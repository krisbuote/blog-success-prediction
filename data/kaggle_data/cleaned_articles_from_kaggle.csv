author,claps,reading_time,link,title,text_content
Justin Lee,8300,11,https://medium.com/swlh/chatbots-were-the-next-big-thing-what-happened-5fc49dd6fa61?source=---------0----------------,Chatbots were the next big thing: what happened? – The Startup – Medium,oh how the headlines blared chatbots were the next big thing our hopes were sky high bright eyed and bushy tailed the industry was ripe for a new era of innovation it was time to start socializing with machines and why wouldn t they be all the road signs pointed towards insane success at the mobile world congress chatbots were the main headliners the conference organizers cited an overwhelming acceptance at the event of the inevitable shift of focus for brands and corporates to chatbots in fact the only significant question around chatbots was who would monopolize the field not whether chatbots would take off in the first place one year on we have an answer to that question no because there isn t even an ecosystem for a platform to dominate chatbots weren t the first technological development to be talked up in grandiose terms and then slump spectacularly the age old hype cycle unfolded in familiar fashion expectations built built and then it all kind of fizzled out the predicted paradim shift didn t materialize and apps are tellingly still alive and well we look back at our breathless optimism and turn to each other slightly baffled is that it that was the chatbot revolution we were promised digit s ethan bloch sums up the general consensus according to dave feldman vice president of product design at heap chatbots didn t just take on one difficult problem and fail they took on several and failed all of them bots can interface with users in different ways the big divide is text vs speech in the beginning of computer interfaces was the written word users had to type commands manually into a machine to get anything done then graphical user interfaces guis came along and saved the day we became entranced by windows mouse clicks icons and hey we eventually got color too meanwhile a bunch of research scientists were busily developing natural language nl interfaces to databases instead of having to learn an arcane database query language another bunch of scientists were developing speech processing software so that you could just speak to your computer rather than having to type this turned out to be a whole lot more difficult than anyone originally realised the next item on the agenda was holding a two way dialog with a machine here s an example dialog dating back to the s with vcr setup system pretty cool right the system takes turns in collaborative way and does a smart job of figuring out what the user wants it was carefully crafted to deal with conversations involving vcrs and could only operate within strict limitations modern day bots whether they use typed or spoken input have to face all these challenges but also work in an efficient and scalable way on a variety of platforms basically we re still trying to achieve the same innovations we were years ago here s where i think we re going wrong an oversized assumption has been that apps are over and would be replaced by bots by pitting two such disparate concepts against one another instead of seeing them as separate entities designed to serve different purposes we discouraged bot development you might remember a similar war cry when apps first came onto the scene ten years ago but do you remember when apps replaced the internet it s said that a new product or service needs to be two of the following better cheaper or faster are chatbots cheaper or faster than apps no not yet at least whether they re better is subjective but i think it s fair to say that today s best bot isn t comparable to today s best app plus nobody thinks that using lyft is too complicated or that it s too hard to order food or buy a dress on an app what is too complicated is trying to complete these tasks with a bot and having the bot fail a great bot can be about as useful as an average app when it comes to rich sophisticated multi layered apps there s no competition that s because machines let us access vast and complex information systems and the early graphical information systems were a revolutionary leap forward in helping us locate those systems modern day apps benefit from decades of research and experimentation why would we throw this away but if we swap the word replace with extend things get much more interesting today s most successful bot experiences take a hybrid approach incorporating chat into a broader strategy that encompasses more traditional elements the next wave will be multimodal apps where you can say what you want like with siri and get back information as a map text or even a spoken response another problematic aspect of the sweeping nature of hype is that it tends to bypass essential questions like these for plenty of companies bots just aren t the right solution the past two years are littered with cases of bots being blindly applied to problems where they aren t needed building a bot for the sake of it letting it loose and hoping for the best will never end well the vast majority of bots are built using decision tree logic where the bot s canned response relies on spotting specific keywords in the user input the advantage of this approach is that it s pretty easy to list all the cases that they are designed to cover and that s precisely their disadvantage too that s because these bots are purely a reflection of the capability fastidiousness and patience of the person who created them and how many user needs and inputs they were able to anticipate problems arise when life refuses to fit into those boxes according to recent reports of the bots on facebook messenger are failing to fulfil simple user requests this is partly a result of developers failing to narrow their bot down to one strong area of focus when we were building growthbot we decided to make it specific to sales and marketers not an all rounder despite the temptation to get overexcited about potential capabilties remember a bot that does one thing well is infinitely more helpful than a bot that does multiple things poorly a competent developer can build a basic bot in minutes but one that can hold a conversation that s another story despite the constant hype around ai we re still a long way from achieving anything remotely human like in an ideal world the technology known as nlp natural language processing should allow a chatbot to understand the messages it receives but nlp is only just emerging from research labs and is very much in its infancy some platforms provide a bit of nlp but even the best is at toddler level capacity for example think about siri understanding your words but not their meaning as matt asay outlines this results in another issue failure to capture the attention and creativity of developers and conversations are complex they re not linear topics spin around each other take random turns restart or abruptly finish today s rule based dialogue systems are too brittle to deal with this kind of unpredictability and statistical approaches using machine learning are just as limited the level of ai required for human like conversation just isn t available yet and in the meantime there are few high quality examples of trailblazing bots to lead the way as dave feldman remarked once upon a time the only way to interact with computers was by typing arcane commands to the terminal visual interfaces using windows icons or a mouse were a revolution in how we manipulate information there s a reasons computing moved from text based to graphical user interfaces guis on the input side it s easier and faster to click than it is to type tapping or selecting is obviously preferable to typing out a whole sentence even with predictive often error prone text on the output side the old adage that a picture is worth a thousand words is usually true we love optical displays of information because we are highly visual creatures it s no accident that kids love touch screens the pioneers who dreamt up graphical interface were inspired by cognitive psychology the study of how the brain deals with communication conversational uis are meant to replicate the way humans prefer to communicate but they end up requiring extra cognitive effort essentially we re swapping something simple for a more complex alternative sure there are some concepts that we can only express using language show me all the ways of getting to a museum that give me steps but don t take longer than minutes but most tasks can be carried out more efficiently and intuitively with guis than with a conversational ui aiming for a human dimension in business interactions makes sense if there s one thing that s broken about sales and marketing it s the lack of humanity brands hide behind ticket numbers feedback forms do not reply emails automated responses and gated contact us forms facebook s goal is that their bots should pass the so called turing test meaning you can t tell whether you are talking to a bot or a human but a bot isn t the same as a human it never will be a conversation encompasses so much more than just text humans can read between the lines leverage contextual information and understand double layers like sarcasm bots quickly forget what they re talking about meaning it s a bit like conversing with someone who has little or no short term memory as hubspot team pinpointed people aren t easily fooled and pretending a bot is a human is guaranteed to diminish returns not to mention the fact that you re lying to your users and even those rare bots that are powered by state of the art nlp and excel at processing and producing content will fall short in comparison and here s the other thing conversational uis are built to replicate the way humans prefer to communicate with other humans but is that how humans prefer to interact with machines not necessarily at the end of the day no amount of witty quips or human like mannerisms will save a bot from conversational failure in a way those early adopters weren t entirely wrong people are yelling at google home to play their favorite song ordering pizza from the domino s bot and getting makeup tips from sephora but in terms of consumer response and developer involvement chatbots haven t lived up to the hype generated circa not even close computers are good at being computers searching for data crunching numbers analyzing opinions and condensing that information computers aren t good at understanding human emotion the state of nlp means they still don t get what we re asking them never mind how we feel that s why it s still impossible to imagine effective customer support sales or marketing without the essential human touch empathy and emotional intelligence for now bots can continue to help us with automated repetitive low level tasks and queries as cogs in a larger more complex system and we did them and ourselves a disservice by expecting so much so soon but that s not the whole story yes our industry massively overestimated the initial impact chatbots would have emphasis on initial as bill gates once said the hype is over and that s a good thing now we can start examining the middle grounded grey area instead of the hyper inflated frantic black and white zone i believe we re at the very beginning of explosive growth this sense of anti climax is completely normal for transformational technology messaging will continue to gain traction chatbots aren t going away nlp and ai are becoming more sophisticated every day developers apps and platforms will continue to experiment with and heavily invest in conversational marketing and i can t wait to see what happens next from a quick cheer to a standing ovation clap to show how much you enjoyed this story head of growth for growthbot messaging conversational strategy hubspot medium s largest publication for makers subscribe to receive our top stories here https goo gl zhclji
Conor Dewey,1400,7,https://towardsdatascience.com/python-for-data-science-8-concepts-you-may-have-forgotten-i-did-825966908393?source=---------1----------------,Python for Data Science: 8 Concepts You May Have Forgotten,if you ve ever found yourself looking up the same question concept or syntax over and over again when programming you re not alone i find myself doing this constantly while it s not unnatural to look things up on stackoverflow or other resources it does slow you down a good bit and raise questions as to your complete understanding of the language we live in a world where there is a seemingly infinite amount of accessible free resources looming just one search away at all times however this can be both a blessing and a curse when not managed effectively an over reliance on these resources can build poor habits that will set you back long term personally i find myself pulling code from similar discussion threads several times rather than taking the time to learn and solidify the concept so that i can reproduce the code myself the next time this approach is lazy and while it may be the path of least resistance in the short term it will ultimately hurt your growth productivity and ability to recall syntax cough interviews down the line recently i ve been working through an online data science course titled python for data science and machine learning on udemy oh god i sound like that guy on youtube over the early lectures in the series i was reminded of some concepts and syntax that i consistently overlook when performing data analysis in python in the interest of solidifying my understanding of these concepts once and for all and saving you guys a couple of stackoverflow searches here s the stuff that i m always forgetting when working with python numpy and pandas i ve included a short description and example for each however for your benefit i will also include links to videos and other resources that explore each concept more in depth as well writing out a for loop every time you need to define some sort of list is tedious luckily python has a built in way to address this problem in just one line of code the syntax can be a little hard to wrap your head around but once you get familiar with this technique you ll use it fairly often see the example above and below for how you would normally go about list comprehension with a for loop vs creating your list with in one simple line with no loops necessary ever get tired of creating function after function for limited use cases lambda functions to the rescue lambda functions are used for creating small one time and anonymous function objects in python basically they let you create a function without creating a function the basic syntax of lambda functions is note that lambda functions can do everything that regular functions can do as long as there s just one expression check out the simple example below and the upcoming video to get a better feel for the power of lambda functions once you have a grasp on lambda functions learning to pair them with the map and filter functions can be a powerful tool specifically map takes in a list and transforms it into a new list by performing some sort of operation on each element in this example it goes through each element and maps the result of itself times to a new list note that the list function simply converts the output to list type the filter function takes in a list and a rule much like map however it returns a subset of the original list by comparing each element against the boolean filtering rule for creating quick and easy numpy arrays look no further than the arange and linspace functions each one has their specific purpose but the appeal here instead of using range is that they output numpy arrays which are typically easier to work with for data science arange returns evenly spaced values within a given interval along with a starting and stopping point you can also define a step size or data type if necessary note that the stopping point is a cut off value so it will not be included in the array output linspace is very similar but with a slight twist linspace returns evenly spaced numbers over a specified interval so given a starting and stopping point as well as a number of values linspace will evenly space them out for you in a numpy array this is especially helpful for data visualizations and declaring axes when plotting you may have ran into this when dropping a column in pandas or summing values in numpy matrix if not then you surely will at some point let s use the example of dropping a column for now i don t know how many times i wrote this line of code before i actually knew why i was declaring axis what i was as you can probably deduce from above set axis to if you want to deal with columns and set it to if you want rows but why is this my favorite reasoning or atleast how i remember this calling the shape attribute from a pandas dataframe gives us back a tuple with the first value representing the number of rows and the second value representing the number of columns if you think about how this is indexed in python rows are at and columns are at much like how we declare our axis value crazy right if you re familiar with sql then these concepts will probably come a lot easier for you anyhow these functions are essentially just ways to combine dataframes in specific ways it can be difficult to keep track of which is best to use at which time so let s review it concat allows the user to append one or more dataframes to each other either below or next to it depending on how you define the axis merge combines multiple dataframes on specific common columns that serve as the primary key join much like merge combines two dataframes however it joins them based on their indices rather than some specified column check out the excellent pandas documentation for specific syntax and more concrete examples as well as some special cases that you may run into think of apply as a map function but made for pandas dataframes or more specifically for series if you re not as familiar series are pretty similar to numpy arrays for the most part apply sends a function to every element along a column or row depending on what you specify you might imagine how useful this can be especially for formatting and manipulating values across a whole dataframe column without having to loop at all last but certainly not least is pivot tables if you re familiar with microsoft excel then you ve probably heard of pivot tables in some respect the pandas built in pivot table function creates a spreadsheet style pivot table as a dataframe note that the levels in the pivot table are stored in multiindex objects on the index and columns of the resulting dataframe that s it for now i hope a couple of these overviews have effectively jogged your memory regarding important yet somewhat tricky methods functions and concepts you frequently encounter when using python for data science personally i know that even the act of writing these out and trying to explain them in simple terms has helped me out a ton if you re interested in receiving my weekly rundown of interesting articles and resources focused on data science machine learning and artificial intelligence then subscribe to self driven data science using the form below if you enjoyed this post feel free to hit the clap button and if you re interested in posts to come make sure to follow me on medium at the link below i ll be writing and shipping every day this month as part of a day challenge this article was originally published on conordewey com from a quick cheer to a standing ovation clap to show how much you enjoyed this story data scientist writer www conordewey com sharing concepts ideas and codes
William Koehrsen,2800,11,https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219?source=---------2----------------,Automated Feature Engineering in Python – Towards Data Science,machine learning is increasingly moving from hand designed models to automatically optimized pipelines using tools such as h tpot and auto sklearn these libraries along with methods such as random search aim to simplify the model selection and tuning parts of machine learning by finding the best model for a dataset with little to no manual intervention however feature engineering an arguably more valuable aspect of the machine learning pipeline remains almost entirely a human labor feature engineering also known as feature creation is the process of constructing new features from existing data to train a machine learning model this step can be more important than the actual model used because a machine learning algorithm only learns from the data we give it and creating features that are relevant to a task is absolutely crucial see the excellent paper a few useful things to know about machine learning typically feature engineering is a drawn out manual process relying on domain knowledge intuition and data manipulation this process can be extremely tedious and the final features will be limited both by human subjectivity and time automated feature engineering aims to help the data scientist by automatically creating many candidate features out of a dataset from which the best can be selected and used for training in this article we will walk through an example of using automated feature engineering with the featuretools python library we will use an example dataset to show the basics stay tuned for future posts using real world data the complete code for this article is available on github feature engineering means building additional features out of existing data which is often spread across multiple related tables feature engineering requires extracting the relevant information from the data and getting it into a single table which can then be used to train a machine learning model the process of constructing features is very time consuming because each new feature usually requires several steps to build especially when using information from more than one table we can group the operations of feature creation into two categories transformations and aggregations let s look at a few examples to see these concepts in action a transformation acts on a single table thinking in terms of python a table is just a pandas dataframe by creating new features out of one or more of the existing columns as an example if we have the table of clients below we can create features by finding the month of the joined column or taking the natural log of the income column these are both transformations because they use information from only one table on the other hand aggregations are performed across tables and use a one to many relationship to group observations and then calculate statistics for example if we have another table with information on the loans of clients where each client may have multiple loans we can calculate statistics such as the average maximum and minimum of loans for each client this process involves grouping the loans table by the client calculating the aggregations and then merging the resulting data into the client data here s how we would do that in python using the language of pandas these operations are not difficult by themselves but if we have hundreds of variables spread across dozens of tables this process is not feasible to do by hand ideally we want a solution that can automatically perform transformations and aggregations across multiple tables and combine the resulting data into a single table although pandas is a great resource there s only so much data manipulation we want to do by hand for more on manual feature engineering check out the excellent python data science handbook fortunately featuretools is exactly the solution we are looking for this open source python library will automatically create many features from a set of related tables featuretools is based on a method known as deep feature synthesis which sounds a lot more imposing than it actually is the name comes from stacking multiple features not because it uses deep learning deep feature synthesis stacks multiple transformation and aggregation operations which are called feature primitives in the vocab of featuretools to create features from data spread across many tables like most ideas in machine learning it s a complex method built on a foundation of simple concepts by learning one building block at a time we can form a good understanding of this powerful method first let s take a look at our example data we already saw some of the dataset above and the complete collection of tables is as follows if we have a machine learning task such as predicting whether a client will repay a future loan we will want to combine all the information about clients into a single table the tables are related through the client id and the loan id variables and we could use a series of transformations and aggregations to do this process by hand however we will shortly see that we can instead use featuretools to automate the process the first two concepts of featuretools are entities and entitysets an entity is simply a table or a dataframe if you think in pandas an entityset is a collection of tables and the relationships between them think of an entityset as just another python data structure with its own methods and attributes we can create an empty entityset in featuretools using the following now we have to add entities each entity must have an index which is a column with all unique elements that is each value in the index must appear in the table only once the index in the clients dataframe is the client idbecause each client has only one row in this dataframe we add an entity with an existing index to an entityset using the following syntax the loans dataframe also has a unique index loan id and the syntax to add this to the entityset is the same as for clients however for the payments dataframe there is no unique index when we add this entity to the entityset we need to pass in the parameter make index true and specify the name of the index also although featuretools will automatically infer the data type of each column in an entity we can override this by passing in a dictionary of column types to the parameter variable types for this dataframe even though missed is an integer this is not a numeric variable since it can only take on discrete values so we tell featuretools to treat is as a categorical variable after adding the dataframes to the entityset we inspect any of them the column types have been correctly inferred with the modification we specified next we need to specify how the tables in the entityset are related the best way to think of a relationship between two tables is the analogy of parent to child this is a one to many relationship each parent can have multiple children in the realm of tables a parent table has one row for every parent but the child table may have multiple rows corresponding to multiple children of the same parent for example in our dataset the clients dataframe is a parent of the loans dataframe each client has only one row in clients but may have multiple rows in loans likewise loans is the parent of payments because each loan will have multiple payments the parents are linked to their children by a shared variable when we perform aggregations we group the child table by the parent variable and calculate statistics across the children of each parent to formalize a relationship in featuretools we only need to specify the variable that links two tables together the clients and the loans table are linked via the client id variable and loans and payments are linked with the loan id the syntax for creating a relationship and adding it to the entityset are shown below the entityset now contains the three entities tables and the relationships that link these entities together after adding entities and formalizing relationships our entityset is complete and we are ready to make features before we can quite get to deep feature synthesis we need to understand feature primitives we already know what these are but we have just been calling them by different names these are simply the basic operations that we use to form new features new features are created in featuretools using these primitives either by themselves or stacking multiple primitives below is a list of some of the feature primitives in featuretools we can also define custom primitives these primitives can be used by themselves or combined to create features to make features with specified primitives we use the ft dfs function standing for deep feature synthesis we pass in the entityset the target entity which is the table where we want to add the features the selected trans primitives transformations and agg primitives aggregations the result is a dataframe of new features for each client because we made clients the target entity for example we have the month each client joined which is a transformation feature primitive we also have a number of aggregation primitives such as the average payment amounts for each client even though we specified only a few feature primitives featuretools created many new features by combining and stacking these primitives the complete dataframe has columns of new features we now have all the pieces in place to understand deep feature synthesis dfs in fact we already performed dfs in the previous function call a deep feature is simply a feature made of stacking multiple primitives and dfs is the name of process that makes these features the depth of a deep feature is the number of primitives required to make the feature for example the mean payments payment amount column is a deep feature with a depth of because it was created using a single aggregation a feature with a depth of two is last loans mean payments payment amount this is made by stacking two aggregations last most recent on top of mean this represents the average payment size of the most recent loan for each client we can stack features to any depth we want but in practice i have never gone beyond a depth of after this point the features are difficult to interpret but i encourage anyone interested to try going deeper we do not have to manually specify the feature primitives but instead can let featuretools automatically choose features for us to do this we use the same ft dfs function call but do not pass in any feature primitives featuretools has built many new features for us to use while this process does automatically create new features it will not replace the data scientist because we still have to figure out what to do with all these features for example if our goal is to predict whether or not a client will repay a loan we could look for the features most correlated with a specified outcome moreover if we have domain knowledge we can use that to choose specific feature primitives or seed deep feature synthesis with candidate features automated feature engineering has solved one problem but created another too many features although it s difficult to say before fitting a model which of these features will be important it s likely not all of them will be relevant to a task we want to train our model on moreover having too many features can lead to poor model performance because the less useful features drown out those that are more important the problem of too many features is known as the curse of dimensionality as the number of features increases the dimension of the data grows it becomes more and more difficult for a model to learn the mapping between features and targets in fact the amount of data needed for the model to perform well scales exponentially with the number of features the curse of dimensionality is combated with feature reduction also known as feature selection the process of removing irrelevant features this can take on many forms principal component analysis pca selectkbest using feature importances from a model or auto encoding using deep neural networks however feature reduction is a different topic for another article for now we know that we can use featuretools to create numerous features from many tables with minimal effort like many topics in machine learning automated feature engineering with featuretools is a complicated concept built on simple ideas using concepts of entitysets entities and relationships featuretools can perform deep feature synthesis to create new features deep feature synthesis in turn stacks feature primitives aggregations which act across a one to many relationship between tables and transformations functions applied to one or more columns in a single table to build new features from multiple tables in future articles i ll show how to use this technique on a real world problem the home credit default risk competition currently being hosted on kaggle stay tuned for that post and in the meantime read this introduction to get started in the competition i hope that you can now use automated feature engineering as an aid in a data science pipeline our models are only as good as the data we give them and automated feature engineering can help to make the feature creation process more efficient for more information on featuretools including advanced usage check out the online documentation to see how featuretools is used in practice read about the work of feature labs the company behind the open source library as always i welcome feedback and constructive criticism and can be reached on twitter koehrsen will from a quick cheer to a standing ovation clap to show how much you enjoyed this story data scientist and master student data science communicator and advocate sharing concepts ideas and codes
Gant Laborde,1300,7,https://medium.freecodecamp.org/machine-learning-how-to-go-from-zero-to-hero-40e26f8aa6da?source=---------3----------------,Machine Learning: how to go from Zero to Hero – freeCodeCamp,if your understanding of a i and machine learning is a big question mark then this is the blog post for you here i gradually increase your awesomenessicitytm by gluing inspirational videos together with friendly text sit down and relax these videos take time and if they don t inspire you to continue to the next section fair enough however if you find yourself at the bottom of this article you ve earned your well rounded knowledge and passion for this new world where you go from there is up to you a i was always cool from moving a paddle in pong to lighting you up with combos in street fighter a i has always revolved around a programmer s functional guess at how something should behave fun but programmers aren t always gifted in programming a i as we often see just google epic game fails to see glitches in a i physics and sometimes even experienced human players regardless a i has a new talent you can teach a computer to play video games understand language and even how to identify people or things this tip of the iceberg new skill comes from an old concept that only recently got the processing power to exist outside of theory i m talking about machine learning you don t need to come up with advanced algorithms anymore you just have to teach a computer to come up with its own advanced algorithm so how does something like that even work an algorithm isn t really written as much as it is sort of bred i m not using breeding as an analogy watch this short video which gives excellent commentary and animations to the high level concept of creating the a i wow right that s a crazy process now how is it that we can t even understand the algorithm when it s done one great visual was when the a i was written to beat mario games as a human we all understand how to play a side scroller but identifying the predictive strategy of the resulting a i is insane impressed there s something amazing about this idea right the only problem is we don t know machine learning and we don t know how to hook it up to video games fortunately for you elon musk already provided a non profit company to do the latter yes in a dozen lines of code you can hook up any a i you want to countless games tasks i have two good answers on why you should care firstly machine learning ml is making computers do things that we ve never made computers do before if you want to do something new not just new to you but to the world you can do it with ml secondly if you don t influence the world the world will influence you right now significant companies are investing in ml and we re already seeing it change the world thought leaders are warning that we can t let this new age of algorithms exist outside of the public eye imagine if a few corporate monoliths controlled the internet if we don t take up arms the science won t be ours i think christian heilmann said it best in his talk on ml the concept is useful and cool we understand it at a high level but what the heck is actually happening how does this work if you want to jump straight in i suggest you skip this section and move on to the next how do i get started section if you re motivated to be a doer in ml you won t need these videos if you re still trying to grasp how this could even be a thing the following video is perfect for walking you through the logic using the classic ml problem of handwriting pretty cool huh that video shows that each layer gets simpler rather than more complicated like the function is chewing data into smaller pieces that end in an abstract concept you can get your hands dirty in interacting with this process on this site by adam harley it s cool watching data go through a trained model but you can even watch your neural network get trained one of the classic real world examples of machine learning in action is the iris data set from in a presentation i attended by javafxpert s overview on machine learning i learned how you can use his tool to visualize the adjustment and back propagation of weights to neurons on a neural network you get to watch it train the neural model even if you re not a java buff the presentation jim gives on all things machine learning is a pretty cool hour introduction into ml concepts which includes more info on many of the examples above these concepts are exciting are you ready to be the einstein of this new era breakthroughs are happening every day so get started now there are tons of resources available i ll be recommending two approaches in this approach you ll understand machine learning down to the algorithms and the math i know this way sounds tough but how cool would it be to really get into the details and code this stuff from scratch if you want to be a force in ml and hold your own in deep conversations then this is the route for you i recommend that you try out brilliant org s app always great for any science lover and take the artificial neural network course this course has no time limits and helps you learn ml while killing time in line on your phone this one costs money after level combine the above with simultaneous enrollment in andrew ng s stanford course on machine learning in weeks this is the course that jim weaver recommended in his video above i ve also had this course independently suggested to me by jen looper everyone provides a caveat that this course is tough for some of you that s a show stopper but for others that s why you re going to put yourself through it and collect a certificate saying you did this course is free you only have to pay for a certificate if you want one with those two courses you ll have a lot of work to do everyone should be impressed if you make it through because that s not simple but more so if you do make it through you ll have a deep understanding of the implementation of machine learning that will catapult you into successfully applying it in new and world changing ways if you re not interested in writing the algorithms but you want to use them to create the next breathtaking website app you should jump into tensorflow and the crash course tensorflow is the de facto open source software library for machine learning it can be used in countless ways and even with javascript here s a crash course plenty more information on available courses and rankings can be found here if taking a course is not your style you re still in luck you don t have to learn the nitty gritty of ml in order to use it today you can efficiently utilize ml as a service in many ways with tech giants who have trained models ready i would still caution you that there s no guarantee that your data is safe or even yours but the offerings of services for ml are quite attractive using an ml service might be the best solution for you if you re excited and able to upload your data to amazon microsoft google i like to think of these services as a gateway drug to advanced ml either way it s good to get started now i have to say thank you to all the aforementioned people and videos they were my inspiration to get started and though i m still a newb in the ml world i m happy to light the path for others as we embrace this awe inspiring age we find ourselves in it s imperative to reach out and connect with people if you take up learning this craft without friendly faces answers and sounding boards anything can be hard just being able to ask and get a response is a game changer add me and add the people mentioned above friendly people with friendly advice helps see i hope this article has inspired you and those around you to learn ml from a quick cheer to a standing ovation clap to show how much you enjoyed this story software consultant adjunct professor published author award winning speaker mentor organizer and immature nerd d lately full of react native tech our community publishes stories worth reading on development design and data science
Emmanuel Ameisen,935,11,https://blog.insightdatascience.com/reinforcement-learning-from-scratch-819b65f074d8?source=---------4----------------,Reinforcement Learning from scratch – Insight Data,want to learn about applied artificial intelligence from leading practitioners in silicon valley new york or toronto learn more about the insight artificial intelligence fellows program are you a company working in ai and would like to get involved in the insight ai fellows program feel free to get in touch recently i gave a talk at the o reilly ai conference in beijing about some of the interesting lessons we ve learned in the world of nlp while there i was lucky enough to attend a tutorial on deep reinforcement learning deep rl from scratch by unity technologies i thought that the session led by arthur juliani was extremely informative and wanted to share some big takeaways below in our conversations with companies we ve seen a rise of interesting deep rl applications tools and results in parallel the inner workings and applications of deep rl such as alphago pictured above can often seem esoteric and hard to understand in this post i will give an overview of core aspects of the field that can be understood by anyone many of the visuals are from the slides of the talk and some are new the explanations and opinions are mine if anything is unclear reach out to me here deep rl is a field that has seen vast amounts of research interest including learning to play atari games beating pro players at dota and defeating go champions contrary to many classical deep learning problems that often focus on perception does this image contain a stop sign deep rl adds the dimension of actions that influence the environment what is the goal and how do i get there in dialog systems for example classical deep learning aims to learn the right response for a given query on the other hand deep reinforcement learning focuses on the right sequences of sentences that will lead to a positive outcome for example a happy customer this makes deep rl particularly attractive for tasks that require planning and adaptation such as manufacturing or self driving however industry applications have trailed behind the rapidly advancing results coming out of the research community a major reason is that deep rl often requires an agent to experiment millions of times before learning anything useful the best way to do this rapidly is by using a simulation environment this tutorial will be using unity to create environments to train agents in for this workshop led by arthur juliani and leon chen their goal was to get every participants to successfully train multiple deep rl algorithms in hours a tall order below is a comprehensive overview of many of the main algorithms that power deep rl today for a more complete set of tutorials arthur juliani wrote an part series starting here deep rl can be used to best the top human players at go but to understand how that s done you first need to understand a few simple concepts starting with much easier problems it all starts with slot machines let s imagine you are faced with chests that you can pick from at each turn each of them have a different average payout and your goal is to maximize the total payout you receive after a fixed number of turns this is a classic problem called multi armed bandits and is where we will start the crux of the problem is to balance exploration which helps us learn about which states are good and exploitation where we now use what we know to pick the best slot machine here we will utilize a value function that maps our actions to an estimated reward called the q function first we ll initialize all q values at equal values then we ll update the q value of each action picking each chest based on how good the payout was after choosing this action this allows us to learn a good value function we will approximate our q function using a neural network starting with a very shallow one that learns a probability distribution by using a softmax over the potential chests while the value function tells us how good we estimate each action to be the policy is the function that determines which actions we end up taking intuitively we might want to use a policy that picks the action with the highest q value this performs poorly in practice as our q estimates will be very wrong at the start before we gather enough experience through trial and error this is why we need to add a mechanism to our policy to encourage exploration one way to do that is to use epsilon greedy which consists of taking a random action with probability epsilon we start with epsilon being close to always choosing random actions and lower epsilon as we go along and learn more about which chests are good eventually we learn which chests are best in practice we might want to take a more subtle approach than either taking the action we think is the best or a random action a popular method is boltzmann exploration which adjust probabilities based on our current estimate of how good each chest is adding in a randomness factor adding different states the previous example was a world in which we were always in the same state waiting to pick from the same chests in front of us most real word problems consist of many different states that is what we will add to our environment next now the background behind chests alternates between colors at each turn changing the average values of the chests this means we need to learn a q function that depends not only on the action the chest we pick but the state what the color of the background is this version of the problem is called contextual multi armed bandits surprisingly we can use the same approach as before the only thing we need to add is an extra dense layer to our neural network that will take in as input a vector representing the current state of the world learning about the consequences of our actions there is another key factor that makes our current problem simpler than mosts in most environments such as in the maze depicted above the actions that we take have an impact on the state of the world if we move up on this grid we might receive a reward or we might receive nothing but the next turn we will be in a different state this is where we finally introduce a need for planning first we will define our q function as the immediate reward in our current state plus the discounted reward we are expecting by taking all of our future actions this solution works if our q estimate of states is accurate so how can we learn a good estimate we will use a method called temporal difference td learning to learn a good q function the idea is to only look at a limited number of steps in the future td for example only uses the next states to evaluate the reward surprisingly we can use td which looks at the current state and our estimate of the reward the next turn and get great results the structure of the network is the same but we need to go through one forward step before receiving the error we then use this error to back propagate gradients like in traditional deep learning and update our value estimates introducing monte carlo another method to estimate the eventual success of our actions is monte carlo estimates this consists of playing out the entire episode with our current policy until we reach an end success by reaching a green block or failure by reaching a red block in the image above and use that result to update our value estimates for each traversed state this allows us to propagate values efficiently in one batch at the end of an episode instead of every time we make a move the cost is that we are introducing noise to our estimates since we attribute very distant rewards to them the world is rarely discrete the previous methods were using neural networks to approximate our value estimates by mapping from a discrete number of states and actions to a value in the maze for example there were states squares and actions move in each adjacent direction in this environment we are trying to learn how to balance a ball on a dimensional paddle by deciding at each time step whether we want to tilt the paddle left or right here the state space becomes continuous the angle of the paddle and the position of the ball the good news is we can still use neural networks to approximate this function a note about off policy vs on policy learning the methods we used previously are off policy methods meaning we can generate data with any strategy using epsilon greedy for example and learn from it on policy methods can only learn from actions that were taken following our policy remember a policy is the method we use to determine which actions to take this constrains our learning process as we have to have an exploration strategy that is built in to the policy itself but allows us to tie results directly to our reasoning and enables us to learn more efficiently the approach we will use here is called policy gradients and is an on policy method previously we were first learning a value function q for each action in each state and then building a policy on top in vanilla policy gradient we still use monte carlo estimates but we learn our policy directly through a loss function that increases the probability of choosing rewarding actions since we are learning on policy we cannot use methods such as epsilon greedy which includes random choices to get our agent to explore the environment the way that we encourage exploration is by using a method called entropy regularization which pushes our probability estimates to be wider and thus will encourage us to make riskier choices to explore the space leveraging deep learning for representations in practice many state of the art rl methods require learning both a policy and value estimates the way we do this with deep learning is by having both be two separate outputs of the same backbone neural network which will make it easier for our neural network to learn good representations one method to do this is advantage actor critic a c we learn our policy directly with policy gradients defined above and learn a value function using something called advantage instead of updating our value function based on rewards we update it based on our advantage which measures how much better or worse an action was than our previous value function estimated it to be this helps make learning more stable compared to simple q learning and vanilla policy gradients learning directly from the screen there is an additional advantage to using deep learning for these methods which is that deep neural networks excel at perceptive tasks when a human plays a game the information received is not a list of states but an image usually of a screen or a board or the surrounding environment image based learning combines a convolutional neural network cnn with rl in this environment we pass in a raw image instead of features and add a layer cnn to our architecture without changing anything else we can even inspect activations to see what the network picks up on to determine value and policy in the example below we can see that the network uses the current score and distant obstacles to estimate the value of the current state while focusing on nearby obstacles for determining actions neat as a side note while toying around with the provided implementation i ve found that visual learning is very sensitive to hyperparameters changing the discount rate slightly for example completely prevented the neural network from learning even on a toy application this is a widely known problem but it is interesting to see it first hand nuanced actions so far we ve played with environments with continuous and discrete state spaces however every environment we studied had a discrete action space we could move in one of four directions or tilt the paddle to the left or right ideally for applications such as self driving cars we would like to learn continuous actions such as turning the steering wheel between and degrees in this environment called d ball world we can choose to tilt the paddle to any value on each of its axes this gives us more control as to how we perform actions but makes the action space much larger we can approach this by approximating our potential choices with gaussian distributions we learn a probability distribution over potential actions by learning the mean and variance of a gaussian distribution and our policy we sample from that distribution simple in theory next steps for the brave there are a few concepts that separate the algorithms described above from state of the art approaches it s interesting to see that conceptually the best robotics and game playing algorithms are not that far away from the ones we just explored that s it for this overview i hope this has been informative and fun if you are looking to dive deeper into the theory of rl give arthur s posts a read or diving deeper by following david silver s ucl course if you are looking to learn more about the projects we do at insight or how we work with companies please check us out below or reach out to me here want to learn about applied artificial intelligence from leading practitioners in silicon valley new york or toronto learn more about the insight artificial intelligence fellows program are you a company working in ai and would like to get involved in the insight ai fellows program feel free to get in touch from a quick cheer to a standing ovation clap to show how much you enjoyed this story ai lead at insight ai emmanuelameisen insight fellows program your bridge to a career in data
Irhum Shafkat,2000,15,https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1?source=---------5----------------,Intuitively Understanding Convolutions for Deep Learning,the advent of powerful and versatile deep learning frameworks in recent years has made it possible to implement convolution layers into a deep learning model an extremely simple task often achievable in a single line of code however understanding convolutions especially for the first time can often feel a bit unnerving with terms like kernels filters channels and so on all stacked onto each other yet convolutions as a concept are fascinatingly powerful and highly extensible and in this post we ll break down the mechanics of the convolution operation step by step relate it to the standard fully connected network and explore just how they build up a strong visual hierarchy making them powerful feature extractors for images the d convolution is a fairly simple operation at heart you start with a kernel which is simply a small matrix of weights this kernel slides over the d input data performing an elementwise multiplication with the part of the input it is currently on and then summing up the results into a single output pixel the kernel repeats this process for every location it slides over converting a d matrix of features into yet another d matrix of features the output features are essentially the weighted sums with the weights being the values of the kernel itself of the input features located roughly in the same location of the output pixel on the input layer whether or not an input feature falls within this roughly same location gets determined directly by whether it s in the area of the kernel that produced the output or not this means the size of the kernel directly determines how many or few input features get combined in the production of a new output feature this is all in pretty stark contrast to a fully connected layer in the above example we have input features and output features if this were a standard fully connected layer you d have a weight matrix of parameters with every output feature being the weighted sum of every single input feature convolutions allow us to do this transformation with only parameters with each output feature instead of looking at every input feature only getting to look at input features coming from roughly the same location do take note of this as it ll be critical to our later discussion before we move on it s definitely worth looking into two techniques that are commonplace in convolution layers padding and strides padding does something pretty clever to solve this pad the edges with extra fake pixels usually of value hence the oft used term zero padding this way the kernel when sliding can allow the original edge pixels to be at its center while extending into the fake pixels beyond the edge producing an output the same size as the input the idea of the stride is to skip some of the slide locations of the kernel a stride of means to pick slides a pixel apart so basically every single slide acting as a standard convolution a stride of means picking slides pixels apart skipping every other slide in the process downsizing by roughly a factor of a stride of means skipping every slides downsizing roughly by factor and so on more modern networks such as the resnet architectures entirely forgo pooling layers in their internal layers in favor of strided convolutions when needing to reduce their output sizes of course the diagrams above only deals with the case where the image has a single input channel in practicality most input images have channels and that number only increases the deeper you go into a network it s pretty easy to think of channels in general as being a view of the image as a whole emphasising some aspects de emphasising others so this is where a key distinction between terms comes in handy whereas in the channel case where the term filter and kernel are interchangeable in the general case they re actually pretty different each filter actually happens to be a collection of kernels with there being one kernel for every single input channel to the layer and each kernel being unique each filter in a convolution layer produces one and only one output channel and they do it like so each of the kernels of the filter slides over their respective input channels producing a processed version of each some kernels may have stronger weights than others to give more emphasis to certain input channels than others eg a filter may have a red kernel channel with stronger weights than others and hence respond more to differences in the red channel features than the others each of the per channel processed versions are then summed together to form one channel the kernels of a filter each produce one version of each channel and the filter as a whole produces one overall output channel finally then there s the bias term the way the bias term works here is that each output filter has one bias term the bias gets added to the output channel so far to produce the final output channel and with the single filter case down the case for any number of filters is identical each filter processes the input with its own different set of kernels and a scalar bias with the process described above producing a single output channel they are then concatenated together to produce the overall output with the number of output channels being the number of filters a nonlinearity is then usually applied before passing this as input to another convolution layer which then repeats this process even with the mechanics of the convolution layer down it can still be hard to relate it back to a standard feed forward network and it still doesn t explain why convolutions scale to and work so much better for image data suppose we have a input and we want to transform it into a grid if we were using a feedforward network we d reshape the input into a vector of length and pass it through a densely connected layer with inputs and outputs one could visualize the weight matrix w for a layer and although the convolution kernel operation may seem a bit strange at first it is still a linear transformation with an equivalent transformation matrix if we were to use a kernel k of size on the reshaped input to get a output the equivalent transformation matrix would be note while the above matrix is an equivalent transformation matrix the actual operation is usually implemented as a very different matrix multiplication the convolution then as a whole is still a linear transformation but at the same time it s also a dramatically different kind of transformation for a matrix with elements there s just parameters which themselves are reused several times each output node only gets to see a select number of inputs the ones inside the kernel there is no interaction with any of the other inputs as the weights to them are set to it s useful to see the convolution operation as a hard prior on the weight matrix in this context by prior i mean predefined network parameters for example when you use a pretrained model for image classification you use the pretrained network parameters as your prior as a feature extractor to your final densely connected layer in that sense there s a direct intuition between why both are so efficient compared to their alternatives transfer learning is efficient by orders of magnitude compared to random initialization because you only really need to optimize the parameters of the final fully connected layer which means you can have fantastic performance with only a few dozen images per class here you don t need to optimize all parameters because we set most of them to zero and they ll stay that way and the rest we convert to shared parameters resulting in only actual parameters to optimize this efficiency matters because when you move from the inputs of mnist to real world images thats over inputs a dense layer attempting to halve the input to inputs would still require over billion parameters for comparison the entirety of resnet has some million parameters so fixing some parameters to and tying parameters increases efficiency but unlike the transfer learning case where we know the prior is good because it works on a large general set of images how do we know this is any good the answer lies in the feature combinations the prior leads the parameters to learn early on in this article we discussed that so with backpropagation coming in all the way from the classification nodes of the network the kernels have the interesting task of learning weights to produce features only from a set of local inputs additionally because the kernel itself is applied across the entire image the features the kernel learns must be general enough to come from any part of the image if this were any other kind of data eg categorical data of app installs this would ve been a disaster for just because your number of app installs and app type columns are next to each other doesn t mean they have any local shared features common with app install dates and time used sure the four may have an underlying higher level feature eg which apps people want most that can be found but that gives us no reason to believe the parameters for the first two are exactly the same as the parameters for the latter two the four could ve been in any consistent order and still be valid pixels however always appear in a consistent order and nearby pixels influence a pixel e g if all nearby pixels are red it s pretty likely the pixel is also red if there are deviations that s an interesting anomaly that could be converted into a feature and all this can be detected from comparing a pixel with its neighbors with other pixels in its locality and this idea is really what a lot of earlier computer vision feature extraction methods were based around for instance for edge detection one can use a sobel edge detection filter a kernel with fixed parameters operating just like the standard one channel convolution for a non edge containing grid eg the background sky most of the pixels are the same value so the overall output of the kernel at that point is for a grid with an vertical edge there is a difference between the pixels to the left and right of the edge and the kernel computes that difference to be non zero activating and revealing the edges the kernel only works only a grids at a time detecting anomalies on a local scale yet when applied across the entire image is enough to detect a certain feature on a global scale anywhere in the image so the key difference we make with deep learning is ask this question can useful kernels be learnt for early layers operating on raw pixels we could reasonably expect feature detectors of fairly low level features like edges lines etc there s an entire branch of deep learning research focused on making neural network models interpretable one of the most powerful tools to come out of that is feature visualization using optimization the idea at core is simple optimize a image usually initialized with random noise to activate a filter as strongly as possible this does make intuitive sense if the optimized image is completely filled with edges that s strong evidence that s what the filter itself is looking for and is activated by using this we can peek into the learnt filters and the results are stunning one important thing to notice here is that convolved images are still images the output of a small grid of pixels from the top left of an image will still be on the top left so you can run another convolution layer on top of another such as the two on the left to extract deeper features which we visualize yet however deep our feature detectors get without any further changes they ll still be operating on very small patches of the image no matter how deep your detectors are you can t detect faces from a grid and this is where the idea of the receptive field comes in a essential design choice of any cnn architecture is that the input sizes grow smaller and smaller from the start to the end of the network while the number of channels grow deeper this as mentioned earlier is often done through strides or pooling layers locality determines what inputs from the previous layer the outputs get to see the receptive field determines what area of the original input to the entire network the output gets to see the idea of a strided convolution is that we only process slides a fixed distance apart and skip the ones in the middle from a different point of view we only keep outputs a fixed distance apart and remove the rest we then apply a nonlinearity to the output and per usual then stack another new convolution layer on top and this is where things get interesting even if were we to apply a kernel of the same size having the same local area to the output of the strided convolution the kernel would have a larger effective receptive field this is because the output of the strided layer still does represent the same image it is not so much cropping as it is resizing only thing is that each single pixel in the output is a representative of a larger area of whose other pixels were discarded from the same rough location from the original input so when the next layer s kernel operates on the output it s operating on pixels collected from a larger area note if you re familiar with dilated convolutions note that the above is not a dilated convolution both are methods of increasing the receptive field but dilated convolutions are a single layer while this takes place on a regular convolution following a strided convolution with a nonlinearity inbetween this expansion of the receptive field allows the convolution layers to combine the low level features lines edges into higher level features curves textures as we see in the mixed a layer followed by a pooling strided layer the network continues to create detectors for even higher level features parts patterns as we see for mixed a the repeated reduction in image size across the network results in by the th block on convolutions input sizes of just compared to inputs of at this point each single pixel represents a grid of pixels which is huge compared to earlier layers where an activation meant detecting an edge here an activation on the tiny grid is one for a very high level feature such as for birds the network as a whole progresses from a small number of filters in case of googlenet detecting low level features to a very large number of filters in the final convolution each looking for an extremely specific high level feature followed by a final pooling layer which collapses each grid into a single pixel each channel is a feature detector with a receptive field equivalent to the entire image compared to what a standard feedforward network would have done the output here is really nothing short of awe inspiring a standard feedforward network would have produced abstract feature vectors from combinations of every single pixel in the image requiring intractable amounts of data to train the cnn with the priors imposed on it starts by learning very low level feature detectors and as across the layers as its receptive field is expanded learns to combine those low level features into progressively higher level features not an abstract combination of every single pixel but rather a strong visual hierarchy of concepts by detecting low level features and using them to detect higher level features as it progresses up its visual hierarchy it is eventually able to detect entire visual concepts such as faces birds trees etc and that s what makes them such powerful yet efficient with image data with the visual hierarchy cnns build it is pretty reasonable to assume that their vision systems are similar to humans and they re really great with real world images but they also fail in ways that strongly suggest their vision systems aren t entirely human like the most major problem adversarial examples examples which have been specifically modified to fool the model adversarial examples would be a non issue if the only tampered ones that caused the models to fail were ones that even humans would notice the problem is the models are susceptible to attacks by samples which have only been tampered with ever so slightly and would clearly not fool any human this opens the door for models to silently fail which can be pretty dangerous for a wide range of applications from self driving cars to healthcare robustness against adversarial attacks is currently a highly active area of research the subject of many papers and even competitions and solutions will certainly improve cnn architectures to become safer and more reliable cnns were the models that allowed computer vision to scale from simple applications to powering sophisticated products and services ranging from face detection in your photo gallery to making better medical diagnoses they might be the key method in computer vision going forward or some other new breakthrough might just be around the corner regardless one thing is for sure they re nothing short of amazing at the heart of many present day innovative applications and are most certainly worth deeply understanding hope you enjoyed this article if you d like to stay connected you ll find me on twitter here if you have a question comments are welcome i find them to be useful to my own learning process as well from a quick cheer to a standing ovation clap to show how much you enjoyed this story curious programmer tinkers around in python and deep learning sharing concepts ideas and codes
Sam Drozdov,2300,6,https://uxdesign.cc/an-intro-to-machine-learning-for-designers-5c74ba100257?source=---------6----------------,An intro to Machine Learning for designers – UX Collective,there is an ongoing debate about whether or not designers should write code wherever you fall on this issue most people would agree that designers should know about code this helps designers understand constraints and empathize with developers it also allows designers to think outside of the pixel perfect box when problem solving for the same reasons designers should know about machine learning put simply machine learning is a field of study that gives computers the ability to learn without being explicitly programmed arthur samuel even though arthur samuel coined the term over fifty years ago only recently have we seen the most exciting applications of machine learning digital assistants autonomous driving and spam free email all exist thanks to machine learning over the past decade new algorithms better hardware and more data have made machine learning an order of magnitude more effective only in the past few years companies like google amazon and apple have made some of their powerful machine learning tools available to developers now is the best time to learn about machine learning and apply it to the products you are building since machine learning is now more accessible than ever before designers today have the opportunity to think about how machine learning can be applied to improve their products designers should be able to talk with software developers about what is possible how to prepare and what outcomes to expect below are a few example applications that should serve as inspiration for these conversations machine learning can help create user centric products by personalizing experiences to the individuals who use them this allows us to improve things like recommendations search results notifications and ads machine learning is effective at finding abnormal content credit card companies use this to detect fraud email providers use this to detect spam and social media companies use this to detect things like hate speech machine learning has enabled computers to begin to understand the things we say natural language processing and the things we see computer vision this allows siri to understand siri set a reminder google photos to create albums of your dog and facebook to describe a photo to those visually impaired machine learning is also helpful in understanding how users are grouped this insight can then be used to look at analytics on a group by group basis from here different features can be evaluated across groups or be rolled out to only a particular group of users machine learning allows us to make predictions about how a user might behave next knowing this we can help prepare for a user s next action for example if we can predict what content a user is planning on viewing we can preload that content so it s immediately ready when they want it depending on the application and what data is available there are different types of machine learning algorithms to choose from i ll briefly cover each of the following supervised learning allows us to make predictions using correctly labeled data labeled data is a group of examples that has informative tags or outputs for example photos with associated hashtags or a house s features eq number of bedrooms location and its price by using supervised learning we can fit a line to the labelled data that either splits the data into categories or represents the trend of the data using this line we are able to make predictions on new data for example we can look at new photos and predict hashtags or look at a new house s features and predict its price if the output we are trying to predict is a list of tags or values we call it classification if the output we are trying to predict is a number we call it regression unsupervised learning is helpful when we have unlabeled data or we are not exactly sure what outputs like an image s hashtags or a house s price are meaningful instead we can identify patterns among unlabeled data for example we can identify related items on an e commerce website or recommend items to someone based on others who made similar purchases if the pattern is a group we call it a cluster if the pattern is a rule e q if this then that we call it an association reinforcement learning doesn t use an existing data set instead we create an agent to collect its own data through trial and error in an environment where it is reinforced with a reward for example an agent can learn to play mario by receiving a positive reward for collecting coins and a negative reward for walking into a goomba reinforcement learning is inspired by the way that humans learn and has turned out to be an effective way to teach computers specifically reinforcement has been effective at training computers to play games like go and dota understanding the problem you are trying to solve and the available data will constrain the types of machine learning you can use e q identifying objects in an image with supervised learning requires a labeled data set of images however constraints are the fruit of creativity in some cases you can set out to collect data that is not already available or consider other approaches even though machine learning is a science it comes with a margin of error it is important to consider how a user s experience might be impacted by this margin of error for example when an autonomous car fails to recognize its surroundings people can get hurt even though machine learning has never been as accessible as it is today it still requires additional resources developers and time to be integrated into a product this makes it important to think about whether the resulting impact justifies the amount of resources needed to implement we have barely covered the tip of the iceberg but hopefully at this point you feel more comfortable thinking about how machine learning can be applied to your product if you are interested in learning more about machine learning here are some helpful resources thanks for reading chat with me on twitter samueldrozdov from a quick cheer to a standing ovation clap to show how much you enjoyed this story digital product designer samueldrozdov com curated stories on user experience usability and product design by fabriciot and caioab
Conor Dewey,252,10,https://towardsdatascience.com/the-big-list-of-ds-ml-interview-resources-2db4f651bd63?source=---------7----------------,The Big List of DS/ML Interview Resources – Towards Data Science,data science interviews certainly aren t easy i know this first hand i ve participated in over individual interviews and phone screens while applying for competitive internships over the last calendar year through this exciting and somewhat at times very painful process i ve accumulated a plethora of useful resources that helped me prepare for and eventually pass data science interviews long story short i ve decided to sort through all my bookmarks and notes in order to deliver a comprehensive list of data science resources with this list by your side you should have more than enough effective tools at your disposal next time you re prepping for a big interview it s worth noting that many of these resources are naturally going to geared towards entry level and intern data science positions as that s where my expertise lies keep that in mind and enjoy here s some of the more general resources covering data science as a whole specifically i highly recommend checking out the first two links regarding data science interview questions while the ebook itself is a couple bucks out of pocket the answers themselves are free on quora these were some of my favorite full coverage questions to practice with right before an interview even data scientists cannot escape the dreaded algorithmic coding interview in my experience this isn t the case of the time but chances are you ll be asked to work through something similar to an easy or medium question on leetcode or hackerrank as far as language goes most companies will let you use whatever language you want personally i did almost all of my algorithmic coding in java even though the positions were targeted at python and r programmers if i had to recommend one thing it s to break out your wallet and invest in cracking the coding interview it absolutely lives up to the hype i plan to continue using it for years to come once the interviewer knows that you can think through problems and code effectively chances are that you ll move onto some more data science specific applications depending on the interviewer and the position you will likely be able to choose between python and r as your tool of choice since i m partial to python my resources below will primarily focus on effectively using pandas and numpy for data analysis a data science interview typically isn t complete without checking your knowledge of sql this can be done over the phone or through a live coding question more likely the latter i ve found that the difficulty level of these questions can vary a good bit ranging from being painfully easy to requiring complex joins and obscure functions our good friend statistics is still crucial for data scientists and it s reflected as such in interviews i had many interviews begin by seeing if i can explain a common statistics or probability concept in simple and concise terms as positions get more experienced i suspect this happens less and less as traditional statistical questions begin to take the more practical form of a b testing scenarios covered later in the post you ll notice that i ve compiled a few more resources here than in other sections this isn t a mistake machine learning is a complex field that is a virtual guarantee in data science interviews today the way that you ll be tested on this is no guarantee however it may come up as a conceptual question regarding cross validation or bias variance tradeoff or it may take the form of a take home assignment with a dataset attached i ve seen both several times so you ve got to be prepared for anything specifically check out the machine learning flashcards below they re only a couple bucks and were my by far my favorite way to quiz myself on any conceptual ml stuff this won t be covered in every single data science interview but it s certainly not uncommon most interviews will have atleast one section solely dedicated to product thinking which often lends itself to a b testing of some sort make sure your familiar with the concepts and statistical background necessary in order to be prepared when it comes up if you have time to spare i took the free online course by udacity and overall i was pretty impressed lastly i wanted to call out all of the posts related to data science jobs and interviewing that i read over and over again to understand not only how to prepare but what to expect as well if you only check out one section here this is the one to focus on this is the layer that sits on top of all the technical skills and application don t overlook it i hope you find these resources useful during your next interview or job search i know i did truthfully i m just glad that i saved these links somewhere lastly this post is part of an ongoing initiative to open source my experience applying and interviewing at data science positions so if you enjoyed this content then be sure to follow me for more stuff like this if you re interested in receiving my weekly rundown of interesting articles and resources focused on data science machine learning and artificial intelligence then subscribe to self driven data science using the form below if you enjoyed this post feel free to hit the clap button and if you re interested in posts to come make sure to follow me on medium at the link below i ll be writing and shipping every day this month as part of a day challenge this article was originally published on conordewey com from a quick cheer to a standing ovation clap to show how much you enjoyed this story data scientist writer www conordewey com sharing concepts ideas and codes
Abhishek Parbhakar,937,6,https://towardsdatascience.com/must-know-information-theory-concepts-in-deep-learning-ai-e54a5da9769d?source=---------8----------------,Must know Information Theory concepts in Deep Learning (AI),information theory is an important field that has made significant contribution to deep learning and ai and yet is unknown to many information theory can be seen as a sophisticated amalgamation of basic building blocks of deep learning calculus probability and statistics some examples of concepts in ai that come from information theory or related fields in the early th century scientists and engineers were struggling with the question how to quantify the information is there a analytical way or a mathematical measure that can tell us about the information content for example consider below two sentences it is not difficult to tell that the second sentence gives us more information since it also tells that bruno is big and brown in addition to being a dog how can we quantify the difference between two sentences can we have a mathematical measure that tells us how much more information second sentence have as compared to the first scientists were struggling with these questions semantics domain and form of data only added to the complexity of the problem then mathematician and engineer claude shannon came up with the idea of entropy that changed our world forever and marked the beginning of digital information age shannon proposed that the semantic aspects of data are irrelevant and nature and meaning of data doesn t matter when it comes to information content instead he quantified information in terms of probability distribution and uncertainty shannon also introduced the term bit that he humbly credited to his colleague john tukey this revolutionary idea not only laid the foundation of information theory but also opened new avenues for progress in fields like artificial intelligence below we discuss four popular widely used and must known information theoretic concepts in deep learning and data sciences also called information entropy or shannon entropy entropy gives a measure of uncertainty in an experiment let s consider two experiments if we compare the two experiments in exp it is easier to predict the outcome as compared to exp so we can say that exp is inherently more uncertain unpredictable than exp this uncertainty in the experiment is measured using entropy therefore if there is more inherent uncertainty in the experiment then it has higher entropy or lesser the experiment is predictable more is the entropy the probability distribution of experiment is used to calculate the entropy a deterministic experiment which is completely predictable say tossing a coin with p h has entropy zero an experiment which is completely random say rolling fair dice is least predictable has maximum uncertainty and has the highest entropy among such experiments another way to look at entropy is the average information gained when we observe outcomes of an random experiment the information gained for a outcome of an experiment is defined as a function of probability of occurrence of that outcome more the rarer is the outcome more is the information gained from observing it for example in an deterministic experiment we always know the outcome so no new information gained is here from observing the outcome and hence entropy is zero for a discrete random variable x with possible outcomes states x x n the entropy in unit of bits is defined as where p x i is the probability of i th outcome of x cross entropy is used to compare two probability distributions it tells us how similar two distributions are cross entropy between two probability distributions p and q defined over same set of outcomes is given by mutual information is a measure of mutual dependency between two probability distributions or random variables it tells us how much information about one variable is carried by the another variable mutual information captures dependency between random variables and is more generalized than vanilla correlation coefficient which captures only the linear relationship mutual information of two discrete random variables x and y is defined as where p x y is the joint probability distribution of x and y and p x and p y are the marginal probability distribution of x and y respectively also called relative entropy kl divergence is another measure to find similarities between two probability distributions it measures how much one distribution diverges from the other suppose we have some data and true distribution underlying it is p but we don t know this p so we choose a new distribution q to approximate this data since q is just an approximation it won t be able to approximate the data as good as p and some information loss will occur this information loss is given by kl divergence kl divergence between p and q tells us how much information we lose when we try to approximate data given by p with q kl divergence of a probability distribution q from another probability distribution p is defined as kl divergence is commonly used in unsupervised machine learning technique variational autoencoders information theory was originally formulated by mathematician and electrical engineer claude shannon in his seminal paper a mathematical theory of communication in note terms experiments random variable ai machine learning deep learning data science have been used loosely above but have technically different meanings in case you liked the article do follow me abhishek parbhakar for more articles related to ai philosophy and economics from a quick cheer to a standing ovation clap to show how much you enjoyed this story finding equilibria among ai philosophy and economics sharing concepts ideas and codes
Aman Dalmia,2300,17,https://blog.usejournal.com/what-i-learned-from-interviewing-at-multiple-ai-companies-and-start-ups-a9620415e4cc?source=---------9----------------,What I learned from interviewing at multiple AI companies and start-ups,over the past months i ve been interviewing at various companies like google s deepmind wadhwani institute of ai microsoft ola fractal analytics and a few others primarily for the roles data scientist software engineer research engineer in the process not only did i get an opportunity to interact with many great minds but also had a peek at myself along with a sense of what people really look for when interviewing someone i believe that if i d had this knowledge before i could have avoided many mistakes and have prepared in a much better manner which is what the motivation behind this post is to be able to help someone bag their dream place of work this post arose from a discussion with one of my juniors on the lack of really fulfilling job opportunities offered through campus placements for people working in ai also when i was preparing i noticed people using a lot of resources but as per my experience over the past months i realised that one can do away with a few minimal ones for most roles in ai all of which i m going to mention at the end of the post i begin with how to get noticed a k a the interview then i provide a list of companies and start ups to apply which is followed by how to ace that interview based on whatever experience i ve had i add a section on what we should strive to work for i conclude with minimal resources you need for preparation note for people who are sitting for campus placements there are two things i d like to add firstly most of what i m going to say except for the last one maybe is not going to be relevant to you for placements but and this is my second point as i mentioned before opportunities on campus are mostly in software engineering roles having no intersection with ai so this post is specifically meant for people who want to work on solving interesting problems using ai also i want to add that i haven t cleared all of these interviews but i guess that s the essence of failure it s the greatest teacher the things that i mention here may not all be useful but these are things that i did and there s no way for me to know what might have ended up making my case stronger to be honest this step is the most important one what makes off campus placements so tough and exhausting is getting the recruiter to actually go through your profile among the plethora of applications that they get having a contact inside the organisation place a referral for you would make it quite easy but in general this part can be sub divided into three keys steps a do the regulatory preparation and do that well so with regulatory preparation i mean a linkedin profile a github profile a portfolio website and a well polished cv firstly your cv should be really neat and concise follow this guide by udacity for cleaning up your cv resume revamp it has everything that i intend to say and i ve been using it as a reference guide myself as for the cv template some of the in built formats on overleaf are quite nice i personally use deedy resume here s a preview as it can be seen a lot of content can be fit into one page however if you really do need more than that then the format linked above would not work directly instead you can find a modified multi page format of the same here the next most important thing to mention is your github profile a lot of people underestimate the potential of this just because unlike linkedin it doesn t have a who viewed your profile option people do go through your github because that s the only way they have to validate what you have mentioned in your cv given that there s a lot of noise today with people associating all kinds of buzzwords with their profile especially for data science open source has a big role to play too with majority of the tools implementations of various algorithms lists of learning resources all being open sourced i discuss the benefits of getting involved in open source and how one can start from scratch in an earlier post here the bare minimum for now should be create a github account if you don t already have one create a repository for each of the projects that you have done add documentation with clear instructions on how to run the code add documentation for each file mentioning the role of each function the meaning of each parameter proper formatting e g pep for python along with a script to automate the previous step optional moving on the third step is what most people lack which is having a portfolio website demonstrating their experience and personal projects making a portfolio indicates that you are really serious about getting into the field and adds a lot of points to the authenticity factor also you generally have space constraints on your cv and tend to miss out on a lot of details you can use your portfolio to really delve deep into the details if you want to and it s highly recommended to include some sort of visualisation or demonstration of the project idea it s really easy to create one too as there are a lot of free platforms with drag and drop features making the process really painless i personally use weebly which is a widely used tool it s better to have a reference to begin with there are a lot of awesome ones out there but i referred to deshraj yadav s personal website to begin with making mine finally a lot of recruiters and start ups have nowadays started using linkedin as their go to platform for hiring a lot of good jobs get posted there apart from recruiters the people working at influential positions are quite active there as well so if you can grab their attention you have a good chance of getting in too apart from that maintaining a clean profile is necessary for people to have the will to connect with you an important part of linkedin is their search tool and for you to show up you must have the relevant keywords interspersed over your profile it took me a lot of iterations and re evaluations to finally have a decent one also you should definitely ask people with or under whom you ve worked with to endorse you for your skills and add a recommendation talking about their experience of working with you all of this increases your chance of actually getting noticed i ll again point towards udacity s guide for linkedin and github profiles all this might seem like a lot but remember that you don t need to do it in a single day or even a week or a month it s a process it never ends setting up everything at first would definitely take some effort but once it s there and you keep updating it regularly as events around you keep happening you ll not only find it to be quite easy but also you ll be able to talk about yourself anywhere anytime without having to explicitly prepare for it because you become so aware about yourself b stay authentic i ve seen a lot of people do this mistake of presenting themselves as per different job profiles according to me it s always better to first decide what actually interests you what would you be happy doing and then search for relevant opportunities not the other way round the fact that the demand for ai talent surpasses the supply for the same gives you this opportunity spending time on your regulatory preparation mentioned above would give you an all around perspective on yourself and help make this decision easier also you won t need to prepare answers to various kinds of questions that you get asked during an interview most of them would come out naturally as you d be talking about something you really care about c networking once you re done with a figured out b networking is what will actually help you get there if you don t talk to people you miss out on hearing about many opportunities that you might have a good shot at it s important to keep connecting with new people each day if not physically then on linkedin so that upon compounding it after many days you have a large and strong network networking is not messaging people to place a referral for you when i was starting off i did this mistake way too often until i stumbled upon this excellent article by mark meloon where he talks about the importance of building a real connection with people by offering our help first another important step in networking is to get your content out for example if you re good at something blog about it and share that blog on facebook and linkedin not only does this help others it helps you as well once you have a good enough network your visibility increases multi fold you never know how one person from your network liking or commenting on your posts may help you reach out to a much broader audience including people who might be looking for someone of your expertise i m presenting this list in alphabetical order to avoid the misinterpretation of any specific preference however i do place a on the ones that i d personally recommend this recommendation is based on either of the following mission statement people personal interaction or scope of learning more than is purely based on the nd and rd factors your interview begins the moment you have entered the room and a lot of things can happen between that moment and the time when you re asked to introduce yourself your body language and the fact that you re smiling while greeting them plays a big role especially when you re interviewing for a start up as culture fit is something that they extremely care about you need to understand that as much as the interviewer is a stranger to you you re a stranger to him her too so they re probably just as nervous as you are it s important to view the interview as more of a conversation between yourself and the interviewer both of you are looking for a mutual fit you are looking for an awesome place to work at and the interviewer is looking for an awesome person like you to work with so make sure that you re feeling good about yourself and that you take the charge of making the initial moments of your conversation pleasant for them and the easiest way i know how to make that happen is to smile there are mostly two types of interviews one where the interviewer has come with come prepared set of questions and is going to just ask you just that irrespective of your profile and the second where the interview is based on your cv i ll start with the second one this kind of interview generally begins with a can you tell me a bit about yourself at this point things are a big no talking about your gpa in college and talking about your projects in detail an ideal statement should be about a minute or two long should give a good idea on what have you been doing till now and it s not restricted to academics you can talk about your hobbies like reading books playing sports meditation etc basically anything that contributes to defining you the interviewer will then take something that you talk about here as a cue for his next question and then the technical part of the interview begins the motive of this kind of interview is to really check whether whatever you have written on your cv is true or not there would be a lot of questions on what could be done differently or if x was used instead of y what would have happened at this point it s important to know the kind of trade offs that is usually made during implementation for e g if the interviewer says that using a more complex model would have given better results then you might say that you actually had less data to work with and that would have lead to overfitting in one of the interviews i was given a case study to work on and it involved designing algorithms for a real world use case i ve noticed that once i ve been given the green flag to talk about a project the interviewers really like it when i talk about it in the following flow problem or previous approaches our approach result intuition the other kind of interview is really just to test your basic knowledge don t expect those questions to be too hard but they would definitely scratch every bit of the basics that you should be having mainly based around linear algebra probability statistics optimisation machine learning and or deep learning the resources mentioned in the minimal resources you need for preparation section should suffice but make sure that you don t miss out one bit among them the catch here is the amount of time you take to answer those questions since these cover the basics they expect that you should be answering them almost instantly so do your preparation accordingly throughout the process it s important to be confident and honest about what you know and what you don t know if there s a question that you re certain you have no idea about say it upfront rather than making aah um sounds if some concept is really important but you are struggling with answering it the interviewer would generally depending on how you did in the initial parts be happy to give you a hint or guide you towards the right solution it s a big plus if you manage to pick their hints and arrive at the correct solution try to not get nervous and the best way to avoid that is by again smiling now we come to the conclusion of the interview where the interviewer would ask you if you have any questions for them it s really easy to think that your interview is done and just say that you have nothing to ask i know many people who got rejected just because of failing at this last question as i mentioned before it s not only you who is being interviewed you are also looking for a mutual fit with the company itself so it s quite obvious that if you really want to join a place you must have many questions regarding the work culture there or what kind of role are they seeing you in it can be as simple as being curious about the person interviewing you there s always something to learn from everything around you and you should make sure that you leave the interviewer with the impression that you re truly interested in being a part of their team a final question that i ve started asking all my interviewers is for a feedback on what they might want me to improve on this has helped me tremendously and i still remember every feedback that i ve gotten which i ve incorporated into my daily life that s it based on my experience if you re just honest about yourself are competent truly care about the company you re interviewing for and have the right mindset you should have ticked all the right boxes and should be getting a congratulatory mail soon we live in an era full of opportunities and that applies to anything that you love you just need to strive to become the best at it and you will find a way to monetise it as gary vaynerchuk just follow him already says this is a great time to be working in ai and if you re truly passionate about it you have so much that you can do with ai you can empower so many people that have always been under represented we keep nagging about the problems surrounding us but there s been never such a time where common people like us can actually do something about those problems rather than just complaining jeffrey hammerbacher founder cloudera had famously said we can do so much with ai than we can ever imagine there are many extremely challenging problems out there which require incredibly smart people like you to put your head down on and solve you can make many lives better time to let go of what is cool or what would look good think and choose wisely any data science interview comprises of questions mostly of a subset of the following four categories computer science math statistics and machine learning if you re not familiar with the math behind deep learning then you should consider going over my last post for resources to understand them however if you are comfortable i ve found that the chapters and of the deep learning book are enough to prepare revise for theoretical questions during such interviews i ve been preparing summaries for a few chapters which you can refer to where i ve tried to even explain a few concepts that i found challenging to understand at first in case you are not willing to go through the entire chapters and if you ve already done a course on probability you should be comfortable answering a few numerical as well for stats covering these topics should be enough now the range of questions here can vary depending on the type of position you are applying for if it s a more traditional machine learning based interview where they want to check your basic knowledge in ml you can complete any one of the following courses machine learning by andrew ng cs machine learning course by caltech professor yaser abu mostafa important topics are supervised learning classification regression svm decision tree random forests logistic regression multi layer perceptron parameter estimation bayes decision rule unsupervised learning k means clustering gaussian mixture models dimensionality reduction pca now if you re applying for a more advanced position there s a high chance that you might be questioned on deep learning in that case you should be very comfortable with convolutional neural networks cnns and or depending upon what you ve worked on recurrent neural networks rnns and their variants and by being comfortable you must know what is the fundamental idea behind deep learning how cnns rnns actually worked what kind of architectures have been proposed and what has been the motivation behind those architectural changes now there s no shortcut for this either you understand them or you put enough time to understand them for cnns the recommended resource is stanford s cs n and cs n for rnns i found this neural network class by hugo larochelle to be really enlightening too refer this for a quick refresher too udacity coming to the aid here too by now you should have figured out that udacity is a really important place for an ml practitioner there are not a lot of places working on reinforcement learning rl in india and i too am not experienced in rl as of now so that s one thing to add to this post sometime in the future getting placed off campus is a long journey of self realisation i realise that this has been another long post and i m again extremely grateful to you for valuing my thoughts i hope that this post finds a way of being useful to you and that it helped you in some way to prepare for your next data science interview better if it did i request you to really think about what i talk about in what we should strive to work for i m very thankful to my friends from iit guwahati for their helpful feedback especially ameya godbole kothapalli vignesh and prabal jain a majority of what i mention here like viewing an interview as a conversation and seeking feedback from our interviewers arose from multiple discussions with prabal who has been advising me constantly on how i can improve my interviewing skills this story is published in noteworthy where thousands come every day to learn about the people ideas shaping the products we love follow our publication to see more product design stories featured by the journal team from a quick cheer to a standing ovation clap to show how much you enjoyed this story ai fanatic math lover dreamer the official journal blog
Sophia Arakelyan,7,4,https://buzzrobot.com/from-ballerina-to-ai-researcher-part-i-46fce67f809b?source=---------1----------------,From Ballerina to AI Researcher: Part I – buZZrobot,last year i published the article from ballerina to ai writer where i described how i embraced the technical part of ai without a technical background but having love and passion for ai i educated myself and was able to build a neural net classifier and do projects in deep rl recently i ve become a participant in the openai scholarship program openai is a non profit that gathers top ai researchers to ensure the safety of ai to benefit humanity every week for the next three months i ll publish blog posts sharing my story of transformation from a person dedicated to years of professional dancing and then writing about tech and ai to actually conducting ai research finding your true calling the key component of happiness my primary goal with the series of blog posts from ballerina to ai researcher is to show that it s never too late to embrace a new field start over again and find your true calling finding work you love is one of the most important components of happiness something that you do every day and invest your time in to grow that makes you feel fulfilled gives you energy something that is a refuge for your soul great things never come easy we have to be able to fight to make great things happen but you can t fight for something you don t believe in especially if you don t feel like it s really important for you and humanity finding that thing is a real challenge i feel lucky that i found my true passion ai to me the technology itself and the ai community researchers scientists people who dedicate their lives to building the most powerful technology of all time with the mission to benefit humanity and make it safe for us is a great source of energy the structure of the blog post series today i m giving an overall intro of what i m going to cover in my from ballerina to ai researcher series i ll dedicate the sequence of blog posts during the openai scholars program to several aspects of ai technology i ll cover those areas that concern me a lot like ai and automation bias in ml dual use of ai etc also the structure of my posts will include some insights on what i m working on right now the final technical project will be available by the end of august and will be open sourced i feel very lucky to have alec radford an experienced researcher as my mentor who guides me in the nlp and nlu research area first week of my scholarship i ve dedicated my first week within the program to learning about the transformer architecture that performs much better on sequential data compared to rnns lstms the novelty of the architecture is its multi head self attention mechanism according to the original paper experiments with the transformer on two machine translation tasks showed the model to be superior in quality while being more parallelizable and requiring significantly less time to train more concretely when rnns or cnns take a sequence as an input it goes through sentences word by word which is a huge obstacle toward parallelization of the process takes more time to train models moreover if sequences are too long the model tends to forget the content of distant positions in sequence or mixes it with the following positions content this is the fundamental problem in dealing with sequential data the transformer architecture reduced this problem thanks to the multi head self attention mechanism i digged into rnn lstm models to catch up with the background information to that end i ve found andrew ng s course on deep learning along with the papers extremely useful to develop insights regarding the transformer i went through the following resources the video by ukasz kaiser from google brain one of the model s creators a blog post with very well elaborated content re the model ran the code tensor tensor and the code using the pytorch framework from this paper to feel the difference between the tf and pytorch frameworks overall the goal within the program is to develop deep comprehension of the nlu research area challenges current state of the art and to formulate and test hypotheses that tackle the most important problems of the field i ll share more on what i m working on in my future articles meanwhile if you have questions feedback please leave a comment if you want to learn more about me here are my facebook and twitter accounts i d appreciate your feedback on my posts such as what topics are most interesting to you that i should consider further coverage on from a quick cheer to a standing ovation clap to show how much you enjoyed this story former ballerina turned ai writer fan of sci fi astrophysics consciousness is the key founder of buzzrobot com the publication aims to cover practical aspects of ai technology use cases along with interviews with notable people in the ai field
Dr. GP Pulipaka,2,6,https://medium.com/@gp_pulipaka/3-ways-to-apply-latent-semantic-analysis-on-large-corpus-text-on-macos-terminal-jupyterlab-colab-7b4dc3e1622?source=---------9----------------,"3 Ways to Apply Latent Semantic Analysis on Large-Corpus Text on macOS Terminal, JupyterLab, and...",latent semantic analysis works on large scale datasets to generate representations to discover the insights through natural language processing there are different approaches to perform the latent semantic analysis at multiple levels such as document level phrase level and sentence level primarily semantic analysis can be summarized into lexical semantics and the study of combining individual words into paragraphs or sentences the lexical semantics classifies and decomposes the lexical items applying lexical semantic structures has different contexts to identify the differences and similarities between the words a generic term in a paragraph or a sentence is hypernym and hyponymy provides the meaning of the relationship between instances of the hyponyms homonyms contain similar syntax or similar spelling with similar structuring with different meanings homonyms are not related to each other book is an example for homonym it can mean for someone to read something or an act of making a reservation with similar spelling form and syntax however the definition is different polysemy is another phenomenon of the words where a single word could be associated with multiple related senses and distinct meanings the word polysemy is a greek word which means many signs python provides nltk library to perform tokenization of the words by chopping the words in larger chunks into phrases or meaningful strings processing words through tokenization produce tokens word lemmatization converts words from the current inflected form into the base form latent semantic analysis applying latent semantic analysis on large datasets of text and documents represents the contextual meaning through mathematical and statistical computation methods on large corpus of text many times latent semantic analysis overtook human scores and subject matter tests conducted by humans the accuracy of latent semantic analysis is high as it reads through machine readable documents and texts at a web scale latent semantic analysis is a technique that applies singular value decomposition and principal component analysis pca the document can be represented with z x y matrix a the rows of the matrix represent the document in the collection the matrix a can represent numerous hundred thousands of rows and columns on a typical large corpus text document applying singular value decomposition develops a set of operations dubbed matrix decomposition natural language processing in python with nltk library applies a low rank approximation to the term document matrix later the low rank approximation aids in indexing and retrieving the document known as latent semantic indexing by clustering the number of words in the document brief overview of linear algebra the a with z x y matrix contains the real valued entries with non negative values for the term document matrix determining the rank of the matrix comes with the number of linearly independent columns or rows in the the matrix the rank of a z y a square c x c represented as diagonal matrix where off diagonal entries are zero examining the matrix if all the c diagonal matrices are one the identity matrix of the dimension c represented by ic for the square z x z matrix a with a vector k which contains not all zeroes for the matrix decomposition applies on the square matrix factored into the product of matrices from eigenvectors this allows to reduce the dimensionality of the words from multi dimensions to two dimensions to view on the plot the dimensionality reduction techniques with principal component analysis and singular value decomposition holds critical relevance in natural language processing the zipfian nature of the frequency of the words in a document makes it difficult to determine the similarity of the words in a static stage hence eigen decomposition is a by product of singular value decomposition as the input of the document is highly asymmetrical the latent semantic analysis is a particular technique in semantic space to parse through the document and identify the words with polysemy with nlkt library the resources such as punkt and wordnet have to be downloaded from nltk deep learning at scale with google colab notebooks training machine learning or deep learning models on cpus could take hours and could be pretty expensive in terms of the programming language efficiency with time and energy of the computer resources google built colab notebooks environment for research and development purposes it runs entirely on the cloud without requiring any additional hardware or software setup for each machine it s entirely equivalent of a jupyter notebook that aids the data scientists to share the colab notebooks by storing on google drive just like any other google sheets or documents in a collaborative environment there are no additional costs associated with enabling gpu at runtime for acceleration on the runtime there are some challenges of uploading the data into colab unlike jupyter notebook that can access the data directly from the local directory of the machine in colab there are multiple options to upload the files from the local file system or a drive can be mounted to load the data through drive fuse wrapper once this step is complete it shows the following log without errors the next step would be generating the authentication tokens to authenticate the google credentials for the drive and colab if it shows successful retrieval of access token then colab is all set at this stage the drive is not mounted yet it will show false when accessing the contents of the text file once the drive is mounted colab has access to the datasets from google drive once the files are accessible the python can be executed similar to executing in jupyter environment colab notebook also displays the results similar to what we see on jupyter notebook pycharm ide the program can be run compiled on pycharm ide environment and run on pycharm or can be executed from osx terminal results from osx terminal jupyter notebook on standalone machine jupyter notebook gives a similar output running the latent semantic analysis on the local machine references gorrell g generalized hebbian algorithm for incremental singular value decomposition in natural language processing retrieved from https www aclweb org anthology e hardeniya n natural language processing python and nltk birmingham england packt publishing landauer t k foltz p w laham d university of colorado at boulder an introduction to latent semantic analysis retrieved from http lsa colorado edu papers dp lsaintro pdf stackoverflow mounting google drive on google colab retrieved from https stackoverflow com questions mounting google drive on google colab stanford university matrix decompositions and latent semantic indexing retrieved from https nlp stanford edu ir book html htmledition matrix decompositions and latent semantic indexing html from a quick cheer to a standing ovation clap to show how much you enjoyed this story ganapathi pulipaka founder and ceo deepsingularity bestselling author big data iot startups sap machinelearning deeplearning datascience
Scott Santens,7300,14,https://medium.com/basic-income/deep-learning-is-going-to-teach-us-all-the-lesson-of-our-lives-jobs-are-for-machines-7c6442e37a49?source=tag_archive---------0----------------,Deep Learning Is Going to Teach Us All the Lesson of Our Lives: Jobs Are for Machines,an alternate version of this article was originally published in the boston globe on december nd a team of scientists led by enrico fermi came back from lunch and watched as humanity created the first self sustaining nuclear reaction inside a pile of bricks and wood underneath a football field at the university of chicago known to history as chicago pile it was celebrated in silence with a single bottle of chianti for those who were there understood exactly what it meant for humankind without any need for words now something new has occurred that again quietly changed the world forever like a whispered word in a foreign language it was quiet in that you may have heard it but its full meaning may not have been comprehended however it s vital we understand this new language and what it s increasingly telling us for the ramifications are set to alter everything we take for granted about the way our globalized economy functions and the ways in which we as humans exist within it the language is a new class of machine learning known as deep learning and the whispered word was a computer s use of it to seemingly out of nowhere defeat three time european go champion fan hui not once but five times in a row without defeat many who read this news considered that as impressive but in no way comparable to a match against lee se dol instead who many consider to be one of the world s best living go players if not the best imagining such a grand duel of man versus machine china s top go player predicted that lee would not lose a single game and lee himself confidently expected to possibly lose one at the most what actually ended up happening when they faced off lee went on to lose all but one of their match s five games an ai named alphago is now a better go player than any human and has been granted the divine rank of dan in other words its level of play borders on godlike go has officially fallen to machine just as jeopardy did before it to watson and chess before that to deep blue so what is go very simply think of go as super ultra mega chess this may still sound like a small accomplishment another feather in the cap of machines as they continue to prove themselves superior in the fun games we play but it is no small accomplishment and what s happening is no game alphago s historic victory is a clear signal that we ve gone from linear to parabolic advances in technology are now so visibly exponential in nature that we can expect to see a lot more milestones being crossed long before we would otherwise expect these exponential advances most notably in forms of artificial intelligence limited to specific tasks we are entirely unprepared for as long as we continue to insist upon employment as our primary source of income this may all sound like exaggeration so let s take a few decade steps back and look at what computer technology has been actively doing to human employment so far let the above chart sink in do not be fooled into thinking this conversation about the automation of labor is set in the future it s already here computer technology is already eating jobs and has been since all work can be divided into four types routine and nonroutine cognitive and manual routine work is the same stuff day in and day out while nonroutine work varies within these two varieties is the work that requires mostly our brains cognitive and the work that requires mostly our bodies manual where once all four types saw growth the stuff that is routine stagnated back in this happened because routine labor is easiest for technology to shoulder rules can be written for work that doesn t change and that work can be better handled by machines distressingly it s exactly routine work that once formed the basis of the american middle class it s routine manual work that henry ford transformed by paying people middle class wages to perform and it s routine cognitive work that once filled us office spaces such jobs are now increasingly unavailable leaving only two kinds of jobs with rosy outlooks jobs that require so little thought we pay people little to do them and jobs that require so much thought we pay people well to do them if we can now imagine our economy as a plane with four engines where it can still fly on only two of them as long as they both keep roaring we can avoid concerning ourselves with crashing but what happens when our two remaining engines also fail that s what the advancing fields of robotics and ai represent to those final two engines because for the first time we are successfully teaching machines to learn i m a writer at heart but my educational background happens to be in psychology and physics i m fascinated by both of them so my undergraduate focus ended up being in the physics of the human brain otherwise known as cognitive neuroscience i think once you start to look into how the human brain works how our mass of interconnected neurons somehow results in what we describe as the mind everything changes at least it did for me as a quick primer in the way our brains function they re a giant network of interconnected cells some of these connections are short and some are long some cells are only connected to one other and some are connected to many electrical signals then pass through these connections at various rates and subsequent neural firings happen in turn it s all kind of like falling dominoes but far faster larger and more complex the result amazingly is us and what we ve been learning about how we work we ve now begun applying to the way machines work one of these applications is the creation of deep neural networks kind of like pared down virtual brains they provide an avenue to machine learning that s made incredible leaps that were previously thought to be much further down the road if even possible at all how it s not just the obvious growing capability of our computers and our expanding knowledge in the neurosciences but the vastly growing expanse of our collective data aka big data big data isn t just some buzzword it s information and when it comes to information we re creating more and more of it every day in fact we re creating so much that a report by sintef estimated that of all information in the world had been created in the prior two years this incredible rate of data creation is even doubling every years thanks to the internet where in every minute we were liking million things on facebook uploading hours of video to youtube and sending tweets everything we do is generating data like never before and lots of data is exactly what machines need in order to learn to learn why imagine programming a computer to recognize a chair you d need to enter a ton of instructions and the result would still be a program detecting chairs that aren t and not detecting chairs that are so how did we learn to detect chairs our parents pointed at a chair and said chair then we thought we had that whole chair thing all figured out so we pointed at a table and said chair which is when our parents told us that was table this is called reinforcement learning the label chair gets connected to every chair we see such that certain neural pathways are weighted and others aren t for chair to fire in our brains what we perceive has to be close enough to our previous chair encounters essentially our lives are big data filtered through our brains the power of deep learning is that it s a way of using massive amounts of data to get machines to operate more like we do without giving them explicit instructions instead of describing chairness to a computer we instead just plug it into the internet and feed it millions of pictures of chairs it can then have a general idea of chairness next we test it with even more images where it s wrong we correct it which further improves its chairness detection repetition of this process results in a computer that knows what a chair is when it sees it for the most part as well as we can the important difference though is that unlike us it can then sort through millions of images within a matter of seconds this combination of deep learning and big data has resulted in astounding accomplishments just in the past year aside from the incredible accomplishment of alphago google s deepmind ai learned how to read and comprehend what it read through hundreds of thousands of annotated news articles deepmind also taught itself to play dozens of atari video games better than humans just by looking at the screen and its score and playing games repeatedly an ai named giraffe taught itself how to play chess in a similar manner using a dataset of million chess positions attaining international master level status in just hours by repeatedly playing itself in an ai even passed a visual turing test by learning to learn in a way that enabled it to be shown an unknown character in a fictional alphabet then instantly reproduce that letter in a way that was entirely indistinguishable from a human given the same task these are all major milestones in ai however despite all these milestones when asked to estimate when a computer would defeat a prominent go player the answer even just months prior to the announcement by google of alphago s victory was by experts essentially maybe in another ten years a decade was considered a fair guess because go is a game so complex i ll just let ken jennings of jeopardy fame another former champion human defeated by ai describe it such confounding complexity makes impossible any brute force approach to scan every possible move to determine the next best move but deep neural networks get around that barrier in the same way our own minds do by learning to estimate what feels like the best move we do this through observation and practice and so did alphago by analyzing millions of professional games and playing itself millions of times so the answer to when the game of go would fall to machines wasn t even close to ten years the correct answer ended up being any time now any time now that s the new go to response in the st century for any question involving something new machines can do better than humans and we need to try to wrap our heads around it we need to recognize what it means for exponential technological change to be entering the labor market space for nonroutine jobs for the first time ever machines that can learn mean nothing humans do as a job is uniquely safe anymore from hamburgers to healthcare machines can be created to successfully perform such tasks with no need or less need for humans and at lower costs than humans amelia is just one ai out there currently being beta tested in companies right now created by ipsoft over the past years she s learned how to perform the work of call center employees she can learn in seconds what takes us months and she can do it in languages because she s able to learn she s able to do more over time in one company putting her through the paces she successfully handled one of every ten calls in the first week and by the end of the second month she could resolve six of ten calls because of this it s been estimated that she can put million people out of a job worldwide viv is an ai coming soon from the creators of siri who ll be our own personal assistant she ll perform tasks online for us and even function as a facebook news feed on steroids by suggesting we consume the media she ll know we ll like best in doing all of this for us we ll see far fewer ads and that means the entire advertising industry that industry the entire internet is built upon stands to be hugely disrupted a world with amelia and viv and the countless other ai counterparts coming online soon in combination with robots like boston dynamics next generation atlas portends is a world where machines can do all four types of jobs and that means serious societal reconsiderations if a machine can do a job instead of a human should any human be forced at the threat of destitution to perform that job should income itself remain coupled to employment such that having a job is the only way to obtain income when jobs for many are entirely unobtainable if machines are performing an increasing percentage of our jobs for us and not getting paid to do them where does that money go instead and what does it no longer buy is it even possible that many of the jobs we re creating don t need to exist at all and only do because of the incomes they provide these are questions we need to start asking and fast fortunately people are beginning to ask these questions and there s an answer that s building up momentum the idea is to put machines to work for us but empower ourselves to seek out the forms of remaining work we as humans find most valuable by simply providing everyone a monthly paycheck independent of work this paycheck would be granted to all citizens unconditionally and its name is universal basic income by adopting ubi aside from immunizing against the negative effects of automation we d also be decreasing the risks inherent in entrepreneurship and the sizes of bureaucracies necessary to boost incomes it s for these reasons it has cross partisan support and is even now in the beginning stages of possible implementation in countries like switzerland finland the netherlands and canada the future is a place of accelerating changes it seems unwise to continue looking at the future as if it were the past where just because new jobs have historically appeared they always will the wef started off by estimating the creation by of million new jobs alongside the elimination of million that s a net loss not a net gain of million jobs in a frequently cited paper an oxford study estimated the automation of about half of all existing jobs by meanwhile self driving vehicles again thanks to machine learning have the capability of drastically impacting all economies especially the us economy as i wrote last year about automating truck driving by eliminating millions of jobs within a short span of time and now even the white house in a stunning report to congress has put the probability at percent that a worker making less than an hour in will eventually lose their job to a machine even workers making as much as an hour face odds of percent to ignore odds like these is tantamount to our now laughable duck and cover strategies for avoiding nuclear blasts during the cold war all of this is why it s those most knowledgeable in the ai field who are now actively sounding the alarm for basic income during a panel discussion at the end of at singularity university prominent data scientist jeremy howard asked do you want half of people to starve because they literally can t add economic value or not before going on to suggest if the answer is not then the smartest way to distribute the wealth is by implementing a universal basic income ai pioneer chris eliasmith director of the centre for theoretical neuroscience warned about the immediate impacts of ai on society in an interview with futurism ai is already having a big impact on our economies my suspicion is that more countries will have to follow finland s lead in exploring basic income guarantees for people moshe vardi expressed the same sentiment after speaking at the annual meeting of the american association for the advancement of science about the emergence of intelligent machines we need to rethink the very basic structure of our economic system we may have to consider instituting a basic income guarantee even baidu s chief scientist and founder of google s google brain deep learning project andrew ng during an onstage interview at this year s deep learning summit expressed the shared notion that basic income must be seriously considered by governments citing a high chance that ai will create massive labor displacement when those building the tools begin warning about the implications of their use shouldn t those wishing to use those tools listen with the utmost attention especially when it s the very livelihoods of millions of people at stake if not then what about when nobel prize winning economists begin agreeing with them in increasing numbers no nation is yet ready for the changes ahead high labor force non participation leads to social instability and a lack of consumers within consumer economies leads to economic instability so let s ask ourselves what s the purpose of the technologies we re creating what s the purpose of a car that can drive for us or artificial intelligence that can shoulder of our workload is it to allow us to work more hours for even less pay or is it to enable us to choose how we work and to decline any pay hours we deem insufficient because we re already earning the incomes that machines aren t what s the big lesson to learn in a century when machines can learn i offer it s that jobs are for machines and life is for people this article was written on a crowdfunded monthly basic income if you found value in this article you can support it along with all my advocacy for basic income with a monthly patron pledge of special thanks to arjun banker steven grimm larry cohen topher hunt aaron marcus kubitza andrew stern keith davis albert wenger richard just chris smothers mark witham david ihnen danielle texeira katie doemland paul wicks jan smole joe esposito jack wagner joe ballou stuart matthews natalie foster chris mccoy michael honey gary aranovich kai wong john david hodge louise whitmore dan o sullivan harish venkatesan michiel dral gerald huff susanne berg cameron ottens kian alavi gray scott kirk israel robert solovay jeff schulman andrew henderson robert f greene martin jordo victor lau shane gordon paolo narciso johan grahn tony destefano erhan altay bryan herdliska stephane boisvert dave shelton rise shine pac luke sampson lee irving kris roadruck amy shaffer thomas welsh olli niinima ki casey young elizabeth balcar masud shah allen bauer all my other funders for their support and my amazing partner katie smith scott santens writes about basic income on his blog you can also follow him here on medium on twitter on facebook or on reddit where he is a moderator for the r basicincome community of over subscribers if you feel others would appreciate this article please click the green heart from a quick cheer to a standing ovation clap to show how much you enjoyed this story new orleans writer focused on the potential for human civilization to gets its act together in the st century moderator of r basicincome on reddit articles discussing the concept of the universal basic income
Adam Geitgey,35000,15,https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471?source=tag_archive---------1----------------,Machine Learning is Fun! – Adam Geitgey – Medium,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in portugue s portugue s alternate tu rkc e franc ais espan ol me xico espan ol espan a polski italiano tie ng vie t or bigger update the content of this article is now available as a full length video course that walks you through every step of the code you can take the course for free and access everything else on lynda com free for days if you sign up with this link have you heard people talking about machine learning but only have a fuzzy idea of what that means are you tired of nodding your way through conversations with co workers let s change that this guide is for anyone who is curious about machine learning but has no idea where to start i imagine there are a lot of people who tried reading the wikipedia article got frustrated and gave up wishing someone would just give them a high level explanation that s what this is the goal is be accessible to anyone which means that there s a lot of generalizations but who cares if this gets anyone more interested in ml then mission accomplished machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem instead of writing code you feed data to the generic algorithm and it builds its own logic based on the data for example one kind of algorithm is a classification algorithm it can put data into different groups the same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not spam without changing a line of code it s the same algorithm but it s fed different training data so it comes up with different classification logic machine learning is an umbrella term covering lots of these kinds of generic algorithms you can think of machine learning algorithms as falling into one of two main categories supervised learning and unsupervised learning the difference is simple but really important let s say you are a real estate agent your business is growing so you hire a bunch of new trainee agents to help you out but there s a problem you can glance at a house and have a pretty good idea of what a house is worth but your trainees don t have your experience so they don t know how to price their houses to help your trainees and maybe free yourself up for a vacation you decide to write a little app that can estimate the value of a house in your area based on it s size neighborhood etc and what similar houses have sold for so you write down every time someone sells a house in your city for months for each house you write down a bunch of details number of bedrooms size in square feet neighborhood etc but most importantly you write down the final sale price using that training data we want to create a program that can estimate how much any other house in your area is worth this is called supervised learning you knew how much each house sold for so in other words you knew the answer to the problem and could work backwards from there to figure out the logic to build your app you feed your training data about each house into your machine learning algorithm the algorithm is trying to figure out what kind of math needs to be done to make the numbers work out this kind of like having the answer key to a math test with all the arithmetic symbols erased from this can you figure out what kind of math problems were on the test you know you are supposed to do something with the numbers on the left to get each answer on the right in supervised learning you are letting the computer work out that relationship for you and once you know what math was required to solve this specific set of problems you could answer to any other problem of the same type let s go back to our original example with the real estate agent what if you didn t know the sale price for each house even if all you know is the size location etc of each house it turns out you can still do some really cool stuff this is called unsupervised learning this is kind of like someone giving you a list of numbers on a sheet of paper and saying i don t really know what these numbers mean but maybe you can figure out if there is a pattern or grouping or something good luck so what could do with this data for starters you could have an algorithm that automatically identified different market segments in your data maybe you d find out that home buyers in the neighborhood near the local college really like small houses with lots of bedrooms but home buyers in the suburbs prefer bedroom houses with lots of square footage knowing about these different kinds of customers could help direct your marketing efforts another cool thing you could do is automatically identify any outlier houses that were way different than everything else maybe those outlier houses are giant mansions and you can focus your best sales people on those areas because they have bigger commissions supervised learning is what we ll focus on for the rest of this post but that s not because unsupervised learning is any less useful or interesting in fact unsupervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer side note there are lots of other types of machine learning algorithms but this is a pretty good place to start as a human your brain can approach most any situation and learn how to deal with that situation without any explicit instructions if you sell houses for a long time you will instinctively have a feel for the right price for a house the best way to market that house the kind of client who would be interested etc the goal of strong ai research is to be able to replicate this ability with computers but current machine learning algorithms aren t that good yet they only work when focused a very specific limited problem maybe a better definition for learning in this case is figuring out an equation to solve a specific problem based on some example data unfortunately machine figuring out an equation to solve a specific problem based on some example data isn t really a great name so we ended up with machine learning instead of course if you are reading this years in the future and we ve figured out the algorithm for strong ai then this whole post will all seem a little quaint maybe stop reading and go tell your robot servant to go make you a sandwich future human so how would you write the program to estimate the value of a house like in our example above think about it for a second before you read further if you didn t know anything about machine learning you d probably try to write out some basic rules for estimating the price of a house like this if you fiddle with this for hours and hours you might end up with something that sort of works but your program will never be perfect and it will be hard to maintain as prices change wouldn t it be better if the computer could just figure out how to implement this function for you who cares what exactly the function does as long is it returns the correct number one way to think about this problem is that the price is a delicious stew and the ingredients are the number of bedrooms the square footage and the neighborhood if you could just figure out how much each ingredient impacts the final price maybe there s an exact ratio of ingredients to stir in to make the final price that would reduce your original function with all those crazy if s and else s down to something really simple like this notice the magic numbers in bold and these are our weights if we could just figure out the perfect weights to use that work for every house our function could predict house prices a dumb way to figure out the best weights would be something like this start with each weight set to run every house you know about through your function and see how far off the function is at guessing the correct price for each house for example if the first house really sold for but your function guessed it sold for you are off by for that single house now add up the squared amount you are off for each house you have in your data set let s say that you had home sales in your data set and the square of how much your function was off for each house was a grand total of that s how wrong your function currently is now take that sum total and divide it by to get an average of how far off you are for each house call this average error amount the cost of your function if you could get this cost to be zero by playing with the weights your function would be perfect it would mean that in every case your function perfectly guessed the price of the house based on the input data so that s our goal get this cost to be as low as possible by trying different weights repeat step over and over with every single possible combination of weights whichever combination of weights makes the cost closest to zero is what you use when you find the weights that work you ve solved the problem that s pretty simple right well think about what you just did you took some data you fed it through three generic really simple steps and you ended up with a function that can guess the price of any house in your area watch out zillow but here s a few more facts that will blow your mind pretty crazy right ok of course you can t just try every combination of all possible weights to find the combo that works the best that would literally take forever since you d never run out of numbers to try to avoid that mathematicians have figured out lots of clever ways to quickly find good values for those weights without having to try very many here s one way first write a simple equation that represents step above now let s re write exactly the same equation but using a bunch of machine learning math jargon that you can ignore for now this equation represents how wrong our price estimating function is for the weights we currently have set if we graph this cost equation for all possible values of our weights for number of bedrooms and sqft we d get a graph that might look something like this in this graph the lowest point in blue is where our cost is the lowest thus our function is the least wrong the highest points are where we are most wrong so if we can find the weights that get us to the lowest point on this graph we ll have our answer so we just need to adjust our weights so we are walking down hill on this graph towards the lowest point if we keep making small adjustments to our weights that are always moving towards the lowest point we ll eventually get there without having to try too many different weights if you remember anything from calculus you might remember that if you take the derivative of a function it tells you the slope of the function s tangent at any point in other words it tells us which way is downhill for any given point on our graph we can use that knowledge to walk downhill so if we calculate a partial derivative of our cost function with respect to each of our weights then we can subtract that value from each weight that will walk us one step closer to the bottom of the hill keep doing that and eventually we ll reach the bottom of the hill and have the best possible values for our weights if that didn t make sense don t worry and keep reading that s a high level summary of one way to find the best weights for your function called batch gradient descent don t be afraid to dig deeper if you are interested on learning the details when you use a machine learning library to solve a real problem all of this will be done for you but it s still useful to have a good idea of what is happening the three step algorithm i described is called multivariate linear regression you are estimating the equation for a line that fits through all of your house data points then you are using that equation to guess the sales price of houses you ve never seen before based where that house would appear on your line it s a really powerful idea and you can solve real problems with it but while the approach i showed you might work in simple cases it won t work in all cases one reason is because house prices aren t always simple enough to follow a continuous line but luckily there are lots of ways to handle that there are plenty of other machine learning algorithms that can handle non linear data like neural networks or svms with kernels there are also ways to use linear regression more cleverly that allow for more complicated lines to be fit in all cases the same basic idea of needing to find the best weights still applies also i ignored the idea of overfitting it s easy to come up with a set of weights that always works perfectly for predicting the prices of the houses in your original data set but never actually works for any new houses that weren t in your original data set but there are ways to deal with this like regularization and using a cross validation data set learning how to deal with this issue is a key part of learning how to apply machine learning successfully in other words while the basic concept is pretty simple it takes some skill and experience to apply machine learning and get useful results but it s a skill that any developer can learn once you start seeing how easily machine learning techniques can be applied to problems that seem really hard like handwriting recognition you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data just feed in the data and watch the computer magically figure out the equation that fits the data but it s important to remember that machine learning only works if the problem is actually solvable with the data that you have for example if you build a model that predicts home prices based on the type of potted plants in each house it s never going to work there just isn t any kind of relationship between the potted plants in each house and the home s sale price so no matter how hard it tries the computer can never deduce a relationship between the two so remember if a human expert couldn t use the data to solve the problem manually a computer probably won t be able to either instead focus on problems where a human could solve the problem but where it would be great if a computer could solve it much more quickly in my mind the biggest problem with machine learning right now is that it mostly lives in the world of academia and commercial research groups there isn t a lot of easy to understand material out there for people who would like to get a broad understanding without actually becoming experts but it s getting a little better every day if you want to try out what you ve learned in this article i made a course that walks you through every step of this article including writing all the code give it a try if you want to go deeper andrew ng s free machine learning class on coursera is pretty amazing as a next step i highly recommend it it should be accessible to anyone who has a comp sci degree and who remembers a very minimal amount of math also you can play around with tons of machine learning algorithms by downloading and installing scikit learn it s a python framework that has black box versions of all the standard algorithms if you liked this article please consider signing up for my machine learning is fun newsletter also please check out the full length course version of this article it covers everything in this article in more detail including writing the actual code in python you can get a free day trial to watch the course if you sign up with this link you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Adam Geitgey,14200,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721?source=tag_archive---------2----------------,Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in portugue s tie ng vie t or italiano are you tired of reading endless news stories about deep learning and not really knowing what that means let s change that this time we are going to learn how to write programs that recognize objects in images using deep learning in other words we re going to explain the black magic that allows google photos to search your photos based on what is in the picture just like part and part this guide is for anyone who is curious about machine learning but has no idea where to start the goal is be accessible to anyone which means that there s a lot of generalizations and we skip lots of details but who cares if this gets anyone more interested in ml then mission accomplished if you haven t already read part and part read them now you might have seen this famous xkcd comic before the goof is based on the idea that any year old child can recognize a photo of a bird but figuring out how to make a computer recognize objects has puzzled the very best computer scientists for over years in the last few years we ve finally found a good approach to object recognition using deep convolutional neural networks that sounds like a a bunch of made up words from a william gibson sci fi novel but the ideas are totally understandable if you break them down one by one so let s do it let s write a program that can recognize birds before we learn how to recognize pictures of birds let s learn how to recognize something much simpler the handwritten number in part we learned about how neural networks can solve complex problems by chaining together lots of simple neurons we created a small neural network to estimate the price of a house based on how many bedrooms it had how big it was and which neighborhood it was in we also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems so let s modify this same neural network to recognize handwritten text but to make the job really simple we ll only try to recognize one letter the numeral machine learning only works when you have data preferably a lot of data so we need lots and lots of handwritten s to get started luckily researchers created the mnist data set of handwritten numbers for this very purpose mnist provides images of handwritten digits each as an x image here are some s from the data set the neural network we made in part only took in a three numbers as the input bedrooms sq feet etc but now we want to process images with our neural network how in the world do we feed images into a neural network instead of just numbers the answer is incredible simple a neural network takes numbers as input to a computer an image is really just a grid of numbers that represent how dark each pixel is to feed an image into our neural network we simply treat the x pixel image as an array of numbers the handle inputs we ll just enlarge our neural network to have input nodes notice that our neural network also has two outputs now instead of just one the first output will predict the likelihood that the image is an and thee second output will predict the likelihood it isn t an by having a separate output for each type of object we want to recognize we can use a neural network to classify objects into groups our neural network is a lot bigger than last time inputs instead of but any modern computer can handle a neural network with a few hundred nodes without blinking this would even work fine on your cell phone all that s left is to train the neural network with images of s and not s so it learns to tell them apart when we feed in an we ll tell it the probability the image is an is and the probability it s not an is vice versa for the counter example images here s some of our training data we can train this kind of neural network in a few minutes on a modern laptop when it s done we ll have a neural network that can recognize pictures of s with a pretty high accuracy welcome to the world of late s era image recognition it s really neat that simply feeding pixels into a neural network actually worked to build image recognition machine learning is magic right well of course it s not that simple first the good news is that our recognizer really does work well on simple images where the letter is right in the middle of the image but now the really bad news our recognizer totally fails to work when the letter isn t perfectly centered in the image just the slightest position change ruins everything this is because our network only learned the pattern of a perfectly centered it has absolutely no idea what an off center is it knows exactly one pattern and one pattern only that s not very useful in the real world real world problems are never that clean and simple so we need to figure out how to make our neural network work in cases where the isn t perfectly centered we already created a really good program for finding an centered in an image what if we just scan all around the image for possible s in smaller sections one section at a time until we find one this approach called a sliding window it s the brute force solution it works well in some limited cases but it s really inefficient you have to check the same image over and over looking for objects of different sizes we can do better than this when we trained our network we only showed it s that were perfectly centered what if we train it with more data including s in all different positions and sizes all around the image we don t even need to collect new training data we can just write a script to generate new images with the s in all kinds of different positions in the image using this technique we can easily create an endless supply of training data more data makes the problem harder for our neural network to solve but we can compensate for that by making our network bigger and thus able to learn more complicated patterns to make the network bigger we just stack up layer upon layer of nodes we call this a deep neural network because it has more layers than a traditional neural network this idea has been around since the late s but until recently training this large of a neural network was just too slow to be useful but once we figured out how to use d graphics cards which were designed to do matrix multiplication really fast instead of normal computer processors working with large neural networks suddenly became practical in fact the exact same nvidia geforce gtx video card that you use to play overwatch can be used to train neural networks incredibly quickly but even though we can make our neural network really big and train it quickly with a d graphics card that still isn t going to get us all the way to a solution we need to be smarter about how we process images into our neural network think about it it doesn t make sense to train a network to recognize an at the top of a picture separately from training it to recognize an at the bottom of a picture as if those were two totally different objects there should be some way to make the neural network smart enough to know that an anywhere in the picture is the same thing without all that extra training luckily there is as a human you intuitively know that pictures have a hierarchy or conceptual structure consider this picture as a human you instantly recognize the hierarchy in this picture most importantly we recognize the idea of a child no matter what surface the child is on we don t have to re learn the idea of child for every possible surface it could appear on but right now our neural network can t do this it thinks that an in a different part of the image is an entirely different thing it doesn t understand that moving an object around in the picture doesn t make it something different this means it has to re learn the identify of each object in every possible position that sucks we need to give our neural network understanding of translation invariance an is an no matter where in the picture it shows up we ll do this using a process called convolution the idea of convolution is inspired partly by computer science and partly by biology i e mad scientists literally poking cat brains with weird probes to figure out how cats process images instead of feeding entire images into our neural network as one grid of numbers we re going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture here s how it s going to work step by step similar to our sliding window search above let s pass a sliding window over the entire original image and save each result as a separate tiny picture tile by doing this we turned our original image into equally sized tiny image tiles earlier we fed a single image into a neural network to see if it was an we ll do the exact same thing here but we ll do it for each individual image tile however there s one big twist we ll keep the same neural network weights for every single tile in the same original image in other words we are treating every image tile equally if something interesting appears in any given tile we ll mark that tile as interesting we don t want to lose track of the arrangement of the original tiles so we save the result from processing each tile into a grid in the same arrangement as the original image it looks like this in other words we ve started with a large image and we ended with a slightly smaller array that records which sections of our original image were the most interesting the result of step was an array that maps out which parts of the original image are the most interesting but that array is still pretty big to reduce the size of the array we downsample it using an algorithm called max pooling it sounds fancy but it isn t at all we ll just look at each x square of the array and keep the biggest number the idea here is that if we found something interesting in any of the four input tiles that makes up each x grid square we ll just keep the most interesting bit this reduces the size of our array while keeping the most important bits so far we ve reduced a giant image down into a fairly small array guess what that array is just a bunch of numbers so we can use that small array as input into another neural network this final neural network will decide if the image is or isn t a match to differentiate it from the convolution step we call it a fully connected network so from start to finish our whole five step pipeline looks like this our image processing pipeline is a series of steps convolution max pooling and finally a fully connected network when solving problems in the real world these steps can be combined and stacked as many times as you want you can have two three or even ten convolution layers you can throw in max pooling wherever you want to reduce the size of your data the basic idea is to start with a large image and continually boil it down step by step until you finally have a single result the more convolution steps you have the more complicated features your network will be able to learn to recognize for example the first convolution step might learn to recognize sharp edges the second convolution step might recognize beaks using it s knowledge of sharp edges the third step might recognize entire birds using it s knowledge of beaks etc here s what a more realistic deep convolutional network like you would find in a research paper looks like in this case they start a x pixel image apply convolution and max pooling twice apply convolution more times apply max pooling and then have two fully connected layers the end result is that the image is classified into one of categories so how do you know which steps you need to combine to make your image classifier work honestly you have to answer this by doing a lot of experimentation and testing you might have to train networks before you find the optimal structure and parameters for the problem you are solving machine learning involves a lot of trial and error now finally we know enough to write a program that can decide if a picture is a bird or not as always we need some data to get started the free cifar data set contains pictures of birds and pictures of things that are not birds but to get even more data we ll also add in the caltech ucsd birds data set that has another bird pics here s a few of the birds from our combined data set and here s some of the non bird images this data set will work fine for our purposes but low res images is still pretty small for real world applications if you want google level performance you need millions of large images in machine learning having more data is almost always more important that having better algorithms now you know why google is so happy to offer you unlimited photo storage they want your sweet sweet data to build our classifier we ll use tflearn tflearn is a wrapper around google s tensorflow deep learning library that exposes a simplified api it makes building convolutional neural networks as easy as writing a few lines of code to define the layers of our network here s the code to define and train the network if you are training with a good video card with enough ram like an nvidia geforce gtx ti or better this will be done in less than an hour if you are training with a normal cpu it might take a lot longer as it trains the accuracy will increase after the first pass i got accuracy after just passes it was already up to after or so passes it capped out around accuracy and additional training didn t help so i stopped it there congrats our program can now recognize birds in images now that we have a trained neural network we can use it here s a simple script that takes in a single image file and predicts if it is a bird or not but to really see how effective our network is we need to test it with lots of images the data set i created held back images for validation when i ran those images through the network it predicted the correct answer of the time that seems pretty good right well it depends our network claims to be accurate but the devil is in the details that could mean all sorts of different things for example what if of our training images were birds and the other were not birds a program that guessed not a bird every single time would be accurate but it would also be useless we need to look more closely at the numbers than just the overall accuracy to judge how good a classification system really is we need to look closely at how it failed not just the percentage of the time that it failed instead of thinking about our predictions as right and wrong let s break them down into four separate categories using our validation set of images here s how many times our predictions fell into each category why do we break our results down like this because not all mistakes are created equal imagine if we were writing a program to detect cancer from an mri image if we were detecting cancer we d rather have false positives than false negatives false negatives would be the worse possible case that s when the program told someone they definitely didn t have cancer but they actually did instead of just looking at overall accuracy we calculate precision and recall metrics precision and recall metrics give us a clearer picture of how well we did this tells us that of the time we guessed bird we were right but it also tells us that we only found of the actual birds in the data set in other words we might not find every bird but we are pretty sure about it when we do find one now that you know the basics of deep convolutional networks you can try out some of the examples that come with tflearn to get your hands dirty with different neural network architectures it even comes with built in data sets so you don t even have to find your own images you also know enough now to start branching and learning about other areas of machine learning why not learn how to use algorithms to train computers how to play atari games next if you liked this article please consider signing up for my machine learning is fun email list i ll only email you when i have something new and awesome to share it s the best way to find out when i write more articles like this you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part part and part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Adam Geitgey,15200,13,https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78?source=tag_archive---------3----------------,Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in portugue s tie ng vie t or italiano have you noticed that facebook has developed an uncanny ability to recognize your friends in your photographs in the old days facebook used to make you to tag your friends in photos by clicking on them and typing in their name now as soon as you upload a photo facebook tags everyone for you like magic this technology is called face recognition facebook s algorithms are able to recognize your friends faces after they have been tagged only a few times it s pretty amazing technology facebook can recognize faces with accuracy which is pretty much as good as humans can do let s learn how modern face recognition works but just recognizing your friends would be too easy we can push this tech to the limit to solve a more challenging problem telling will ferrell famous actor apart from chad smith famous rock musician so far in part and we ve used machine learning to solve isolated problems that have only one step estimating the price of a house generating new data based on existing data and telling if an image contains a certain object all of those problems can be solved by choosing one machine learning algorithm feeding in data and getting the result but face recognition is really a series of several related problems as a human your brain is wired to do all of this automatically and instantly in fact humans are too good at recognizing faces and end up seeing faces in everyday objects computers are not capable of this kind of high level generalization at least not yet so we have to teach them how to do each step in this process separately we need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step in other words we will chain together several machine learning algorithms let s tackle this problem one step at a time for each step we ll learn about a different machine learning algorithm i m not going to explain every single algorithm completely to keep this from turning into a book but you ll learn the main ideas behind each one and you ll learn how you can build your own facial recognition system in python using openface and dlib the first step in our pipeline is face detection obviously we need to locate the faces in a photograph before we can try to tell them apart if you ve used any camera in the last years you ve probably seen face detection in action face detection is a great feature for cameras when the camera can automatically pick out faces it can make sure that all the faces are in focus before it takes the picture but we ll use it for a different purpose finding the areas of the image we want to pass on to the next step in our pipeline face detection went mainstream in the early s when paul viola and michael jones invented a way to detect faces that was fast enough to run on cheap cameras however much more reliable solutions exist now we re going to use a method invented in called histogram of oriented gradients or just hog for short to find faces in an image we ll start by making our image black and white because we don t need color data to find faces then we ll look at every single pixel in our image one at a time for every single pixel we want to look at the pixels that directly surrounding it our goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it then we want to draw an arrow showing in which direction the image is getting darker if you repeat that process for every single pixel in the image you end up with every pixel being replaced by an arrow these arrows are called gradients and they show the flow from light to dark across the entire image this might seem like a random thing to do but there s a really good reason for replacing the pixels with gradients if we analyze pixels directly really dark images and really light images of the same person will have totally different pixel values but by only considering the direction that brightness changes both really dark images and really bright images will end up with the same exact representation that makes the problem a lot easier to solve but saving the gradient for every single pixel gives us way too much detail we end up missing the forest for the trees it would be better if we could just see the basic flow of lightness darkness at a higher level so we could see the basic pattern of the image to do this we ll break up the image into small squares of x pixels each in each square we ll count up how many gradients point in each major direction how many point up point up right point right etc then we ll replace that square in the image with the arrow directions that were the strongest the end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way to find faces in this hog image all we have to do is find the part of our image that looks the most similar to a known hog pattern that was extracted from a bunch of other training faces using this technique we can now easily find faces in any image if you want to try this step out yourself using python and dlib here s code showing how to generate and view hog representations of images whew we isolated the faces in our image but now we have to deal with the problem that faces turned different directions look totally different to a computer to account for this we will try to warp each picture so that the eyes and lips are always in the sample place in the image this will make it a lot easier for us to compare faces in the next steps to do this we are going to use an algorithm called face landmark estimation there are lots of ways to do this but we are going to use the approach invented in by vahid kazemi and josephine sullivan the basic idea is we will come up with specific points called landmarks that exist on every face the top of the chin the outside edge of each eye the inner edge of each eyebrow etc then we will train a machine learning algorithm to be able to find these specific points on any face here s the result of locating the face landmarks on our test image now that we know were the eyes and mouth are we ll simply rotate scale and shear the image so that the eyes and mouth are centered as best as possible we won t do any fancy d warps because that would introduce distortions into the image we are only going to use basic image transformations like rotation and scale that preserve parallel lines called affine transformations now no matter how the face is turned we are able to center the eyes and mouth are in roughly the same position in the image this will make our next step a lot more accurate if you want to try this step out yourself using python and dlib here s the code for finding face landmarks and here s the code for transforming the image using those landmarks now we are to the meat of the problem actually telling faces apart this is where things get really interesting the simplest approach to face recognition is to directly compare the unknown face we found in step with all the pictures we have of people that have already been tagged when we find a previously tagged face that looks very similar to our unknown face it must be the same person seems like a pretty good idea right there s actually a huge problem with that approach a site like facebook with billions of users and a trillion photos can t possibly loop through every previous tagged face to compare it to every newly uploaded picture that would take way too long they need to be able to recognize faces in milliseconds not hours what we need is a way to extract a few basic measurements from each face then we could measure our unknown face the same way and find the known face with the closest measurements for example we might measure the size of each ear the spacing between the eyes the length of the nose etc if you ve ever watched a bad crime show like csi you know what i am talking about ok so which measurements should we collect from each face to build our known face database ear size nose length eye color something else it turns out that the measurements that seem obvious to us humans like eye color don t really make sense to a computer looking at individual pixels in an image researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself deep learning does a better job than humans at figuring out which parts of a face are important to measure the solution is to train a deep convolutional neural network just like we did in part but instead of training the network to recognize pictures objects like we did last time we are going to train it to generate measurements for each face the training process works by looking at face images at a time then the algorithm looks at the measurements it is currently generating for each of those three images it then tweaks the neural network slightly so that it makes sure the measurements it generates for and are slightly closer while making sure the measurements for and are slightly further apart after repeating this step millions of times for millions of images of thousands of different people the neural network learns to reliably generate measurements for each person any ten different pictures of the same person should give roughly the same measurements machine learning people call the measurements of each face an embedding the idea of reducing complicated raw data like a picture into a list of computer generated numbers comes up a lot in machine learning especially in language translation the exact approach for faces we are using was invented in by researchers at google but many similar approaches exist this process of training a convolutional neural network to output face embeddings requires a lot of data and computer power even with an expensive nvidia telsa video card it takes about hours of continuous training to get good accuracy but once the network has been trained it can generate measurements for any face even ones it has never seen before so this step only needs to be done once lucky for us the fine folks at openface already did this and they published several trained networks which we can directly use thanks brandon amos and team so all we need to do ourselves is run our face images through their pre trained network to get the measurements for each face here s the measurements for our test image so what parts of the face are these numbers measuring exactly it turns out that we have no idea it doesn t really matter to us all that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person if you want to try this step yourself openface provides a lua script that will generate embeddings all images in a folder and write them to a csv file you run it like this this last step is actually the easiest step in the whole process all we have to do is find the person in our database of known people who has the closest measurements to our test image you can do that by using any basic machine learning classification algorithm no fancy deep learning tricks are needed we ll use a simple linear svm classifier but lots of classification algorithms could work all we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match running this classifier takes milliseconds the result of the classifier is the name of the person so let s try out our system first i trained a classifier with the embeddings of about pictures each of will ferrell chad smith and jimmy falon then i ran the classifier on every frame of the famous youtube video of will ferrell and chad smith pretending to be each other on the jimmy fallon show it works and look how well it works for faces in different poses even sideways faces let s review the steps we followed now that you know how this all works here s instructions from start to finish of how run this entire face recognition pipeline on your own computer update you can still follow the steps below to use openface however i ve released a new python based face recognition library called face recognition that is much easier to install and use so i d recommend trying out face recognition first instead of continuing below i even put together a pre configured virtual machine with face recognition opencv tensorflow and lots of other deep learning tools pre installed you can download and run it on your computer very easily give the virtual machine a shot if you don t want to install all these libraries yourself original openface instructions if you liked this article please consider signing up for my machine learning is fun newsletter you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Xiaohan Zeng,48000,13,https://medium.com/@XiaohanZeng/i-interviewed-at-five-top-companies-in-silicon-valley-in-five-days-and-luckily-got-five-job-offers-25178cf74e0f?source=tag_archive---------4----------------,"I interviewed at five top companies in Silicon Valley in five days, and luckily got five job offers",in the five days from july th to th i interviewed at linkedin salesforce einstein google airbnb and facebook and got all five job offers it was a great experience and i feel fortunate that my efforts paid off so i decided to write something about it i will discuss how i prepared review the interview process and share my impressions about the five companies i had been at groupon for almost three years it s my first job and i have been working with an amazing team and on awesome projects we ve been building cool stuff making impact within the company publishing papers and all that but i felt my learning rate was being annealed read slowing down yet my mind was craving more also as a software engineer in chicago there are so many great companies that all attract me in the bay area life is short and professional life shorter still after talking with my wife and gaining her full support i decided to take actions and make my first ever career change although i m interested in machine learning positions the positions at the five companies are slightly different in the title and the interviewing process three are machine learning engineer linkedin google facebook one is data engineer salesforce and one is software engineer in general airbnb therefore i needed to prepare for three different areas coding machine learning and system design since i also have a full time job it took me months in total to prepare here is how i prepared for the three areas while i agree that coding interviews might not be the best way to assess all your skills as a developer there is arguably no better way to tell if you are a good engineer in a short period of time imo it is the necessary evil to get you that job i mainly used leetcode and geeksforgeeks for practicing but hackerrank and lintcode are also good places i spent several weeks going over common data structures and algorithms then focused on areas i wasn t too familiar with and finally did some frequently seen problems due to my time constraints i usually did two problems per day here are some thoughts this area is more closely related to the actual working experience many questions can be asked during system design interviews including but not limited to system architecture object oriented design database schema design distributed system design scalability etc there are many resources online that can help you with the preparation for the most part i read articles on system design interviews architectures of large scale systems and case studies here are some resources that i found really helpful although system design interviews can cover a lot of topics there are some general guidelines for how to approach the problem with all that said the best way to practice for system design interviews is to actually sit down and design a system i e your day to day work instead of doing the minimal work go deeper into the tools frameworks and libraries you use for example if you use hbase rather than simply using the client to run some ddl and do some fetches try to understand its overall architecture such as the read write flow how hbase ensures strong consistency what minor major compactions do and where lru cache and bloom filter are used in the system you can even compare hbase with cassandra and see the similarities and differences in their design then when you are asked to design a distributed key value store you won t feel ambushed many blogs are also a great source of knowledge such as hacker noon and engineering blogs of some companies as well as the official documentation of open source projects the most important thing is to keep your curiosity and modesty be a sponge that absorbs everything it is submerged into machine learning interviews can be divided into two aspects theory and product design unless you are have experience in machine learning research or did really well in your ml course it helps to read some textbooks classical ones such as the elements of statistical learning and pattern recognition and machine learning are great choices and if you are interested in specific areas you can read more on those make sure you understand basic concepts such as bias variance trade off overfitting gradient descent l l regularization bayes theorem bagging boosting collaborative filtering dimension reduction etc familiarize yourself with common formulas such as bayes theorem and the derivation of popular models such as logistic regression and svm try to implement simple models such as decision trees and k means clustering if you put some models on your resume make sure you understand it thoroughly and can comment on its pros and cons for ml product design understand the general process of building a ml product here s what i tried to do here i want to emphasize again on the importance of remaining curious and learning continuously try not to merely using the api for spark mllib or xgboost and calling it done but try to understand why stochastic gradient descent is appropriate for distributed training or understand how xgboost differs from traditional gbdt e g what is special about its loss function why it needs to compute the second order derivative etc i started by replying to hr s messages on linkedin and asking for referrals after a failed attempt at a rock star startup which i will touch upon later i prepared hard for several months and with help from my recruiters i scheduled a full week of onsites in the bay area i flew in on sunday had five full days of interviews with around interviewers at some best tech companies in the world and very luckily got job offers from all five of them all phone screenings are standard the only difference is in the duration for some companies like linkedin it s one hour while for facebook and airbnb it s minutes proficiency is the key here since you are under the time gun and usually you only get one chance you would have to very quickly recognize the type of problem and give a high level solution be sure to talk to the interviewer about your thinking and intentions it might slow you down a little at the beginning but communication is more important than anything and it only helps with the interview do not recite the solution as the interviewer would almost certainly see through it for machine learning positions some companies would ask ml questions if you are interviewing for those make sure you brush up your ml skills as well to make better use of my time i scheduled three phone screenings in the same afternoon one hour apart from each the upside is that you might benefit from the hot hand and the downside is that the later ones might be affected if the first one does not go well so i don t recommend it for everyone one good thing about interviewing with multiple companies at the same time is that it gives you certain advantages i was able to skip the second round phone screening with airbnb and salesforce because i got the onsite at linkedin and facebook after only one phone screening more surprisingly google even let me skip their phone screening entirely and schedule my onsite to fill the vacancy after learning i had four onsites coming in the next week i knew it was going to make it extremely tiring but hey nobody can refuse a google onsite invitation linkedin this is my first onsite and i interviewed at the sunnyvale location the office is very neat and people look very professional as always the sessions are one hour each coding questions are standard but the ml questions can get a bit tough that said i got an email from my hr containing the preparation material which was very helpful and in the end i did not see anything that was too surprising i heard the rumor that linkedin has the best meals in the silicon valley and from what i saw if it s not true it s not too far from the truth acquisition by microsoft seems to have lifted the financial burden from linkedin and freed them up to do really cool things new features such as videos and professional advertisements are exciting as a company focusing on professional development linkedin prioritizes the growth of its own employees a lot of teams such as ads relevance and feed ranking are expanding so act quickly if you want to join salesforce einstein rock star project by rock star team the team is pretty new and feels very much like a startup the product is built on the scala stack so type safety is a real thing there great talks on the optimus prime library by matthew tovbin at scala days chicago and leah mcguire at spark summit west i interviewed at their palo alto office the team has a cohesive culture and work life balance is great there everybody is passionate about what they are doing and really enjoys it with four sessions it is shorter compared to the other onsite interviews but i wish i could have stayed longer after the interview matthew even took me for a walk to the hp garage google absolutely the industry leader and nothing to say about it that people don t already know but it s huge like really really huge it took me minutes to ride a bicycle to meet my friends there also lines for food can be too long forever a great place for developers i interviewed at one of the many buildings on the mountain view campus and i don t know which one it is because it s huge my interviewers all look very smart and once they start talking they are even smarter it would be very enjoyable to work with these people one thing that i felt special about google s interviews is that the analysis of algorithm complexity is really important make sure you really understand what big o notation means airbnb fast expanding unicorn with a unique culture and arguably the most beautiful office in the silicon valley new products such as experiences and restaurant reservation high end niche market and expansion into china all contribute to a positive prospect perfect choice if you are risk tolerant and want a fast growing pre ipo experience airbnb s coding interview is a bit unique because you ll be coding in an ide instead of whiteboarding so your code needs to compile and give the right answer some problems can get really hard and they ve got the one of a kind cross functional interviews this is how airbnb takes culture seriously and being technically excellent doesn t guarantee a job offer for me the two cross functionals were really enjoyable i had casual conversations with the interviewers and we all felt happy at the end of the session overall i think airbnb s onsite is the hardest due to the difficulty of the problems longer duration and unique cross functional interviews if you are interested be sure to understand their culture and core values facebook another giant that is still growing fast and smaller and faster paced compared to google with its product lines dominating the social network market and big investments in ai and vr i can only see more growth potential for facebook in the future with stars like yann lecun and yangqing jia it s the perfect place if you are interested in machine learning i interviewed at building the one with the rooftop garden and ocean view and also where zuckerberg s office is located i m not sure if the interviewers got instructions but i didn t get clear signs whether my solutions were correct although i believed they were by noon the prior four days started to take its toll and i was having a headache i persisted through the afternoon sessions but felt i didn t do well at all i was a bit surprised to learn that i was getting an offer from them as well generally i felt people there believe the company s vision and are proud of what they are building being a company with half a trillion market cap and growing facebook is a perfect place to grow your career at this is a big topic that i won t cover in this post but i found this article to be very helpful some things that i do think are important all successes start with failures including interviews before i started interviewing for these companies i failed my interview at databricks in may back in april xiangrui contacted me via linkedin asking me if i was interested in a position on the spark mllib team i was extremely thrilled because i use spark and love scala databricks engineers are top notch and spark is revolutionizing the whole big data world it is an opportunity i couldn t miss so i started interviewing after a few days the bar is very high and the process is quite long including one pre screening questionnaire one phone screening one coding assignment and one full onsite i managed to get the onsite invitation and visited their office in downtown san francisco where treasure island can be seen my interviewer were incredibly intelligent yet equally modest during the interviews i often felt being pushed to the limits it was fine until one disastrous session where i totally messed up due to insufficient skills and preparation and it ended up a fiasco xiangrui was very kind and walked me to where i wanted to go after the interview was over and i really enjoyed talking to him i got the rejection several days later it was expected but i felt frustrated for a few days nonetheless although i missed the opportunity to work there i wholeheartedly wish they will continue to make greater impact and achievements from the first interview in may to finally accepting the job offer in late september my first career change was long and not easy it was difficult for me to prepare because i needed to keep doing well at my current job for several weeks i was on a regular schedule of preparing for the interview till am getting up at am the next day and fully devoting myself to another day at work interviewing at five companies in five days was also highly stressful and risky and i don t recommend doing it unless you have a very tight schedule but it does give you a good advantage during negotiation should you secure multiple offers i d like to thank all my recruiters who patiently walked me through the process the people who spend their precious time talking to me and all the companies that gave me the opportunities to interview and extended me offers lastly but most importantly i want to thank my family for their love and support my parents for watching me taking the first and every step my dear wife for everything she has done for me and my daughter for her warming smile thanks for reading through this long post you can find me on linkedin or twitter xiaohan zeng ps since the publication of this post it has unexpectedly received some attention i would like to thank everybody for the congratulations and shares and apologize for not being able to respond to each of them this post has been translated into some other languages it has been reposted in tech in asia breaking into startups invited me to a live video streaming together with sophia ciocca covershr did a short qna with me from a quick cheer to a standing ovation clap to show how much you enjoyed this story critical mind romantic heart
Gil Fewster,3300,5,https://medium.freecodecamp.org/the-mind-blowing-ai-announcement-from-google-that-you-probably-missed-2ffd31334805?source=tag_archive---------5----------------,The mind-blowing AI announcement from Google that you probably missed.,disclaimer i m not an expert in neural networks or machine learning since originally writing this article many people with far more expertise in these fields than myself have indicated that while impressive what google have achieved is evolutionary not revolutionary in the very least it s fair to say that i m guilty of anthropomorphising in parts of the text i ve left the article s content unchanged because i think it s interesting to compare the gut reaction i had with the subsequent comments of experts in the field i strongly encourage readers to browse the comments after reading the article for some perspectives more sober and informed than my own in the closing weeks of google published an article that quietly sailed under most people s radars which is a shame because it may just be the most astonishing article about machine learning that i read last year don t feel bad if you missed it not only was the article competing with the pre christmas rush that most of us were navigating it was also tucked away on google s research blog beneath the geektastic headline zero shot translation with google s multilingual neural machine translation system this doesn t exactly scream must read does it especially when you ve got projects to wind up gifts to buy and family feuds to be resolved all while the advent calendar relentlessly counts down the days until christmas like some kind of chocolate filled yuletide doomsday clock luckily i m here to bring you up to speed here s the deal up until september of last year google translate used phrase based translation it basically did the same thing you and i do when we look up key words and phrases in our lonely planet language guides it s effective enough and blisteringly fast compared to awkwardly thumbing your way through a bunch of pages looking for the french equivalent of please bring me all of your cheese and don t stop until i fall over but it lacks nuance phrase based translation is a blunt instrument it does the job well enough to get by but mapping roughly equivalent words and phrases without an understanding of linguistic structures can only produce crude results this approach is also limited by the extent of an available vocabulary phrase based translation has no capacity to make educated guesses at words it doesn t recognize and can t learn from new input all that changed in september when google gave their translation tool a new engine the google neural machine translation system gnmt this new engine comes fully loaded with all the hot buzzwords like neural network and machine learning the short version is that google translate got smart it developed the ability to learn from the people who used it it learned how to make educated guesses about the content tone and meaning of phrases based on the context of other words and phrases around them and here s the bit that should make your brain explode it got creative google translate invented its own language to help it translate more effectively what s more nobody told it to it didn t develop a language or interlingua as google call it because it was coded to it developed a new language because the software determined over time that this was the most efficient way to solve the problem of translation stop and think about that for a moment let it sink in a neural computing system designed to translate content from one human language into another developed its own internal language to make the task more efficient without being told to do so in a matter of weeks i ve added a correction retraction of this paragraph in the notes to understand what s going on we need to understand what zero shot translation capability is here s google s mike schuster nikhil thorat and melvin johnson from the original blog post here you can see an advantage of google s new neural machine over the old phrase based approach the gmnt is able to learn how to translate between two languages without being explicitly taught this wouldn t be possible in a phrase based model where translation is dependent upon an explicit dictionary to map words and phrases between each pair of languages being translated and this leads the google engineers onto that truly astonishing discovery of creation so there you have it in the last weeks of as journos around the world started penning their was this the worst year in living memory thinkpieces google engineers were quietly documenting a genuinely astonishing breakthrough in software engineering and linguistics i just thought maybe you d want to know ok to really understand what s going on we probably need multiple computer science and linguistics degrees i m just barely scraping the surface here if you ve got time to get a few degrees or if you ve already got them please drop me a line and explain it all me to slowly update in my excitement it s fair to say that i ve exaggerated the idea of this as an intelligent system at least so far as we would think about human intelligence and decision making make sure you read chris mcdonald s comment after the article for a more sober perspective update nafrondel s excellent detailed reply is also a must read for an expert explanation of how neural networks function from a quick cheer to a standing ovation clap to show how much you enjoyed this story a tinkerer our community publishes stories worth reading on development design and data science
Adam Geitgey,10400,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3?source=tag_archive---------6----------------,Machine Learning is Fun! Part 2 – Adam Geitgey – Medium,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in italiano espan ol franc ais tu rkc e portugue s tie ng vie t or in part we said that machine learning is using generic algorithms to tell you something interesting about your data without writing any code specific to the problem you are solving if you haven t already read part read it now this time we are going to see one of these generic algorithms do something really cool create video game levels that look like they were made by humans we ll build a neural network feed it existing super mario levels and watch new ones pop out just like part this guide is for anyone who is curious about machine learning but has no idea where to start the goal is be accessible to anyone which means that there s a lot of generalizations and we skip lots of details but who cares if this gets anyone more interested in ml then mission accomplished back in part we created a simple algorithm that estimated the value of a house based on its attributes given data about a house like this we ended up with this simple estimation function in other words we estimated the value of the house by multiplying each of its attributes by a weight then we just added those numbers up to get the house s value instead of using code let s represent that same function as a simple diagram however this algorithm only works for simple problems where the result has a linear relationship with the input what if the truth behind house prices isn t so simple for example maybe the neighborhood matters a lot for big houses and small houses but doesn t matter at all for medium sized houses how could we capture that kind of complicated detail in our model to be more clever we could run this algorithm multiple times with different of weights that each capture different edge cases now we have four different price estimates let s combine those four price estimates into one final estimate we ll run them through the same algorithm again but using another set of weights our new super answer combines the estimates from our four different attempts to solve the problem because of this it can model more cases than we could capture in one simple model let s combine our four attempts to guess into one big diagram this is a neural network each node knows how to take in a set of inputs apply weights to them and calculate an output value by chaining together lots of these nodes we can model complex functions there s a lot that i m skipping over to keep this brief including feature scaling and the activation function but the most important part is that these basic ideas click it s just like lego we can t model much with one single lego block but we can model anything if we have enough basic lego blocks to stick together the neural network we ve seen always returns the same answer when you give it the same inputs it has no memory in programming terms it s a stateless algorithm in many cases like estimating the price of house that s exactly what you want but the one thing this kind of model can t do is respond to patterns in data over time imagine i handed you a keyboard and asked you to write a story but before you start my job is to guess the very first letter that you will type what letter should i guess i can use my knowledge of english to increase my odds of guessing the right letter for example you will probably type a letter that is common at the beginning of words if i looked at stories you wrote in the past i could narrow it down further based on the words you usually use at the beginning of your stories once i had all that data i could use it to build a neural network to model how likely it is that you would start with any given letter our model might look like this but let s make the problem harder let s say i need to guess the next letter you are going to type at any point in your story this is a much more interesting problem let s use the first few words of ernest hemingway s the sun also rises as an example what letter is going to come next you probably guessed n the word is probably going to be boxing we know this based on the letters we ve already seen in the sentence and our knowledge of common words in english also the word middleweight gives us an extra clue that we are talking about boxing in other words it s easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of english to solve this problem with a neural network we need to add state to our model each time we ask our neural network for an answer we also save a set of our intermediate calculations and re use them the next time as part of our input that way our model will adjust its predictions based on the input that it has seen recently keeping track of state in our model makes it possible to not just predict the most likely first letter in the story but to predict the most likely next letter given all previous letters this is the basic idea of a recurrent neural network we are updating the network each time we use it this allows it to update its predictions based on what it saw most recently it can even model patterns over time as long as we give it enough of a memory predicting the next letter in a story might seem pretty useless what s the point one cool use might be auto predict for a mobile phone keyboard but what if we took this idea to the extreme what if we asked the model to predict the next most likely character over and over forever we d be asking it to write a complete story for us we saw how we could guess the next letter in hemingway s sentence let s try generating a whole story in the style of hemingway to do this we are going to use the recurrent neural network implementation that andrej karpathy wrote andrej is a deep learning researcher at stanford and he wrote an excellent introduction to generating text with rnns you can view all the code for the model on github we ll create our model from the complete text of the sun also rises characters using unique letters including punctuation uppercase lowercase etc this data set is actually really small compared to typical real world applications to generate a really good model of hemingway s style it would be much better to have at several times as much sample text but this is good enough to play around with as an example as we just start to train the rnn it s not very good at predicting letters here s what it generates after a loops of training you can see that it has figured out that sometimes words have spaces between them but that s about it after about iterations things are looking more promising the model has started to identify the patterns in basic sentence structure it s adding periods at the ends of sentences and even quoting dialog a few words are recognizable but there s also still a lot of nonsense but after several thousand more training iterations it looks pretty good at this point the algorithm has captured the basic pattern of hemingway s short direct dialog a few sentences even sort of make sense compare that with some real text from the book even by only looking for patterns one character at a time our algorithm has reproduced plausible looking prose with proper formatting that is kind of amazing we don t have to generate text completely from scratch either we can seed the algorithm by supplying the first few letters and just let it find the next few letters for fun let s make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of er he and the s not bad but the really mind blowing part is that this algorithm can figure out patterns in any sequence of data it can easily generate real looking recipes or fake obama speeches but why limit ourselves human language we can apply this same idea to any kind of sequential data that has a pattern in nintendo released super mario makertm for the wii u gaming system this game lets you draw out your own super mario brothers levels on the gamepad and then upload them to the internet so you friends can play through them you can include all the classic power ups and enemies from the original mario games in your levels it s like a virtual lego set for people who grew up playing super mario brothers can we use the same model that generated fake hemingway text to generate fake super mario brothers levels first we need a data set for training our model let s take all the outdoor levels from the original super mario brothers game released in this game has levels and about of them have the same outdoor style so we ll stick to those to get the designs for each level i took an original copy of the game and wrote a program to pull the level designs out of the game s memory super mario bros is a year old game and there are lots of resources online that help you figure out how the levels were stored in the game s memory extracting level data from an old video game is a fun programming exercise that you should try sometime here s the first level from the game which you probably remember if you ever played it if we look closely we can see the level is made of a simple grid of objects we could just as easily represent this grid as a sequence of characters with one character representing each object we ve replaced each object in the level with a letter and so on using a different letter for each different kind of object in the level i ended up with text files that looked like this looking at the text file you can see that mario levels don t really have much of a pattern if you read them line by line the patterns in a level really emerge when you think of the level as a series of columns so in order for the algorithm to find the patterns in our data we need to feed the data in column by column figuring out the most effective representation of your input data called feature selection is one of the keys of using machine learning algorithms well to train the model i needed to rotate my text files by degrees this made sure the characters were fed into the model in an order where a pattern would more easily show up just like we saw when creating the model of hemingway s prose a model improves as we train it after a little training our model is generating junk it sort of has an idea that s and s should show up a lot but that s about it it hasn t figured out the pattern yet after several thousand iterations it s starting to look like something the model has almost figured out that each line should be the same length it has even started to figure out some of the logic of mario the pipes in mario are always two blocks wide and at least two blocks high so the p s in the data should appear in x clusters that s pretty cool with a lot more training the model gets to the point where it generates perfectly valid data let s sample an entire level s worth of data from our model and rotate it back horizontal this data looks great there are several awesome things to notice finally let s take this level and recreate it in super mario maker play it yourself if you have super mario maker you can play this level by bookmarking it online or by looking it up using level code ac f c the recurrent neural network algorithm we used to train our model is the same kind of algorithm used by real world companies to solve hard problems like speech detection and language translation what makes our model a toy instead of cutting edge is that our model is generated from very little data there just aren t enough levels in the original super mario brothers game to provide enough data for a really good model if we could get access to the hundreds of thousands of user created super mario maker levels that nintendo has we could make an amazing model but we can t because nintendo won t let us have them big companies don t give away their data for free as machine learning becomes more important in more industries the difference between a good program and a bad program will be how much data you have to train your models that s why companies like google and facebook need your data so badly for example google recently open sourced tensorflow its software toolkit for building large scale machine learning applications it was a pretty big deal that google gave away such important capable technology for free this is the same stuff that powers google translate but without google s massive trove of data in every language you can t create a competitor to google translate data is what gives google its edge think about that the next time you open up your google maps location history or facebook location history and notice that it stores every place you ve ever been in machine learning there s never a single way to solve a problem you have limitless options when deciding how to pre process your data and which algorithms to use often combining multiple approaches will give you better results than any single approach readers have sent me links to other interesting approaches to generating super mario levels if you liked this article please consider signing up for my machine learning is fun email list i ll only email you when i have something new and awesome to share it s the best way to find out when i write more articles like this you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
David Venturi,10600,20,https://medium.freecodecamp.org/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0?source=tag_archive---------7----------------,"Every single Machine Learning course on the internet, ranked by your reviews",a year and a half ago i dropped out of one of the best computer science programs in canada i started creating my own data science master s program using online resources i realized that i could learn everything i needed through edx coursera and udacity instead and i could learn it faster more efficiently and for a fraction of the cost i m almost finished now i ve taken many data science related courses and audited portions of many more i know the options out there and what skills are needed for learners preparing for a data analyst or data scientist role so i started creating a review driven guide that recommends the best courses for each subject within data science for the first guide in the series i recommended a few coding classes for the beginner data scientist then it was statistics and probability classes then introductions to data science also data visualization for this guide i spent a dozen hours trying to identify every online machine learning course offered as of may extracting key bits of information from their syllabi and reviews and compiling their ratings my end goal was to identify the three best courses available and present them to you below for this task i turned to none other than the open source class central community and its database of thousands of course ratings and reviews since class central founder dhawal shah has kept a closer eye on online courses than arguably anyone else in the world dhawal personally helped me assemble this list of resources each course must fit three criteria we believe we covered every notable course that fits the above criteria since there are seemingly hundreds of courses on udemy we chose to consider the most reviewed and highest rated ones only there s always a chance that we missed something though so please let us know in the comments section if we left a good course out we compiled average ratings and number of reviews from class central and other review sites to calculate a weighted average rating for each course we read text reviews and used this feedback to supplement the numerical ratings we made subjective syllabus judgment calls based on three factors a popular definition originates from arthur samuel in machine learning is a subfield of computer science that gives computers the ability to learn without being explicitly programmed in practice this means developing computer programs that can make predictions based on data just as humans can learn from experience so can computers where data experience a machine learning workflow is the process required for carrying out a machine learning project though individual projects can differ most workflows share several common tasks problem evaluation data exploration data preprocessing model training testing deployment etc below you ll find helpful visualization of these core steps the ideal course introduces the entire process and provides interactive examples assignments and or quizzes where students can perform each task themselves first off let s define deep learning here is a succinct description as would be expected portions of some of the machine learning courses contain deep learning content i chose not to include deep learning only courses however if you are interested in deep learning specifically we ve got you covered with the following article my top three recommendations from that list would be several courses listed below ask students to have prior programming calculus linear algebra and statistics experience these prerequisites are understandable given that machine learning is an advanced discipline missing a few subjects good news some of this experience can be acquired through our recommendations in the first two articles programming statistics of this data science career guide several top ranked courses below also provide gentle calculus and linear algebra refreshers and highlight the aspects most relevant to machine learning for those less familiar stanford university s machine learning on coursera is the clear current winner in terms of ratings reviews and syllabus fit taught by the famous andrew ng google brain founder and former chief scientist at baidu this was the class that sparked the founding of coursera it has a star weighted average rating over reviews released in it covers all aspects of the machine learning workflow though it has a smaller scope than the original stanford class upon which it is based it still manages to cover a large number of techniques and algorithms the estimated timeline is eleven weeks with two weeks dedicated to neural networks and deep learning free and paid options are available ng is a dynamic yet gentle instructor with a palpable experience he inspires confidence especially when sharing practical implementation tips and warnings about common pitfalls a linear algebra refresher is provided and ng highlights the aspects of calculus most relevant to machine learning evaluation is automatic and is done via multiple choice quizzes that follow each lesson and programming assignments the assignments there are eight of them can be completed in matlab or octave which is an open source version of matlab ng explains his language choice though python and r are likely more compelling choices in with the increased popularity of those languages reviewers note that that shouldn t stop you from taking the course a few prominent reviewers noted the following columbia university s machine learning is a relatively new offering that is part of their artificial intelligence micromasters on edx though it is newer and doesn t have a large number of reviews the ones that it does have are exceptionally strong professor john paisley is noted as brilliant clear and clever it has a star weighted average rating over reviews the course also covers all aspects of the machine learning workflow and more algorithms than the above stanford offering columbia s is a more advanced introduction with reviewers noting that students should be comfortable with the recommended prerequisites calculus linear algebra statistics probability and coding quizzes programming assignments and a final exam are the modes of evaluation students can use either python octave or matlab to complete the assignments the course s total estimated timeline is eight to ten hours per week over twelve weeks it is free with a verified certificate available for purchase below are a few of the aforementioned sparkling reviews machine learning a ztm on udemy is an impressively detailed offering that provides instruction in both python and r which is rare and can t be said for any of the other top courses it has a star weighted average rating over reviews which makes it the most reviewed course of the ones considered it covers the entire machine learning workflow and an almost ridiculous in a good way number of algorithms through hours of on demand video the course takes a more applied approach and is lighter math wise than the above two courses each section starts with an intuition video from eremenko that summarizes the underlying theory of the concept being taught de ponteves then walks through implementation with separate videos for both python and r as a bonus the course includes python and r code templates for students to download and use on their own projects there are quizzes and homework challenges though these aren t the strong points of the course eremenko and the superdatascience team are revered for their ability to make the complex simple also the prerequisites listed are just some high school mathematics so this course might be a better option for those daunted by the stanford and columbia offerings a few prominent reviewers noted the following our pick had a weighted average rating of out of stars over reviews let s look at the other alternatives sorted by descending rating a reminder that deep learning only courses are not included in this guide you can find those here the analytics edge massachusetts institute of technology edx more focused on analytics in general though it does cover several machine learning topics uses r strong narrative that leverages familiar real world examples challenging ten to fifteen hours per week over twelve weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews python for data science and machine learning bootcamp jose portilla udemy has large chunks of machine learning content but covers the whole data science process more of a very detailed intro to python amazing course though not ideal for the scope of this guide hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews data science and machine learning bootcamp with r jose portilla udemy the comments for portilla s above course apply here as well except for r hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews machine learning series lazy programmer inc udemy taught by a data scientist big data engineer full stack software engineer with an impressive resume lazy programmer currently has a series of machine learning focused courses on udemy in total the courses have ratings and almost all of them have stars a useful course ordering is provided in each individual course s description uses python cost varies depending on udemy discounts which are frequent machine learning georgia tech udacity a compilation of what was three separate courses supervised unsupervised and reinforcement learning part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree oms bite sized videos as is udacity s style friendly professors estimated timeline of four months free it has a star weighted average rating over reviews implementing predictive analytics with spark in azure hdinsight microsoft edx introduces the core concepts of machine learning and a variety of algorithms leverages several big data friendly tools including apache spark scala and hadoop uses both python and r four hours per week over six weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews data science and machine learning with python hands on frank kane udemy uses python kane has nine years of experience at amazon and imdb nine hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews scala and spark for big data and machine learning jose portilla udemy big data focus specifically on implementation in scala and spark ten hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews machine learning engineer nanodegree udacity udacity s flagship machine learning program which features a best in class project review system and career support the program is a compilation of several individual udacity courses which are free co created by kaggle estimated timeline of six months currently costs usd per month with a tuition refund available for those who graduate within months it has a star weighted average rating over reviews learning from data introductory machine learning california institute of technology edx enrollment is currently closed on edx but is also available via caltech s independent platform see below it has a star weighted average rating over reviews learning from data introductory machine learning yaser abu mostafa california institute of technology a real caltech course not a watered down version reviews note it is excellent for understanding machine learning theory the professor yaser abu mostafa is popular among students and also wrote the textbook upon which this course is based videos are taped lectures with lectures slides picture in picture uploaded to youtube homework assignments are pdf files the course experience for online students isn t as polished as the top three recommendations it has a star weighted average rating over reviews mining massive datasets stanford university machine learning with a focus on big data introduces modern distributed file systems and mapreduce ten hours per week over seven weeks free it has a star weighted average rating over reviews aws machine learning a complete guide with python chandra lingam udemy a unique focus on cloud based machine learning and specifically amazon web services uses python nine hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews introduction to machine learning face detection in python holczer balazs udemy uses python eight hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews statlearning statistical learning stanford university based on the excellent textbook an introduction to statistical learning with applications in r and taught by the professors who wrote it reviewers note that the mooc isn t as good as the book citing thin exercises and mediocre videos five hours per week over nine weeks free it has a star weighted average rating over reviews machine learning specialization university of washington coursera great courses but last two classes including the capstone project were canceled reviewers note that this series is more digestable read easier for those without strong technical backgrounds than other top machine learning courses e g stanford s or caltech s be aware that the series is incomplete with recommender systems deep learning and a summary missing free and paid options available it has a star weighted average rating over reviews from to machine learning nlp python cut to the chase loony corn udemy a down to earth shy but confident take on machine learning techniques taught by four person team with decades of industry experience together uses python cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews principles of machine learning microsoft edx uses r python and microsoft azure machine learning part of the microsoft professional program certificate in data science three to four hours per week over six weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews big data statistical inference and machine learning queensland university of technology futurelearn a nice brief exploratory machine learning course with a focus on big data covers a few tools like r h o flow and weka only three weeks in duration at a recommended two hours per week but one reviewer noted that six hours per week would be more appropriate free and paid options available it has a star weighted average rating over reviews genomic data science and clustering bioinformatics v university of california san diego coursera for those interested in the intersection of computer science and biology and how it represents an important frontier in modern science focuses on clustering and dimensionality reduction part of ucsd s bioinformatics specialization free and paid options available it has a star weighted average rating over reviews intro to machine learning udacity prioritizes topic breadth and practical tools in python over depth and theory the instructors sebastian thrun and katie malone make this class so fun consists of bite sized videos and quizzes followed by a mini project for each lesson currently part of udacity s data analyst nanodegree estimated timeline of ten weeks free it has a star weighted average rating over reviews machine learning for data analysis wesleyan university coursera a brief intro machine learning and a few select algorithms covers decision trees random forests lasso regression and k means clustering part of wesleyan s data analysis and interpretation specialization estimated timeline of four weeks free and paid options available it has a star weighted average rating over reviews programming with python for data science microsoft edx produced by microsoft in partnership with coding dojo uses python eight hours per week over six weeks free and paid options available it has a star weighted average rating over reviews machine learning for trading georgia tech udacity focuses on applying probabilistic machine learning approaches to trading decisions uses python part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree oms estimated timeline of four months free it has a star weighted average rating over reviews practical machine learning johns hopkins university coursera a brief practical introduction to a number of machine learning algorithms several one two star reviews expressing a variety of concerns part of jhu s data science specialization four to nine hours per week over four weeks free and paid options available it has a star weighted average rating over reviews machine learning for data science and analytics columbia university edx introduces a wide range of machine learning topics some passionate negative reviews with concerns including content choices a lack of programming assignments and uninspiring presentation seven to ten hours per week over five weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews recommender systems specialization university of minnesota coursera strong focus one specific type of machine learning recommender systems a four course specialization plus a capstone project which is a case study taught using lenskit an open source toolkit for recommender systems free and paid options available it has a star weighted average rating over reviews machine learning with big data university of california san diego coursera terrible reviews that highlight poor instruction and evaluation some noted it took them mere hours to complete the whole course part of ucsd s big data specialization free and paid options available it has a star weighted average rating over reviews practical predictive analytics models and methods university of washington coursera a brief intro to core machine learning concepts one reviewer noted that there was a lack of quizzes and that the assignments were not challenging part of uw s data science at scale specialization six to eight hours per week over four weeks free and paid options available it has a star weighted average rating over reviews the following courses had one or no reviews as of may machine learning for musicians and artists goldsmiths university of london kadenze unique students learn algorithms software tools and machine learning best practices to make sense of human gesture musical audio and other real time data seven sessions in length audit free and premium usd per month options available it has one star review applied machine learning in python university of michigan coursera taught using python and the scikit learn toolkit part of the applied data science with python specialization scheduled to start may th free and paid options available applied machine learning microsoft edx taught using various tools including python r and microsoft azure machine learning note microsoft produces the course includes hands on labs to reinforce the lecture content three to four hours per week over six weeks free with a verified certificate available for purchase machine learning with python big data university taught using python targeted towards beginners estimated completion time of four hours big data university is affiliated with ibm free machine learning with apache systemml big data university taught using apache systemml which is a declarative style language designed for large scale machine learning estimated completion time of eight hours big data university is affiliated with ibm free machine learning for data science university of california san diego edx doesn t launch until january programming examples and assignments are in python using jupyter notebooks eight hours per week over ten weeks free with a verified certificate available for purchase introduction to analytics modeling georgia tech edx the course advertises r as its primary programming tool five to ten hours per week over ten weeks free with a verified certificate available for purchase predictive analytics gaining insights from big data queensland university of technology futurelearn brief overview of a few algorithms uses hewlett packard enterprise s vertica analytics platform as an applied tool start date to be announced two hours per week over four weeks free with a certificate of achievement available for purchase introduccio n al machine learning universitas telefo nica miri ada x taught in spanish an introduction to machine learning that covers supervised and unsupervised learning a total of twenty estimated hours over four weeks machine learning path step dataquest taught in python using dataquest s interactive in browser platform multiple guided projects and a plus project where you build your own machine learning system using your own data subscription required the following six courses are offered by datacamp datacamp s hybrid teaching style leverages video and text based instruction with lots of examples through an in browser code editor a subscription is required for full access to each course introduction to machine learning datacamp covers classification regression and clustering algorithms uses r fifteen videos and exercises with an estimated timeline of six hours supervised learning with scikit learn datacamp uses python and scikit learn covers classification and regression algorithms seventeen videos and exercises with an estimated timeline of four hours unsupervised learning in r datacamp provides a basic introduction to clustering and dimensionality reduction in r sixteen videos and exercises with an estimated timeline of four hours machine learning toolbox datacamp teaches the big ideas in machine learning uses r videos and exercises with an estimated timeline of four hours machine learning with the experts school budgets datacamp a case study from a machine learning competition on drivendata involves building a model to automatically classify items in a school s budget datacamp s supervised learning with scikit learn is a prerequisite fifteen videos and exercises with an estimated timeline of four hours unsupervised learning in python datacamp covers a variety of unsupervised learning algorithms using python scikit learn and scipy the course ends with students building a recommender system to recommend popular musical artists thirteen videos and exercises with an estimated timeline of four hours machine learning tom mitchell carnegie mellon university carnegie mellon s graduate introductory machine learning course a prerequisite to their second graduate level course statistical machine learning taped university lectures with practice problems homework assignments and a midterm all with solutions posted online a version of the course also exists cmu is one of the best graduate schools for studying machine learning and has a whole department dedicated to ml free statistical machine learning larry wasserman carnegie mellon university likely the most advanced course in this guide a follow up to carnegie mellon s machine learning course taped university lectures with practice problems homework assignments and a midterm all with solutions posted online free undergraduate machine learning nando de freitas university of british columbia an undergraduate machine learning course lectures are filmed and put on youtube with the slides posted on the course website the course assignments are posted as well no solutions though de freitas is now a full time professor at the university of oxford and receives praise for his teaching abilities in various forums graduate version available see below machine learning nando de freitas university of british columbia a graduate machine learning course the comments in de freitas undergraduate course above apply here as well this is the fifth of a six piece series that covers the best online courses for launching yourself into the data science field we covered programming in the first article statistics and probability in the second article intros to data science in the third article and data visualization in the fourth the final piece will be a summary of those articles plus the best online courses for other key topics such as data wrangling databases and even software engineering if you re looking for a complete list of data science online courses you can find them on class central s data science and big data subject page if you enjoyed reading this check out some of class central s other pieces if you have suggestions for courses i missed let me know in the responses if you found this helpful click the so more people will see it here on medium this is a condensed version of my original article published on class central where i ve included detailed course syllabi from a quick cheer to a standing ovation clap to show how much you enjoyed this story curriculum lead projects datacamp i created my own data science master s program our community publishes stories worth reading on development design and data science
Michael Jordan,34000,16,https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7?source=tag_archive---------8----------------,Artificial Intelligence — The Revolution Hasn’t Happened Yet,artificial intelligence ai is the mantra of the current era the phrase is intoned by technologists academicians journalists and venture capitalists alike as with many phrases that cross over from technical academic fields into general circulation there is significant misunderstanding accompanying the use of the phrase but this is not the classical case of the public not understanding the scientists here the scientists are often as befuddled as the public the idea that our era is somehow seeing the emergence of an intelligence in silicon that rivals our own entertains all of us enthralling us and frightening us in equal measure and unfortunately it distracts us there is a different narrative that one can tell about the current era consider the following story which involves humans computers data and life or death decisions but where the focus is something other than intelligence in silicon fantasies when my spouse was pregnant years ago we had an ultrasound there was a geneticist in the room and she pointed out some white spots around the heart of the fetus those are markers for down syndrome she noted and your risk has now gone up to in she further let us know that we could learn whether the fetus in fact had the genetic modification underlying down syndrome via an amniocentesis but amniocentesis was risky the risk of killing the fetus during the procedure was roughly in being a statistician i determined to find out where these numbers were coming from to cut a long story short i discovered that a statistical analysis had been done a decade previously in the uk where these white spots which reflect calcium buildup were indeed established as a predictor of down syndrome but i also noticed that the imaging machine used in our test had a few hundred more pixels per square inch than the machine used in the uk study i went back to tell the geneticist that i believed that the white spots were likely false positives that they were literally white noise she said ah that explains why we started seeing an uptick in down syndrome diagnoses a few years ago it s when the new machine arrived we didn t do the amniocentesis and a healthy girl was born a few months later but the episode troubled me particularly after a back of the envelope calculation convinced me that many thousands of people had gotten that diagnosis that same day worldwide that many of them had opted for amniocentesis and that a number of babies had died needlessly and this happened day after day until it somehow got fixed the problem that this episode revealed wasn t about my individual medical care it was about a medical system that measured variables and outcomes in various places and times conducted statistical analyses and made use of the results in other places and times the problem had to do not just with data analysis per se but with what database researchers call provenance broadly where did data arise what inferences were drawn from the data and how relevant are those inferences to the present situation while a trained human might be able to work all of this out on a case by case basis the issue was that of designing a planetary scale medical system that could do this without the need for such detailed human oversight i m also a computer scientist and it occurred to me that the principles needed to build planetary scale inference and decision making systems of this kind blending computer science with statistics and taking into account human utilities were nowhere to be found in my education and it occurred to me that the development of such principles which will be needed not only in the medical domain but also in domains such as commerce transportation and education were at least as important as those of building ai systems that can dazzle us with their game playing or sensorimotor skills whether or not we come to understand intelligence any time soon we do have a major challenge on our hands in bringing together computers and humans in ways that enhance human life while this challenge is viewed by some as subservient to the creation of artificial intelligence it can also be viewed more prosaically but with no less reverence as the creation of a new branch of engineering much like civil engineering and chemical engineering in decades past this new discipline aims to corral the power of a few key ideas bringing new resources and capabilities to people and doing so safely whereas civil engineering and chemical engineering were built on physics and chemistry this new engineering discipline will be built on ideas that the preceding century gave substance to ideas such as information algorithm data uncertainty computing inference and optimization moreover since much of the focus of the new discipline will be on data from and about humans its development will require perspectives from the social sciences and humanities while the building blocks have begun to emerge the principles for putting these blocks together have not yet emerged and so the blocks are currently being put together in ad hoc ways thus just as humans built buildings and bridges before there was civil engineering humans are proceeding with the building of societal scale inference and decision making systems that involve machines humans and the environment just as early buildings and bridges sometimes fell to the ground in unforeseen ways and with tragic consequences many of our early societal scale inference and decision making systems are already exposing serious conceptual flaws and unfortunately we are not very good at anticipating what the next emerging serious flaw will be what we re missing is an engineering discipline with its principles of analysis and design the current public dialog about these issues too often uses ai as an intellectual wildcard one that makes it difficult to reason about the scope and consequences of emerging technology let us begin by considering more carefully what ai has been used to refer to both recently and historically most of what is being called ai today particularly in the public sphere is what has been called machine learning ml for the past several decades ml is an algorithmic field that blends ideas from statistics computer science and many other disciplines see below to design algorithms that process data make predictions and help make decisions in terms of impact on the real world ml is the real thing and not just recently indeed that ml would grow into massive industrial relevance was already clear in the early s and by the turn of the century forward looking companies such as amazon were already using ml throughout their business solving mission critical back end problems in fraud detection and supply chain prediction and building innovative consumer facing services such as recommendation systems as datasets and computing resources grew rapidly over the ensuing two decades it became clear that ml would soon power not only amazon but essentially any company in which decisions could be tied to large scale data new business models would emerge the phrase data science began to be used to refer to this phenomenon reflecting the need of ml algorithms experts to partner with database and distributed systems experts to build scalable robust ml systems and reflecting the larger social and environmental scope of the resulting systems this confluence of ideas and technology trends has been rebranded as ai over the past few years this rebranding is worthy of some scrutiny historically the phrase ai was coined in the late s to refer to the heady aspiration of realizing in software and hardware an entity possessing human level intelligence we will use the phrase human imitative ai to refer to this aspiration emphasizing the notion that the artificially intelligent entity should seem to be one of us if not physically at least mentally whatever that might mean this was largely an academic enterprise while related academic fields such as operations research statistics pattern recognition information theory and control theory already existed and were often inspired by human intelligence and animal intelligence these fields were arguably focused on low level signals and decisions the ability of say a squirrel to perceive the three dimensional structure of the forest it lives in and to leap among its branches was inspirational to these fields ai was meant to focus on something different the high level or cognitive capability of humans to reason and to think sixty years later however high level reasoning and thought remain elusive the developments which are now being called ai arose mostly in the engineering fields associated with low level pattern recognition and movement control and in the field of statistics the discipline focused on finding patterns in data and on making well founded predictions tests of hypotheses and decisions indeed the famous backpropagation algorithm that was rediscovered by david rumelhart in the early s and which is now viewed as being at the core of the so called ai revolution first arose in the field of control theory in the s and s one of its early applications was to optimize the thrusts of the apollo spaceships as they headed towards the moon since the s much progress has been made but it has arguably not come about from the pursuit of human imitative ai rather as in the case of the apollo spaceships these ideas have often been hidden behind the scenes and have been the handiwork of researchers focused on specific engineering challenges although not visible to the general public research and systems building in areas such as document retrieval text classification fraud detection recommendation systems personalized search social network analysis planning diagnostics and a b testing have been a major success these are the advances that have powered companies such as google netflix facebook and amazon one could simply agree to refer to all of this as ai and indeed that is what appears to have happened such labeling may come as a surprise to optimization or statistics researchers who wake up to find themselves suddenly referred to as ai researchers but labeling of researchers aside the bigger problem is that the use of this single ill defined acronym prevents a clear understanding of the range of intellectual and commercial issues at play the past two decades have seen major progress in industry and academia in a complementary aspiration to human imitative ai that is often referred to as intelligence augmentation ia here computation and data are used to create services that augment human intelligence and creativity a search engine can be viewed as an example of ia it augments human memory and factual knowledge as can natural language translation it augments the ability of a human to communicate computing based generation of sounds and images serves as a palette and creativity enhancer for artists while services of this kind could conceivably involve high level reasoning and thought currently they don t they mostly perform various kinds of string matching and numerical operations that capture patterns that humans can make use of hoping that the reader will tolerate one last acronym let us conceive broadly of a discipline of intelligent infrastructure ii whereby a web of computation data and physical entities exists that makes human environments more supportive interesting and safe such infrastructure is beginning to make its appearance in domains such as transportation medicine commerce and finance with vast implications for individual humans and societies this emergence sometimes arises in conversations about an internet of things but that effort generally refers to the mere problem of getting things onto the internet not to the far grander set of challenges associated with these things capable of analyzing those data streams to discover facts about the world and interacting with humans and other things at a far higher level of abstraction than mere bits for example returning to my personal anecdote we might imagine living our lives in a societal scale medical system that sets up data flows and data analysis flows between doctors and devices positioned in and around human bodies thereby able to aid human intelligence in making diagnoses and providing care the system would incorporate information from cells in the body dna blood tests environment population genetics and the vast scientific literature on drugs and treatments it would not just focus on a single patient and a doctor but on relationships among all humans just as current medical testing allows experiments done on one set of humans or animals to be brought to bear in the care of other humans it would help maintain notions of relevance provenance and reliability in the way that the current banking system focuses on such challenges in the domain of finance and payment and while one can foresee many problems arising in such a system involving privacy issues liability issues security issues etc these problems should properly be viewed as challenges not show stoppers we now come to a critical issue is working on classical human imitative ai the best or only way to focus on these larger challenges some of the most heralded recent success stories of ml have in fact been in areas associated with human imitative ai areas such as computer vision speech recognition game playing and robotics so perhaps we should simply await further progress in domains such as these there are two points to make here first although one would not know it from reading the newspapers success in human imitative ai has in fact been limited we are very far from realizing human imitative ai aspirations unfortunately the thrill and fear of making even limited progress on human imitative ai gives rise to levels of over exuberance and media attention that is not present in other areas of engineering second and more importantly success in these domains is neither sufficient nor necessary to solve important ia and ii problems on the sufficiency side consider self driving cars for such technology to be realized a range of engineering problems will need to be solved that may have little relationship to human competencies or human lack of competencies the overall transportation system an ii system will likely more closely resemble the current air traffic control system than the current collection of loosely coupled forward facing inattentive human drivers it will be vastly more complex than the current air traffic control system specifically in its use of massive amounts of data and adaptive statistical modeling to inform fine grained decisions it is those challenges that need to be in the forefront and in such an effort a focus on human imitative ai may be a distraction as for the necessity argument it is sometimes argued that the human imitative ai aspiration subsumes ia and ii aspirations because a human imitative ai system would not only be able to solve the classical problems of ai as embodied e g in the turing test but it would also be our best bet for solving ia and ii problems such an argument has little historical precedent did civil engineering develop by envisaging the creation of an artificial carpenter or bricklayer should chemical engineering have been framed in terms of creating an artificial chemist even more polemically if our goal was to build chemical factories should we have first created an artificial chemist who would have then worked out how to build a chemical factory a related argument is that human intelligence is the only kind of intelligence that we know and that we should aim to mimic it as a first step but humans are in fact not very good at some kinds of reasoning we have our lapses biases and limitations moreover critically we did not evolve to perform the kinds of large scale decision making that modern ii systems must face nor to cope with the kinds of uncertainty that arise in ii contexts one could argue that an ai system would not only imitate human intelligence but also correct it and would also scale to arbitrarily large problems but we are now in the realm of science fiction such speculative arguments while entertaining in the setting of fiction should not be our principal strategy going forward in the face of the critical ia and ii problems that are beginning to emerge we need to solve ia and ii problems on their own merits not as a mere corollary to a human imitative ai agenda it is not hard to pinpoint algorithmic and infrastructure challenges in ii systems that are not central themes in human imitative ai research ii systems require the ability to manage distributed repositories of knowledge that are rapidly changing and are likely to be globally incoherent such systems must cope with cloud edge interactions in making timely distributed decisions and they must deal with long tail phenomena whereby there is lots of data on some individuals and little data on most individuals they must address the difficulties of sharing data across administrative and competitive boundaries finally and of particular importance ii systems must bring economic ideas such as incentives and pricing into the realm of the statistical and computational infrastructures that link humans to each other and to valued goods such ii systems can be viewed as not merely providing a service but as creating markets there are domains such as music literature and journalism that are crying out for the emergence of such markets where data analysis links producers and consumers and this must all be done within the context of evolving societal ethical and legal norms of course classical human imitative ai problems remain of great interest as well however the current focus on doing ai research via the gathering of data the deployment of deep learning infrastructure and the demonstration of systems that mimic certain narrowly defined human skills with little in the way of emerging explanatory principles tends to deflect attention from major open problems in classical ai these problems include the need to bring meaning and reasoning into systems that perform natural language processing the need to infer and represent causality the need to develop computationally tractable representations of uncertainty and the need to develop systems that formulate and pursue long term goals these are classical goals in human imitative ai but in the current hubbub over the ai revolution it is easy to forget that they are not yet solved ia will also remain quite essential because for the foreseeable future computers will not be able to match humans in their ability to reason abstractly about real world situations we will need well thought out interactions of humans and computers to solve our most pressing problems and we will want computers to trigger new levels of human creativity not replace human creativity whatever that might mean it was john mccarthy while a professor at dartmouth and soon to take a position at mit who coined the term ai apparently to distinguish his budding research agenda from that of norbert wiener then an older professor at mit wiener had coined cybernetics to refer to his own vision of intelligent systems a vision that was closely tied to operations research statistics pattern recognition information theory and control theory mccarthy on the other hand emphasized the ties to logic in an interesting reversal it is wiener s intellectual agenda that has come to dominate in the current era under the banner of mccarthy s terminology this state of affairs is surely however only temporary the pendulum swings more in ai than in most fields but we need to move beyond the particular historical perspectives of mccarthy and wiener we need to realize that the current public dialog on ai which focuses on a narrow subset of industry and a narrow subset of academia risks blinding us to the challenges and opportunities that are presented by the full scope of ai ia and ii this scope is less about the realization of science fiction dreams or nightmares of super human machines and more about the need for humans to understand and shape technology as it becomes ever more present and influential in their daily lives moreover in this understanding and shaping there is a need for a diverse set of voices from all walks of life not merely a dialog among the technologically attuned focusing narrowly on human imitative ai prevents an appropriately wide range of voices from being heard while industry will continue to drive many developments academia will also continue to play an essential role not only in providing some of the most innovative technical ideas but also in bringing researchers from the computational and statistical disciplines together with researchers from other disciplines whose contributions and perspectives are sorely needed notably the social sciences the cognitive sciences and the humanities on the other hand while the humanities and the sciences are essential as we go forward we should also not pretend that we are talking about something other than an engineering effort of unprecedented scale and scope society is aiming to build new kinds of artifacts these artifacts should be built to work as claimed we do not want to build systems that help us with medical treatments transportation options and commercial opportunities to find out after the fact that these systems don t really work that they make errors that take their toll in terms of human lives and happiness in this regard as i have emphasized there is an engineering discipline yet to emerge for the data focused and learning focused fields as exciting as these latter fields appear to be they cannot yet be viewed as constituting an engineering discipline moreover we should embrace the fact that what we are witnessing is the creation of a new branch of engineering the term engineering is often invoked in a narrow sense in academia and beyond with overtones of cold affectless machinery and negative connotations of loss of control by humans but an engineering discipline can be what we want it to be in the current era we have a real opportunity to conceive of something historically new a human centric engineering discipline i will resist giving this emerging discipline a name but if the acronym ai continues to be used as placeholder nomenclature going forward let s be aware of the very real limitations of this placeholder let s broaden our scope tone down the hype and recognize the serious challenges ahead michael i jordan from a quick cheer to a standing ovation clap to show how much you enjoyed this story michael i jordan is a professor in the department of electrical engineering and computer sciences and the department of statistics at uc berkeley
Eran Kampf,57,3,https://developerzen.com/data-mining-handling-missing-values-the-database-bd2241882e72?source=tag_archive---------0----------------,Data Mining — Handling Missing Values the Database – DeveloperZen,i ve recently answered predicting missing data values in a database on stackoverflow and thought it deserved a mention on developerzen one of the important stages of data mining is preprocessing where we prepare the data for mining real world data tends to be incomplete noisy and inconsistent and an important task when preprocessing the data is to fill in missing values smooth out noise and correct inconsistencies if we specifically look at dealing with missing data there are several techniques that can be used choosing the right technique is a choice that depends on the problem domain the data s domain sales data crm data and our goal for the data mining process so how can you handle missing values in your database this is usually done when the class label is missing assuming your data mining goal is classification or many attributes are missing from the row not just one however you ll obviously get poor performance if the percentage of such rows is high for example let s say we have a database of students enrolment data age sat score state of residence etc and a column classifying their success in college to low medium and high let s say our goal is to build a model predicting a student s success in college data rows who are missing the success column are not useful in predicting success so they could very well be ignored and removed before running the algorithm decide on a new global constant value like unknown n a or minus infinity that will be used to fill all the missing values this technique is used because sometimes it just doesn t make sense to try and predict the missing value for example let s look at the students enrollment database again assuming the state of residence attribute data is missing for some students filling it up with some state doesn t really makes sense as opposed to using something like n a replace missing values of an attribute with the mean or median if its discrete value for that attribute in the database for example in a database of us family incomes if the average income of a us family is x you can use that value to replace missing income values instead of using the mean or median of a certain attribute calculated by looking at all the rows in a database we can limit the calculations to the relevant class to make the value more relevant to the row we re looking at let s say you have a cars pricing database that among other things classifies cars to luxury and low budget and you re dealing with missing values in the cost field replacing missing cost of a luxury car with the average cost of all luxury cars is probably more accurate than the value you d get if you factor in the low budget cars the value can be determined using regression inference based tools using bayesian formalism decision trees clustering algorithms k mean median etc for example we could use clustering algorithms to create clusters of rows which will then be used for calculating an attribute mean or median as specified in technique another example could be using a decision tree to try and predict the probable value in the missing attribute according to other attributes in the data i d suggest looking into regression and decision trees first id tree generation as they re relatively easy and there are plenty of examples on the net additional notes originally published at www developerzen com on august from a quick cheer to a standing ovation clap to show how much you enjoyed this story maker of things big data geek food lover the essence of software development
Oliver Lindberg,1,7,https://medium.com/the-lindberg-interviews/interview-with-googles-alfred-spector-on-voice-search-hybrid-intelligence-and-more-2f6216aa480c?source=tag_archive---------0----------------,"Interview with Google’s Alfred Spector on voice search, hybrid intelligence and more",google s a pretty good search engine right well you ain t seen nothing yet vp of research alfred spector talks to oliver lindberg about the technologies emerging from google labs from voice search to hybrid intelligence and beyond this article originally appeared in issue of net magazine in and was republished at www techradar com google has always been tight lipped about products that haven t launched yet it s no secret however that thanks to the company s bottom up culture its engineers are working on tons of new projects at the same time following the mantra of release early release often the speed at which the search engine giant is churning out tools is staggering at the heart of it all is alfred spector google s vice president of research and special initiatives one of the areas google is making significant advances in is voice search spector is astounded by how rapidly it s come along the google mobile app features search by voice capabilities that are available for the iphone blackberry windows mobile and android all versions understand english including us uk australian and indian english accents but the latest addition for nokia s phones even introduces mandarin speech recognition which because of its many different accents and tonal characteristics posed a huge engineering challenge it s the most spoken language in the world but as it isn t exactly keyboard friendly voice search could become immensely popular in china voice is one of these grand technology challenges in computer science spector explains can a computer understand the human voice it s been worked on for many decades and what we ve realised over the last couple of years is that search particularly on handheld devices is amenable to voice as an import mechanism it s very valuable to be able to use voice all of us know that no matter how good the keyboard it s tricky to type exactly the right thing into a searchbar while holding your backpack and everything else to get a computer to take account of your voice is no mean feat of course one idea is to take all of the voices that the system hears over time into one huge pan human voice model so on the one hand we have a voice that s higher and with an english accent and on the other hand my voice which is deeper and with an american accent they both go into one model or it just becomes personalised to the individual voice scientists are a little unclear as to which is the best approach the research department is also making progress in machine translation google translate already features languages including swahili and yiddish the latest version introduces instant real time translation phonetic input and text to speech support in english we re able to go from any language to any of the others and there are times so possibilities spector explains we re focusing on increasing the number of languages because we d like to handle even those languages where there s not an enormous volume of usage it will make the web far more valuable to more people if they can access the english or chinese language web for example but we also continue to focus on quality because almost always the translations are valuable but imperfect sometimes it comes from training our translation system over more raw data so we have say eu documents in english and french and can compare them and learn rules for translation the other approach is to bring more knowledge into translation for example we re using more syntactic knowledge today and doing automated parsing with language it s been a grand challenge of the field since the late s now it s finally achieved mass usage the team led by scientist franz josef och has been collecting data for more than languages and the google translator toolkit which makes use of the wisdom of the crowds now even supports languages many of which are minority languages the editor enables users to translate text correct the automatic translation and publish it spector thinks that this approach is the future as computers become even faster handling more and more data a lot of it in the cloud machines learn from users and thus become smarter he calls this concept hybrid intelligence it s very difficult to solve these technological problems without human input he says it s hard to create a robot that s as clever smart and knowledgeable of the world as we humans are but it s not as tough to build a computational system like google which extends what we do greatly and gradually learns something about the world from us but that requires our interpretation to make it really successful we need to get computers and people communicating in both directions so the computer learns from the human and makes the human more effective examples of hybrid intelligence are google suggest which instantly offers popular searches as you type a search query and the did you mean feature in google search which corrects you when you misspell a query in the search bar the more you use it the better the system gets training computers to become seemingly more intelligent poses major hurdles for google s engineers computers don t train as efficiently as people do spector explains let s take the chess example if a kasparov was the educator we could count on almost anything he says as being accurate but if you tried to learn from a million chess players you learn from my children as well who play chess but they re and eight they ll be right sometimes and not right other times there s noise in that and some of the noise is spam one also has to have careful regard for privacy issues by collecting enormous amounts of data google hopes to create a powerful database that eventually will understand the relationship between words for example a dog is an animal and a dog has four legs the challenge is to try to establish these relationships automatically using tons of information instead of having experts teach the system this database would then improve search results and language translations because it would have a better understanding of the meaning of the words there s also a lot of research around conceptual search let s take a video of a couple in front of the empire state building we watch the video and it s clear they re on their honeymoon but what is the video about is it about love or honeymoons or is it about renting office space it s a fundamentally challenging problem one example of conceptual search is google image swirl which was added to labs in november enter a keyword and you get a list of images clicking on each one brings up a cluster of related pictures click on any of them to expand the wonder wheel further google notes that they re not just the most relevant images the algorithm determines the most relevant group of images with similar appearance and meaning to improve the world s data google continues to focus on the importance of the open internet another labs project google fusion tables facilitates data management in the cloud it enables users to create tables filter and aggregate data merge it with other data sources and visualise it with google maps or the google visualisation api the data sets can then be published shared or kept private and commented on by people around the world it s an example of open collaboration spector says if it s public we can crawl it to make it searchable and easily visible to people we hired one of the best database researchers in the world alon halevy to lead it google is aiming to make more information available more easily across multiple devices whether it s images videos speech or maps no matter which language we re using spector calls the impact totally transparent processing it revolutionises the role of computation in day today life the computer can break down all these barriers to communication and knowledge no matter what device we re using we have access to things we can do translations there are books or government documents and some day we hope to have medical records whatever you want no matter where you are you can find it spector retired in early and now serves as the cto of two sigma investments this article originally appeared in issue of net magazine in and was republished at www techradar com photography by andy short from a quick cheer to a standing ovation clap to show how much you enjoyed this story independent editor and content consultant founder and captain of pixelpioneers co founder and curator of www generateconf com former editor of netmag interviews with leading tech entrepreneurs and web designers conducted by oliverlindberg at netmag
Xu Wenhao,1,4,https://xuwenhao.com/%E5%BB%BA%E8%AE%AE%E7%9A%84%E7%A8%8B%E5%BA%8F%E5%91%98%E5%AD%A6%E4%B9%A0lda%E7%AE%97%E6%B3%95%E7%9A%84%E6%AD%A5%E9%AA%A4-54168e081bc1?source=tag_archive---------0----------------,建议的程序员学习LDA算法的步骤 – 蒸汽与魔法,lda cs blei dirichlet ai machine learning prml blei tony share lda science topic models vs unstructured data prml graphic models lda search engine gibbs sampling for the uninitiated mark steyvers tom griffiths probabilistic topic models lda gibbs sampling plda lda blei lda prml from a quick cheer to a standing ovation clap to show how much you enjoyed this story facebook messenger chatbot machine learning big data
Netflix Technology Blog,439,9,https://medium.com/netflix-techblog/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429?source=tag_archive---------0----------------,Netflix Recommendations: Beyond the 5 stars (Part 1),by xavier amatriain and justin basilico personalization science and engineering in this two part blog post we will open the doors of one of the most valued netflix assets our recommendation system in part we will relate the netflix prize to the broader recommendation challenge outline the external components of our personalized service and highlight how our task has evolved with the business in part we will describe some of the data and models that we use and discuss our approach to algorithmic innovation that combines offline machine learning experimentation with online ab testing enjoy and remember that we are always looking for more star talent to add to our great team so please take a look at our jobs page in we announced the netflix prize a machine learning and data mining competition for movie rating prediction we offered million to whoever improved the accuracy of our existing system called cinematch by we conducted this competition to find new ways to improve the recommendations we provide to our members which is a key part of our business however we had to come up with a proxy question that was easier to evaluate and quantify the root mean squared error rmse of the predicted rating the race was on to beat our rmse of with the finish line of reducing it to or less a year into the competition the korbell team won the first progress prize with an improvement they reported more than hours of work in order to come up with the final combination of algorithms that gave them this prize and they gave us the source code we looked at the two underlying algorithms with the best performance in the ensemble matrix factorization which the community generally called svd singular value decomposition and restricted boltzmann machines rbm svd by itself provided a rmse while rbm alone provided a competitive but slightly worse rmse a linear blend of these two reduced the error to to put these algorithms to use we had to work to overcome some limitations for instance that they were built to handle million ratings instead of the more than billion that we have and that they were not built to adapt as members added more ratings but once we overcame those challenges we put the two algorithms into production where they are still used as part of our recommendation engine if you followed the prize competition you might be wondering what happened with the final grand prize ensemble that won the m two years later this is a truly impressive compilation and culmination of years of work blending hundreds of predictive models to finally cross the finish line we evaluated some of the new methods offline but the additional accuracy gains that we measured did not seem to justify the engineering effort needed to bring them into a production environment also our focus on improving netflix personalization had shifted to the next level by then in the remainder of this post we will explain how and why it has shifted one of the reasons our focus in the recommendation algorithms has changed is because netflix as a whole has changed dramatically in the last few years netflix launched an instant streaming service in one year after the netflix prize began streaming has not only changed the way our members interact with the service but also the type of data available to use in our algorithms for dvds our goal is to help people fill their queue with titles to receive in the mail over the coming days and weeks selection is distant in time from viewing people select carefully because exchanging a dvd for another takes more than a day and we get no feedback during viewing for streaming members are looking for something great to watch right now they can sample a few videos before settling on one they can consume several in one session and we can observe viewing statistics such as whether a video was watched fully or only partially another big change was the move from a single website into hundreds of devices the integration with the roku player and the xbox were announced in two years into the netflix competition just a year later netflix streaming made it into the iphone now it is available on a multitude of devices that go from a myriad of android devices to the latest appletv two years ago we went international with the launch in canada in we added latin american countries and territories to the list and just recently we launched in uk and ireland today netflix has more than million subscribers in countries those subscribers streamed billion hours from hundreds of different devices in the last quarter of every day they add million movies and tv shows to the queue and generate million ratings we have adapted our personalization algorithms to this new scenario in such a way that now of what people watch is from some sort of recommendation we reached this point by continuously optimizing the member experience and have measured significant gains in member satisfaction whenever we improved the personalization for our members let us now walk you through some of the techniques and approaches that we use to produce these recommendations we have discovered through the years that there is tremendous value to our subscribers in incorporating recommendations to personalize as much of netflix as possible personalization starts on our homepage which consists of groups of videos arranged in horizontal rows each row has a title that conveys the intended meaningful connection between the videos in that group most of our personalization is based on the way we select rows how we determine what items to include in them and in what order to place those items take as a first example the top row this is our best guess at the ten titles you are most likely to enjoy of course when we say you we really mean everyone in your household it is important to keep in mind that netflix personalization is intended to handle a household that is likely to have different people with different tastes that is why when you see your top you are likely to discover items for dad mom the kids or the whole family even for a single person household we want to appeal to your range of interests and moods to achieve this in many parts of our system we are not only optimizing for accuracy but also for diversity another important element in netflix personalization is awareness we want members to be aware of how we are adapting to their tastes this not only promotes trust in the system but encourages members to give feedback that will result in better recommendations a different way of promoting trust with the personalization component is to provide explanations as to why we decide to recommend a given movie or show we are not recommending it because it suits our business needs but because it matches the information we have from you your explicit taste preferences and ratings your viewing history or even your friends recommendations on the topic of friends we recently released our facebook connect feature in out of the countries we operate all but the us because of concerns with the vppa law knowing about your friends not only gives us another signal to use in our personalization algorithms but it also allows for different rows that rely mostly on your social circle to generate recommendations some of the most recognizable personalization in our service is the collection of genre rows these range from familiar high level categories like comedies and dramas to highly tailored slices such as imaginative time travel movies from the s each row represents layers of personalization the choice of genre itself the subset of titles selected within that genre and the ranking of those titles members connect with these rows so well that we measure an increase in member retention by placing the most tailored rows higher on the page instead of lower as with other personalization elements freshness and diversity is taken into account when deciding what genres to show from the thousands possible we present an explanation for the choice of rows using a member s implicit genre preferences recent plays ratings and other interactions or explicit feedback provided through our taste preferences survey we will also invite members to focus a row with additional explicit preference feedback when this is lacking similarity is also an important source of personalization in our service we think of similarity in a very broad sense it can be between movies or between members and can be in multiple dimensions such as metadata ratings or viewing data furthermore these similarities can be blended and used as features in other models similarity is used in multiple contexts for example in response to a member s action such as searching or adding a title to the queue it is also used to generate rows of adhoc genres based on similarity to titles that a member has interacted with recently if you are interested in a more in depth description of the architecture of the similarity system you can read about it in this past post on the blog in most of the previous contexts be it in the top row the genres or the similars ranking the choice of what order to place the items in a row is critical in providing an effective personalized experience the goal of our ranking system is to find the best possible ordering of a set of items for a member within a specific context in real time we decompose ranking into scoring sorting and filtering sets of movies for presentation to a member our business objective is to maximize member satisfaction and month to month subscription retention which correlates well with maximizing consumption of video content we therefore optimize our algorithms to give the highest scores to titles that a member is most likely to play and enjoy now it is clear that the netflix prize objective accurate prediction of a movie s rating is just one of the many components of an effective recommendation system that optimizes our members enjoyment we also need to take into account factors such as context title popularity interest evidence novelty diversity and freshness supporting all the different contexts in which we want to make recommendations requires a range of algorithms that are tuned to the needs of those contexts in the next part of this post we will talk in more detail about the ranking problem we will also dive into the data and models that make all the above possible and discuss our approach to innovating in this space on to part originally published at techblog netflix com on april from a quick cheer to a standing ovation clap to show how much you enjoyed this story learn more about how netflix designs builds and operates our systems and engineering organizations learn about netflix s world class engineering efforts company culture product developments and more
Netflix Technology Blog,365,10,https://medium.com/netflix-techblog/netflix-recommendations-beyond-the-5-stars-part-2-d9b96aa399f5?source=tag_archive---------1----------------,Netflix Recommendations: Beyond the 5 stars (Part 2),by xavier amatriain and justin basilico personalization science and engineering in part one of this blog post we detailed the different components of netflix personalization we also explained how netflix personalization and the service as a whole have changed from the time we announced the netflix prize the m prize delivered a great return on investment for us not only in algorithmic innovation but also in brand awareness and attracting stars no pun intended to join our team predicting movie ratings accurately is just one aspect of our world class recommender system in this second part of the blog post we will give more insight into our broader personalization technology we will discuss some of our current models data and the approaches we follow to lead innovation and research in this space the goal of recommender systems is to present a number of attractive items for a person to choose from this is usually accomplished by selecting some items and sorting them in the order of expected enjoyment or utility since the most common way of presenting recommended items is in some form of list such as the various rows on netflix we need an appropriate ranking model that can use a wide variety of information to come up with an optimal ranking of the items for each of our members if you are looking for a ranking function that optimizes consumption an obvious baseline is item popularity the reason is clear on average a member is most likely to watch what most others are watching however popularity is the opposite of personalization it will produce the same ordering of items for every member thus the goal becomes to find a personalized ranking function that is better than item popularity so we can better satisfy members with varying tastes recall that our goal is to recommend the titles that each member is most likely to play and enjoy one obvious way to approach this is to use the member s predicted rating of each item as an adjunct to item popularity using predicted ratings on their own as a ranking function can lead to items that are too niche or unfamiliar being recommended and can exclude items that the member would want to watch even though they may not rate them highly to compensate for this rather than using either popularity or predicted rating on their own we would like to produce rankings that balance both of these aspects at this point we are ready to build a ranking prediction model using these two features there are many ways one could construct a ranking function ranging from simple scoring methods to pairwise preferences to optimization over the entire ranking for the purposes of illustration let us start with a very simple scoring approach by choosing our ranking function to be a linear combination of popularity and predicted rating this gives an equation of the form frank u v w p v w r u v b where u user v video item p popularity and r predicted rating this equation defines a two dimensional space like the one depicted below once we have such a function we can pass a set of videos through our function and sort them in descending order according to the score you might be wondering how we can set the weights w and w in our model the bias b is constant and thus ends up not affecting the final ordering in other words in our simple two dimensional model how do we determine whether popularity is more or less important than predicted rating there are at least two possible approaches to this you could sample the space of possible weights and let the members decide what makes sense after many a b tests this procedure might be time consuming and not very cost effective another possible answer involves formulating this as a machine learning problem select positive and negative examples from your historical data and let a machine learning algorithm learn the weights that optimize your goal this family of machine learning problems is known as learning to rank and is central to application scenarios such as search engines or ad targeting note though that a crucial difference in the case of ranked recommendations is the importance of personalization we do not expect a global notion of relevance but rather look for ways of optimizing a personalized model as you might guess apart from popularity and rating prediction we have tried many other features at netflix some have shown no positive effect while others have improved our ranking accuracy tremendously the graph below shows the ranking improvement we have obtained by adding different features and optimizing the machine learning algorithm many supervised classification methods can be used for ranking typical choices include logistic regression support vector machines neural networks or decision tree based methods such as gradient boosted decision trees gbdt on the other hand a great number of algorithms specifically designed for learning to rank have appeared in recent years such as ranksvm or rankboost there is no easy answer to choose which model will perform best in a given ranking problem the simpler your feature space is the simpler your model can be but it is easy to get trapped in a situation where a new feature does not show value because the model cannot learn it or the other way around to conclude that a more powerful model is not useful simply because you don t have the feature space that exploits its benefits the previous discussion on the ranking algorithms highlights the importance of both data and models in creating an optimal personalized experience for our members at netflix we are fortunate to have many relevant data sources and smart people who can select optimal algorithms to turn data into product features here are some of the data sources we can use to optimize our recommendations so what about the models one thing we have found at netflix is that with the great availability of data both in quantity and types a thoughtful approach is required to model selection training and testing we use all sorts of machine learning approaches from unsupervised methods such as clustering algorithms to a number of supervised classifiers that have shown optimal results in various contexts this is an incomplete list of methods you should probably know about if you are working in machine learning for personalization consumer data science the abundance of source data measurements and associated experiments allow us to operate a data driven organization netflix has embedded this approach into its culture since the company was founded and we have come to call it consumer data science broadly speaking the main goal of our consumer science approach is to innovate for members effectively the only real failure is the failure to innovate or as thomas watson sr founder of ibm put it if you want to increase your success rate double your failure rate we strive for an innovation culture that allows us to evaluate ideas rapidly inexpensively and objectively and once we test something we want to understand why it failed or succeeded this lets us focus on the central goal of improving our service for our members so how does this work in practice it is a slight variation over the traditional scientific process called a b testing or bucket testing when we execute a b tests we track many different metrics but we ultimately trust member engagement e g hours of play and retention tests usually have thousands of members and anywhere from to cells exploring variations of a base idea we typically have scores of a b tests running in parallel a b tests let us try radical ideas or test many approaches at the same time but the key advantage is that they allow our decisions to be data driven you can read more about our approach to a b testing in this previous tech blog post or in some of the quora answers by our chief product officer neil hunt an interesting follow up question that we have faced is how to integrate our machine learning approaches into this data driven a b test culture at netflix we have done this with an offline online testing process that tries to combine the best of both worlds the offline testing cycle is a step where we test and optimize our algorithms prior to performing online a b testing to measure model performance offline we track multiple metrics used in the machine learning community from ranking measures such as normalized discounted cumulative gain mean reciprocal rank or fraction of concordant pairs to classification metrics such as accuracy precision recall or f score we also use the famous rmse from the netflix prize or other more exotic metrics to track different aspects like diversity we keep track of how well those metrics correlate to measurable online gains in our a b tests however since the mapping is not perfect offline performance is used only as an indication to make informed decisions on follow up tests once offline testing has validated a hypothesis we are ready to design and launch the a b test that will prove the new feature valid from a member perspective if it does we will be ready to roll out in our continuous pursuit of the better product for our members the diagram below illustrates the details of this process an extreme example of this innovation cycle is what we called the top marathon this was a focused week effort to quickly test dozens of algorithmic ideas related to improving our top row think of it as a month hackathon with metrics different teams and individuals were invited to contribute ideas and code in this effort we rolled out different ideas as a b tests each week and kept track of the offline and online metrics the winning results are already part of our production system the netflix prize abstracted the recommendation problem to a proxy question of predicting ratings but member ratings are only one of the many data sources we have and rating predictions are only part of our solution over time we have reformulated the recommendation problem to the question of optimizing the probability a member chooses to watch a title and enjoys it enough to come back to the service more data availability enables better results but in order to get those results we need to have optimized approaches appropriate metrics and rapid experimentation to excel at innovating personalization it is insufficient to be methodical in our research the space to explore is virtually infinite at netflix we love choosing and watching movies and tv shows we focus our research by translating this passion into strong intuitions about fruitful directions to pursue under utilized data sources better feature representations more appropriate models and metrics and missed opportunities to personalize we use data mining and other experimental approaches to incrementally inform our intuition and so prioritize investment of effort as with any scientific pursuit there s always a contribution from lady luck but as the adage goes luck favors the prepared mind finally above all we look to our members as the final judges of the quality of our recommendation approach because this is all ultimately about increasing our members enjoyment in their own netflix experience we are always looking for more people to join our team of prepared minds make sure you take a look at our jobs page originally published at techblog netflix com on june from a quick cheer to a standing ovation clap to show how much you enjoyed this story learn more about how netflix designs builds and operates our systems and engineering organizations learn about netflix s world class engineering efforts company culture product developments and more
Wolf Garbe,6,6,https://medium.com/@wolfgarbe/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f?source=tag_archive---------2----------------,1000x Faster Spelling Correction algorithm (2012) – Wolf Garbe – Medium,update an improved symspell implementation is now x faster update symspellcompound with compound aware spelling correction update benchmark of symspell bk tree und norvig s spell correct recently i answered a question on quora about spelling correction for search engines when i described our symspell algorithm i was pointed to peter norvig s page where he outlined his approach both algorithms are based on edit distance damerau levenshtein distance both try to find the dictionary entries with smallest edit distance from the query term if the edit distance is the term is spelled correctly if the edit distance is the dictionary term is used as spelling suggestion but symspell uses a different way to search the dictionary resulting in a significant performance gain and language independence three ways to search for minimum edit distance in a dictionary naive approachthe obvious way of doing this is to compute the edit distance from the query term to each dictionary term before selecting the string s of minimum edit distance as spelling suggestion this exhaustive search is inordinately expensive source christopher d manning prabhakar raghavan hinrich schu tze introduction to information retrieval the performance can be significantly improved by terminating the edit distance calculation as soon as a threshold of or has been reached peter norviggenerate all possible terms with an edit distance deletes transposes replaces inserts from the query term and search them in the dictionary for a word of length n an alphabet size a an edit distance d there will be n deletions n transpositions a n alterations and a n insertions for a total of n an a terms at search time source peter norvig how to write a spelling corrector this is much better than the naive approach but still expensive at search time terms for n a d and language dependent because the alphabet is used to generate the terms which is different in many languages and huge in chinese a unicode han characters symmetric delete spelling correction symspell generate terms with an edit distance deletes only from each dictionary term and add them together with the original term to the dictionary this has to be done only once during a pre calculation step generate terms with an edit distance deletes only from the input term and search them in the dictionary for a word of length n an alphabet size of a an edit distance of there will be just n deletions for a total of n terms at search time this is three orders of magnitude less expensive terms for n and d and language independent the alphabet is not required to generate deletes the cost of this approach is the pre calculation time and storage space of x deletes for every original dictionary entry which is acceptable in most cases the number x of deletes for a single dictionary entry depends on the maximum edit distance x n for edit distance x n n for edit distance x n d n d for edit distance d combinatorics k out of n combinations without repetitions and k n d e g for a maximum edit distance of and an average word length of and dictionary entries we need to additionally store deletes remark during the precalculation different words in the dictionary might lead to same delete term delete sun delete sin sn while we generate only one new dictionary entry sn inside we need to store both original terms as spelling correction suggestion sun sin remark there are four different comparison pair types the last comparison type is required for replaces and transposes only but we need to check whether the suggested dictionary term is really a replace or an adjacent transpose of the input term to prevent false positives of higher edit distance bank bnak and bank bink but bank kanb and bank xban and bank baxn remark instead of a dedicated spelling dictionary we are using the search engine index itself this has several benefits remark we have implemented query suggestions completion in a similar fashion this is a good way to prevent spelling errors in the first place every newly indexed word whose frequency is over a certain threshold is stored as a suggestion to all of its prefixes they are created in the index if they do not yet exist as we anyway provide an instant search feature the lookup for suggestions comes also at almost no extra cost multiple terms are sorted by the number of results stored in the index reasoningthe symspell algorithm exploits the fact that the edit distance between two terms is symmetrical we are using variant because the delete only transformation is language independent and three orders of magnitude less expensive where does the speed come from computational complexity the symspell algorithm is constant time o time i e independent of the dictionary size but depending on the average term length and maximum edit distance because our index is based on a hash table which has an average search time complexity of o comparison to other approaches bk trees have a search time of o log dictionary size whereas the symspell algorithm is constant time o time i e independent of the dictionary size tries have a comparable search performance to our approach but a trie is a prefix tree which requires a common prefix this makes it suitable for autocomplete or search suggestions but not applicable for spell checking if your typing error is e g in the first letter than you have no common prefix hence the trie will not work for spelling correction application possible application fields of the symspell algorithm are those of fast approximate dictionary string matching spell checkers for word processors and search engines correction systems for optical character recognition natural language translation based on translation memory record linkage de duplication matching dna sequences fuzzy string searching and fraud detection source codethe c implementation of the symmetric delete spelling correction algorithm is released on github as open source under the mit license https github com wolfgarbe symspell portsthere are ports in c crystal go java javascript python ruby rust scala swift available originally published at blog faroo com on june from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder seekstorm search as a service faroo p p search http www seekstorm com https github com wolfgarbe https www quora com profile wolf garbe
Paul Christiano,43,31,https://ai-alignment.com/a-formalization-of-indirect-normativity-7e44db640160?source=tag_archive---------3----------------,Formalizing indirect normativity – AI Alignment,this post outlines a formalization of what nick bostrom calls indirect normativity i don t think it s an adequate solution to the ai control problem but to my knowledge it was the first precise specification of a goal that meets the not terrible bar i e which does not lead to terrible consequences if pursued without any caveats or restrictions the proposal outlined here was sketched in early while i was visiting fhi and was my first serious foray into ai control when faced with the challenge of writing down precise moral principles adhering to the standards demanded in mathematics moral philosophers encounter two serious difficulties in light of these difficulties a moral philosopher might simply declare it is not my place to aspire to mathematical standards of precision ethics as a project inherently requires shared language understanding and experience it becomes impossible or meaningless without them this may be a defensible philosophical position but unfortunately the issue is not entirely philosophical in the interest of building institutions or machines which reliably pursue what we value we may one day be forced to describe precisely what we value in a way that does not depend on charitable or common sense interpretation in the same way that we today must describe what we want done precisely to computers often with considerable effort if some aspects of our values cannot be described formally then it may be more difficult to use institutions or machines to reliably satisfy them this is not to say that describing our values formally is necessary to satisfying them merely that it might make it easier since we are focusing on finding any precise and satisfactory moral theory rather than resolving disputes in moral philosophy we will adopt a consequentialist approach without justification and focus on axiology moreover we will begin from the standpoint of expected utility maximization and leave aside questions about how or over what space the maximization is performed we aim to mathematically define a utility function u such that we would be willing to build a hypothetical machine which exceptionlessly maximized u possibly at the catastrophic expense of any other values we will assume that the machine has an ability to reason which at least rivals that of humans and is willing to tolerate arbitrarily complex definitions of u within its ability to reason about them we adopt an indirect approach rather than specifying what exactly we want we specify a process for determining what we want this process is extremely complex so that any computationally limited agent will always be uncertain about the process output however by reasoning about the process it is possible to make judgments about which action has the highest expected utility in light of this uncertainty for example i might adopt the principle a state of affairs is valuable to the extent that i would judge it valuable after a century of reflection in general i will be uncertain about what i would say after a century but i can act on the basis of my best guesses after a century i will probably prefer worlds with more happiness and so today i should prefer worlds with more happiness after a century i have only a small probability of valuing trees feelings and so today i should go out of my way to avoid hurting them if it is either instrumentally useful or extremely easy as i spend more time thinking my beliefs about what i would say after a century may change and i will start to pursue different states of affairs even though the formal definition of my values is static similarly i might desire to think about the value of trees feelings if i expect that my opinions are unstable if i spend a month thinking about trees my current views will then be a much better predictor of my views after a hundred years and if i know better whether or not trees feelings are valuable i can make better decisions this example is quite informal but it communicates the main idea of the approach we stress that the value of our contribution if any is in the possibility of a precise formulation our proposal itself will be relatively informal instead it is a description of how you would arrive at a precise formulation the use of indirection seems to be necessary to achieve the desired level of precision our proposal contains only two explicit steps each of these steps requires substantial elaboration but we must also specify what we expect the human to do with these tools this proposal is best understood in the context of other fantastic seeming proposals such as my utility is whatever i would write down if i reflected for a thousand years without interruption or biological decay the counterfactual events which take place within the definition are far beyond the realm our intuition recognizes as realistic and have no place except in thought experiments but to the extent that we can reason about these counterfactuals and change our behavior on the basis of that reasoning if so motivated we can already see how such fantastic situations could affect our more prosaic reality the remainder of this document consists of brief elaboration of some of these steps and a few arguments about why this is a desirable process the first step of our proposal is a high fidelity mathematical model of human cognition we will set aside philosophical troubles and assume that the human brain is a purely physical system which may be characterized mathematically even granting this it is not clear how we can realistically obtain such a characterization the most obvious approach to characterizing a brain is to combine measurements of its behavior or architecture with an understanding of biology chemistry and physics this project represents a massive engineering effort which is currently just beginning most pessimistically our proposal could be postponed until this project s completion this could still be long before the mathematical characterization of the brain becomes useful for running experiments or automating human activities because we are interested only in a definition we do not care about having the computational resources necessary to simulate the brain an impractical mathematical definition however may be much easier to obtain we can define a model of a brain in terms of exhaustive searches which could never be practically carried out for example given some observations of a neuron we can formally define a brute force search for a model of that neuron similarly given models of individual neurons we may be able to specify a brute force search over all ways of connecting those neurons which account for our observations of the brain say some data acquired through functional neuroimaging it may be possible to carry out this definition without exploiting any structural knowledge about the brain beyond what is necessary to measure it effectively by collecting imaging data for a human exposed to a wide variety of stimuli we can recover a large corpus of data which must be explained by any model of a human brain moreover by using our explicit knowledge of human cognition we can algorithmically generate an extensive range of tests which identify a successful simulation by probing responses to questions or performance on games or puzzles in fact this project may be possible using existing resources the complexity of the human brain is not as unapproachable as it may at first appear though it may contain synapses each described by many parameters it can be specified much more compactly a newborn s brain can be specified by about bits of genetic information together with a recipe for a physical simulation of development the human brain appears to form new long term memories at a rate of bits per second suggesting that it may be possible to specify an adult brain using additional bits of experiential information this suggests that it may require only about bits of information to specify a human brain which is at the limits of what can be reasonably collected by existing technology for functional neuroimaging this discussion has glossed over at least one question what do we mean by brain emulation human cognition does not reside in a physical system with sharp boundaries and it is not clear how you would define or use a simulation of the input output behavior of such an object we will focus on some system which does have precisely defined input output behavior and which captures the important aspects of human cognition consider a system containing a human a keyboard a monitor and some auxiliary instruments well insulated from the environment except for some wires carrying inputs to the monitor and outputs from the keyboard and auxiliary instruments and wires carrying power the inputs to this system are simply screens to be displayed on the monitor say delivered as a sequence to be displayed one after another at frames per second while the outputs are the information conveyed from the keyboard and the other measuring apparatuses also delivered as a sequence of data dumps each recording activity from the last th of a second this human in a box system can be easily formally defined if a precise description of a human brain and coarse descriptions of the human body and the environment are available alternatively the input output behavior of the human in a box can be directly observed and a computational model constructed for the entire system let h be a mathematical definition of the resulting randomized function from input sequences in in in k to the next output out k h is by design a good approximation to what the human would output if presented with any particular input sequence using h we can mathematically define what would happen if the human interacted with a wide variety of systems for example if we deliver out k as the input to an abstract computer running some arbitrary software and then define in k as what the screen would next display we can mathematically define the distribution over transcripts which would have arisen if the human had interacted with the abstract computer this computer could be running an interactive shell a video game or a messaging client note that h reflects the behavior of a particular human in a particular mental state this state is determined by the process used to design h or the data used to learn it in general we can control h by choosing an appropriate human and providing appropriate instructions training more emulations could be produced by similar measures if necessary using only a single human may seem problematic but we will not rely on this lone individual to make all relevant ethical judgments instead we will try to select a human with the motivational stability to carry out the subsequent steps faithfully which will define u using the judgment of a community consisting of many humans this discussion has been brief and has necessarily glossed over several important difficulties one difficulty is the danger of using computationally unbounded brute force search given the possibility of short programs which exhibit goal oriented behavior another difficulty is that unless the emulation project is extremely conservative the models it produces are not likely to be fully functional humans their thoughts may be blurred in various ways they may be missing many memories or skills and they may lack important functionalities such as long term memory formation or emotional expression the scope of these issues depends on the availability of data from which to learn the relevant aspects of human cognition realistic proposals along these lines will need to accommodate these shortcomings relying on distorted emulations as a tool to construct increasingly accurate models for any idealized software with a distinguished instruction return we can use h to mathematically define the distribution over return values which would result if the human were to interact with that software we will informally define a particular program t which provides a rich environment in which the remainder of our proposal can be implemented from a technical perspective this will be the last step of our proposal the remaining steps will be reflected only in the intentions and behavior of the human being simulated in h fix a convenient and adequately expressive language say a dialect of python designed to run on an abstract machine t implements a standard interface for an interactive shell in this language the user can look through all of the past instructions that have been executed and their return values rendered as strings or execute a new instruction we also provide symbols representing h and t themselves as functions from sequences of k inputs to a value for the kth output we also provide some useful information such as a snapshot of the internet and some information about the process used to create h and t which we encode as a bit string and store in a single environment variable data we assume that our language of choice has a return instruction and we have t return whenever the user executes this instruction some care needs to be taken to define the behavior if t enters an infinite loop we want to minimize the probability that the human accidentally hangs the terminal with catastrophic consequences but we cannot provide a complete safety net without running into unresolvable issues with self reference we define u to be the value returned by h interacting with t if h represented an unfortunate mental state then this interaction could be short and unproductive the simulated human could just decide to type return and be done with it however by choosing an appropriate human to simulate and inculcating an appropriate mental state we can direct the process further we intend for h to use the resources in t to initiate a larger deliberative process for example the first step of this process may be to instantiate many copies of h interacting with variants of messaging clients which are in contact with each other the return value from the original process could then be defined as the value returned by a designated leader from this community or as a majority vote amongst the copies of h or so on another step might be to create appropriate realistic virtual environments for simulated brains rather than confining them to boxes for motivational stability it may be helpful to design various coordination mechanisms involving frameworks for interaction cached mental states which are frequently re instantiated or sanity checks whereby one copy of h monitors the behavior of another the resulting communities of simulated brains then engage in a protracted planning process ensuring that subsequent steps can be carried out safely or developing alternative approaches the main priority of this community is to reduce the probability of errors as far as possible exactly what constitutes an error will be discussed at more length later at the end of this process we obtain a formal definition of a new protocol h which submits its inputs for consideration to a large community and then produces its outputs using some deliberation mechanism democratic vote one leader using the rest of the community as advisors etc the next step requires our community of simulated brains to construct a detailed simulation of earth which they can observe and manipulate once they have such a simulation they have access to all of the data which would have been available on earth in particular they can now explore many possible futures and construct simulations for each living human in order to locate earth we will again leverage an exhaustive search first h decides on informal desiderata for an earth simulation these are likely to be as follows once h has decided on the desiderata it uses a brute force search to find a simulation satisfying them for each possible program it instantiates a new copy of h tasked with evaluating whether that program is an acceptable simulation we then define e to be a uniform distribution over programs which pass this evaluation we might have doubts about whether this process produces the real earth perhaps even once we have verified that it is identical according to a laundry list of measures it may still be different in other important ways there are two reasons why we might care about such differences first if the simulated earth has a substantially different set of people than the real earth then a different set of people will be involved in the subsequent decision making if we care particularly about the opinions of the people who actually exist which the reader might well being amongst such people then this may be unsatisfactory second if events transpire significantly differently on the simulated earth than the real earth value judgments designed to guide behavior appropriately in the simulated earth may lead to less appropriate behaviors in the real earth this will not be a problem if our ultimate definition of u consists of universalizable ethical principles but we will see that u might take other forms these concerns are addressed by a few broad arguments first checking a detailed but arbitrary laundry list actually provides a very strong guarantee for example if this laundry list includes verifying a snapshot of the internet then every event or person documented on the internet must exist unchanged and every keystroke of every person composing a document on the internet must not be disturbed if the world is well interconnected then it may be very difficult to modify parts of the world without having substantial effects elsewhere and so if a long enough arbitrary list of properties is fixed we expect nearly all of the world to be the same as well second if the essential character of the world is fixed but detailed are varied we should expect the sort of moral judgments reached by consensus to be relatively constant finally if the system whose behavior depends on these moral judgments is identical between the real and simulated worlds then outputting a u which causes that system to behave a certain way in the simulated world will also cause that system to behave that way in the real world once h has defined a simulation of the world which permits inspection and intervention by careful trial and error h can inspect a variety of possible futures in particular they can find interventions which cause the simulated human society to conduct a real brain emulation project and produce high fidelity brain scans for all living humans once these scans have been obtained h can use them to define u as the output of a new community h which draws on the expertise of all living humans operating under ideal conditions there are two important degrees of flexibility how to arrange the community for efficient communication and deliberation and how to delegate the authority to define u in terms of organization the distinction between different approaches is probably not very important for example it would probably be perfectly satisfactory to start from a community of humans interacting with each other over something like the existing internet but on abstract secure infrastructure more important are the safety measures which would be in place and the mechanism for resolving differences of value between different simulated humans the basic approach to resolving disputes is to allow each human to independently create a utility function u each bounded in the interval and then to return their average this average can either be unweighted or can be weighted by a measure of each individual s influence in the real world in accordance with a game theoretic notion like the shapley value applied to abstract games or simulations of the original world more sophisticated mechanisms are also possible and may be desirable of course these questions can and should be addressed in part by h during its deliberation in the previous step after all h has access to an unlimited length of time to deliberate and has infinitely powerful computational aids the role of our reasoning at this stage is simply to suggest that we can reasonably expect h to discover effective solutions as when discussing discovering a brain simulation by brute force we have skipped over some critical issues in this section in general brute force searches particularly over programs which we would like to run are quite dangerous because such searches will discover many programs with destructive goal oriented behaviors to deal with these issues in both cases we must rely on patience and powerful safety measures once we have a formal description of a community of interacting humans given as much time as necessary to deliberate and equipped with infinitely powerful computational aids it becomes increasingly difficult to make coherent predictions about their behavior critically though we can also become increasingly confident that the outcome of their behavior will reflect their intentions we sketch some possibilities to illustrate the degree of flexibility available perhaps the most natural possibility is for this community to solve some outstanding philosophical problems and to produce a utility function which directly captures their preferences however even if they quickly discovered a formulation which appeared to be attractive they would still be wise to spend a great length of time and to leverage some of these other techniques to ensure that their proposed solution was really satisfactory another natural possibility is to eschew a comprehensive theory of ethics and define value in terms of the community s judgment we can define a utility function in terms of the hypothetical judgments of astronomical numbers of simulated humans collaboratively evaluating the goodness of a state of affairs by examining its history at the atomic level understanding the relevant higher order structure and applying human intuitions it seems quite likely that the community will gradually engage in self modifications enlarging their cognitive capacity along various dimensions as they come to understand the relevant aspects of cognition and judge such modifications to preserve their essential character either independently or as an outgrowth of this process they may gradually or abruptly pass control to machine intelligences which they are suitably confident expresses their values this process could be used to acquire the power necessary to define a utility function in one of the above frameworks or understanding value preserving self modification or machine intelligence may itself prove an important ingredient in formalizing what it is we value any of these operations would be performed only after considerable analysis when the original simulated humans were extremely confident in the desirability of the results whatever path they take and whatever coordination mechanisms they use eventually they will output a utility function u we then define u if u u if u and u u otherwise at this point we have offered a proposal for formally defining a function u we have made some general observations about what this definition entails but now we may wonder to what extent u reflects our values or more relevantly to what extent our values are served by the creation of u maximizers concerns may be divided into a few natural categories we respond to each of these objections in turn if the process works as intended we will reach a stage in which a large community of humans reflects on their values undergoes a process of discovery and potentially self modification and then outputs its result we may be concerned that this dynamic does not adequately capture what we value for example we may believe that some other extrapolation dynamic captures our values or that it is morally desirable to act on the basis of our current beliefs without further reflection or that the presence of realistic disruptions such as the threat of catastrophe has an important role in shaping our moral deliberation the important observation in the defense of our proposal is that whatever objections we could think of today we could think of within the simulation if upon reflection we decide that too much reflection is undesirable we can simply change our plans appropriately if we decide that realistic interference is important for moral deliberation we can construct a simulation in which such interference occurs or determine our moral principles by observing moral judgments in our own world s possible futures there is some chance that this proposal is inadequate for some reason which won t be apparent upon reflection but then by definition this is a fact which we cannot possibly hope to learn by deliberating now it therefore seems quite difficult to maintain objections to the proposal along these lines one aspect of the proposal does get locked in however after being considered by only one human rather than by a large civilization the distribution of authority amongst different humans and the nature of mechanisms for resolving differing value judgments here we have two possible defenses one is that the mechanism for resolving such disagreements can be reflected on at length by the individual simulated in h this individual can spend generations of subjective time and greatly expand her own cognitive capacities while attempting to determine the appropriate way to resolve such disagreements however this defense is not completely satisfactory we may be able to rely on this individual to produce a very technically sound and generally efficient proposal but the proposal itself is quite value laden and relying on one individual to make such a judgment is in some sense begging the question a second more compelling defense is that the structure of our world has already provided a mechanism for resolving value disagreements by assigning decision making weight in a way that depends on current influence for example as determined by the simulated ability of various coalitions to achieve various goals we can generate a class of proposals which are at a minimum no worse than the status quo of course these considerations will also be shaped by the conditions surrounding the creation or maintenance of systems which will be guided by u for example if a nation were to create a u maximizer they might first adopt an internal policy for assigning influence on u by performing this decision making in an idealized environment we can also reduce the likelihood of destructive conflict and increase the opportunities for mutually beneficial bargaining we may have moral objections to codifying this sort of might makes right policy favoring a more democratic proposal or something else entirely but as a matter of empirical fact a more cosmopolitan proposal will be adopted only if it is supported by those with the appropriate forms of influence a situation which is unchanged by precisely codifying existing power structure finally the values of the simulations in this process may diverge from the values of the original human models for one reaosn or another for example the simulated humans may predictably disagree with the original models about ethical questions by virtue of probably having no physical instantiation that is the output of this process is defined in terms of what a particular human would do in a situation which that human knows will never come to pass if i ask what would i do if i were to wake up in a featureless room and told that the future of humanity depended on my actions the answer might begin with become distressed that i am clearly inhabiting a hypothetical situation and adjust my ethical views to take into account the fact that people in hypothetical situations apparently have relevant first person experience setting aside the question of whether such adjustments are justified they at least raise the possibility that our values may diverge from those of the simulations in this process these changes might be minimized by understanding their nature in advance and treating them on a case by case basis if we can become convinced that our understanding is exhaustive for example we could try and use humans who robustly employ updateless decision theories which never undergo such predictable changes or we could attempt to engineer a situation in which all of the humans being emulated do have physical instantiations and naive self interest for those emulations aligns roughly with the desired behavior for example by allowing the early emulations to write themselves into our world we can imagine many ways in which this process can fail to work as intended the original brain emulations may accurately model human behavior the original subject may deviate from the intended plans or simulated humans can make an error when interacting with their virtual environment which causes the process to get hijacked by some unintended dynamic we can argue that the proposal is likely to succeed and can bolster the argument in various ways by reducing the number of assumptions necessary for succees building in fault tolerance justifying each assumption more rigorously and so on however we are unlikely to eliminate the possibility of error therefore we need to argue that if the process fails with some small probability the resulting values will only be slightly disturbed this is the reason for requiring u to lie in the interval we will see that this restriction bounds the damage which may be done by an unlikely failure if the process fails with some small probability then we can represent the resulting utility function as u u u where u is the intended utility function and u is a utility function produced by some arbitrary error process now consider two possible states of affairs a and b such that u a u b u b then since u we have u a u a u a u b u b u b u b thus if a is substantially better than b according to u then a is better than b according to u this shows that a small probability of error whether coming from the stochasticity of our process or an agent s uncertainty about the process output has only a small effect on the resulting values moreover the process contains a humans who have access to a simulation of our world this implies in particular that they have access to a simulation of whatever u maximizing agents exist in the world and they have knowledge of those agents beliefs about u this allows them to choose u with perfect knowledge of the effects of error in these agents judgments in some cases this will allow them to completely negate the effect of error terms for example if the randomness in our process causes a perfectly cooperate community of simulated humans to control u with probability and causes an arbitrary adversary to control it with probability then the simulated humans can spend half of their mass outputting a utility function which exactly counters the effect of the adversary in general the situation is not quite so simple the fraction of mass controlled by any particular coalition will vary as the system s uncertainty about u varies and so it will be impossible to counteract the effect of an error term in a way which is time independent instead we will argue later that an appropriate choice of a bounded and noisy u can be used to achieve a very wide variety of effective behaviors of u maximizers overcoming the limitations both of bounded utility maximization and of noisy specification of utility functions many possible problems with this scheme were described or implicitly addressed above but that discussion was not exhaustive and there are some classes of errors that fall through the cracks one interesting class of failures concerns changes in the values of the hypothetical human h this human is in a very strange situation and it seems quite possible that the physical universe we know contains extremely few instances of that situation especially as the process unfolds and becomes more exotic so h s first person experience of this situation may lead to significant changes in h s views for example our intuition that our own universe is valuable seems to be derived substantially from our judgment that our own first person experiences are valuable if hypothetically we found ourselves in a very alien universe it seems quite plausible that we would judge the experiences within that universe to be morally valuable as well depending perhaps on our initial philosophical inclinations another example concerns our self interest much of individual humans values seem to depend on their own anticipations about what will happen to them especially when faced with the prospect of very negative outcomes if hypothetically we woke up in a completely non physical situation it is not exactly clear what we would anticipate and this may distort our behavior would we anticipate the planned thought experiment occurring as planned would we focus our attention on those locations in the universe where a simulation of the thought experiment might be occurring this possibility is particularly troubling in light of the incentives our scheme creates anyone who can manipulate h s behavior can have a significant effect on the future of our world and so many may be motivated to create simulations of h a realistic u maximizer will not be able to carry out the process described in the definition of u in fact this process probably requires immensely more computing resources than are available in the universe it may even involve the reaction of a simulated human to watching a simulation of the universe to what extent can we make robust guarantees about the behavior of such an agent we have already touched on this difficulty when discussing the maxim a state of affairs is valuable to the extent i would judge it valuable after a century of reflection we cannot generally predict our own judgments in a hundred years time but we can have well founded beliefs about those judgments and act on the basis of those beliefs we can also have beliefs about the value of further deliberation and can strike a balance between such deliberation and acting on our current best guess a u maximizer faces a similar set of problems it cannot understand the exact form of u but it can still have well founded beliefs about u and about what sorts of actions are good according to u for example if we suppose that the u maximizer can carry out any reasoning that we can carry out then the u maximizer knows to avoid anything which we suspect would be bad according to u for example torturing humans even if the u maximizer cannot carry out this reasoning as long as it can recognize that humans have powerful predictive models for other humans it can simply appropriate those models either by carrying out reasoning inspired by human models or by simply asking moreover the community of humans being simulated in our process has access to a simulation of whatever u maximizer is operating under this uncertainty and has a detailed understanding of that uncertainty this allows the community to shape their actions in a way with predictable to the u maximizer consequences it is easily conceivable that our values cannot be captured by a bounded utility function easiest to imagine is the possibility that some states of the world are much better than others in a way that requires unbounded utility functions but it is also conceivable that the framework of utility maximization is fundamentally not an appropriate one for guiding such an agent s action or that the notion of utility maximization hides subtleties which we do not yet appreciate we will argue that it is possible to transform bounded utility maximization into an arbitrary alternative system of decision making by designing a utility function which rewards worlds in which the u maximizer replaced itself with an alternative decision maker it is straightforward to design a utility function which is maximized in worlds where any particular u maximizer converted itself into a non u maximizer even if no simple characterization can be found for the desired act we can simply instantiate many communities of humans to look over a world history and decide whether or not they judge the u maximizer to have acted appropriately the more complicated question is whether a realistic u maximizer can be made to convert itself into a non u maximizer given that it is logically uncertain about the nature of u it is at least conceivable that it couldn t if the desirability of some other behavior is only revealed by philosophical considerations which are too complex to ever be discovered by physically limited agents then we should not expect any physically limited u maximizer to respond to those considerations of course in this case we could also not expect normal human deliberation to correctly capture our values the relevant question is whether a u maximizer could switch to a different normative framework if an ordinary investment of effort by human society revealed that a different normative framework was more appropriate if a u maximizer does not spend any time investigating this possibility than it may not be expected to act on it but to the extent that we assign a significant probability to the simulated humans deciding that a different normative framework is more appropriate and to the extent that the u maximizer is able to either emulate or accept our reasoning it will also assign a significant probability to this possibility unless it is able to rule it out by more sophisticated reasoning if we and the u maximizer expect the simulations to output a u which rewards a switch to a different normative framework and this possibility is considered seriously then u maximization entails exploring this possibility if these explorations suggest that the simulated humans probably do recommend some particular alternative framework and will output a u which assigns high value to worlds in which this framework is adopted and low value to worlds in which it isn t then a u maximizer will change frameworks such a change of frameworks may involve sweeping action in the world for example the u maximizer may have created many other agents which are pursuing activities instrumentally useful to maximizing u these agents may then need to be destroyed or altered anticipating this possibility the u maximizer is likely to take actions to ensure that its current best guess about u does not get locked in this argument suggests that a u maximizer could adopt an arbitrary alternative framework if it were feasible to conclude that humans would endorse that framework upon reflection our proposal appears to be something of a cop out in that it declines to directly take a stance on any ethical issues indeed not only do we fail to specify a utility function ourselves but we expect the simulations to which we have delegated the problem to in turn delegate it at least a few more times clearly at some point this process must bottom out with actual value judgments and we may be concerned that this sort of passing the buck is just obscuring deeper problems which will arise when the process does bottom out as observed above whatever such concerns we might have can also be discovered by the simulations we create if there is some fundamental difficulty which always arises when trying to assign values then we certainly have not exacerbated this problem by delegation nevertheless there are at least two coherent objections one might raise both of these objections can be met with a single response in the current world we face a broad range of difficult and often urgent problems by passing the buck the first time we delegate resolution of ethical challenges to a civilization which does not have to deal with some of these difficulties in particular it faces no urgent existential threats this allows us to divert as much energy as possible to dealing with practical problems today while still capturing most of the benefits of nearly arbitrarily extensive ethical deliberation this process is defined in terms of the behavior of unthinkably many hypothetical brain emulations it is conceivable that the moral status of these emulations may be significant we must make a distinction between two possible sources of moral value it could be the case that a u maximizer carries out simulations on physical hardware in order to better understand u and these simulations have moral value or it could be the case that the hypothetical emulations themselves have moral value in the first case we can remark that the moral value of such simulations is itself incorporated into the definition of u therefore a u maximizer will be sensitive to the possible suffering of simulations it runs while trying to learn about u as long as it believes that we may might be concerned about the simulations welfare upon reflection it can rely as much as possible on approaches which do not involve running simulations which deprive simulations of the first person experience of discomfort or which estimate outcomes by running simulations in more pleasant circumstances if the u maximizer is able to foresee that we will consider certain sacrifices in simulation welfare worthwhile then it will make those sacrifices in general in the same way that we can argue that estimates of u reflect our values over states of affairs we can argue that estimates of u reflects our values over processes for learning about u in the second case a u maximizer in our world may have little ability to influence the welfare of hypothetical simulations invoked in the definition of u however the possible disvalue of these simulations experiences are probably seriously diminished in general the moral value of such hypothetical simulations experiences is somewhat dubious if we simply write down the definition of u these simulations seem to have no more reality than story book characters whose activities we describe the best arguments for their moral relevance comes from the great causal significance of their decisions if the actions of a powerful u maximizer depend on its beliefs about what a particular simulation would do in a particular situation including for example that simulation s awareness of discomfort or fear or confusion at the absurdity of the hypothetical situation in which they find themselves then it may be the case that those emotional responses are granted moral significance however although we may define astronomical numbers of hypothetical simulations the detailed emotional responses of very view of these simulations will play an important role in the definition of u moreover for the most part the existences of the hypothetical simulations we define are extremely well controlled by those simulations themselves and may be expected to be counted as unusually happy by the lights of the simulations themselves the early simulations who have less such control are created from an individual who has provided consent and is selected to find such situations particularly non distressing finally we observe that u can exert control over the experiences of even hypothetical simulations if the early simulations would experience morally relevant suffering because of their causal significance but the later simulations they generate robustly disvalue this suffering the later simulations can simulate each other and ensure that they all take the same actions eliminating the causal significance of the earlier simulations originally published at ordinaryideas wordpress com on april from a quick cheer to a standing ovation clap to show how much you enjoyed this story openai aligning ai systems with human interests
Robbie Tilton,3,15,https://medium.com/@robbietilton/emotional-computing-with-ai-3513884055fa?source=tag_archive---------4----------------,Emotional Computing – Robbie Tilton – Medium,investigating the human to computer relationship through reverse engineering the turing test humans are getting closer to creating a computer with the ability to feel and think although the processes of the human brain are at large unknown computer scientists have been working to simulate the human capacity to feel and understand emotions this paper explores what it means to live in an age where computers can have emotional depth and what this means for the future of human to computer interactions in an experiment between a human and a human disguised as a computer the turing test is reverse engineered in order to understand the role computers will play as they become more adept to the processes of the human mind implications for this study are discussed and the direction for future research suggested the computer is a gateway technology that has opened up new ways of creation communication and expression computers in first world countries are a standard household item approximately of americans owning one as of us census bereau and are utilized as a tool to achieve a diverse range of goals as this product continues to become more globalized transistors are becoming smaller processors are becoming faster hard drives are holding information in new networked patterns and humans are adapting to the methods of interaction expected of machines at the same time with more powerful computers and quicker means of communication many researchers are exploring how a computer can serve as a tool to simulate the brains cognition if a computer is able to achieve the same intellectual and emotional properties as the human brain we could potentially understand how we ourselves think and feel coined by mit the term affective computing relates to computation of emotion or the affective phenomena and is a study that breaks down complex processes of the brain relating them to machine like activities marvin minsky rosalind picard clifford nass and scott brave along with many others have contributed to this field and what it would mean to have a computer that could fully understand its users in their research it is very clear that humans have the capacity to associate human emotions and personality traits with a machine nass and brave but can a human ever truly treat machine as a person in this paper we will uncover what it means for humans to interact with machines of greater intelligence and attempt to predict the future of human to computer interactions the human to computer relationship is continuously evolving and is dependent on the software interface users interact with with regards to current wide scale interfaces osx windows linux ios and android the tools and abilities that a computer provide remains to be the central focus of computational advancements for commercial purposes this relationship to software is driven by utilitarian needs and humans do not expect emotional comprehension or intellectually equivalent thoughts in their household devices as face tracking eye tracking speech recognition and kinetic recognition are advancing in their experimental laboratories it is anticipated that these technologies will eventually make their way to the mainstream market to provide a new relationship to what a computer can understand about its users and how a user can interact with a computer this paper is not about if a computer will have the ability to feel and love its user but asks the question to what capacity will humans be able to reciprocate feelings to a machine how does intelligence quotient iq differ from emotional quotient eq an iq is a representational relationship of intelligence that measures cognitive abilities like learning understanding and dealing with new situations an eq is a method of measuring emotional intelligence and the ability to both use emotions and cognitive skills cherry advances in computer iq have been astonishing and have proved that machines are capable of answering difficult questions accurately are able to hold a conversation with human like understanding and allow for emotional connections between a human and machine the turing test in particular has shown the machines ability to think and even fool a person into believing that it is a human turing test explained in detail in section machines like deep blue watson eliza svetlana cleverbot and many more have all expanded the perceptions of what a computer is and can be if an increased computational iq can allow a human to computer relationship to feel more like a human to human interaction what would the advancement of computational eq bring us peter robinson a professor at the university of cambridge states that if a computer understands its users feelings that it can then respond with an interaction that is more intuitive for its users robinson in essence eq advocates feel that it can facilitate a more natural interaction process where collaboration can occur with a computer in alan turing s computing machinery and intelligence turing a variant on the classic british parlor imitation game is proposed the original game revolves around three players a man a a woman b and an interrogator the interrogator stays in a room apart from a and b and only can communicate to the participants through text based communication a typewriter or instant messenger style interface when the game begins one contestant a or b is asked to pretend to be the opposite gender and to try and convince the interrogator of this at the same time the opposing participant is given full knowledge that the other contestant is trying to fool the interrogator with alan turing s computational background he took this imitation game one step further by replacing one of the participants a or b with a machine thus making the investigator try and depict if he she was speaking to a human or machine in turing proposed that by the average interrogator would not have more than a percent chance of making the right identification after five minutes of questioning the turing test was first passed in with eliza by joseph weizenbaum a chat robot programmed to act like a rogerian psychotherapist weizenbaum in kenneth colby created a similar bot called parry that incorporated more personality than eliza and was programmed to act like a paranoid schizophrenic bowden since these initial victories for the test the st century has proven to continue to provide machines with more human like qualities and traits that have made people fall in love with them convinced them of being human and have human like reasoning brian christian the author of the most human human argues that the problem with designing artificial intelligence with greater ability is that even though these machines are capable of learning and speaking that they have no self they are mere accumulations of identities and thoughts that are foreign to the machine and have no central identity of their own he also argues that people are beginning to idealize the machine and admire machines capabilities more than their fellow humans in essence he argues humans are evolving to become more like machines with less of a notion of self christian turing states we like to believe that man is in some subtle way superior to the rest of creation and it is likely to be quite strong in intellectual people since they value the power of thinking more highly than others and are more inclined to base their belief in the superiority of man on this power if this is true will humans idealize the future of the machine for its intelligence or will they remain an inferior being as an object of our creation reversing the turing test allows us to understand how humans will treat machines when machines provide an equivalent emotional and intellectual capacity this also hits directly on jefferson lister s quote not until a machine can write a sonnet or compose a concerto because of thoughts and emotions felt and not by the chance fall of symbols could we agree that machine equals brain that is not only write it but know that it had written it participants were given a chat room simulation between two participants a a human interrogator and b a human disguised as a computer in this simulation a and b were both placed in different rooms to avoid influence and communicated through a text based interface a was informed that b was an advanced computer chat bot with the capacity to feel understand learn and speak like a human b was informed to be his or herself text based communication was chosen to follow turing s argument that a computers voice should not help an interrogator determine if it s a human or computer pairings of participants were chosen to participate in the interaction one at a time to avoid influence from other participants each experiment was five minutes in length to replicate turing s time restraints twenty eight graduate students were recruited from the nyu interactive telecommunications program to participate in the study male and female the experiment was evenly distributed across men and women after being recruited in person participants were directed to a website that gave instructions and ran the experiment upon entering the website a participants were told that we were in the process of evaluating an advanced cloud based computing system that had the capacity to feel emotion understand learn and converse like a human b participants were instructed that they would be communicating with another person through text and to be themselves they were also told that participant a thinks they are a computer but that they shouldn t act like a computer or pretend to be one in any way this allowed a to explicitly understand that they were talking to a computer while b knew a perspective and explicitly were not going to play the role of a computer participants were then directed to communicate with the bot or human freely without restrictions after five minutes of conversation the participants were asked to stop and then filled out a questionnaire participants were asked to rate iq and eq of the person they were conversing with a participants perceived the following of b iq not good barely acceptable okay great excellent iq average rating eq not good barely acceptable okay great excellent eq average rating ability to hold a conversation not good barely acceptable okay great excellent ability to hold a conversation average b participants perceived the following of a iq not good barely acceptable okay great excellent iq average rating eq not good barely acceptable okay great excellent eq average rating ability to hold a conversation not good barely acceptable okay great excellent ability to hold a conversation average overall a participants gave the perceived chabot higher ratings than b participants gave a in particular the highest rating was in regards to the chat bot s iq this data states that people viewed the chat bot to be more intellectually competent it also implies that people talking with bots decrease their iq eq and conversation ability when communicating with computers a participants were allowed to decide their username within the chat system to best reflect how they wanted to portray themselves to the machine b participants were designated the gender neutral name bot in an attempt to ganger gender perceptions for the machine the male to female ratio was divided evenly with all participants being male and being female a participants of the time thought b was a male a female and gender neutral on the other hand b participants of the time thought a was a male a female and gender neutral the usernames a chose are as follows hihi inessah somade willzing jihyun g ann divagrrl thisdoug jono minion p itslynnburke from these results it is clear that people associate the male gender and gender neutrality with machines it also demonstrates that people modify their identities when speaking with machines b participants were asked if they would like to pursue a friendship with the person they chatted with of participants responded affirmatively that they would indeed like to pursue a friendship while said maybe or no one response stated i would like to continue the conversation but i don t think i would be enticed to pursue a friendship another responded maybe i like people who are intellectually curious but i worry that the person might be a bit of a smart ass overall the participant disguised as a machine may or may not pursue a friendship after five minutes of text based conversation b participants were also asked if they felt a cared about their feelings stated that a indeed did care about their feelings stated that they weren t sure if a cared about their feelings and stated that a did not care about their feelings these results indicate a user s lack of attention to b s emotional state a participants were asked what they felt could be improved about the b participants the following improvements were noted should be funny give it a better sense of humor it can be better if he knows about my friends or preference the response was inconsistent and too slow it should share more about itself your algorithm is prime prude just like that letdown siri well i guess i liked it better but it should be more engaged and human consistency not after the first cold prompt it pushed me on too many questions i felt that it gave up on answering and the response time was a bit slow outsource the chatbot to fluent english speakers elsewhere and pretend they are bots if the responses are this slow to this many inquiries then it should be about the same experience i was very impressed with its parsing ability so far not as much with its reasoning i think some parameters for the conversation would help like ask a question maybe make the response faster i was confused at first because i asked a question waited a bit then asked another question waited and then got a response from the bot the responses from this indicate that even if a computer is a human that its user may not necessarily be fully satisfied with its performance the response implies that each user would like the machine to accommodate his or her needs in order to cause less personality and cognitive friction with several participant comments incorporating response time it also indicates people expect machines to have consistent response times humans clearly vary in speed when listening thinking and responding but it is expected of machines to act in a rhythmic fashion it also suggests that there is an expectation that a machine will answer all questions asked and will not ask its users more questions than perceived necessary a participants were asked if they felt b s artificial intelligence could improve their relationship to computers if integrated in their daily products of participants responded affirmatively that they felt this could improve their relationship well i think i prefer talking to a person better but yes for ipod smart phones etc would be very handy for everyday use products yes especially iphone is always with me so it can track my daily behaviors that makes the algorithm smarter possibly i should have queries it for information that would have been more relevant to me absolutely yes the which responded negatively had doubts that it would be necessary or desirable not sure it might creep me out if it were i like siri as much as the next gal but honestly we re approaching the uncanny valley now its not clear to me why this type of relationship needs to improve i think human relationships still need a lot of work nope i still prefer flesh sacks no the findings of the paper are relevant to the future of affective computation whether a super computer with a human like iq and eq can improve the human to computer interaction the uncertainty of computational equivalency that turing brought forth is indeed an interesting starting point to understand what we want out of the future of computers the responses from the experiment affirm gender perceptions of machines and show how we display ourselves to machines it seems that we limit our intelligence limit our emotions and obscure our identities when communicating to a machine this leads us to question if we would want to give our true self to a computer if it doesn t have a self of its own it also could indicate that people censor themselves for machines because they lack a similarity that bonds humans to humans or that there s a stigma associated with placing information in a digital device the inverse relationship is also shown through the data that people perceive a bots iq eq and discussion ability to be high even though the chat bot was indeed a human this data can imply humans perceive bots to not have restrictions and to be competent at certain procedures the results also imply that humans aren t really sure what they want out of artificial intelligence in the future and that we are not certain that an affective computer would even enjoy a users company and or conversation the results also state that we currently think of computers as a very personal device that should be passive not active but reactive when interacted with it suggests a consistent reliability we expect upon machines and that we expect to take more information from a machine than it takes from us a major limitation of this experiment is the sample size and sample diversity the sample size of twenty eight students is too small to fully understand and gather a stable result set it was also only conducted with nyu interactive telecommunications students who all have extensive experience with computers and technology to get a more accurate assessment of emotions a more diverse sample range needs to be taken five minutes is a short amount of time to create an emotional connection or friendship to stay true to the turing tests limitations this was enforced but further relational understanding could be understood if more time was granted beside the visual interface of the chat window it would be important to show the emotions of participant b through a virtual avatar not having this visual feedback could have limited emotional resonance with participants a time is also a limitation people aren t used to speaking to inquisitive machines yet and even through a familiar interface a chat room many participants haven t held conversations with machines previously perhaps if chat bots become more active conversational participants in commercial applications users will feel less censored to give themselves to the conversation in addition to the refinements noted in the limitations described above there are several other experiments for possible future studies for example investigating a long term human to bot relationship this would provide a better understanding toward the emotions a human can share with a machine and how a machine can reciprocate these emotions it would also better allow computer scientists to understand what really creates a significant relationship when physical limitations are present future studies should attempt to push these results further by understanding how a larger sample reacts to a computer algorithm with higher intellectual and emotional understanding it should also attempt to understand the boundaries of emotional computing and what is ideal for the user and what is ideal for the machine without compromising either parties capacities this paper demonstrates the diverse range of emotions that people can feel for affective computation and indicates that we are not in a time where computational equivalency is fully desired or accepted positive reactions indicate that there is optimism for more adept artificial intelligence and that there is interest in the field for commercial use it also provides insight that humans limit themselves when communicating with machines and that inversely machines don t limit themselves when communicating with humans books articlesbowden m minds as machine a history of cognitive science oxford university press christian b the most human human marvin m the emotion machine commonsense thinking artificial intelligence and the future of the human mind simon schuster paperbacks nass c brave s wired for speech how voice activates and advances the human computer relationship mit press nass c brave s hutchinson k computers that care investigating the effects of orientation of emotion exhibited by an embodied computer agent human computer studies elsevier picard r affective computing mit press searle j minds brains and programs cambridge university press turing a computing machinery and intelligence mind stor wilson r keil f the mit encyclopedia of the cognitive sciences mit press weizenbaum j eliza a computer program for the study of natural language communication between man and machine communications of the acm websites cherry k what is emotional intelligence http psychology about com od personalitydevelopment a emotionalintell htm epstein r clever bots radio lab http www radiolab org may clever bots ibm deep blue ibm http www research ibm com deepblue ibm watson ibm http www ibm com innovation us watson index html leavitt d i took the turing test new york times http www nytimes com books review book review the most human human by brian christian html personal robotics group nexi mit http robotic media mit edu robinson p the emotional computer camrbidge ideas http www cam ac uk research news the emotional computer us census bereau households with a computer and internet use to http www census gov hhes computer s eliza mit http www manifestation com neurotoys eliza php from a quick cheer to a standing ovation clap to show how much you enjoyed this story
Netflix Technology Blog,330,11,https://medium.com/netflix-techblog/system-architectures-for-personalization-and-recommendation-e081aa94b5d8?source=tag_archive---------0----------------,System Architectures for Personalization and Recommendation,by xavier amatriain and justin basilico in our previous posts about netflix personalization we highlighted the importance of using both data and algorithms to create the best possible experience for netflix members we also talked about the importance of enriching the interaction and engaging the user with the recommendation system today we re exploring another important piece of the puzzle how to create a software architecture that can deliver this experience and support rapid innovation coming up with a software architecture that handles large volumes of existing data is responsive to user interactions and makes it easy to experiment with new recommendation approaches is not a trivial task in this post we will describe how we address some of these challenges at netflix to start with we present an overall system diagram for recommendation systems in the following figure the main components of the architecture contain one or more machine learning algorithms the simplest thing we can do with data is to store it for later offline processing which leads to part of the architecture for managing offline jobs however computation can be done offline nearline or online online computation can respond better to recent events and user interaction but has to respond to requests in real time this can limit the computational complexity of the algorithms employed as well as the amount of data that can be processed offline computation has less limitations on the amount of data and the computational complexity of the algorithms since it runs in a batch manner with relaxed timing requirements however it can easily grow stale between updates because the most recent data is not incorporated one of the key issues in a personalization architecture is how to combine and manage online and offline computation in a seamless manner nearline computation is an intermediate compromise between these two modes in which we can perform online like computations but do not require them to be served in real time model training is another form of computation that uses existing data to generate a model that will later be used during the actual computation of results another part of the architecture describes how the different kinds of events and data need to be handled by the event and data distribution system a related issue is how to combine the different signals and models that are needed across the offline nearline and online regimes finally we also need to figure out how to combine intermediate recommendation results in a way that makes sense for the user the rest of this post will detail these components of this architecture as well as their interactions in order to do so we will break the general diagram into different sub systems and we will go into the details of each of them as you read on it is worth keeping in mind that our whole infrastructure runs across the public amazon web services cloud as mentioned above our algorithmic results can be computed either online in real time offline in batch or nearline in between each approach has its advantages and disadvantages which need to be taken into account for each use case online computation can respond quickly to events and use the most recent data an example is to assemble a gallery of action movies sorted for the member using the current context online components are subject to an availability and response time service level agreements sla that specifies the maximum latency of the process in responding to requests from client applications while our member is waiting for recommendations to appear this can make it harder to fit complex and computationally costly algorithms in this approach also a purely online computation may fail to meet its sla in some circumstances so it is always important to think of a fast fallback mechanism such as reverting to a precomputed result computing online also means that the various data sources involved also need to be available online which can require additional infrastructure on the other end of the spectrum offline computation allows for more choices in algorithmic approach such as complex algorithms and less limitations on the amount of data that is used a trivial example might be to periodically aggregate statistics from millions of movie play events to compile baseline popularity metrics for recommendations offline systems also have simpler engineering requirements for example relaxed response time slas imposed by clients can be easily met new algorithms can be deployed in production without the need to put too much effort into performance tuning this flexibility supports agile innovation at netflix we take advantage of this to support rapid experimentation if a new experimental algorithm is slower to execute we can choose to simply deploy more amazon ec instances to achieve the throughput required to run the experiment instead of spending valuable engineering time optimizing performance for an algorithm that may prove to be of little business value however because offline processing does not have strong latency requirements it will not react quickly to changes in context or new data ultimately this can lead to staleness that may degrade the member experience offline computation also requires having infrastructure for storing computing and accessing large sets of precomputed results nearline computation can be seen as a compromise between the two previous modes in this case computation is performed exactly like in the online case however we remove the requirement to serve results as soon as they are computed and can instead store them allowing it to be asynchronous the nearline computation is done in response to user events so that the system can be more responsive between requests this opens the door for potentially more complex processing to be done per event an example is to update recommendations to reflect that a movie has been watched immediately after a member begins to watch it results can be stored in an intermediate caching or storage back end nearline computation is also a natural setting for applying incremental learning algorithms in any case the choice of online nearline offline processing is not an either or question all approaches can and should be combined there are many ways to combine them we already mentioned the idea of using offline computation as a fallback another option is to precompute part of a result with an offline process and leave the less costly or more context sensitive parts of the algorithms for online computation even the modeling part can be done in a hybrid offline online manner this is not a natural fit for traditional supervised classification applications where the classifier has to be trained in batch from labeled data and will only be applied online to classify new inputs however approaches such as matrix factorization are a more natural fit for hybrid online offline modeling some factors can be precomputed offline while others can be updated in real time to create a more fresh result other unsupervised approaches such as clustering also allow for offline computation of the cluster centers and online assignment of clusters these examples point to the possibility of separating our model training into a large scale and potentially complex global model training on the one hand and a lighter user specific model training or updating phase that can be performed online much of the computation we need to do when running personalization machine learning algorithms can be done offline this means that the jobs can be scheduled to be executed periodically and their execution does not need to be synchronous with the request or presentation of the results there are two main kinds of tasks that fall in this category model training and batch computation of intermediate or final results in the model training jobs we collect relevant existing data and apply a machine learning algorithm produces a set of model parameters which we will henceforth refer to as the model this model will usually be encoded and stored in a file for later consumption although most of the models are trained offline in batch mode we also have some online learning techniques where incremental training is indeed performed online batch computation of results is the offline computation process defined above in which we use existing models and corresponding input data to compute results that will be used at a later time either for subsequent online processing or direct presentation to the user both of these tasks need refined data to process which usually is generated by running a database query since these queries run over large amounts of data it can be beneficial to run them in a distributed fashion which makes them very good candidates for running on hadoop via either hive or pig jobs once the queries have completed we need a mechanism for publishing the resulting data we have several requirements for that mechanism first it should notify subscribers when the result of a query is ready second it should support different repositories not only hdfs but also s or cassandra for instance finally it should transparently handle errors allow for monitoring and alerting at netflix we use an internal tool named hermes that provides all of these capabilities and integrates them into a coherent publish subscribe framework it allows data to be delivered to subscribers in near real time in some sense it covers some of the same use cases as apache kafka but it is not a message event queue system regardless of whether we are doing an online or offline computation we need to think about how an algorithm will handle three kinds of inputs models data and signals models are usually small files of parameters that have been previously trained offline data is previously processed information that has been stored in some sort of database such as movie metadata or popularity we use the term signals to refer to fresh information we input to algorithms this data is obtained from live services and can be made of user related information such as what the member has watched recently or context data such as session device date or time our goal is to turn member interaction data into insights that can be used to improve the member s experience for that reason we would like the various netflix user interface applications smart tvs tablets game consoles etc to not only deliver a delightful user experience but also collect as many user events as possible these actions can be related to clicks browsing viewing or even the content of the viewport at any time events can then be aggregated to provide base data for our algorithms here we try to make a distinction between data and events although the boundary is certainly blurry we think of events as small units of time sensitive information that need to be processed with the least amount of latency possible these events are routed to trigger a subsequent action or process such as updating a nearline result set on the other hand we think of data as more dense information units that might need to be processed and stored for later use here the latency is not as important as the information quality and quantity of course there are user events that can be treated as both events and data and therefore sent to both flows at netflix our near real time event flow is managed through an internal framework called manhattan manhattan is a distributed computation system that is central to our algorithmic architecture for recommendation it is somewhat similar to twitter s storm but it addresses different concerns and responds to a different set of internal requirements the data flow is managed mostly through logging through chukwa to hadoop for the initial steps of the process later we use hermes as our publish subscribe mechanism the goal of our machine learning approach is to come up with personalized recommendations these recommendation results can be serviced directly from lists that we have previously computed or they can be generated on the fly by online algorithms of course we can think of using a combination of both where the bulk of the recommendations are computed offline and we add some freshness by post processing the lists with online algorithms that use real time signals at netflix we store offline and intermediate results in various repositories to be later consumed at request time the primary data stores we use are cassandra evcache and mysql each solution has advantages and disadvantages over the others mysql allows for storage of structured relational data that might be required for some future process through general purpose querying however the generality comes at the cost of scalability issues in distributed environments cassandra and evcache both offer the advantages of key value stores cassandra is a well known and standard solution when in need of a distributed and scalable no sql store cassandra works well in some situations however in cases where we need intensive and constant write operations we find evcache to be a better fit the key issue however is not so much where to store them as to how to handle the requirements in a way that conflicting goals such as query complexity read write latency and transactional consistency meet at an optimal point for each use case in previous posts we have highlighted the importance of data models and user interfaces for creating a world class recommendation system when building such a system it is critical to also think of the software architecture in which it will be deployed we want the ability to use sophisticated machine learning algorithms that can grow to arbitrary complexity and can deal with huge amounts of data we also want an architecture that allows for flexible and agile innovation where new approaches can be developed and plugged in easily plus we want our recommendation results to be fresh and respond quickly to new data and user actions finding the sweet spot between these desires is not trivial it requires a thoughtful analysis of requirements careful selection of technologies and a strategic decomposition of recommendation algorithms to achieve the best outcomes for our members we are always looking for great engineers to join our team if you think you can help us be sure to look at our jobs page originally published at techblog netflix com on march from a quick cheer to a standing ovation clap to show how much you enjoyed this story learn more about how netflix designs builds and operates our systems and engineering organizations learn about netflix s world class engineering efforts company culture product developments and more
James Faghmous ,187,6,https://medium.com/@nomadic_mind/new-to-machine-learning-avoid-these-three-mistakes-73258b3848a4?source=tag_archive---------1----------------,New to Machine Learning? Avoid these three mistakes,machine learning ml is one of the hottest fields in data science as soon as ml entered the mainstream through amazon netflix and facebook people have been giddy about what they can learn from their data however modern machine learning i e not the theoretical statistical learning that emerged in the s is very much an evolving field and despite its many successes we are still learning what exactly can ml do for data practitioners i gave a talk on this topic earlier this fall at northwestern university and i wanted to share these cautionary tales with a wider audience machine learning is a field of computer science where algorithms improve their performance at a certain task as more data are observed to do so algorithms select a hypothesis that best explains the data at hand with the hope that the hypothesis would generalize to future unseen data take the left panel in the figure in the header the crosses denote the observed data projected in a two dimensional space in this case house prices and their corresponding size in square meters the blue line is the algorithm s best hypothesis to explain the observed data it states there is a linear relationship between the price and size of a house as the house s size increases so does its price in linear increments now using this hypothesis i can predict the price of an unseen datapoint based on its size as the dimensions of the data increase the hypotheses that explain the data become more complex however given that we are using a finite sample of observations to learn our hypothesis finding an adequate hypothesis that generalizes to unseen data is nontrivial there are three major pitfalls one can fall into that will prevent you from having a generalizable model and hence the conclusions of your hypothesis will be in doubt occam s razor is a principle attributed to william of occam a th century philosopher occam s razor advocates for choosing the simplest hypothesis that explains your data yet no simpler while this notion is simple and elegant it is often misunderstood to mean that we must select the simplest hypothesis possible regardless of performance in their paper in nature johan nyberg and colleagues used a level artificial neural network to predict seasonal hurricane counts using two or three environmental variables the authors reported stellar accuracy in predicting seasonal north atlantic hurricane counts however their model violates occam s razor and most certainly doesn t generalize to unseen data the razor was violated when the hypothesis or model selected to describe the relationship between environmental data and seasonal hurricane counts was generated using a four layer neural network a four layer neural network can model virtually any function no matter how complex and could fit a small dataset very well but fail to generalize to unseen data the rightmost panel in the top figure shows such incident the hypothesis selected by the algorithm the blue curve to explain the data is so complex that it fits through every single data point that is for any given house size in the training data i can give you with pinpoint accuracy the price it would sell for it doesn t take much to observe that even a human couldn t be that accurate we could give you a very close estimate of the price but to predict the selling price of a house within a few dollars every single time is impossible the pitfall of selecting too complex a hypothesis is known as overfitting think of overfitting as memorizing as opposed to learning if you are a child and you are memorizing how to add numbers you may memorize the sums of any pair of integers between and however when asked to calculate you will be unable to because you have never seen or and therefore couldn t memorize their sum that s what happens to an overfitted model it gets too lazy to learn the general principle that explains the data and instead memorizes the data data leakage occurs when the data you are using to learn a hypothesis happens to have the information you are trying to predict the most basic form of data leakage would be to use the same data that we want to predict as input to our model e g use the price of a house to predict the price of the same house however most often data leakage occurs subtly and inadvertently for example one may wish to learn for anomalies as opposed to raw data that is a deviations from a long term mean however many fail to remove the test data before computing the anomalies and hence the anomalies carry some information about the data you want to predict since they influenced the mean and standard deviation before being removed the are several ways to avoid data leakage as outlined by claudia perlich in her great paper on the subject however there is no silver bullet sometimes you may inherit a corrupt dataset without even realizing it one way to spot data leakage is if you are doing very poorly on unseen independent data for example say you got a dataset from someone that spanned but you started collecting you own data from onward if your model s performance is poor on the newly collected data it may be a sign of data leakage you must resist the urge to retrain the model with both the potentially corrupt and new data instated either try to identify the causes of poor performance on the new data or better yet independently reconstruct the entire dataset as a rule of thumb your best defense is to always be mindful of the possibility of data leakage in any dataset sampling bias is the case when you shortchange your model by training it on a biased or non random dataset which results in a poorly generalizable hypothesis in the case of housing prices sampling bias occurs if for some reason all the house prices sizes you collected were of huge mansions however when it was time to test your model and the first price you needed to predict was that of a bedroom apartment you couldn t predict it sampling bias happens very frequently mainly because as humans we are notorious for being biased nonrandom samplers one of the most common examples of this bias happens in startups and investing if you attend any business school course they will use all these case studies of how to build a successful company such case studies actually depict the anomalies and not the norm as most companies fail for every apple that became a success there were other startups that died trying so to build an automated data driven investment strategy you would need samples from both successful and unsuccessful companies the figure above figure is a concrete example of sampling bias say you want to predict whether a tornado is going to originate at certain location based on two environmental conditions wind shear and convective available potential energy cape we don t have to worry about what these variables actually mean but figure shows the wind shear and cape associated with tornado cases we can fit a model to these data but it will certainly not generalize because we failed to include shear and cape values when tornados did not occur in order for our model to separate between positive tornados and negative no tornados events we must train it using both populations there you have it being mindful of these limitations does not guarantee that your ml algorithm will solve all your problems but it certainly reduces the risk of being disappointed when your model doesn t generalize to unseen data now go on young jedi train your model you must from a quick cheer to a standing ovation clap to show how much you enjoyed this story nomadic mind sometimes the difference between success and failure is the same as between and living is in the details
Datafiniti,3,5,https://blog.datafiniti.co/classifying-websites-with-neural-networks-39123a464055?source=tag_archive---------2----------------,Classifying Websites with Neural Networks – Knowledge from Data: The Datafiniti Blog,at datafiniti we have a strong need for converting unstructured web content into structured data for example we d like to find a page like and do the following both of these are hard things for a computer to do in an automated manner while it s easy for you or me to realize that the above web page is selling some jeans a computer would have a hard time making the distinction from the above page from either of the following web pages or both of these pages share many similarities to the actual product page but also have many key differences the real challenge though is that if we look at the entire set of possible web pages those similarities and differences become somewhat blurred which means hard and fast rules for classifications will fail often in fact we can t even rely on just looking at the underlying html since there are huge variations in how product pages are laid out in html while we could try and develop a complicated set of rules to account for all the conditions that perfectly identify a product page doing so would be extremely time consuming and frankly incredibly boring work instead we can try using a classical technique out of the artificial intelligence handbook neural networks here s a quick primer on neural networks let s say we want to know whether any particular mushroom is poisonous or not we re not entirely sure what determines this but we do have a record of mushrooms with their diameters and heights along with which of these mushrooms were poisonous to eat for sure in order to see if we could use diameter and heights to determine poisonous ness we could set up the following equation a diameter b height or for not poisonous poisonous we would then try various combinations of a and b for all possible diameters and heights until we found a combination that correctly determined poisonous ness for as many mushrooms as possible neural networks provide a structure for using the output of one set of input data to adjust a and b to the most likely best values for the next set of input data by constantly adjusting a and b this way we can quickly get to the best possible values for them in order to introduce more complex relationships in our data we can introduce hidden layers in this model which would end up looking something like for a more detailed explanation of neural networks you can check out the following links in our product page classifier algorithm we setup a neural network with input layer with nodes hidden layer with nodes and output layer with output nodes our input layer modeled several features including our output layer had the following our algorithm for the neural network took the following steps the ultimate output is two sets of input layers t and t that we can use in a matrix equation to predict page type for any given web page this works like so so how did we do in order to determine how successful we were in our predictions we need to determine how to measure success in general we want to measure how many true positive tp results as compared to false positives fp and false negatives fn conventional measurements for these are our implementation had the following results these scores are just over our training set of course the actual scores on real life data may be a bit lower but not by much this is pretty good we should have an algorithm on our hands that can accurately classify product pages about of the time of course identifying product pages isn t enough we also want to pull out the actual structured data in particular we re interested in product name price and any unique identifiers e g upc ean isbn this information would help us fill out our product search we don t actually use neural networks for doing this neural networks are better suited toward classification problems and extracting data from a web page is a different type of problem instead we use a variety of heuristics specific to each attribute we re trying to extract for example for product name we look at the h and h tags and use a few metrics to determine the best choice we ve been able to achieve around a accuracy here we may go into the actual metrics and methodology for developing them in a separate post we feel pretty good about our ability to classify and extract product data the extraction part could be better but it s steadily being improved in the meantime we re also working on classifying other types of pages such as business data company team pages event data and more as we roll out these classifiers and data extractors we re including each one in our crawl of the entire internet this means that we can scan the entire internet and pull out any available data that exists out there exciting stuff you can connect with us and learn more about our business people product and property apis and datasets by selecting one of the options below from a quick cheer to a standing ovation clap to show how much you enjoyed this story instant access to web data building the world s largest database of web data follow our journey
Arjan Haring 🔮🔨,3,8,https://medium.com/i-love-experiments/reinventing-social-sciences-in-the-era-of-big-data-d255f3e391f3?source=tag_archive---------3----------------,Reinventing Social Sciences in the Era of Big Data – I love experiments – Medium,sune lehmann is an associate professor at dtu informatics technical university of denmark in the past he has worked as a postdoctoral fellow at institute for quantitative social science at harvard university and the college of computer and information science at northeasthern university before that he was at laszlo baraba si s center for complex network research at northeastern university and the center for cancer systems biology at the dana farber cancer institute i wouldn t call him stupid he is okay well he is actually pretty great forget that he is freaking fantastic we should get him over for one of our events and so we did sune spoke at the nd projectwaalhalla this time let s begin at the beginning before we dive in deeper your main research project has to do with measuring real social networks with high resolution i know for a fact you don t mean d printed social networks but what are you aiming for and how are you going to get there my humble research goal is to reinvent social sciences in the age of big data my background is in mathematical analysis of large networks but over the past years i ve slowly grown more and more interested in understanding social systems as a scientist i was blown away by the promise of all of the digital traces of human behavior collected as a consequence of cheap hard drives and databases everywhere but in spite of the promise of big data the results so far have been less exciting than i had hoped for all the hype deep new scientific insights from big data are far and few between a central hypothesis in my work is that in order to advance our quantitative understanding of social interaction we cannot get by with noisy incomplete big data we need good data let me explain why and use my own field as an example let s say you have a massive cell phone data set from a telco that provides service to or the population of a large country of million people that s something like million people and easily terabytes of monthly data so a massive dataset but when you start thinking about the network you run into problems the standard approach is to simply look at the network between the individuals in your sample assuming that people are randomly sampled and links are randomly distributed you realize that of the population corresponds to only of the links is of cell phone calls enough to understand how the network works with only one in ten links remaining in the dataset the social structure almost completely erased and it gets worse telecommunication is only one small biased aspect of human communication human interactions may also unfold face to face via text message email facebook skype etc and these streams are collected in silos where we cannot generally identify individuals entities across datasets so if we think about all these ways we can communicate access to only one in ten of my cell phone contacts is very likely insufficient for making valid inferences and the worst part is that we can t know without access to the full data set we can t even tell what we can and can t tell from a sample so when i started out as an assistant professor i decided to change the course of my career and move from sitting comfortably in front of my computer as a computational theoretical scientist to becoming an experimenter to try and attack this problem head on now a few of years later we have put together a dataset of human social interactions that is unparalleled in terms of quality and size we recording social interactions within more than students at my university using top of the line cell phones as censors we can capture detailed interaction patterns such as face to face via bluetooth social network data e g facebook and twitter via apps telecommunication data from call logs and geolocation via gps wifi we like to call this type of data deep data a densely connected group of participants all the links observations across many communication channels high frequency observations minute by minute scale but with long observation windows years of collection and with behavioral data supplemented by classic questionnaires as well as the possibility of running intervention experiments but my expertise and ultimate interest is not in building a deep data collection platform although that has been a lot of fun i want to get back to the questions that motivated the enthusiasm for computational social science in the first place reinventing social sciences is what it s all about what can we learn from just one channel now that we know about all the communication channels we can begin to understand what kind of things one may learn from a single channel let s get quantitative about the usefulness of e g large cell phone data sets or facebook when that s the only data available my heart is still with the network science in some ways this whole project is designed to build a system that will really take us places in terms of modeling human social networks lots of network science is still about unweighted undirected static networks we are already using this dataset to create better models for dynamic multiplex networks understanding spreading processes influence behavior disease etc is a central goal if we look a bit forward in time we have an system where n is big enough to perform intervention experiments with randomized controls etc we re still far from implementing this goal but we re working on finding the right questions and working closely with social scientists to get our protocols for these questions just right what a coincidence we are all about modeling behavior and learning across channels and with contagionapi prominently on our product roadmap we want to start dabbling with spreading processes as well in the near future what would you say were major challenges the last years in modeling behavior and what do see as biggest challenges opportunities for the future there are many challenges although we ve made amazing progress in network science for example it s still a fact that our fundamental understanding of dynamic multi channel networks is still in its infancy there aren t a lot of easily interpretable models that really explain the underlying networks so that s an area with lots of challenges and corresponding opportunities and when we want to figure out questions about things taking place on networks we run into all kinds of problems about how to do statistics right brilliant statisticians have shown that homophily and contagion are generically confounded in observational social network studies on that front guys like sinan aral are doing really exciting work using interventions to get at some of the issues but there is still lots to do in that area finally privacy is a big issue we re working closely with collaborators at the mit medialab to develop new responsible solutions and we ve already gotten far on that topic but in terms of data sharing that respects the privacy of study participants there is still a long way to go but since studies of digital traces of human behavior will not be going away anytime soon we have to make progress in this area and oh yeah why does this all matter and should we be concerned by these things i think there are many reasons to be concerned and excited the more we learn about how systems work the more we are able to influence them to control them that is also true for systems of humans if we think about spreading of disease it d be great to know how to slow down or stop the spread of sars or similar contagious viruses or as a society we may be able to increase spread of things we support such as tolerance good exercise habits etc and similarly we can use an understanding influence in social systems to inhibit negative behavior such as intolerance smoking etc and all this ties into another good reason to be concerned companies like google facebook apple or governmental agencies like nsa are committing serious resources to research in this area it s not a coincidence that both google and facebook are developing their own cell phones but none of these walled off players are sharing their results they re simply applying them to the public in my opinion that s one of the key problems of the current state of affairs the imbalance of information we hand over our personal data to powerful corporations but have nearly zero insight into a what they know about us and b what they re doing with all the stuff they know about us by doing research that is open collaborative explicit about privacy and public i hope we can act as a counter point and work to diminish the information gap okay great but should companies be interested in the stuff you are doing and if so why i think so one of the exciting things about this area is that basic research is very close to applied research insight into the mechanisms that drive human nature is indeed valuable for companies i presume that s why science rockstars exists for example note from the editor not stupid at all we already know that human behavior can be influenced significantly with nudging that certain kinds of collective behaviors influence our opinions and purchasing behaviors the more we uncover about the details of these mechanism the more precise and effective we can be about influencing others let s discuss the ethics of this another time but it s not just marketing if used for good this is the science of what makes people happy so inside organizations work like this could be used to re think organizational structures incentives etc to make employees happier more fulfilled or if we think about organizations as organisms having access to realtime information about employees can be thought of as a nervous system for the company allowing for faster reaction times when crises arise identification of pain points etc finally for the medical field we know that genes only explain part of what makes us sick being able to quantify and analyze behavior means knowing more about the environment the nurture part of nurture vs nature in that sense detailed data on how we behave could also help us understand how to be healthier originally published at www sciencerockstars com on november from a quick cheer to a standing ovation clap to show how much you enjoyed this story let s fix the future scientific advisor jadatascience a blog series about the discipline of business experimentation how to run and learn from experiments in different contexts is a complex matter but lays at the heart of innovation
Eventbrite,10,8,https://medium.com/@eventbrite/multi-index-locality-sensitive-hashing-for-fun-and-profit-ee04292a6e37?source=tag_archive---------4----------------,Multi-Index Locality Sensitive Hashing for Fun and Profit,one way that we deal with this volume of data is to cluster up all the similar messages together to find patterns in behavior of senders for example if someone is contacting thousands of different organizers with similar messages that behavior is suspect and will be examined the big question is how can we compare every single message we see with every other message efficiently and accurately in this article we ll be exploring a technique known as multi index locality sensitive hashing to perform the the comparison efficiently we pre process the data with a series of steps let s first define what similar messages are here we have and example of two similar messages a and b to our human eyes of course they re similar but we want determine this similarity quantitatively the solution is to break up the message into tokens and then treat each message as a bag of tokens the simplest naive way to do tokenization is to split up a message on spaces punctuation and convert each character to lowercase so our result from our tokenization of the above messages would be i ll leave as an exercise to the reader to come up with more interesting ways to do tokenization for handling contractions plurals foreign languages etc to calculate the similarity between these two bags of tokens we ll use an estimation known as the jaccard similarity coefficient this is defined as the ratio of sizes of the intersection and union of a and b therefore in our example we ll then set a threshold above which we will consider two messages to be similar so then when given a set of m messages we simply compute the similarity of a message to every other message this works in theory but in practice there are cases where this metric is unreliable eg if one message is significantly longer than the other not to mention horribly inefficient o n m where n is the number of tokens per message we need do things smarter one problem with doing a simple jaccard similarity is that the scale of the value changes with the size number of tokens of the message to address this we can transform our tokens with a method known as minhash here s a psuedo code snippet the interesting property of the minhash transformation is that it leaves us with a constant n number of hashes and that chosen hashes will be in the same positions in the vector after the minhash transformation the jaccard similarity can be approximated by an element wise comparison of two hash vectors implemented as pseudo code above so we can stop here but we re having so much fun and we can do so much better notice when we do comparison we have to to o n integer comparisons and if we have m messages then comparing every message to each other is o n m integer comparisons this is still not acceptable to reduce the time complexity of comparing minhashes to each other we can do better with a technique known as bit sampling the main idea is that we don t need to know the exact value of each hash but only that the hashes are equal at their respective positions in each hash vector with this insight let s only look at the least significant bit lsb of each hash value more pseudo code when comparing two messages if the hashes are equal in the same position in the minhash vector then the bits in the equivalent position after bit sampling should be also equal so we can emulate the jaccard similarity of two minhashes by counting the equal bits in the two bit vectors aka the hamming distance and dividing by the number of bits of course two different hashes will have the same lsb of the time to increase our efficacy we would pick a large n initially here is some naive and inefficient pseudo code in practice more efficient implementations of the bitsimilarity function can calculate in near o time for reasonable sizes of n bit twiddling hacks this means that when comparing m messages to each other we ve reduced the time complexity to o m but wait there s more remember how i said we have a lot of data o m is still unreasonable when m is a very large number of messages so we need to try to reduce the number of comparisons to make using a divide and conquer strategy lets start with an example where we set n and we want to have a bitsimilarity of in the worst case to do this we need of the bits to be equal or bits unequal we will refer to the number of unequal bits as the radius of the bit vectors ie if two bit vectors are within a certain radius of bits then they are similar the unequal bits can be found by taking the bit wise xor of the two bit vectors for example if we split up xor mask into chunks of bits then at least one chunk will have exactly zero or exactly one of the bit differences pigeonhole principal more generally if we split xor mask of size n into k chunks with an expected radius r then at least one chunk is guaranteed to have floor r k or less bits unequal for the purpose of explanation we will assume that we have chosen all the parameters such that floor r k now you re wondering how this piece of logic help us we can now design a data structure lshtable to index the bit vectors to reduce the number of bitsimilarity comparisons drastically but increase memory consumption in o m fast search in hamming space with multi index hashing we will define lshtable with some pseudo code basically in lshtable initialization we create k hash tables for each k chunks during add of a bit vector we split the bit vector into k chunks for each of these chunks we add the original bit vector into the associated hash table under the index chunk upon the lookup of a bit vector we once again split it into chunks and for each chunk look up the associated hash table for a chunk that s close zero or one bits off the returned list is a set of candidate bit vectors to check bitsimilarity because of the property explained in the previous section at least one hash table will contain a set of candidates that contains a similar bit vector to compare every m message to every other message we first insert its bit vector into an lshtable an o k operation k is constant then to find similar messages we simply do a lookup from the lshtable another o k operation and then check bitsimilarity for each of the candidates returned the number of candidates to check is usually on the order of m n k if at all therefore the time complexity to compare all m messages to each other is o m m n k in practice n and k are empirically chosen such that n k m so the final time complexity is o m remember we started with o n m phew what a ride so we ve detailed how to find similar messages in a very large set of messages efficiently by using multi index locality sensitivity hashing we can reduce the time complexity of from quadratic with a very high constant to near linear with a more manageable constant i should also mention that many of the ancillary pseudo code excerpts used here describe the most naive implementation of each method and are for instructive purposes only from a quick cheer to a standing ovation clap to show how much you enjoyed this story we help bring the world together through live experiences
Akash Shende,1,3,https://medium.com/@akash0x53/color-based-object-segmentation-baf8044ec6a3?source=tag_archive---------7----------------,Color Based Object Segmentation – Akash Shende – Medium,in this picture pranav mistry is using color marker on his fingers to track the gesture and his wearable computer perform action based on gestures that sounds easy but no it s not computer need to understand those color marker first for that it needs to separate marker from any surroundings segmentation can be helpful to achieve this various methods are available for segmentation however this article talks about robust color based object segmentation create binary mask that separates blue t shirt from rest to find blue t shirt in given image i used opencv s inrange method which takes color or greyscale image lower higher range value as its parameter and returns binary image where pixel value set to when input pixel doesn t fall in specified range otherwise pixel value set to with the help of this function and after determining range values i ended up with this mask but you can see there are problems it s not able to create mask for complete t shirt also it mask eyes which aren t blue this is happening because light from one side of body whitens the right side at the same time creates shadow in left region thus it creates different shades of blue and results into partial segmentation normalization of color plane reduces variation in light by averaging pixel values thus it removes highlighted and shadowed region and make image flatten following image is free from highlights shadows and it is divided into one large green background blue t shirt and skin now the inrange method able to mask only t shirt following function converts a pixel at x y location into its corresponding normalized rgb pixel let r g b are pixel values then normalized pixel g x y is calculated as divide the individual color component by sum of all color components and multiply by division results into floating point number in range of to and as this is bit image result is scaled up by this function accepts bit rgb image matrix of size x and returns normalized rgb image originally published at akash x github io on april from a quick cheer to a standing ovation clap to show how much you enjoyed this story python
Hrishikesh Huilgolkar,1,4,https://medium.com/@hrishikeshio/traveling-santa-problem-an-incompetent-algorists-attempt-49ad9d26b26?source=tag_archive---------8----------------,Traveling santa Problem — An incompetent algorist’s attempt,kaggle announced the traveling santa problem in the christmas season i joined in excitedly but soon realized this is not an easy problem solving this problem would require expertise on data structures and some good familiarity with tsp problems and its many heuristic algorithms i had neither i had to find a way to deal with this problem i compenseted my lack of algorithmic expertise with common sense logic and intuition i finished th out of total competitors i did some research on packaged tsp solvers and top tsp algorithms i found concorde but i could not get it to work on my ubuntu machine so i settled with lkh which uses lin kernighan heuristic for solving tsp and related problems i wrote scripts for file conversions and for running lkh lkh easily solved my tsp problem in around hours but it was just one path i still had to figure out how to make it find the second path a simple idea to get disjoint paths is to generate first path and then make weight of those edges infinite and run lkh on the problem again but this required the problem to be in distance matrix format then i found a major problem problem ram too lowcreating distance matrix for points was unimaginable it would requirememory for one digit assuming memory for one digit bytes memory required which is gb correct me if i am wrong solution a simple solution was to divide the map in manageable chunks i used scipy s distance matrix creation function scipy spatial distance pdist it creates distance matrix from coordinates the matrix created by pdist is in compressed form a flattened matrix of upper diagonal elements scipy spatial distance squareform can create a square distance matrix from compressed matrix but that would waste a lot of ram so i created a custom function which divided compressed matrix by rows so lkh can read it input coordinates output of pdist compressed upper column output of squreform uncompressed square matrix output of my function which processed compressed matrix upper diagonal elements lots of ram saved i tried using manhattan distance instead of euclidean distance but after dividing the problem in grids time taken by distance calculation was manageable so i stuck with euclidean distance through trial and error i found that on my laptop with gb ram a by grid in the above format was manageable for both creating distance matrix and for lkh i ran lkh on resulting distance matrices and joined the individual solutions i joined the resulting solutions in different combination for both paths so as to avoid common paths i got with this method i tried time limit on lkh algorithm from seconds i reduced it to seconds but it made the results slightly worse mingle the solution above was good but it could have been better the problem was that the first path was so good that the second path struggled to find good path the difference between the two paths was big path mpath mfor a long time i thought this would require either solving both paths simultaneously or using genetic algorithm or similar algorithm to combine both paths both were pretty difficult to implement then i got a simple idea my map was divided in squares if i combine squares of first path and squares of second path i will have a path whose distance will be approximately average of the two paths i tried this trick and used different combinations of the two paths squares and got the best score of for new path select blue squres from old path and grey square from old path use remaining squares for new path remove cross lines my squares were joined in a zigzag manner i removed the zig zag lines for a further improvement i scored which was my best score another idea was to make end point of one square and the beginning point of next square as near as possible but i couldn t implement the idea before deadline my score was around points away from the first place which was not bad public repo https bitbucket org hrishikeshio traveling santa more documentation for source code coming soon originally published at www blogicious com on january from a quick cheer to a standing ovation clap to show how much you enjoyed this story blockchain cryptocurrencies and the decentralised future
Adam Geitgey,35000,15,https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471?source=tag_archive---------0----------------,Machine Learning is Fun! – Adam Geitgey – Medium,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in portugue s portugue s alternate tu rkc e franc ais espan ol me xico espan ol espan a polski italiano tie ng vie t or bigger update the content of this article is now available as a full length video course that walks you through every step of the code you can take the course for free and access everything else on lynda com free for days if you sign up with this link have you heard people talking about machine learning but only have a fuzzy idea of what that means are you tired of nodding your way through conversations with co workers let s change that this guide is for anyone who is curious about machine learning but has no idea where to start i imagine there are a lot of people who tried reading the wikipedia article got frustrated and gave up wishing someone would just give them a high level explanation that s what this is the goal is be accessible to anyone which means that there s a lot of generalizations but who cares if this gets anyone more interested in ml then mission accomplished machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem instead of writing code you feed data to the generic algorithm and it builds its own logic based on the data for example one kind of algorithm is a classification algorithm it can put data into different groups the same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not spam without changing a line of code it s the same algorithm but it s fed different training data so it comes up with different classification logic machine learning is an umbrella term covering lots of these kinds of generic algorithms you can think of machine learning algorithms as falling into one of two main categories supervised learning and unsupervised learning the difference is simple but really important let s say you are a real estate agent your business is growing so you hire a bunch of new trainee agents to help you out but there s a problem you can glance at a house and have a pretty good idea of what a house is worth but your trainees don t have your experience so they don t know how to price their houses to help your trainees and maybe free yourself up for a vacation you decide to write a little app that can estimate the value of a house in your area based on it s size neighborhood etc and what similar houses have sold for so you write down every time someone sells a house in your city for months for each house you write down a bunch of details number of bedrooms size in square feet neighborhood etc but most importantly you write down the final sale price using that training data we want to create a program that can estimate how much any other house in your area is worth this is called supervised learning you knew how much each house sold for so in other words you knew the answer to the problem and could work backwards from there to figure out the logic to build your app you feed your training data about each house into your machine learning algorithm the algorithm is trying to figure out what kind of math needs to be done to make the numbers work out this kind of like having the answer key to a math test with all the arithmetic symbols erased from this can you figure out what kind of math problems were on the test you know you are supposed to do something with the numbers on the left to get each answer on the right in supervised learning you are letting the computer work out that relationship for you and once you know what math was required to solve this specific set of problems you could answer to any other problem of the same type let s go back to our original example with the real estate agent what if you didn t know the sale price for each house even if all you know is the size location etc of each house it turns out you can still do some really cool stuff this is called unsupervised learning this is kind of like someone giving you a list of numbers on a sheet of paper and saying i don t really know what these numbers mean but maybe you can figure out if there is a pattern or grouping or something good luck so what could do with this data for starters you could have an algorithm that automatically identified different market segments in your data maybe you d find out that home buyers in the neighborhood near the local college really like small houses with lots of bedrooms but home buyers in the suburbs prefer bedroom houses with lots of square footage knowing about these different kinds of customers could help direct your marketing efforts another cool thing you could do is automatically identify any outlier houses that were way different than everything else maybe those outlier houses are giant mansions and you can focus your best sales people on those areas because they have bigger commissions supervised learning is what we ll focus on for the rest of this post but that s not because unsupervised learning is any less useful or interesting in fact unsupervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer side note there are lots of other types of machine learning algorithms but this is a pretty good place to start as a human your brain can approach most any situation and learn how to deal with that situation without any explicit instructions if you sell houses for a long time you will instinctively have a feel for the right price for a house the best way to market that house the kind of client who would be interested etc the goal of strong ai research is to be able to replicate this ability with computers but current machine learning algorithms aren t that good yet they only work when focused a very specific limited problem maybe a better definition for learning in this case is figuring out an equation to solve a specific problem based on some example data unfortunately machine figuring out an equation to solve a specific problem based on some example data isn t really a great name so we ended up with machine learning instead of course if you are reading this years in the future and we ve figured out the algorithm for strong ai then this whole post will all seem a little quaint maybe stop reading and go tell your robot servant to go make you a sandwich future human so how would you write the program to estimate the value of a house like in our example above think about it for a second before you read further if you didn t know anything about machine learning you d probably try to write out some basic rules for estimating the price of a house like this if you fiddle with this for hours and hours you might end up with something that sort of works but your program will never be perfect and it will be hard to maintain as prices change wouldn t it be better if the computer could just figure out how to implement this function for you who cares what exactly the function does as long is it returns the correct number one way to think about this problem is that the price is a delicious stew and the ingredients are the number of bedrooms the square footage and the neighborhood if you could just figure out how much each ingredient impacts the final price maybe there s an exact ratio of ingredients to stir in to make the final price that would reduce your original function with all those crazy if s and else s down to something really simple like this notice the magic numbers in bold and these are our weights if we could just figure out the perfect weights to use that work for every house our function could predict house prices a dumb way to figure out the best weights would be something like this start with each weight set to run every house you know about through your function and see how far off the function is at guessing the correct price for each house for example if the first house really sold for but your function guessed it sold for you are off by for that single house now add up the squared amount you are off for each house you have in your data set let s say that you had home sales in your data set and the square of how much your function was off for each house was a grand total of that s how wrong your function currently is now take that sum total and divide it by to get an average of how far off you are for each house call this average error amount the cost of your function if you could get this cost to be zero by playing with the weights your function would be perfect it would mean that in every case your function perfectly guessed the price of the house based on the input data so that s our goal get this cost to be as low as possible by trying different weights repeat step over and over with every single possible combination of weights whichever combination of weights makes the cost closest to zero is what you use when you find the weights that work you ve solved the problem that s pretty simple right well think about what you just did you took some data you fed it through three generic really simple steps and you ended up with a function that can guess the price of any house in your area watch out zillow but here s a few more facts that will blow your mind pretty crazy right ok of course you can t just try every combination of all possible weights to find the combo that works the best that would literally take forever since you d never run out of numbers to try to avoid that mathematicians have figured out lots of clever ways to quickly find good values for those weights without having to try very many here s one way first write a simple equation that represents step above now let s re write exactly the same equation but using a bunch of machine learning math jargon that you can ignore for now this equation represents how wrong our price estimating function is for the weights we currently have set if we graph this cost equation for all possible values of our weights for number of bedrooms and sqft we d get a graph that might look something like this in this graph the lowest point in blue is where our cost is the lowest thus our function is the least wrong the highest points are where we are most wrong so if we can find the weights that get us to the lowest point on this graph we ll have our answer so we just need to adjust our weights so we are walking down hill on this graph towards the lowest point if we keep making small adjustments to our weights that are always moving towards the lowest point we ll eventually get there without having to try too many different weights if you remember anything from calculus you might remember that if you take the derivative of a function it tells you the slope of the function s tangent at any point in other words it tells us which way is downhill for any given point on our graph we can use that knowledge to walk downhill so if we calculate a partial derivative of our cost function with respect to each of our weights then we can subtract that value from each weight that will walk us one step closer to the bottom of the hill keep doing that and eventually we ll reach the bottom of the hill and have the best possible values for our weights if that didn t make sense don t worry and keep reading that s a high level summary of one way to find the best weights for your function called batch gradient descent don t be afraid to dig deeper if you are interested on learning the details when you use a machine learning library to solve a real problem all of this will be done for you but it s still useful to have a good idea of what is happening the three step algorithm i described is called multivariate linear regression you are estimating the equation for a line that fits through all of your house data points then you are using that equation to guess the sales price of houses you ve never seen before based where that house would appear on your line it s a really powerful idea and you can solve real problems with it but while the approach i showed you might work in simple cases it won t work in all cases one reason is because house prices aren t always simple enough to follow a continuous line but luckily there are lots of ways to handle that there are plenty of other machine learning algorithms that can handle non linear data like neural networks or svms with kernels there are also ways to use linear regression more cleverly that allow for more complicated lines to be fit in all cases the same basic idea of needing to find the best weights still applies also i ignored the idea of overfitting it s easy to come up with a set of weights that always works perfectly for predicting the prices of the houses in your original data set but never actually works for any new houses that weren t in your original data set but there are ways to deal with this like regularization and using a cross validation data set learning how to deal with this issue is a key part of learning how to apply machine learning successfully in other words while the basic concept is pretty simple it takes some skill and experience to apply machine learning and get useful results but it s a skill that any developer can learn once you start seeing how easily machine learning techniques can be applied to problems that seem really hard like handwriting recognition you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data just feed in the data and watch the computer magically figure out the equation that fits the data but it s important to remember that machine learning only works if the problem is actually solvable with the data that you have for example if you build a model that predicts home prices based on the type of potted plants in each house it s never going to work there just isn t any kind of relationship between the potted plants in each house and the home s sale price so no matter how hard it tries the computer can never deduce a relationship between the two so remember if a human expert couldn t use the data to solve the problem manually a computer probably won t be able to either instead focus on problems where a human could solve the problem but where it would be great if a computer could solve it much more quickly in my mind the biggest problem with machine learning right now is that it mostly lives in the world of academia and commercial research groups there isn t a lot of easy to understand material out there for people who would like to get a broad understanding without actually becoming experts but it s getting a little better every day if you want to try out what you ve learned in this article i made a course that walks you through every step of this article including writing all the code give it a try if you want to go deeper andrew ng s free machine learning class on coursera is pretty amazing as a next step i highly recommend it it should be accessible to anyone who has a comp sci degree and who remembers a very minimal amount of math also you can play around with tons of machine learning algorithms by downloading and installing scikit learn it s a python framework that has black box versions of all the standard algorithms if you liked this article please consider signing up for my machine learning is fun newsletter also please check out the full length course version of this article it covers everything in this article in more detail including writing the actual code in python you can get a free day trial to watch the course if you sign up with this link you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Shivon Zilis,1200,10,https://medium.com/@shivon/the-current-state-of-machine-intelligence-f76c20db2fe1?source=tag_archive---------1----------------,The Current State of Machine Intelligence – Shivon Zilis – Medium,the machine intelligence landscape and post can be found here i spent the last three months learning about every artificial intelligence machine learning or data related startup i could find my current list has of them to be exact yes i should find better things to do with my evenings and weekends but until then why do this a few years ago investors and startups were chasing big data i helped put together a landscape on that industry now we re seeing a similar explosion of companies calling themselves artificial intelligence machine learning or somesuch collectively i call these machine intelligence i ll get into the definitions in a second our fund bloomberg beta which is focused on the future of work has been investing in these approaches i created this landscape to start to put startups into context i m a thesis oriented investor and it s much easier to identify crowded areas and see white space once the landscape has some sort of taxonomy what is machine intelligence anyway i mean machine intelligence as a unifying term for what others call machine learning and artificial intelligence some others have used the term before without quite describing it or understanding how laden this field has been with debates over descriptions i would have preferred to avoid a different label but when i tried either artificial intelligence or machine learning both proved to too narrow when i called it artificial intelligence too many people were distracted by whether certain companies were true ai and when i called it machine learning many thought i wasn t doing justice to the more ai esque like the various flavors of deep learning people have immediately grasped machine intelligence so here we are computers are learning to think read and write they re also picking up human sensory function with the ability to see and hear arguably to touch taste and smell though those have been of a lesser focus machine intelligence technologies cut across a vast array of problem types from classification and clustering to natural language processing and computer vision and methods from support vector machines to deep belief networks all of these technologies are reflected on this landscape what this landscape doesn t include however important is big data technologies some have used this term interchangeably with machine learning and artificial intelligence but i want to focus on the intelligence methods rather than data storage and computation pieces of the puzzle for this landscape though of course data technologies enable machine intelligence which companies are on the landscape i considered thousands of companies so while the chart is crowded it s still a small subset of the overall ecosystem admissions rates to the chart were fairly in line with those of yale or harvard and perhaps equally arbitrary i tried to pick companies that used machine intelligence methods as a defining part of their technology many of these companies clearly belong in multiple areas but for the sake of simplicity i tried to keep companies in their primary area and categorized them by the language they use to describe themselves instead of quibbling over whether a company used nlp accurately in its self description if you want to get a sense for innovations at the heart of machine intelligence focus on the core technologies layer some of these companies have apis that power other applications some sell their platforms directly into enterprise some are at the stage of cryptic demos and some are so stealthy that all we have is a few sentences to describe them the most exciting part for me was seeing how much is happening in the application space these companies separated nicely into those that reinvent the enterprise industries and ourselves if i were looking to build a company right now i d use this landscape to help figure out what core and supporting technologies i could package into a novel industry application everyone likes solving the sexy problems but there are an incredible amount of unsexy industry use cases that have massive market opportunities and powerful enabling technologies that are begging to be used for creative applications e g watson developer cloud alchemyapi reflections on the landscape we ve seen a few great articles recently outlining why machine intelligence is experiencing a resurgence documenting the enabling factors of this resurgence kevin kelly for example chalks it up to cheap parallel computing large datasets and better algorithms i focused on understanding the ecosystem on a company by company level and drawing implications from that yes it s true machine intelligence is transforming the enterprise industries and humans alike on a high level it s easy to understand why machine intelligence is important but it wasn t until i laid out what many of these companies are actually doing that i started to grok how much it is already transforming everything around us as kevin kelly more provocatively put it the business plans of the next startups are easy to forecast take x and add ai in many cases you don t even need the x machine intelligence will certainly transform existing industries but will also likely create entirely new ones machine intelligence is enabling applications we already expect like automated assistants siri adorable robots jibo and identifying people in images like the highly effective but unfortunately named deepface however it s also doing the unexpected protecting children from sex trafficking reducing the chemical content in the lettuce we eat helping us buy shoes online that fit our feet precisely and destroying s classic video games many companies will be acquired i was surprised to find that over of the eligible non public companies on the slide have been acquired it was in stark contrast to big data landscape we created which had very few acquisitions at the time no jaw will drop when i reveal that google is the number one acquirer though there were more than different acquirers just for the companies on this chart my guess is that by the end of almost another will be acquired for thoughts on which specific ones will get snapped up in the next year you ll have to twist my arm big companies have a disproportionate advantage especially those that build consumer products the giants in search google baidu social networks facebook linkedin pinterest content netflix yahoo mobile apple and e commerce amazon are in an incredible position they have massive datasets and constant consumer interactions that enable tight feedback loops for their algorithms and these factors combine to create powerful network effects and they have the most to gain from the low hanging fruit that machine intelligence bears best in class personalization and recommendation algorithms have enabled these companies success it s both impressive and disconcerting that facebook recommends you add the person you had a crush on in college and netflix tees up that perfect guilty pleasure sitcom now they are all competing in a new battlefield the move to mobile winning mobile will require lots of machine intelligence state of the art natural language interfaces like apple s siri visual search like amazon s firefly and dynamic question answering technology that tells you the answer instead of providing a menu of links all of the search companies are wrestling with this large enterprise companies ibm and microsoft have also made incredible strides in the field though they don t have the same human facing requirements so are focusing their attention more on knowledge representation tasks on large industry datasets like ibm watson s application to assist doctors with diagnoses the talent s in the new ai vy league in the last years most of the best minds in machine intelligence especially the hardcore ai types worked in academia they developed new machine intelligence methods but there were few real world applications that could drive business value now that real world applications of more complex machine intelligence methods like deep belief nets and hierarchical neural networks are starting to solve real world problems we re seeing academic talent move to corporate settings facebook recruited nyu professors yann lecun and rob fergus to their ai lab google hired university of toronto s geoffrey hinton baidu wooed andrew ng it s important to note that they all still give back significantly to the academic community one of lecun s lab mandates is to work on core research to give back to the community hinton spends half of his time teaching ng has made machine intelligence more accessible through coursera but it is clear that a lot of the intellectual horsepower is moving away from academia for aspiring minds in the space these corporate labs not only offer lucrative salaries and access to the godfathers of the industry but the most important ingredient data these labs offer talent access to datasets they could never get otherwise the imagenet dataset is fantastic but can t compare to what facebook google and baidu have in house as a result we ll likely see corporations become the home of many of the most important innovations in machine intelligence and recruit many of the graduate students and postdocs that would have otherwise stayed in academia there will be a peace dividend big companies have an inherent advantage and it s likely that the ones who will win the machine intelligence race will be even more powerful than they are today however the good news for the rest of the world is that the core technology they develop will rapidly spill into other areas both via departing talent and published research similar to the big data revolution which was sparked by the release of google s bigtable and bigquery papers we will see corporations release equally groundbreaking new technologies into the community those innovations will be adapted to new industries and use cases that the googles of the world don t have the dna or desire to tackle opportunities for entrepreneurs my company does deep learning for x few words will make you more popular in that is if you can credibly say them deep learning is a particularly popular method in the machine intelligence field that has been getting a lot of attention google facebook and baidu have achieved excellent results with the method for vision and language based tasks and startups like enlitic have shown promising results as well yes it will be an overused buzzword with excitement ahead of results and business models but unlike the hundreds of companies that say they do big data it s much easier to cut to the chase in terms of verifying credibility here if you re paying attention the most exciting part about the deep learning method is that when applied with the appropriate levels of care and feeding it can replace some of the intuition that comes from domain expertise with automatically learned features the hope is that in many cases it will allow us to fundamentally rethink what a best in class solution is as an investor who is curious about the quirkier applications of data and machine intelligence i can t wait to see what creative problems deep learning practitioners try to solve i completely agree with jeff hawkins when he says a lot of the killer applications of these types of technologies will sneak up on us i fully intend to keep an open mind acquihire as a business model people say that data scientists are unicorns in short supply the talent crunch in machine intelligence will make it look like we had a glut of data scientists in the data field many people had industry experience over the past decade most hardcore machine intelligence work has only been in academia we won t be able to grow this talent overnight this shortage of talent is a boon for founders who actually understand machine intelligence a lot of companies in the space will get seed funding because there are early signs that the acquihire price for a machine intelligence expert is north of x that of a normal technical acquihire take for example deep mind where price per technical head was somewhere between m if we choose to consider it in the acquihire category i ve had multiple friends ask me only semi jokingly shivon should i just round up all of my smartest friends in the ai world and call it a company to be honest i m not sure what to tell them at bloomberg beta we d rather back companies building for the long term but that doesn t mean this won t be a lucrative strategy for many enterprising founders a good demo is disproportionately valuable in machine intelligence i remember watching watson play jeopardy when it struggled at the beginning i felt really sad for it when it started trouncing its competitors i remember cheering it on as if it were the toronto maple leafs in the stanley cup finals disclaimers i was an ibmer at the time so was biased towards my team the maple leafs have not made the finals during my lifetime yet so that was purely a hypothetical why do these awe inspiring demos matter the last wave of technology companies to ipo didn t have demos that most of us would watch so why should machine intelligence companies the last wave of companies were very computer like database companies enterprise applications and the like sure i d like to see a x more performant database but most people wouldn t care machine intelligence wins and loses on demos because the technology is very human enough to inspire shock and awe business models tend to take a while to form so they need more funding for longer period of time to get them there they are fantastic acquisition bait watson beat the world s best humans at trivia even if it thought toronto was a us city deepmind blew people away by beating video games vicarious took on captcha there are a few companies still in stealth that promise to impress beyond that and i can t wait to see if they get there demo or not i d love to talk to anyone using machine intelligence to change the world there s no industry too unsexy no problem too geeky i d love to be there to help so don t be shy i hope this landscape chart sparks a conversation the goal to is make this a living document and i want to know if there are companies or categories missing i welcome feedback and would like to put together a dynamic visualization where i can add more companies and dimensions to the data methods used data types end users investment to date location etc so that folks can interact with it to better explore the space questions and comments please email me thank you to andrew paprocki aria haghighi beau cronin ben lorica doug fulop david andrzejewski eric berlow eric jonas gary kazantsev gideon mann greg smithies heidi skinner jack clark jon lehr kurt keutzer lauren barless pete skomoroch pete warden roger magoulas sean gourley stephen purpura wes mckinney zach bogue the quid team and the bloomberg beta team for your ever helpful perspectives disclaimer bloomberg beta is an investor in adatao alation aviso brightfunnel context relevant mavrx newsle orbital insights pop up archive and two others on the chart that are still undisclosed we re also investors in a few other machine intelligence companies that aren t focusing on areas that were a fit for this landscape so we left them off for the full resolution version of the landscape please click here from a quick cheer to a standing ovation clap to show how much you enjoyed this story partner at bloomberg beta all about machine intelligence for good equal parts nerd and athlete straight up canadian stereotype and proud of it
AirbnbEng,369,11,https://medium.com/airbnb-engineering/architecting-a-machine-learning-system-for-risk-941abbba5a60?source=tag_archive---------2----------------,Architecting a Machine Learning System for Risk – Airbnb Engineering & Data Science – Medium,by naseem hakim aaron keys at airbnb we want to build the world s most trusted community guests trust airbnb to connect them with world class hosts for unique and memorable travel experiences airbnb hosts trust that guests will treat their home with the same care and respect that they would their own the airbnb review system helps users find community members who earn this trust through positive interactions with others and the ecosystem as a whole prospers the overwhelming majority of web users act in good faith but unfortunately there exists a small number of bad actors who attempt to profit by defrauding websites and their communities the trust and safety team at airbnb works across many disciplines to help protect our users from these bad actors ideally before they have the opportunity to impart negativity on the community there are many different kinds of risk that online businesses may have to protect against with varying exposure depending on the particular business for example email providers devote significant resources to protecting users from spam whereas payments companies deal more with credit card chargebacks we can mitigate the potential for bad actors to carry out different types of attacks in different ways many risks can be mitigated through user facing changes to the product that require additional verification from the user for example requiring email confirmation or implementing fa to combat account takeovers as many banks have done scripted attacks are often associated with a noticeable increase in some measurable metric over a short period of time for example a sudden increase in reservations in a particular city could be a result of excellent marketing or fraud fraudulent actors often exhibit repetitive patterns as we recognize these patterns we can apply heuristics to predict when they are about to occur again and help stop them for complex evolving fraud vectors heuristics eventually become too complicated and therefore unwieldy in such cases we turn to machine learning which will be the focus of this blog post for a more detailed look at other aspects of online risk management check out ohad samet s great ebook different risk vectors can require different architectures for example some risk vectors are not time critical but require computationally intensive techniques to detect an offline architecture is best suited for this kind of detection for the purposes of this post we are focusing on risks requiring realtime or near realtime action from a broad perspective a machine learning pipeline for these kinds of risk must balance two important goals these may seem like competing goals since optimizing for realtime calculations during a web transaction creates a focus on speed and reliability whereas optimizing for model building and iteration creates more of a focus on flexibility at airbnb engineering and data teams have worked closely together to develop a framework that accommodates both goals a fast robust scoring framework with an agile model building pipeline in keeping with our service oriented architecture we built a separate fraud prediction service to handle deriving all the features for a particular model when a critical event occurs in our system e g a reservation is created we query the fraud prediction service for this event this service can then calculate all the features for the reservation creation model and send these features to our openscoring service which is described in more detail below the openscoring service returns a score and a decision based on a threshold we ve set and the fraud prediction service can then use this information to take action i e put the reservation on hold the fraud prediction service has to be fast to ensure that we are taking action on suspicious events in near realtime like many of our backend services for which performance is critical it is built in java and we parallelize the database queries necessary for feature generation however we also want the freedom to occasionally do some heavy computation in deriving features so we run it asynchronously so that we are never blocking for reservations etc this asynchronous model works for many situations where a few seconds of delay in fraud detection has no negative effect it s worth noting however that there are cases where you may want to react in realtime to block transactions in which case a synchronous query and precomputed features may be necessary this service is built in a very modular way and exposes an internal restful api making adding new events and models easy openscoring is a java service that provides a json rest interface to the java predictive model markup language pmml evaluator jpmml both jpmml and openscoring are open source projects released under the apache license and authored by villu ruusmann edit the most recent version is licensed the under agpl the jpmml backend of openscoring consumes pmml an xml markup language that encodes several common types of machine learning models including tree models logit models svms and neural networks we have streamlined openscoring for a production environment by adding several features including kafka logging and statsd monitoring andy kramolisch has modified openscoring to permit using several models simultaneously as described below there are several considerations that we weighed carefully before moving forward with openscoring after considering all of these factors we decided that openscoring best satisfied our two pronged goal of having a fast and robust yet flexible machine learning framework a schematic of our model building pipeline using pmml is illustrated above the first step involves deriving features from the data stored on the site since the combination of features that gives the optimal signal is constantly changing we store the features in a json format which allows us to generalize the process of loading and transforming features based on their names and types we then transform the raw features through bucketing or binning values and replacing missing values with reasonable estimates to improve signal we also remove features that are shown to be statistically unimportant from our dataset while we omit most of the details regarding how we perform these transformations for brevity here it is important to recognize that these steps take a significant amount of time and care we then use our transformed features to train and cross validate the model using our favorite pmml compatible machine learning library and upload the pmml model to openscoring the final model is tested and then used for decision making if it becomes the best performer the model training step can be performed in any language with a library that outputs pmml one commonly used and well supported library is the r pmml package as illustrated below generating a pmml with r requires very little code this r script has the advantage of simplicity and a script similar to this is a great way to start building pmmls and to get a first model into production in the long run however a setup like this has some disadvantages first our script requires that we perform feature transformation as a pre processing step and therefore we have add these transformation instructions to the pmml by editing it afterwards the r pmml package supports many pmml transformations and data manipulations but it is far from universal we deploy the model as a separate step post model training and so we have to manually test it for validity which can be a time consuming process yet another disadvantage of r is that the implementation of the pmml exporter is somewhat slow for a random forest model with many features and many trees however we ve found that simply re writing the export function in c decreases run time by a factor of from a few days to a few seconds we can get around the drawbacks of r while maintaining its advantages by building a pipeline based on python and scikit learn scikit learn is a python package that supports many standard machine learning models and includes helpful utilities for validating models and performing feature transformations we find that python is a more natural language than r for ad hoc data manipulation and feature extraction we automate the process of feature extraction based on a set of rules encoded in the names and types of variables in the features json thus new features can be incorporated into the model pipeline with no changes to the existing code deployment and testing can also be performed automatically in python by using its standard network libraries to interface with openscoring standard model performance tests precision recall roc curves etc are carried out using sklearn s built in capabilities sklearn does not support pmml export out of the box so have written an in house exporter for particular sklearn classifiers when the pmml file is uploaded to openscoring it is automatically tested for correspondence with the scikit learn model it represents because feature transformation model building model validation deployment and testing are all carried out in a single script a data scientist or engineer is able to quickly iterate on a model based on new features or more recent data and then rapidly deploy the new model into production although this blog post has focused mostly on our architecture and model building pipeline the truth is that much of our time has been spent elsewhere our process was very successful for some models but for others we encountered poor precision recall initially we considered whether we were experiencing a bias or a variance problem and tried using more data and more features however after finding no improvement we started digging deeper into the data and found that the problem was that our ground truth was not accurate consider chargebacks as an example a chargeback can be not as described nad or fraud this is a simplification and grouping both types of chargebacks together for a single model would be a bad idea because legitimate users can file nad chargebacks this is an easy problem to resolve and not one we actually had agents categorize chargebacks as part of our workflow however there are other types of attacks where distinguishing legitimate activity from illegitimate is more subtle and necessitated the creation of new data stores and logging pipelines most people who ve worked in machine learning will find this obvious but it s worth re stressing towards this end sometimes you don t know what data you re going to need until you ve seen a new attack especially if you haven t worked in the risk space before or have worked in the risk space but only in a different sector so the best advice we can offer in this case is to log everything throw it all in hdfs whether you need it now or not in the future you can always use this data to backfill new data stores if you find it useful this can be invaluable in responding to a new attack vector although our current ml pipeline uses scikit learn and openscoring our system is constantly evolving our current setup is a function of the stage of the company and the amount of resources both in terms of personnel and data that are currently available smaller companies may only have a few ml models in production and a small number of analysts and can take time to manually curate data and train the model in many non standardized steps larger companies might have many many models and require a high degree of automation and get a sizable boost from online training a unique challenge of working at a hyper growth company is that landscape fundamentally changes year over year and pipelines need to adjust to account for this as our data and logging pipelines improve investing in improved learning algorithms will become more worthwhile and we will likely shift to testing new algorithms incorporating online learning and expanding on our model building framework to support larger data sets additionally some of the most important opportunities to improve our models are based on insights into our unique data feature selection and other aspects our risk systems that we are not able to share publicly we would like to acknowledge the other engineers and analysts who have contributed to these critical aspects of this project we work in a dynamic highly collaborative environment and this project is an example of how engineers and data scientists at airbnb work together to arrive at a solution that meets a diverse set of needs if you re interested in learning more contact us about our data science and engineering teams originally published at nerds airbnb com on june from a quick cheer to a standing ovation clap to show how much you enjoyed this story creative engineers and data scientists building a world where you can belong anywhere http airbnb io creative engineers and data scientists building a world where you can belong anywhere http airbnb io
Yingjie Miao ,43,6,https://medium.com/kifi-engineering/from-word2vec-to-doc2vec-an-approach-driven-by-chinese-restaurant-process-93d3602eaa31?source=tag_archive---------3----------------,From word2vec to doc2vec: an approach driven by Chinese restaurant process,google s word vec project has created lots of interests in the text mining community it s a neural network language model that is both supervised and unsupervised unsupervised in the sense that you only have to provide a big corpus say english wiki supervised in the sense that the model cleverly generates supervised learning tasks from the corpus how two approaches known as continuous bag of words cbow and skip gram see figure in this paper cbow forces the neural net to predict current word by surrounding words and skip gram forces the neural net to predict surrounding words of the current word training is essentially a classic back propagation method with a few optimization and approximation tricks e g hierarchical softmax word vectors generated by the neural net have nice semantic and syntactic behaviors semantically ios is close to android syntactically boys minus boy is close to girls minus girl one can checkout more examples here although this provides high quality word vectors there is still no clear way to combine them into a high quality document vector in this article we discuss one possible heuristic inspired by a stochastic process called chinese restaurant process crp basic idea is to use crp to drive a clustering process and summing word vectors in the right cluster imagine we have an document about chicken recipe it contains words like chicken pepper salt cheese it also contains words like use buy definitely my the the word vec model gives us a vector for each word one could naively sum up every word vector as the doc vector this clearly introduces lots of noise a better heuristic is to use a weighted sum based on other information like idf or part of speech pos tag the question is could we be more selective when adding terms if this is a chicken recipe document i shouldn t even consider words like definitely use my in the summation one can argue that idf based weights can significantly reduce noise of boring words like the and is however for words like definitely overwhelming the idfs are not necessarily small as you would hope it s natural to think that if we can first group words into clusters words like chicken pepper may stay in one cluster along with other clusters of junk words if we can identify the relevant clusters and only summing up word vectors from relevant clusters we should have a good doc vector this boils down to clustering the words in the document one can of course use off the shelf algorithms like k means but most these algorithms require a distance metric word vec behaves nicely by cosine similarity this doesn t necessarily mean it behaves as well under eucledian distance even after projection to unit sphere it s perhaps best to use geodesic distance it would be nice if we can directly work with cosine similarity we have done a quick experiment on clustering words driven by crp like stochastic process it worked surprisingly well so far now let s explain crp imagine you go to a chinese restaurant there are already n tables with different number of peoples there is also an empty table crp has a hyperparamter r which can be regarded as the imagined number of people on the empty table you go to one of the n tables with probability proportional to existing number of people on the table for the empty table the number is r if you go to one of the n existing tables you are done if you decide to sit down at the empty table the chinese restaurant will automatically create a new empty table in that case the next customer comes in will choose from n tables including the new empty table inspired by crp we tried the following variations of crp to include the similarity factor common setup is the following we are given m vectors to be clustered we maintain two things cluster sum not centroid and vectors in clusters we iterate through vectors for current vector v suppose we have n clusters already now we find the cluster c whose cluster sum is most similar to current vector call this score sim v c variant v creates a new cluster with probability n otherwise v goes to cluster c variant if sim v c n goes to cluster c otherwise with probability n it creates a new cluster and with probability n n it goes to c in any of the two variants if v goes to a cluster we update cluster sum and cluster membership there is one distinct difference to traditional crp if we don t go to empty table we deterministically go to the most similar table in practice we find these variants create similar results one difference is that variant tend to have more clusters and smaller clusters variant tend to have fewer but larger clusters the examples below are from variant for example for a chicken recipe document the clusters look like this apparently the first cluster is most relevant now let s take the cluster sum vector which is the sum of all vectors from this cluster and test if it really preserves semantic below is a snippet of python console we trained word vector using the c implementation on a fraction of english wiki and read the model file using python library gensim model word vec c below denotes the cluster looks like the semantic is preserved well it s convincing that we can use this as the doc vector the recipe document seems easy now let s try something more challenging like a news article news articles tend to tell stories and thus has less concentrated topic words we tried the clustering on this article titled signals on radar puzzle officials in hunt for malaysian jet we got clusters again looks decent note that this is a simple pass clustering process and we don t have to specify number of clusters could be very helpful for latency sensitive services there is still a missing step how to find out the relevant cluster s we haven t yet done extensive experiments on this part a few heuristics to consider there are other problems to think about how do we merge clusters based on similarity among cluster sum vectors or averaging similarity between cluster members what is the minimal set of words that can reconstruct cluster sum vector in the sense of cosine similarity this could be used as a semantic keyword extraction method conclusion google s word vec provides powerful word vectors we are interested in using these vectors to generate high quality document vectors in an efficient way we tried a strategy based on a variant of chinese restaurant process and obtained interesting results there are some open problems to explore and we would like to hear what you think appendix python style pseudo code for similarity driven crp we wrote this post while working on kifi connecting people with knowledge learn more originally published at eng kifi com on march from a quick cheer to a standing ovation clap to show how much you enjoyed this story the kifi engineering blog
Pinterest Engineering,113,6,https://medium.com/@Pinterest_Engineering/building-a-smarter-home-feed-ad1918fdfbe3?source=tag_archive---------4----------------,Building a smarter home feed – Pinterest Engineering – Medium,chris pinchak pinterest engineer discovery the home feed should be a reflection of what each user cares about content is sourced from inputs such as people and boards the user follows interests and recommendations to ensure we maintain fast reliable and personalized home feeds we built the smart feed with the following design values in mind different sources of pins should be mixed together at different rates some pins should be selectively dropped or deferred until a later time some sources may produce pins of poor quality for a user so instead of showing everything available immediately we can be selective about what to show and what to hold back for a future session pins should be arranged in the order of best first rather than newest first for some sources newer pins are intuitively better while for others newness is less important we shifted away from our previously time ordered home feed system and onto a more flexible one the core feature of the smart feed architecture is its separation of available but unseen content and content that s already been presented to the user we leverage knowledge of what the user hasn t yet seen to our advantage when deciding how the feed evolves over time smart feed is a composition of three independent services each of which has a specific role in the construction of a home feed the smart feed worker is the first to process pins and has two primary responsibilities to accept incoming pins and assign some score proportional to their quality or value to the receiving user and to remember these scored pins in some storage for later consumption essentially the worker manages pins as they become newly available such as those from the repins of the people the user follows pins have varying value to the receiving user so the worker is tasked with deciding the magnitude of their subjective quality incoming pins are currently obtained from three separate sources repins made by followed users related pins and pins from followed interests each is scored by the worker and then inserted into a pool for that particular type of pin each pool is a priority queue sorted on score and belongs to a single user newly added pins mix with those added before allowing the highest quality pins to be accessible over time at the front of the queue pools can be implemented in a variety of ways so long as the priority queue requirement is met we choose to do this by exploiting the key based sorting of hbase each key is a combination of user score and pin such that for any user we may scan a list of available pins according to their score newly added triples will be inserted at their appropriate location to maintain the score order this combination of user score and pin into a key value can be used to create a priority queue in other storage systems aside from hbase a property we may use in the future depending on evolving storage requirements distinct from the smart feed worker the smart feed content generator is concerned primarily with defining what new means in the context of a home feed when a user accesses the home feed we ask the content generator for new pins since their last visit the generator decides the quantity composition and arrangement of new pins to return in response to this request the content generator assembles available pins into chunks for consumption by the user as part of their home feed the generator is free to choose any arrangement based on a variety of input signals and may elect to use some or all of the pins available in the pools pins that are selected for inclusion in a chunk are thereafter removed from from the pools so they cannot be returned as part of subsequent chunks the content generator is generally free to perform any rearrangements it likes but is bound to the priority queue nature of the pools when the generator asks for n pins from a pool it ll get the n highest scoring i e best pins available therefore the generator doesn t need to concern itself with finding the best available content but instead with how the best available content should be presented in addition to providing high availability of the home feed the smart feed service is responsible for combining new pins returned by the content generator with those that previously appeared in the home feed we can separate these into the chunk returned by the content generator and the materialized feed managed by the smart feed service the materialized feed represents a frozen view of the feed as it was the last time the user viewed it to the materialized pins we add the pins from the content generator in the chunk the service makes no decisions about order instead it adds the pins in exactly the order given by the chunk because it has a fairly low rate of reading and writing the materialized feed is likely to suffer from fewer availability events in addition feeds can be trimmed to restrict them to a maximum size the need for less storage means we can easily increase the availability and reliability of the materialized feed through replication and the use of faster storage hardware the smart feed service relies on the content generator to provide new pins if the generator experiences a degradation in performance the service can gracefully handle the loss of its availability in the event the content generator encounters an exception while generating a chunk or if it simply takes too long to produce one the smart feed service will return the content contained in the materialized feed in this instance the feed will appear to the end user as unchanged from last time future feed views will produce chunks as large as or larger than the last so that eventually the user will see new pins by moving to smart feed we achieved the goals of a highly flexible architecture and better control over the composition of home feeds the home feed is now powered by three separate services each with a well defined role in its production and distribution the individual services can be altered or replaced with components that serve the same general purpose the use of pools to buffer pins according to their quality allows us a greater amount of control over the composition of home feeds continuing with this project we intend to better model users preferences with respect to pins in their home feeds our accuracy of recommendation quality varies considerably over our user base and we would benefit from using preference information gathered from recent interactions with the home feed knowledge of personal preference will also help us order home feeds so the pins of most value can be discovered with the least amount of effort if you re interested in tackling challenges and making improvements like this join our team chris pinchak is a software engineer at pinterest acknowledgements this technology was built in collaboration with dan feng dmitry chechik raghavendra prabhu jeremy carroll xun liu varun sharma joe lau yuchen liu tian ying chang and yun park this team as well as people from across the company helped make this project a reality with their technical insights and invaluable feedback from a quick cheer to a standing ovation clap to show how much you enjoyed this story inventive engineers building the first visual discovery engine billion ideas and counting https careers pinterest com careers engineering
Nikhil Dandekar,116,3,https://towardsdatascience.com/what-makes-a-good-data-scientist-engineer-a8b4d7948a86?source=tag_archive---------5----------------,What makes a good data scientist/engineer? – Towards Data Science,the term data scientist has been used lately to describe a wide variety of skills roles in this post i will focus on a particular flavor of data scientist i will talk about the qualities needed to be a good data scientist engineer who ships relevance products to users some examples of relevance products are these folks need to be strong at data science and engineering to be successful some places call these folks as machine learning engineers since most of the work they do involves machine learning more generally i feel relevance engineer is a good term to describe them relevance engineers have a common set of skills that they draw upon to get their jobs done the list below doesn t include some of the known obvious skills you obviously need to be smart you obviously need to have or be able to learn quickly the required book knowledge but beyond that there are a bunch of not so obvious skills that you can t learn from a book here are some of those in no particular order this list is by no means exhaustive but does capture some of the qualities of the smartest folks i have worked with happy to hear what you think thanks to peter bailey and andrew hogue for feedback on the initial revisions in this post feature means a software feature not a machine learning feature from a quick cheer to a standing ovation clap to show how much you enjoyed this story engineering manager doing machine learning google previously worked on ml and search at quora foursquare and bing sharing concepts ideas and codes
Jeff Smith,20,7,https://medium.com/data-engineering/modeling-madly-8b2c72eb52be?source=tag_archive---------6----------------,Modeling Madly – Data Engineering – Medium,i recently wrapped up my second hackathon at intent media you can see my summary of one of our previous hackathons here these past two hackathons i ve taken on some slightly different challenges than people usually go after in a hackathon developing new machine learning models while i ve been working on data science and machine learning systems for a while i ve found that trying to do so under extreme constraints can be a distinctly different experience a very good data hacker can easily find themselves with a great idea at a hackathon but with little to nothing to demo at the end accepting that my personal experience is just my own let me offer three tips for building new models at a hackathon when you re doing a more traditional web app hack at a hackathon you can almost run out of time and still come up with something pretty good as long as you get that last bug fixed before the demo this is a great characteristic to build into the plan of a hack but one that simply does not apply to a machine learning hack think about what happens when you do find that last bug in a machine learning project you still need to potentially do all of the below that s no just hit refresh workflow even with a well oiled workflow some of those tasks can take all of the time your average one day hackathon is scheduled for take for example training a production grade model using say hadoop can take a lot of time even if you have the cash to spin up a fair sized cluster of ec instances what that means for your hack can vary but you re just asking for trouble if you don t start with that fact taken into account in the scope and goals of your project a solid project design is absolutely crucial if you re going to hope to take all of the little steps involved in getting your model ready to demo which leads me to my next point one of the best things about working in data science is all of the really smart people but of course the corollary is that one of the worst things about working in data science is all of the really smart people sharp engineers and data scientists can take the nugget of an idea and envision a useful powerful suite of products that would take years to build which is not so useful when you have a day or two mature dataists know just how much ambition is too much and plan accordingly i happen to be lucky enough to work with some very smart and very mature data scientists and engineers so this has not been a problem for either of my last few hacks but i m just lucky that way you might not be so lucky unrealistic ambitions are a constant danger in a machine learning hack running along the edge of all activities like a precipice beckoning you to dive off and see where you land if you take one thing away from this post let it be this don t dive off the cliff just don t do it you won t like where you land you ll wind up with more questions than answers and you ll have nothing to show come demo time moreover your fellow devs who worked on apps and not models will simply not understand what you spent your time on what does a precipice look like it could be a novel distance metric it could be a fundamental improvement to a widely used technique like svrs or it could just be something really benign sounding like a longer training set i would say that even choosing to pose the problem as a regression one instead of a classification one could qualify the danger originates in the intrinsic tension between the rigorous and exploratory mode of academic data science machine learning education and the pedal to the metal pace mandated by a hackathon they are very different modes of working and you re just going to have suspend some of your good habits for a day or so if you want to have something to demo this last point can be the trickiest to put in practice but i think it can totally be the difference between a project that feels like a hack and one that feels like just getting warmed up on a weeklong story if you ve figured out how to scope your project appropriately and designed something that can really be built in a day or two you can still actually fail to do so i think it can the difference can easily come down to technology choices for example i currently make my living writing cascalog clojure and java on top of hadoop to process files stored in s i know these tools well enough to pay my rent but i would absolutely hesitate to use any of them in a tight paced context i have spent weeks trying to understand a single cascalog bug seriously if you know the language python offers an unbeatable value proposition for this use case scikit learn has nearly everything you could imagine needing pandas numpy and scipy are all sitting there to be brought in when appropriate and don t forget how awesome it can be to prototype in a purpose built exploratory development environment like ipython but this is machine learning and sometimes our data is just big maybe even web scale some people hate these phrases but they serve a purpose we don t all use hadoop out of love for horrendously complex java applications big data is not just statistics on a mac pro although it can often look like that scale can be a real necessity even in a hackathon when it is there are no easy answers if you re lucky maybe you can actually work with multiple hour model learning times if you re really lucky you might be using spark and not hadoop in which case it might not take hours to learn your model my point is that insofar as you have a choice choose the leaner meaner tool the one that will let you do more with less input required from you don t use that c library that promises awesome runtime but with python bindings that you ve never tried you ll never figure out its quirks in time write as little data cleanup code as you can manage commands like dropna can save you precious minutes to hours and if you can get your data from database or an api instead of files then for the love of cthulhu do it hell even if you have to load your data from files to a database first it might be worth your time sql is one of the highest productivity rapid prototyping tools i know and though i love to bash on the clunkiness of hadoop there are even ways of taking some serious pain out of using it under pressure depending on what you re doing elastic map reduce or predictionio can get you to the point of being productive much faster i love hackathons and their variations they remind me of the fun old days in grad school furiously hacking away to come up with something interesting to say about definitionally uncertain stuff the furious pace and the pragmatic compromises are part of the fun compared to things like pitch events hackathons have way less problems even if they have their issues as well at their best they re about the love of unconstrained creation i ve tried to do machine learning hacks because it s just so damn cool to go from zero to having a program that makes decisions it amazes me every time it works and doubly so when i can manage to get something working on a deadline taking on a challenge like building a new model in a hackathon is also a great learning experience especially if you get to work as part of a strong team machine learning in the real world is an even larger topic than its academic cousin and there s always interesting things to learn hackathons can be great places to rapidly iterate through approaches and learn from your teammates how to build things better and faster that s pretty likely to come in handy sometime the main part of the post is over but i wanted to make sure to leave a note for anyone who was interested in what we hack at intent media or what we build for our customers we re hiring all sorts of smart people to build systems for machine learning and more please reach out if you want to hear more about how and why we do what we do from a quick cheer to a standing ovation clap to show how much you enjoyed this story author of reactive machine learning systems manningbooks building ais for fun and profit friend of animals laying the foundation of tomorrow s big data
Chris Jagers,45,5,https://medium.com/@chrisjagers/the-wolfram-language-b853337f8427?source=tag_archive---------7----------------,Explaining the Wolfram Language – Chris Jagers – Medium,many people are already familiar with apple s voice search called siri or the search engine behind it called wolfram alpha this search engine can use natural language to search vast sets of data and even compute math however this is just a tiny fraction of what the language can do and i don t even think it s a good introduction to what s possible to understand the raw power of the underlying technology you really have to understand what it is and a little about how it works the wolfram website has wonderful documentation and explanations but for the uninitiated it can seem bewildering they have repackaged the language so many different ways that it can be hard for the beginner to understand exactly what it is that s why i want to venture my own introduction let s start with it s origins mathematica was designed as a desktop tool for computational research and exploration it continued evolving and the breakthrough was realizing those symbols could be anything images sounds algorithms geometry data sets anything so it became more than just a language stephen wolfram calls this a knowledge based language because it has smart objects built in that can be computed the language doesn t simply find results it computes results into actual models analysis and other symbolic objects the real power is that the results remain symbolic objects that can be further manipulated symbolically i e embed in another symbolic object operate on it in short anything can be computed pretty abstract i know don t worry we ll get to examples soon the actual syntax is a combination of objects and operators which are grouped and ordered by square brackets the stuff at the center of the formula gets read first and then it expands out like a russian doll out of many potential examples i have carefully selected one from their site to illustrate it s simplicity and power let s say we want our system to determine the difference between poetry and prose this would be difficult to program directly because there are so many variables and the differences are so subtle with wolfram language that hard stuff becomes easy you can train it recognize the difference very quickly here s how it works let s use shakespeare for an example first scan all of hamlet and call that type of stuff prose then scan all of shakespeare s sonnets and call that stuff poetry easy next train the system with machine learning classify and predict are the two big functions we want to classify which is poetry and which is prose wolfram looks at our situation and instantly determines that the markov method is the best for differentiating among all the subtle differences between prose and poetry that s it any system using this bit of training will automatically be able to detect the difference between poetry and prose with a high degree of accuracy the key to this accuracy is the size of the data set you really need at millions of data points to train it reliably but with wolfram many of those data sets are already built in easy this is just one tiny example to illustrate what the language looks like and how it goes beyond symbols to work with computable objects we could continue translating poems into interactive maps and interactions into music and so on how does wolfram compare with other products like apache hadoop and others well it s a totally different thing in those products everything is manual the various axis and all the variables are manually defined instead wolfram intelligently applies formulas and makes choices to optimize results based on specific conditions it makes the hard stuff automatic plus it s capable of much more than machine learning that s just one example of hundreds sound d geometry language images etc and a mixture of them all mathematica is still the most powerful and polished way to access the wolfram language their new programming cloud and other cloud offerings signal serious intent to move to the web but it is still early days the language is very mature for desktop exploration and some companies have even made mathematica applications for small scale internal use which can be quite useful even though the wolfram website has signaled intent to make it more broadly deployable within commercial services i don t think this is the proper way to use the language within my own company we find wolfram extremely handy for research but not deployment within a web based product in short it isn t performant commercial products require more than a powerful language they are made within an ecosystem of services and vendors that all have to work together without machine learning built into the native cloud where data is stored it can t be deployed in a saas product in a way that lives up to expectations while stephen wolfram would love for his language to be used within commercial products i think he resents having to play nice with lower level languages his alternative of making api requests across the web isn t a good way to embed intelligence within products and i don t think we will ever see entire saas products built entirely with a functional language programming is the art of automation the wolfram community is full of very smart people using the language for research and exploration they represent the cutting edge of computation personally i m looking forward to when we can see intelligence woven into commercial and consumer products that solve real problems for people on a daily basis from a quick cheer to a standing ovation clap to show how much you enjoyed this story ceo of learning machine www learningmachine com
John Wittenauer,2,9,https://medium.com/@jdwittenauer/machine-learning-exercises-in-python-part-1-60db0df846a4?source=tag_archive---------8----------------,"Machine Learning Exercises In Python, Part 1 – John Wittenauer – Medium",this content originally appeared on curious insight this post is part of a series covering the exercises from andrew ng s machine learning class on coursera the original code exercise text and data files for this post are available here part simple linear regressionpart multivariate linear regressionpart logistic regressionpart multivariate logistic regressionpart neural networkspart support vector machinespart k means clustering pcapart anomaly detection recommendation one of the pivotal moments in my professional development this year came when i discovered coursera i d heard of the mooc phenomenon but had not had the time to dive in and take a class earlier this year i finally pulled the trigger and signed up for andrew ng s machine learning class i completed the whole thing from start to finish including all of the programming exercises the experience opened my eyes to the power of this type of education platform and i ve been hooked ever since this blog post will be the first in a series covering the programming exercises from andrew s class one aspect of the course that i didn t particularly care for was the use of octave for assignments although octave matlab is a fine platform most real world data science is done in either r or python certainly there are other languages and tools being used but these two are unquestionably at the top of the list since i m trying to develop my python skills i decided to start working through the exercises from scratch in python the full source code is available at my ipython repo on github you ll also find the data used in these exercises and the original exercise pdfs in sub folders off the root directory if you re interested while i can explain some of the concepts involved in this exercise along the way it s impossible for me to convey all the information you might need to fully comprehend it if you re really interested in machine learning but haven t been exposed to it yet i encourage you to check out the class it s completely free and there s no commitment whatsoever with that let s get started in the first part of exercise we re tasked with implementing simple linear regression to predict profits for a food truck suppose you are the ceo of a restaurant franchise and are considering different cities for opening a new outlet the chain already has trucks in various cities and you have data for profits and populations from the cities you d like to figure out what the expected profit of a new food truck might be given only the population of the city that it would be placed in let s start by examining the data which is in a file called ex data txt in the data directory of my repository above first we need to import a few libraries now let s get things rolling we can use pandas to load the data into a data frame and display the first few rows using the head function note medium can t render tables the full example is here another useful function that pandas provides out of the box is the describe function which calculates some basic statistics on a data set this is helpful to get a feel for the data during the exploratory analysis stage of a project note medium can t render tables the full example is here examining stats about your data can be helpful but sometimes you need to find ways to visualize it too fortunately this data set only has one dependent variable so we can toss it in a scatter plot to get a better idea of what it looks like we can use the plot function provided by pandas for this which is really just a wrapper for matplotlib it really helps to actually look at what s going on doesn t it we can clearly see that there s a cluster of values around cities with smaller populations and a somewhat linear trend of increasing profit as the size of the city increases now let s get to the fun part implementing a linear regression algorithm in python from scratch if you re not familiar with linear regression it s an approach to modeling the relationship between a dependent variable and one or more independent variables if there s one independent variable then it s called simple linear regression and if there s more than one independent variable then it s called multiple linear regression there are lots of different types and variances of linear regression that are outside the scope of this discussion so i won t go into that here but to put it simply we re trying to create a linear model of the data x using some number of parameters theta that describes the variance of the data such that given a new data point that s not in x we could accurately predict what the outcome y would be without actually knowing what y is in this implementation we re going to use an optimization technique called gradient descent to find the parameters theta if you re familiar with linear algebra you may be aware that there s another way to find the optimal parameters for a linear model called the normal equation which basically solves the problem at once using a series of matrix calculations however the issue with this approach is that it doesn t scale very well for large data sets in contrast we can use variants of gradient descent and other optimization methods to scale to data sets of unlimited size so for machine learning problems this approach is more practical okay that s enough theory let s write some code the first thing we need is a cost function the cost function evaluates the quality of our model by calculating the error between our model s prediction for a data point using the model parameters and the actual data point for example if the population for a given city is and we predicted that it was our error is assuming an l or least squares loss function we do this for each data point in x and sum the result to get the cost here s the function notice that there are no loops we re taking advantage of numpy s linear algrebra capabilities to compute the result as a series of matrix operations this is far more computationally efficient than an unoptimizted for loop in order to make this cost function work seamlessly with the pandas data frame we created above we need to do some manipulating first we need to insert a column of s at the beginning of the data frame in order to make the matrix operations work correctly i won t go into detail on why this is needed but it s in the exercise text if you re interested basically it accounts for the intercept term in the linear equation second we need to separate our data into independent variables x and our dependent variable y finally we re going to convert our data frames to numpy matrices and instantiate a parameter matirx one useful trick to remember when debugging matrix operations is to look at the shape of the matrices you re dealing with it s also helpful to remember when walking through the steps in your head that matrix multiplications look like i x j j x k i x k where i j and k are the shapes of the relative dimensions of the matrix l l l l l l okay so now we can try out our cost function remember the parameters were initialized to so the solution isn t optimal yet but we can see if it works so far so good now we need to define a function to perform gradient descent on the parameters theta using the update rules defined in the exercise text here s the function for gradient descent the idea with gradient descent is that for each iteration we compute the gradient of the error term in order to figure out the appropriate direction to move our parameter vector in other words we re calculating the changes to make to our parameters in order to reduce the error thus bringing our solution closer to the optimal solution i e best fit this is a fairly complex topic and i could easily devote a whole blog post just to discussing gradient descent if you re interested in learning more i would recommend starting with this article and branching out from there once again we re relying on numpy and linear algebra for our solution you may notice that my implementation is not optimal in particular there s a way to get rid of that inner loop and update all of the parameters at once i ll leave it up to the reader to figure it out for now i ll cover it in a later post now that we ve got a way to evaluate solutions and a way to find a good solution it s time to apply this to our data set matrix note that we ve initialized a few new variables here if you look closely at the gradient descent function it has parameters called alpha and iters alpha is the learning rate it s a factor in the update rule for the parameters that helps determine how quickly the algorithm will converge to the optimal solution iters is just the number of iterations there is no hard and fast rule for how to initialize these parameters and typically some trial and error is involved we now have a parameter vector descibing what we believe is the optimal linear model for our data set one quick way to evaluate just how good our regression model is might be to look at the total error of our new solution on the data set that s certainly a lot better than but it s not a very intuitive way to look at it fortunately we have some other techniques at our disposal we re now going to use matplotlib to visualize our solution remember the scatter plot from before let s overlay a line representing our model on top of a scatter plot of the data to see how well it fits we can use numpy s linspace function to create an evenly spaced series of points within the range of our data and then evaluate those points using our model to see what the expected profit would be we can then turn it into a line graph and plot it not bad our solution looks like and optimal linear model of the data set since the gradient decent function also outputs a vector with the cost at each training iteration we can plot that as well notice that the cost always decreases this is an example of what s called a convex optimization problem if you were to plot the entire solution space for the problem i e plot the cost as a function of the model parameters for every possible value of the parameters you would see that it looks like a bowl shape with a basin representing the optimal solution that s all for now in part we ll finish off the first exercise by extending this example to more than variable i ll also show how the above solution can be reached by using a popular machine learning library called scikit learn to comment on this article check out the original post at curious insight follow me on twitter to get new post updates from a quick cheer to a standing ovation clap to show how much you enjoyed this story data scientist engineer author investor entrepreneur
Pinterest Engineering,25,7,https://medium.com/@Pinterest_Engineering/building-the-interests-platform-73a3a3755c21?source=tag_archive---------9----------------,Building the interests platform – Pinterest Engineering – Medium,ningning hu pinterest engineer discovery the core value of pinterest is to help people find the things they care about by connecting them to pins and people that relate to their interests we re building a service that s powered by people and supercharged with technology the interest graph the connections that make up the pinterest index creates bridges between pins boards and pinners it s our job to build a system that helps people to collect the things they love and connect them to communities of engaged people who share similar interests and can help them discover more from categories like travel fitness and humor to more niche areas like vintage motorcycles craft beer or japanese architecture we re building a visual discovery tool for all interests the interests platform is built to support this vision specifically it s responsible for producing high quality data on interests interest relationships and their association with pins boards and pinners figure feedback loop between machine intelligence and human curation in contrast with conventional methods of generating such data which rely primarily on machine learning and data mining techniques our system relies heavily on human curation the ultimate goal is to build a system that s both machine and human powered creating a feedback mechanism by which human curated data helps drive improvements in our machine algorithms and vice versa figure system components raw input to the system includes existing data about pins boards pinners and search queries as well as explicit human curation signals about interests with this data we re able to construct a continuously evolving interest dictionary which provides the foundation to support other key components such as interest feeds interest recommendations and related interests from a technology standpoint interests are text strings that represent entities for which a group of pinners might have a shared passion we generated an initial collection of interests by extracting frequently occurring n grams from pin and board descriptions as well as board titles and filtering these n grams using custom built grammars while this approach provided a high coverage set of interests we found many terms to be malformed phrases for instance we would extract phrases such as lamborghini yellow instead of yellow lamborghini this proved problematic because we wanted interest terms to represent how pinners would describe them and so we employed a variety of methods to eliminate malformed interests terms we first compared terms with repeated search queries performed by a group of pinners over a few months intuitively this criterion matches well with the notion that an interest should be an entity for which a group of pinners are passionate later we filtered the candidate set through public domain ontologies like wikipedia titles these ontologies were primarily used to validate proper nouns as opposed to common phrases as all available ontologies represented only a subset of possible interests this is especially true for pinterest where pinners themselves curate special interests like mid century modern style finally we also maintain an internal blacklist to filter abusive words and x rated terms as well as pinterest specific stop words like love this filtering is especially important to interest terms which might be recommended to millions of users we arrived at a fair quality collection of interests following the above algorithmic approaches in order to understand the quality of our efforts we gave a term subset of our collection to a third party vendor which used crowdsourcing to rate our data to be rigorous we composed a set of four criteria by which users would evaluate candidate interests terms is it english is it a valid phrase in grammar is it a standalone concept is it a proper name the crowdsourced ratings were both interesting if not somewhat expected there was a low rate of agreement amongst raters with especially high discrepancy in determining whether an interest s term represented a standalone concept despite the ambiguity we were able to confirm that of the collection generated using the above algorithms satisfied our interests criteria this type of effort however is not easy to scale the real solution is to allow pinners to provide both implicit and explicit signals to help us determine the validity of an interest implicit signals behaviors like clicking and viewing while explicit signals include asking pinners to specifically provide information which can be actions like a thumbs up thumbs down starring or skipping recommendations to capture all the signals used for defining the collections of terms we built a dictionary that stores all the data associated with each interest including invalid interests and the reason why it s invalid this service plays a key role in human curation by aggregating signals from different people on top of this dictionary service we can build different levels of reviewing system with the interests dictionary we can associate pins boards and pinners with representative interests one of the initial ways we experimented with this was launching a preview of a page where pinners can explore their interests figure exploring interests in order to match interests to pinners we need to aggregate all the information related with a person s interests at its core our system recommends interests based upon pins with which a pinner interacts every pin on pinterest has been collected and given context by someone who thinks it s important and in doing so is helping other people discover great content each individual pin is an incredibly rich source of data as discussed in a previous blog post on discovery data model one pin often has multiple copies different people may pin it from different sources and the same pin can be repinned multiple times during this process each pin accumulates numerous unique textual descriptions which allows us to connect pins with interests terms with high precision however this conceptually simple process requires non trivial engineering effort to scale to the amount of pins and pinners that the service has today the data process pipeline managed by pinball composes over hadoop jobs and runs periodically to update the user interest mapping to capture users latest interest information the initial feedback on the explore interests page has been positive proving the capabilities of our system we ll continue testing different ways of exposing a person s interests and related content based on implicit signals as well as explicit signals such as the ability to create custom categories of interests related interests are an important way of enabling the ability to browse interests and discover new ones to compute related interests we simply combine the co occurrence relationship for interests computed at pin and board levels figure computing related interests the quality of the related interests is surprisingly high given the simplicity of the algorithm we attribute this effect to the cleanness of pinterest data text data on pins tend to be very concise and contain less noise than other types of data like web pages also related interests calculation already makes use of boards which are heavily curated by people vs machines in regards to organizing related content we find that utilizing the co occurrence of interest terms at the level of both pins and boards provides the best tradeoff between achieving high precision as well as recall when computing the related interests one of the initial ways we began showing people related content was through related pins when you pin an object you ll see a recommendation for a related board with that same pin so you can explore similar objects additionally if you scroll beneath a pin you ll see pins from other people who ve also pinned that original object at this point of all pins have related pins and we ve seen growth in engagement with related pins in the last six months interests feeds provide pinners with a continuous feed of pins that are highly related our feeds are populated using a variety of sources including search and through our annotation pipeline a key property of the feed is flow only feeds with decent flow can attract pinners to come back repeatedly thereby maintaining high engagement in order to optimize for our feeds we ve utilized a number of real time indexing and retrieval systems including real time search real time annotating and also human curation for some of the interests to ensure quality we need to guarantee quality from all sources for that purpose we measure the engagement of pins from each source and address quality issue accordingly figure how interest feeds are generated accurately capturing pinner interests and interest relationships and making this data understandable and actionable for tens of millions of people collecting tens of billions of pins is not only an engineering challenge but also a product design one we re just at the beginning as we continue to improve the data and design ways to empower people to provide feedback that allows us to build a hybrid system combining machine and human curation to power discovery results of these effort will be reflected in future product releases if you re interested in building new ways of helping people discover the things they care about join our team acknowledgements the core team members for the interests backend platform are ningning hu leon lin ryan shih and yuan wei many other folks from other parts of the company especially the discovery team and the infrastructure teams have provided very useful feedback and help along the way to make the ongoing project successful ningning hu is an engineer at pinterest from a quick cheer to a standing ovation clap to show how much you enjoyed this story inventive engineers building the first visual discovery engine billion ideas and counting https careers pinterest com careers engineering
Christopher Nguyen,991,8,https://medium.com/deep-learning-101/algorithms-of-the-mind-10eb13f61fc4?source=tag_archive---------0----------------,Algorithms of the Mind – Deep Learning 101 – Medium,what machine learning teaches us about ourselves originally published at blog arimo com follow me on twitter to keep informed of interesting developments on these topics science often follows technology because inventions give us new ways to think about the world and new phenomena in need of explanation or so aram harrow an mit physics professor counter intuitively argues in why now is the right time to study quantum computing he suggests that the scientific idea of entropy could not really be conceived until steam engine technology necessitated understanding of thermodynamics quantum computing similarly arose from attempts to simulate quantum mechanics on ordinary computers so what does all this have to do with machine learning much like steam engines machine learning is a technology intended to solve specific classes of problems yet results from the field are indicating intriguing possibly profound scientific clues about how our own brains might operate perceive and learn the technology of machine learning is giving us new ways to think about the science of human thought and imagination five years ago deep learning pioneer geoff hinton who currently splits his time between the university of toronto and google published the following demo hinton had trained a five layer neural network to recognize handwritten digits when given their bitmapped images it was a form of computer vision one that made handwriting machine readable but unlike previous works on the same topic where the main objective is simply to recognize digits hinton s network could also run in reverse that is given the concept of a digit it can regenerate images corresponding to that very concept we are seeing quite literally a machine imagining an image of the concept of the magic is encoded in the layers between inputs and outputs these layers act as a kind of associative memory mapping back and forth from image and concept from concept to image all in one neural network but beyond the simplistic brain inspired machine vision technology here the broader scientific question is whether this is how human imagination visualization works if so there s a huge a ha moment here after all isn t this something our brains do quite naturally when we see the digit we think of the concept conversely when someone says we can conjure up in our minds eye an image of the digit is it all a kind of running backwards by the brain from concept to images or sound smell feel etc through the information encoded in the layers aren t we watching this network create new pictures and perhaps in a more advanced version even new internal connections as it does so if visual recognition and imagination are indeed just back and forth mapping between images and concepts what s happening between those layers do deep neural networks have some insight or analogies to offer us here let s first go back years to immanuel kant s critique of pure reason in which he argues that intuition is nothing but the representation of phenomena kant railed against the idea that human knowledge could be explained purely as empirical and rational thought it is necessary he argued to consider intuitions in his definitions intuitions are representations left in a person s mind by sensory perceptions where as concepts are descriptions of empirical objects or sensory data together these make up human knowledge fast forwarding two centuries later berkeley cs professor alyosha efros who specializes in visual understanding pointed out that there are many more things in our visual world than we have words to describe them with using word labels to train models efros argues exposes our techniques to a language bottleneck there are many more un namable intuitions than we have words for in training deep networks such as the seminal cat recognition work led by quoc le at google stanford we re discovering that the activations in successive layers appear to go from lower to higher conceptual levels an image recognition network encodes bitmaps at the lowest layer then apparent corners and edges at the next layer common shapes at the next and so on these intermediate layers don t necessarily have any activations corresponding to explicit high level concepts like cat or dog yet they do encode a distributed representation of the sensory inputs only the final output layer has such a mapping to human defined labels because they are constrained to match those labels therefore the above encodings and labels seem to correspond to exactly what kant referred to as intuitions and concepts in yet another example of machine learning technology revealing insights about human thought the network diagram above makes you wonder whether this is how the architecture of intuition albeit vastly simplified is being expressed if as efros has pointed out there are a lot more conceptual patterns than words can describe then do words constrain our thoughts this question is at the heart of the sapir whorf or linguistic relativity hypothesis and the debate about whether language completely determines the boundaries of our cognition or whether we are unconstrained to conceptualize anything regardless of the languages we speak in its strongest form the hypothesis posits that the structure and lexicon of languages constrain how one perceives and conceptualizes the world one of the most striking effects of this is demonstrated in the color test shown here when asked to pick out the one square with a shade of green that s distinct from all the others the himba people of northern namibia who have distinct words for the two shades of green can find it almost instantly the rest of us however have a much harder time doing so the theory is that once we have words to distinguish one shade from another our brains will train itself to discriminate between the shades so the difference would become more and more obvious over time in seeing with our brain not with our eyes language drives perception with machine learning we also observe something similar in supervised learning we train our models to best match images or text audio etc against provided labels or categories by definition these models are trained to discriminate much more effectively between categories that have provided labels than between other possible categories for which we have not provided labels when viewed from the perspective of supervised machine learning this outcome is not at all surprising so perhaps we shouldn t be too surprised by the results of the color experiment above either language does indeed influence our perception of the world in the same way that labels in supervised machine learning influence the model s ability to discriminate among categories and yet we also know that labels are not strictly required to discriminate between cues in google s cat recognizing brain the network eventually discovers the concept of cat dog etc all by itself even without training the algorithm against explicit labels after this unsupervised training whenever the network is fed an image belonging to a certain category like cats the same corresponding set of cat neurons always gets fired up simply by looking at the vast set of training images this network has discovered the essential patterns of each category as well as the differences of one category vs another in the same way an infant who is repeatedly shown a paper cup would soon recognize the visual pattern of such a thing even before it ever learns the words paper cup to attach that pattern to a name in this sense the strong form of the sapir whorf hypothesis cannot be entirely correct we can and do discover concepts even without the words to describe them supervised and unsupervised machine learning turn out to represent the two sides of the controversy s coin and if we recognized them as such perhaps sapir whorf would not be such a controversy and more of a reflection of supervised and unsupervised human learning i find these correspondences deeply fascinating and we ve only scratched the surface philosophers psychologists linguists and neuroscientists have studied these topics for a long time the connection to machine learning and computer science is more recent especially with the advances in big data and deep learning when fed with huge amounts of text images or audio data the latest deep learning architectures are demonstrating near or even better than human performance in language translation image classification and speech recognition every new discovery in machine learning demystifies a bit more of what may be going on in our brains we re increasingly able to borrow from the vocabulary of machine learning to talk about our minds thanks to sonal chokshi and vu pham for extensive review edits also chrisjagers chickamade from a quick cheer to a standing ovation clap to show how much you enjoyed this story arimoinc ceo co founder leader entrepreneur hacker xoogler executive professor dataviz parallelcomputing deeplearning former googleapps fundamentals and latest developments in deeplearning
Per Harald Borgen,2100,6,https://medium.com/learning-new-stuff/machine-learning-in-a-week-a0da25d59850?source=tag_archive---------1----------------,Machine Learning in a Week – Learning New Stuff – Medium,getting into machine learning ml can seem like an unachievable task from the outside however after dedicating one week to learning the basics of the subject i found it to be much more accessible than i anticipated this article is intended to give others who re interested in getting into ml a roadmap of how to get started drawing from the experiences i made in my intro week before my machine learning week i had been reading about the subject for a while and had gone through half of andrew ng s course on coursera and a few other theoretical courses so i had a tiny bit of conceptual understanding of ml though i was completely unable to transfer any of my knowledge into code this is what i wanted to change i wanted to be able to solve problems with ml by the end of the week even through this meant skipping a lot of fundamentals and going for a top down approach instead of bottoms up after asking for advice on hacker news i came to the conclusion that python s scikit learn module was the best starting point this module gives you a wealth of algorithms to choose from reducing the actual machine learning to a few lines of code i started off the week by looking for video tutorials which involved scikit learn i finally landed on sentdex s tutorial on how to use ml for investing in stocks which gave me the necessary knowledge to move on to the next step the good thing about the sentdex tutorial is that the instructor takes you through all the steps of gathering the data as you go along you realize that fetching and cleaning up the data can be much more time consuming than doing the actually machine learning so the ability to write scripts to scrape data from files or crawl the web are essential skills for aspiring machine learning geeks i have re watched several of the videos later on to help me when i ve been stuck with problems so i d recommend you to do the same however if you already know how to scrape data from websites this tutorial might not be the perfect fit as a lot of the videos evolve around data fetching in that case the udacity s intro to machine learning might be a better place to start tuesday i wanted to see if i could use what i had learned to solve an actual problem as another developer in my coding cooperative was working on bank of england s data visualization competition i teamed up with him to check out the datasets the bank has released the most interesting data was their household surveys this is an annual survey the bank perform on a few thousand households regarding money related subjects the problem we decided to solve was the following i played around with the dataset spent a few hours cleaning up the data and used the scikit learn map to find a suitable algorithm for the problem we ended up with a success ratio at around which isn t impressive at all but the machine did at least manage to guess a little better than flipping a coin which would have given a success rate at seeing results is like fuel to your motivation so i d recommend you doing this for yourself once you have a basic grasp of how to use scikit learn after playing around with various scikit learn modules i decided to try and write a linear regression algorithm from the ground up i wanted to do this because i felt and still feel that i really don t understand what s happening on under the hood luckily the coursera course goes into detail on how a few of the algorithms work which came to great use at this point more specifically it describes the underlying concepts of using linear regression with gradient descent this has definitely been the most effective of learning technique as it forces you to understand the steps that are going on under the hood i strongly recommend you to do this at some point i plan to rewrite my own implementations of more complex algorithms as i go along but i prefer doing this after i ve played around with the respective algorithms in scikit learn on thursday i started doing kaggle s introductory tutorials kaggle is a platform for machine learning competitions where you can submit solutions to problems released by companies or organizations i recommend you trying out kaggle after having a little bit of a theoretical and practical understanding of machine learning you ll need this in order to start using kaggle otherwise it will be more frustrating than rewarding the bag of words tutorial guides you through every steps you need to take in order to enter a submission to a competition plus gives you a brief and exciting introduction into natural language processing nlp i ended the tutorial with much higher interest in nlp than i had when entering it friday i continued working on the kaggle tutorials and also started udacity s intro to machine learning i m currently half ways through and find it quite enjoyable it s a lot easier the coursera course as it doesn t go in depth in the algorithms but it s also more practical as it teaches you scikit learn which is a whole lot easier to apply to the real world than writing algorithms from the ground up in octave as you do in the coursera course doing it for a week hasn t just been great fun it has also helped my awareness of its usefulness of machine learning in society the more i learn about it the more i see which areas it can be used to solve problems choose a top down approach if you re not ready for the heavy stuff and get into problem solving as quickly as possible good luck thanks for reading my name is per i m a co founder of scrimba a better way to teach and learn code if you ve read this far i d recommend you to check out this demo from a quick cheer to a standing ovation clap to show how much you enjoyed this story co founder of scrimba the next generation platform for teaching and learning code https scrimba com a publication about improving your technical skills
Ahmed El Deeb,593,7,https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89?source=tag_archive---------2----------------,What to do with “small” data? – Rants on Machine Learning – Medium,by ahmed el deeb many technology companies now have teams of smart data scientists versed in big data infrastructure tools and machine learning algorithms but every now and then a data set with very few data points turns up and none of these algorithms seem to be working properly anymore what the hell is happening what can you do about it most data science relevance and machine learning activities in technology companies have been focused around big data and scenarios with huge data sets sets where the rows represent documents users files queries songs images etc things that are in the thousands hundreds of thousands millions or even billions the infrastructure tools and algorithms to deal with these kinds of data sets have been evolving very quickly and improving continuously during the last decade or so and most data scientists and machine learning practitioners have gained experience is such situations have grown accustomed to the appropriate algorithms and gained good intuitions about the usual trade offs bias variance flexibility stability hand crafted features vs feature learning etc but small data sets still arise in the wild every now and then and often they are trickier to handle require a different set of algorithms and a different set of skills small data sets arise is several situations problems of small data are numerous but mainly revolve around high variance hire a statistician i m not kidding statisticians are the original data scientists the field of statistics was developed when data was much harder to come by and as such was very aware of small sample problems statistical tests parametric models bootstrapping and other useful mathematical tools are the domain of classical statistics not modern machine learning lacking a good general purpose statistician get a marine biologist a zoologist a psychologist or anyone who was trained in a domain that deals with small sample experiments the closer to your domain the better if you don t want to hire a statistician full time on your team make it a temporary consultation but hiring a classically trained statistician could be a very good investment stick to simple models more precisely stick to a limited set of hypotheses one way to look at predictive modeling is as a search problem from an initial set of possible models which is the most appropriate model to fit our data in a way each data point we use for fitting down votes all models that make it unlikely or up vote models that agree with it when you have heaps of data you can afford to explore huge sets of models hypotheses effectively and end up with one that is suitable when you don t have so many data points to begin with you need to start from a fairly small set of possible hypotheses e g the set of all linear models with non zero weights the set of decision trees with depth the set of histograms with equally spaced bins this means that you rule out complex hypotheses like those that deal with non linearity or feature interactions this also means that you can t afford to fit models with too many degrees of freedom too many weights or parameters whenever appropriate use strong assumptions e g no negative weights no interaction between features specific distributions etc to restrict the space of possible hypotheses pool data when possible are you building a personalized spam filter try building it on top of a universal model trained for all users are you modeling gdp for a specific country try fitting your models on gdp for all countries for which you can get data maybe using importance sampling to emphasize the country you re interested in are you trying to predict the eruptions of a specific volcano you get the idea limit experimentation don t over use your validation set if you try too many different techniques and use a hold out set to compare between them be aware of the statistical power of the results you are getting and be aware that the performance you are getting on this set is not a good estimator for out of sample performance do clean up your data with small data sets noise and outliers are especially troublesome cleaning up your data could be crucial here to get sensible models alternatively you can restrict your modeling to techniques especially designed to be robust to outliers e g quantile regression do perform feature selection i am not a big fan of explicit feature selection i typically go for regularization and model averaging next two points to avoid over fitting but if the data is truly limiting sometimes explicit feature selection is essential wherever possible use domain expertise to do feature selection or elimination as brute force approaches e g all subsets or greedy forward selection are as likely to cause over fitting as including all features do use regularization regularization is an almost magical solution that constraints model fitting and reduces the effective degrees of freedom without reducing the actual number of parameters in the model l regularization produces models with fewer non zero parameters effectively performing implicit feature selection which could be desirable for explainability of performance in production while l regularization produces models with more conservative closer to zero parameters and is effectively similar to having strong zero centered priors for the parameters in the bayesian world l is usually better for prediction accuracy than l do use model averaging model averaging has similar effects to regularization is that it reduces variance and enhances generalization but it is a generic technique that can be used with any type of models or even with heterogeneous sets of models the downside here is that you end up with huge collections of models which could be slow to evaluate or awkward to deploy to a production system two very reasonable forms of model averaging are bagging and bayesian model averaging try bayesian modeling and model averaging again not a favorite technique of mine but bayesian inference may be well suited for dealing with smaller data sets especially if you can use domain expertise to construct sensible priors prefer confidence intervals to point estimates it is usually a good idea to get an estimate of confidence in your prediction in addition to producing the prediction itself for regression analysis this usually takes the form of predicting a range of values that is calibrated to cover the true value of the time or in the case of classification it could be just a matter of producing class probabilities this becomes more crucial with small data sets as it becomes more likely that certain regions in your feature space are less represented than others model averaging as referred to in the previous two points allows us to do that pretty easily in a generic way for regression classification and density estimation it is also useful to do that when evaluating your models producing confidence intervals on the metrics you are using to compare model performance is likely to save you from jumping to many wrong conclusions this could be a somewhat long list of things to do or try but they all revolve around three main themes constrained modeling smoothing and quantification of uncertainty most figures used in this post were taken from the book pattern recognition and machine learning by christopher bishop from a quick cheer to a standing ovation clap to show how much you enjoyed this story relevance engineer machine learning practitioner and hobbyist former entrepreneur rants about machine learning and its future
Matt Fogel,938,4,https://medium.com/swlh/the-7-best-data-science-and-machine-learning-podcasts-e8f0d5a4a419?source=tag_archive---------3----------------,The 7 Best Data Science and Machine Learning Podcasts,data science and machine learning have long been interests of mine but now that i m working on fuzzy ai and trying to make ai and machine learning accessible to all developers i need to keep on top of all the news in both fields my preferred way to do this is through listening to podcasts i ve listened to a bunch of machine learning and data science podcasts in the last few months so i thought i d share my favorites a great starting point on some of the basics of data science and machine learning every other week they release a minute episode where hosts kyle and linda polich give a short primer on topics like k means clustering natural language processing and decision tree learning often using analogies related to their pet parrot yoshi this is the only place where you ll learn about k means clustering via placement of parrot droppings website itunes hosted by katie malone and ben jaffe of online education startup udacity this weekly podcast covers diverse topics in data science and machine learning teaching specific concepts like hidden markov models and how they apply to real world problems and datasets they make complex topics extremely accessible website itunes each week hosts chris albon and jonathon morgan both experienced technologists and data scientists talk about the latest news in data science over drinks listening to partially derivative is a great way to keep up on the latest data news website itunes this podcast features ben lorica o reilly media s chief data scientist speaking with other experts about timely big data and data science topics it can often get quite technical but the topics of discussion are always really interesting website itunes data stories is a little more focused on data visualization than data science but there is often some interesting overlap between the topics every other week enrico bertini and moritz stefaner cover diverse topics in data with their guests recent episodes about smart cities and nicholas felton s annual reports are particularly interesting website itunes billing itself as a gentle introduction to artificial intelligence and machine learning this podcast can still get quite technical and complex covering topics like how to reason about uncertain events using fuzzy set theory and fuzzy measure theory and how to represent knowledge using logical rules website itunes the newest podcasts on this list with episodes released as of this writing every other week hosts katherine gorman and ryan adams speak with a guest about their work and news stories related to machine learning website itunes feel i ve unfairly left a podcast off this list leave me a note to let me know published in startups wanderlust and life hacking from a quick cheer to a standing ovation clap to show how much you enjoyed this story cofounder of fuzzyai helping developers make their software smarter faster medium s largest publication for makers subscribe to receive our top stories here https goo gl zhclji
Illia Polosukhin,255,3,https://medium.com/@ilblackdragon/tensorflow-tutorial-part-1-c559c63c0cb1?source=tag_archive---------4----------------,TensorFlow Tutorial— Part 1 – Illia Polosukhin – Medium,upd april scikit flow has been merged into tensorflow since version and now called tensorflow learn or tf learn google released a machine learning framework called tensorflow and it s taking the world by storm k stars on github a lot of publicity and general excitement in between ai researchers now but how you to use it for something regular problem data scientist may have and if you are ai researcher we will build up to interesting problems over time a reasonable question why as a data scientist who already has a number of tools in your toolbox r scikit learn etc you care about yet another framework the answer is two part let s start with simple example take titanic dataset from kaggle first make sure you have installed tensorflow and scikit learn with few helpful libs including scikit flow that is simplifying a lot of work with tensorflow you can get dataset and the code from http github com ilblackdragon tf examples quick look at the data use ipython or ipython notebook for ease of interactive exploration let s test how we can predict survived class based on float variables in scikit learn we separate dataset into features and target fill in n a in the data with zeros and build a logistic regression predicting on the training data gives us some measure of accuracy of cause it doesn t properly evaluate the model quality and test dataset should be used but for simplicity we will look at train only for now now using tf learn previously scikit flow congratulations you just built your first tensorflow model tf learn is a library that wraps a lot of new apis by tensorflow with nice and familiar scikit learn api tensorflow is all about a building and executing graph this is a very powerful concept but it is also cumbersome to start with looking under the hood of tf learn we just used three parts even as you get more familiar with tensorflow pieces of scikit flow will be useful like graph actions and layers and host of other ops and tools see future posts for examples of handling categorical variables text and images part deep neural networks custom tensorflow models with scikit flow and digit recognition with convolutional networks from a quick cheer to a standing ovation clap to show how much you enjoyed this story co founder near ai teaching machines to code i m tweeting as ilblackdragon
Ahmed El Deeb,283,3,https://medium.com/rants-on-machine-learning/the-unreasonable-effectiveness-of-random-forests-f33c3ce28883?source=tag_archive---------5----------------,The Unreasonable Effectiveness of Random Forests – Rants on Machine Learning – Medium,it s very common for machine learning practitioners to have favorite algorithms it s a bit irrational since no algorithm strictly dominates in all applications the performance of ml algorithms varies wildly depending on the application and the dimensionality of the dataset and even for a given problem and a given dataset any single model will likely be beaten by an ensemble of diverse models trained by diverse algorithms anyway but people have favorites nevertheless some like svms for the elegance of their formulation or the quality of the available implementations some like decision rules for their simplicity and interpretability and some are crazy about neural networks for their flexibility my favorite out of the box algorithm is as you might have guessed the random forest and it s the second modeling technique i typically try on any given data set after a linear model this beautiful visualization from scikit learn illustrates the modelling capacity of a decision forest here s a paper by leo breiman the inventor of the algorithms describing random forests here s another amazing paper by rich caruana et al evaluating several supervised learning algorithms on many different datasets from a quick cheer to a standing ovation clap to show how much you enjoyed this story relevance engineer machine learning practitioner and hobbyist former entrepreneur rants about machine learning and its future
Christophe Bourguignat,157,4,https://medium.com/@chris_bour/6-tricks-i-learned-from-the-otto-kaggle-challenge-a9299378cd61?source=tag_archive---------6----------------,6 Tricks I Learned From The OTTO Kaggle Challenge – Christophe Bourguignat – Medium,here are a few things i learned from the otto group kaggle competition i had the chance to team up with great kaggle master xavier conort and the french community as a whole has been very active teaming with xavier has been the opportunity to practice some ensembling technics we heavily used stacking we added to an initial set of features new features being the predictions of n different classifiers random forest gbm neural networks and then retrained p classifiers over the n features and finally made a weighted average of the p outputs we tested two tricks this is one of the great functionalities of the last scikit learn version it allows to rescale the classifier predictions by taking observations predicted within a segments e g and comparing to the actual truth ratio of these observation e g with means that a rescaling is needed here is a mini notebook explaining how to use calibration and demonstrating how well it worked on the otto challenge data at the beginning of the competition it appeared quickly that once again gradient boosting trees was one of the best performing algorithm provided that you find the right hyper parameters on the scikit learn implementation most important hyper parameters are learning rate the shrinkage parameter n estimators the number of boosting stages and max depth limits the number of nodes in the tree the best value depends on the interaction of the input variables min samples split and min samples leaf can also be a way to control depth of the trees for optimal performance i also discovered that two other parameters were crucial for this competition i must admit i never paid attention on it before this challenge namely subsample the fraction of samples to be used for fitting the individual base learners and max features the number of features to consider when looking for the best split the problem was to find a way to quickly find the best hyperparameters combination i first discovered gridsearchcv that makes an exhaustive search over specified parameter ranges as always with scikit learn it has a convenient programming interface handling for example smoothly cross validation and parallel distributing of search however the number of parameters to tune and their range was too large to discover the best ones in the acceptable time frame i had in mind typically while sleeping i e to hours i had to fall back to an other option i then used randomizedsearchcv that appeared in version with this method search is done randomly on a subspace of parameters it gives generally very good results as described in this paper and i was able to find a suitable parameter set within a few hours note that some competitors like french kaggler amine used hyperopt for hyperparameters optimization xgboost is a gradient boosting implementation heavily used by kagglers and i now understand why i never used it before but it was a hot topic discussed in the forum i decided to have a look at it even if its main interface is in r but there is a python api that i didn t use yet xgboost is much faster than scikit learn and gave better prediction it will remain for sure part of my toolblox someone posted on the forum he was right it has been for me the opportunity to play with neural networks for the first time several implementations have been used by the competitors h o keras cxxnet i personally used lasagne main challenges was to fine tune the number of layers number of neurons dropout and learning rate here is a notebook on what i learned one of the secret of the competition was to run several times the same algorithm with random selection of observations and features and take the average of the output to do that easily i discovered the scikit learn baggingclassifier meta estimator it hides the tedious complexity of looping over model fits random subsets selection and averaging and exposes easy fit predict proba entry points from a quick cheer to a standing ovation clap to show how much you enjoyed this story data enthusiast bigdata datascience machinelearning frenchdata kaggle
I'Boss Potiwarakorn,128,3,https://medium.com/o-v-e-r-f-i-t-t-e-d/machine-learning-%E0%B9%80%E0%B8%A3%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B8%AD%E0%B8%B0%E0%B9%84%E0%B8%A3-%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B9%84%E0%B8%9B%E0%B8%97%E0%B8%B3%E0%B9%84%E0%B8%A1-4a1dcae85b96?source=tag_archive---------7----------------,"Machine Learning เรียนอะไร, รู้ไปทําไม – O v e r f i t t e d – Medium",machine learning machine learning machine learning machine learning machine learning machine learning internet machine learning google search machn leaning google machine learning spam filtering face recognition handwriting recognition marketing machine learning customer segmentation customer churn prediction facebook s friend suggestions facebook machine learning requirement arthur samuel computer gaming artificial intelligence machine learning machine learning machine learning a b a b machine learning artificial intelligence data mining ai artificial intelligence data mining machine learning ai intelligent agent machine learning search ai machine learning ai data mining knowledge discovery data information data mining ai machine learning database system insight machine learning algorithm supervised learning unsupervised learning supervised learning training data label feature classification supervised learning discrete regression continuous linear regression line of best fit regression training data training data model y george e p box model y x supervised learning unsupervised learning supervised learning classification unsupervised learning clustering classification clustering label feature clustering classification feature visualize dimensionality reduction dimension reduction visualize feature performance space complexity machine learning algorithm basic semi supervised learning reinforcement learning konpat machine learning machine learning d from a quick cheer to a standing ovation clap to show how much you enjoyed this story software choreographer at thoughtworks functional programming devops and machine learning enthusiast a half score day a story blog by league of machine learning from chulalongkorn university
samim,301,4,https://medium.com/@samim/generating-stories-about-images-d163ba41e4ed?source=tag_archive---------8----------------,Generating Stories about Images – samim – Medium,stories are a fundamental human tool that we use to communicate thought creating a stories about a image is a difficult task that many struggle with new machine learning experiments are enabling us to generate stories based on the content of images this experiment explores how to generate little romantic stories about images incl guest star taylor swift neural storyteller is a recently published experiment by ryan kiros university of toronto it combines recurrent neural networks rnn skip thoughts vectors and other techniques to generate little story about images neural storyteller s outputs are creative and often comedic it is open source this experiment started by running randomly selected web images through neural storyteller and experimenting with hyper parameters neural storyteller comes with pre trained models one trained on million passages of romance novels the other trained on taylor swift lyrics inputs and outputs were manually filtered and recombined into two videos using romantic novel model voices generated with a text to speech using taylor swift model combined with a well known swift instrumental neural storyteller gives us a fascinating glimpse into the future of storytelling even though these technologies are not fully mature yet the art of storytelling is bound to change in the near future authors will be training custom models combining styles across genres and generating text with images sounds exploring this exiting new medium is rewarding from a quick cheer to a standing ovation clap to show how much you enjoyed this story designer code magician working at the intersection of hci machine learning creativity building tools for enlightenment narrative engineering
AirbnbEng,517,9,https://medium.com/airbnb-engineering/how-airbnb-uses-machine-learning-to-detect-host-preferences-18ce07150fa3?source=tag_archive---------9----------------,How Airbnb uses Machine Learning to Detect Host Preferences,by bar ifrach at airbnb we seek to match people who are looking for accommodation guests with those looking to rent out their place hosts guests reach out to hosts whose listings they wish to stay in however a match succeeds only if the host also wants to accommodate the guest i first heard about airbnb in from a friend he offered his nice apartment on the site when he traveled to see his family during our vacations from grad school his main goal was to fit as many booked nights as possible into the weeks when he was away my friend would accept or reject requests depending on whether or not the request would help him to maximize his occupancy about two years later i joined airbnb as a data scientist i remembered my friend s behavior and was curious to discover what affects hosts decisions to accept accommodation requests and how airbnb could increase acceptances and matches on the platform what started as a small research project resulted in the development of a machine learning model that learns our hosts preferences for accommodation requests based on their past behavior for each search query that a guest enters on airbnb s search engine our model computes the likelihood that relevant hosts will want to accommodate the guest s request then we surface likely matches more prominently in the search results in our a b testing the model showed about a increase in booking conversion resulting in many more matches on airbnb in this blog post i outline the process that brought us to this model i kicked off my research into hosts acceptances by checking if other hosts maximized their occupancy like my friend every accommodation request falls in a sequence or in a window of available days in the calendar such as on april in the calendar shown below the gray days surrounding the window are either blocked by the host or already booked if accepted and booked a request may leave the host with a sub window before the check in date check in gap april and or a sub window after the check out check out gap april a host looking to have a high occupancy will try to avoid such gaps indeed when i plotted hosts tendency to accept over the sum of the check in gap and the check out gap in the example above as in the next plot i found the effect that i expected to see hosts were more likely to accept requests that fit well in their calendar and minimize gap days but do all hosts try to maximize occupancy and prefer stays with short gaps perhaps some hosts are not interested in maximizing their occupancy and would rather host occasionally and maybe hosts in big markets like my friend are different from hosts in smaller markets indeed when i looked at listings from big and small markets separately i found that they behaved quite differently hosts in big markets care a lot about their occupancy a request with no gaps is almost likelier to be accepted than one with gap nights for small markets i found the opposite effect hosts prefer to have a small number of nights between requests so hosts in different markets have different preferences but it seems likely that even within a market hosts may prefer different stays a similar story revealed itself when i looked at hosts tendency to accept based on other characteristics of the accommodation request for example on average airbnb hosts prefer accommodation requests that are at least a week in advance over last minute requests but perhaps some hosts prefer short notice the plot below looks at the dispersion of hosts preferences for last minute stays less than days versus far in advance stays more than days indeed the dispersion in preferences reveals that some hosts like last minute stays better than far in advance stays those in the bottom right even though on average hosts prefer longer notice i found similar dispersion in hosts tendency to accept other trip characteristics like the number of guests whether it is a weekend trip etc all these findings pointed to the same conclusion if we could promote in our search results hosts who would be more likely to accept an accommodation request resulting from that search query we would expect to see happier guests and hosts and more matches that turned into fun vacations or productive business trips in other words we could personalize our search results but not in the way you might expect typically personalized search results promote results that would fit the unique preferences of the searcher the guest at a two sided marketplace like airbnb we also wanted to personalize search by the preference of the hosts whose listings would appear in the search results encouraged by my findings i joined forces with another data scientist and a software engineer to create a personalized search signal we set out to associate hosts prior acceptance and decline decisions by the following characteristics of the trip check in date check out date and number of guests by adding host preferences to our existing ranking model capturing guest preferences we hoped to enable more and better matches at first glance this seems like a perfect case for collaborative filtering we have users hosts and items trips and we want to understand the preference for those items by combining historical ratings accept decline with statistical learning from similar hosts however the application does not fully fit in the collaborative filtering framework for two reasons with these points in mind we decided to massage the problem into something resembling collaborative filtering we used the multiplicity of responses for the same trip to reduce the noise coming from the latent factors in the guest host interaction to do so we considered hosts average response to a certain trip characteristic in isolation instead of looking at the combination of trip length size of guest party size of calendar gap and so on we looked at each of these trip characteristics by itself with this coarser structure of preferences we were able to resolve some of the noise in our data as well as the potentially conflicting labels for the same trip we used the mean acceptance rate for each trip characteristic as a proxy for preference still our data set was relatively sparse on average for each trip characteristic we could not determine the preference for about of hosts because they never received an accommodation request that met those trip characteristics as a method of imputation we smoothed the preference using a weight function that for each trip characteristic averages the median preference of hosts in the region with the host s preference the weight on the median preference is when the host has no data points and goes to monotonically the more data points the host has using these newly defined preferences we created predictions for host acceptances using a l regularized logistic regression essentially we combine the preferences for different trip characteristics into a single prediction for the probability of acceptance the weight the preference of each trip characteristic has on the acceptance decision is the coefficient that comes out of the logistic regression to improve the prediction we include a few more geographic and host specific features in the logistic regression this flow chart summarizes the modeling technique we ran this model on segments of hosts on our cluster using a user generated function udf on hive the udf is written in python its inputs are accommodation requests hosts response to them and a few other host features depending on the flag passed to it the udf either builds the preferences for the different trip characteristics or trains the logistic regression model using scikit learn our main off line evaluation metric for the model was mean squared error mse which is more appropriate in a setting when we care about the predicted probability more than about classification in our off line evaluation of the model we were able to get a decrease in mse over our previous model that captured host acceptance probability this was a promising result but we still had to test the performance of the model live on our site to test the online performance of the model we launched an experiment that used the predicted probability of host acceptance as a significant weight in our ranking algorithm that also includes many other features that capture guests preferences every time a guest in the treatment group entered a search query our model predicted the probability of acceptance for all relevant hosts and influenced the order in which listings were presented to the guest ranking likelier matches higher we evaluated the experiment by looking at multiple metrics but the most important one was the likelihood that a guest requesting accommodation would get a booking booking conversion we found a lift in our booking conversion and a significant increase in the number of successful matches between guests and hosts after concluding the initial experiment we made a few more optimizations that improved conversion by approximately another and then launched the experiment to of users this was an exciting outcome for our first full fledged personalization search signal and a sizable contributor to our success first this project taught us that in a two sided marketplace personalization can be effective on the buyer as well as the seller side second the project taught us that sometimes you have to roll up your sleeves and build a machine learning model tailored for your own application in this case the application did not quite fit in the collaborative filtering and a multilevel model with host fixed effect was too computationally demanding and not suited for a sparse data set while building our own model took more time it was a fun learning experience finally this project would not have succeeded without the fantastic work of spencer de mars and lukasz dziurzynski originally published at nerds airbnb com on april from a quick cheer to a standing ovation clap to show how much you enjoyed this story creative engineers and data scientists building a world where you can belong anywhere http airbnb io creative engineers and data scientists building a world where you can belong anywhere http airbnb io
Adam Geitgey,14200,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721?source=tag_archive---------1----------------,Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in portugue s tie ng vie t or italiano are you tired of reading endless news stories about deep learning and not really knowing what that means let s change that this time we are going to learn how to write programs that recognize objects in images using deep learning in other words we re going to explain the black magic that allows google photos to search your photos based on what is in the picture just like part and part this guide is for anyone who is curious about machine learning but has no idea where to start the goal is be accessible to anyone which means that there s a lot of generalizations and we skip lots of details but who cares if this gets anyone more interested in ml then mission accomplished if you haven t already read part and part read them now you might have seen this famous xkcd comic before the goof is based on the idea that any year old child can recognize a photo of a bird but figuring out how to make a computer recognize objects has puzzled the very best computer scientists for over years in the last few years we ve finally found a good approach to object recognition using deep convolutional neural networks that sounds like a a bunch of made up words from a william gibson sci fi novel but the ideas are totally understandable if you break them down one by one so let s do it let s write a program that can recognize birds before we learn how to recognize pictures of birds let s learn how to recognize something much simpler the handwritten number in part we learned about how neural networks can solve complex problems by chaining together lots of simple neurons we created a small neural network to estimate the price of a house based on how many bedrooms it had how big it was and which neighborhood it was in we also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems so let s modify this same neural network to recognize handwritten text but to make the job really simple we ll only try to recognize one letter the numeral machine learning only works when you have data preferably a lot of data so we need lots and lots of handwritten s to get started luckily researchers created the mnist data set of handwritten numbers for this very purpose mnist provides images of handwritten digits each as an x image here are some s from the data set the neural network we made in part only took in a three numbers as the input bedrooms sq feet etc but now we want to process images with our neural network how in the world do we feed images into a neural network instead of just numbers the answer is incredible simple a neural network takes numbers as input to a computer an image is really just a grid of numbers that represent how dark each pixel is to feed an image into our neural network we simply treat the x pixel image as an array of numbers the handle inputs we ll just enlarge our neural network to have input nodes notice that our neural network also has two outputs now instead of just one the first output will predict the likelihood that the image is an and thee second output will predict the likelihood it isn t an by having a separate output for each type of object we want to recognize we can use a neural network to classify objects into groups our neural network is a lot bigger than last time inputs instead of but any modern computer can handle a neural network with a few hundred nodes without blinking this would even work fine on your cell phone all that s left is to train the neural network with images of s and not s so it learns to tell them apart when we feed in an we ll tell it the probability the image is an is and the probability it s not an is vice versa for the counter example images here s some of our training data we can train this kind of neural network in a few minutes on a modern laptop when it s done we ll have a neural network that can recognize pictures of s with a pretty high accuracy welcome to the world of late s era image recognition it s really neat that simply feeding pixels into a neural network actually worked to build image recognition machine learning is magic right well of course it s not that simple first the good news is that our recognizer really does work well on simple images where the letter is right in the middle of the image but now the really bad news our recognizer totally fails to work when the letter isn t perfectly centered in the image just the slightest position change ruins everything this is because our network only learned the pattern of a perfectly centered it has absolutely no idea what an off center is it knows exactly one pattern and one pattern only that s not very useful in the real world real world problems are never that clean and simple so we need to figure out how to make our neural network work in cases where the isn t perfectly centered we already created a really good program for finding an centered in an image what if we just scan all around the image for possible s in smaller sections one section at a time until we find one this approach called a sliding window it s the brute force solution it works well in some limited cases but it s really inefficient you have to check the same image over and over looking for objects of different sizes we can do better than this when we trained our network we only showed it s that were perfectly centered what if we train it with more data including s in all different positions and sizes all around the image we don t even need to collect new training data we can just write a script to generate new images with the s in all kinds of different positions in the image using this technique we can easily create an endless supply of training data more data makes the problem harder for our neural network to solve but we can compensate for that by making our network bigger and thus able to learn more complicated patterns to make the network bigger we just stack up layer upon layer of nodes we call this a deep neural network because it has more layers than a traditional neural network this idea has been around since the late s but until recently training this large of a neural network was just too slow to be useful but once we figured out how to use d graphics cards which were designed to do matrix multiplication really fast instead of normal computer processors working with large neural networks suddenly became practical in fact the exact same nvidia geforce gtx video card that you use to play overwatch can be used to train neural networks incredibly quickly but even though we can make our neural network really big and train it quickly with a d graphics card that still isn t going to get us all the way to a solution we need to be smarter about how we process images into our neural network think about it it doesn t make sense to train a network to recognize an at the top of a picture separately from training it to recognize an at the bottom of a picture as if those were two totally different objects there should be some way to make the neural network smart enough to know that an anywhere in the picture is the same thing without all that extra training luckily there is as a human you intuitively know that pictures have a hierarchy or conceptual structure consider this picture as a human you instantly recognize the hierarchy in this picture most importantly we recognize the idea of a child no matter what surface the child is on we don t have to re learn the idea of child for every possible surface it could appear on but right now our neural network can t do this it thinks that an in a different part of the image is an entirely different thing it doesn t understand that moving an object around in the picture doesn t make it something different this means it has to re learn the identify of each object in every possible position that sucks we need to give our neural network understanding of translation invariance an is an no matter where in the picture it shows up we ll do this using a process called convolution the idea of convolution is inspired partly by computer science and partly by biology i e mad scientists literally poking cat brains with weird probes to figure out how cats process images instead of feeding entire images into our neural network as one grid of numbers we re going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture here s how it s going to work step by step similar to our sliding window search above let s pass a sliding window over the entire original image and save each result as a separate tiny picture tile by doing this we turned our original image into equally sized tiny image tiles earlier we fed a single image into a neural network to see if it was an we ll do the exact same thing here but we ll do it for each individual image tile however there s one big twist we ll keep the same neural network weights for every single tile in the same original image in other words we are treating every image tile equally if something interesting appears in any given tile we ll mark that tile as interesting we don t want to lose track of the arrangement of the original tiles so we save the result from processing each tile into a grid in the same arrangement as the original image it looks like this in other words we ve started with a large image and we ended with a slightly smaller array that records which sections of our original image were the most interesting the result of step was an array that maps out which parts of the original image are the most interesting but that array is still pretty big to reduce the size of the array we downsample it using an algorithm called max pooling it sounds fancy but it isn t at all we ll just look at each x square of the array and keep the biggest number the idea here is that if we found something interesting in any of the four input tiles that makes up each x grid square we ll just keep the most interesting bit this reduces the size of our array while keeping the most important bits so far we ve reduced a giant image down into a fairly small array guess what that array is just a bunch of numbers so we can use that small array as input into another neural network this final neural network will decide if the image is or isn t a match to differentiate it from the convolution step we call it a fully connected network so from start to finish our whole five step pipeline looks like this our image processing pipeline is a series of steps convolution max pooling and finally a fully connected network when solving problems in the real world these steps can be combined and stacked as many times as you want you can have two three or even ten convolution layers you can throw in max pooling wherever you want to reduce the size of your data the basic idea is to start with a large image and continually boil it down step by step until you finally have a single result the more convolution steps you have the more complicated features your network will be able to learn to recognize for example the first convolution step might learn to recognize sharp edges the second convolution step might recognize beaks using it s knowledge of sharp edges the third step might recognize entire birds using it s knowledge of beaks etc here s what a more realistic deep convolutional network like you would find in a research paper looks like in this case they start a x pixel image apply convolution and max pooling twice apply convolution more times apply max pooling and then have two fully connected layers the end result is that the image is classified into one of categories so how do you know which steps you need to combine to make your image classifier work honestly you have to answer this by doing a lot of experimentation and testing you might have to train networks before you find the optimal structure and parameters for the problem you are solving machine learning involves a lot of trial and error now finally we know enough to write a program that can decide if a picture is a bird or not as always we need some data to get started the free cifar data set contains pictures of birds and pictures of things that are not birds but to get even more data we ll also add in the caltech ucsd birds data set that has another bird pics here s a few of the birds from our combined data set and here s some of the non bird images this data set will work fine for our purposes but low res images is still pretty small for real world applications if you want google level performance you need millions of large images in machine learning having more data is almost always more important that having better algorithms now you know why google is so happy to offer you unlimited photo storage they want your sweet sweet data to build our classifier we ll use tflearn tflearn is a wrapper around google s tensorflow deep learning library that exposes a simplified api it makes building convolutional neural networks as easy as writing a few lines of code to define the layers of our network here s the code to define and train the network if you are training with a good video card with enough ram like an nvidia geforce gtx ti or better this will be done in less than an hour if you are training with a normal cpu it might take a lot longer as it trains the accuracy will increase after the first pass i got accuracy after just passes it was already up to after or so passes it capped out around accuracy and additional training didn t help so i stopped it there congrats our program can now recognize birds in images now that we have a trained neural network we can use it here s a simple script that takes in a single image file and predicts if it is a bird or not but to really see how effective our network is we need to test it with lots of images the data set i created held back images for validation when i ran those images through the network it predicted the correct answer of the time that seems pretty good right well it depends our network claims to be accurate but the devil is in the details that could mean all sorts of different things for example what if of our training images were birds and the other were not birds a program that guessed not a bird every single time would be accurate but it would also be useless we need to look more closely at the numbers than just the overall accuracy to judge how good a classification system really is we need to look closely at how it failed not just the percentage of the time that it failed instead of thinking about our predictions as right and wrong let s break them down into four separate categories using our validation set of images here s how many times our predictions fell into each category why do we break our results down like this because not all mistakes are created equal imagine if we were writing a program to detect cancer from an mri image if we were detecting cancer we d rather have false positives than false negatives false negatives would be the worse possible case that s when the program told someone they definitely didn t have cancer but they actually did instead of just looking at overall accuracy we calculate precision and recall metrics precision and recall metrics give us a clearer picture of how well we did this tells us that of the time we guessed bird we were right but it also tells us that we only found of the actual birds in the data set in other words we might not find every bird but we are pretty sure about it when we do find one now that you know the basics of deep convolutional networks you can try out some of the examples that come with tflearn to get your hands dirty with different neural network architectures it even comes with built in data sets so you don t even have to find your own images you also know enough now to start branching and learning about other areas of machine learning why not learn how to use algorithms to train computers how to play atari games next if you liked this article please consider signing up for my machine learning is fun email list i ll only email you when i have something new and awesome to share it s the best way to find out when i write more articles like this you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part part and part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Adam Geitgey,15200,13,https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78?source=tag_archive---------2----------------,Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in portugue s tie ng vie t or italiano have you noticed that facebook has developed an uncanny ability to recognize your friends in your photographs in the old days facebook used to make you to tag your friends in photos by clicking on them and typing in their name now as soon as you upload a photo facebook tags everyone for you like magic this technology is called face recognition facebook s algorithms are able to recognize your friends faces after they have been tagged only a few times it s pretty amazing technology facebook can recognize faces with accuracy which is pretty much as good as humans can do let s learn how modern face recognition works but just recognizing your friends would be too easy we can push this tech to the limit to solve a more challenging problem telling will ferrell famous actor apart from chad smith famous rock musician so far in part and we ve used machine learning to solve isolated problems that have only one step estimating the price of a house generating new data based on existing data and telling if an image contains a certain object all of those problems can be solved by choosing one machine learning algorithm feeding in data and getting the result but face recognition is really a series of several related problems as a human your brain is wired to do all of this automatically and instantly in fact humans are too good at recognizing faces and end up seeing faces in everyday objects computers are not capable of this kind of high level generalization at least not yet so we have to teach them how to do each step in this process separately we need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step in other words we will chain together several machine learning algorithms let s tackle this problem one step at a time for each step we ll learn about a different machine learning algorithm i m not going to explain every single algorithm completely to keep this from turning into a book but you ll learn the main ideas behind each one and you ll learn how you can build your own facial recognition system in python using openface and dlib the first step in our pipeline is face detection obviously we need to locate the faces in a photograph before we can try to tell them apart if you ve used any camera in the last years you ve probably seen face detection in action face detection is a great feature for cameras when the camera can automatically pick out faces it can make sure that all the faces are in focus before it takes the picture but we ll use it for a different purpose finding the areas of the image we want to pass on to the next step in our pipeline face detection went mainstream in the early s when paul viola and michael jones invented a way to detect faces that was fast enough to run on cheap cameras however much more reliable solutions exist now we re going to use a method invented in called histogram of oriented gradients or just hog for short to find faces in an image we ll start by making our image black and white because we don t need color data to find faces then we ll look at every single pixel in our image one at a time for every single pixel we want to look at the pixels that directly surrounding it our goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it then we want to draw an arrow showing in which direction the image is getting darker if you repeat that process for every single pixel in the image you end up with every pixel being replaced by an arrow these arrows are called gradients and they show the flow from light to dark across the entire image this might seem like a random thing to do but there s a really good reason for replacing the pixels with gradients if we analyze pixels directly really dark images and really light images of the same person will have totally different pixel values but by only considering the direction that brightness changes both really dark images and really bright images will end up with the same exact representation that makes the problem a lot easier to solve but saving the gradient for every single pixel gives us way too much detail we end up missing the forest for the trees it would be better if we could just see the basic flow of lightness darkness at a higher level so we could see the basic pattern of the image to do this we ll break up the image into small squares of x pixels each in each square we ll count up how many gradients point in each major direction how many point up point up right point right etc then we ll replace that square in the image with the arrow directions that were the strongest the end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way to find faces in this hog image all we have to do is find the part of our image that looks the most similar to a known hog pattern that was extracted from a bunch of other training faces using this technique we can now easily find faces in any image if you want to try this step out yourself using python and dlib here s code showing how to generate and view hog representations of images whew we isolated the faces in our image but now we have to deal with the problem that faces turned different directions look totally different to a computer to account for this we will try to warp each picture so that the eyes and lips are always in the sample place in the image this will make it a lot easier for us to compare faces in the next steps to do this we are going to use an algorithm called face landmark estimation there are lots of ways to do this but we are going to use the approach invented in by vahid kazemi and josephine sullivan the basic idea is we will come up with specific points called landmarks that exist on every face the top of the chin the outside edge of each eye the inner edge of each eyebrow etc then we will train a machine learning algorithm to be able to find these specific points on any face here s the result of locating the face landmarks on our test image now that we know were the eyes and mouth are we ll simply rotate scale and shear the image so that the eyes and mouth are centered as best as possible we won t do any fancy d warps because that would introduce distortions into the image we are only going to use basic image transformations like rotation and scale that preserve parallel lines called affine transformations now no matter how the face is turned we are able to center the eyes and mouth are in roughly the same position in the image this will make our next step a lot more accurate if you want to try this step out yourself using python and dlib here s the code for finding face landmarks and here s the code for transforming the image using those landmarks now we are to the meat of the problem actually telling faces apart this is where things get really interesting the simplest approach to face recognition is to directly compare the unknown face we found in step with all the pictures we have of people that have already been tagged when we find a previously tagged face that looks very similar to our unknown face it must be the same person seems like a pretty good idea right there s actually a huge problem with that approach a site like facebook with billions of users and a trillion photos can t possibly loop through every previous tagged face to compare it to every newly uploaded picture that would take way too long they need to be able to recognize faces in milliseconds not hours what we need is a way to extract a few basic measurements from each face then we could measure our unknown face the same way and find the known face with the closest measurements for example we might measure the size of each ear the spacing between the eyes the length of the nose etc if you ve ever watched a bad crime show like csi you know what i am talking about ok so which measurements should we collect from each face to build our known face database ear size nose length eye color something else it turns out that the measurements that seem obvious to us humans like eye color don t really make sense to a computer looking at individual pixels in an image researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself deep learning does a better job than humans at figuring out which parts of a face are important to measure the solution is to train a deep convolutional neural network just like we did in part but instead of training the network to recognize pictures objects like we did last time we are going to train it to generate measurements for each face the training process works by looking at face images at a time then the algorithm looks at the measurements it is currently generating for each of those three images it then tweaks the neural network slightly so that it makes sure the measurements it generates for and are slightly closer while making sure the measurements for and are slightly further apart after repeating this step millions of times for millions of images of thousands of different people the neural network learns to reliably generate measurements for each person any ten different pictures of the same person should give roughly the same measurements machine learning people call the measurements of each face an embedding the idea of reducing complicated raw data like a picture into a list of computer generated numbers comes up a lot in machine learning especially in language translation the exact approach for faces we are using was invented in by researchers at google but many similar approaches exist this process of training a convolutional neural network to output face embeddings requires a lot of data and computer power even with an expensive nvidia telsa video card it takes about hours of continuous training to get good accuracy but once the network has been trained it can generate measurements for any face even ones it has never seen before so this step only needs to be done once lucky for us the fine folks at openface already did this and they published several trained networks which we can directly use thanks brandon amos and team so all we need to do ourselves is run our face images through their pre trained network to get the measurements for each face here s the measurements for our test image so what parts of the face are these numbers measuring exactly it turns out that we have no idea it doesn t really matter to us all that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person if you want to try this step yourself openface provides a lua script that will generate embeddings all images in a folder and write them to a csv file you run it like this this last step is actually the easiest step in the whole process all we have to do is find the person in our database of known people who has the closest measurements to our test image you can do that by using any basic machine learning classification algorithm no fancy deep learning tricks are needed we ll use a simple linear svm classifier but lots of classification algorithms could work all we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match running this classifier takes milliseconds the result of the classifier is the name of the person so let s try out our system first i trained a classifier with the embeddings of about pictures each of will ferrell chad smith and jimmy falon then i ran the classifier on every frame of the famous youtube video of will ferrell and chad smith pretending to be each other on the jimmy fallon show it works and look how well it works for faces in different poses even sideways faces let s review the steps we followed now that you know how this all works here s instructions from start to finish of how run this entire face recognition pipeline on your own computer update you can still follow the steps below to use openface however i ve released a new python based face recognition library called face recognition that is much easier to install and use so i d recommend trying out face recognition first instead of continuing below i even put together a pre configured virtual machine with face recognition opencv tensorflow and lots of other deep learning tools pre installed you can download and run it on your computer very easily give the virtual machine a shot if you don t want to install all these libraries yourself original openface instructions if you liked this article please consider signing up for my machine learning is fun newsletter you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Adam Geitgey,10400,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3?source=tag_archive---------3----------------,Machine Learning is Fun! Part 2 – Adam Geitgey – Medium,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in italiano espan ol franc ais tu rkc e portugue s tie ng vie t or in part we said that machine learning is using generic algorithms to tell you something interesting about your data without writing any code specific to the problem you are solving if you haven t already read part read it now this time we are going to see one of these generic algorithms do something really cool create video game levels that look like they were made by humans we ll build a neural network feed it existing super mario levels and watch new ones pop out just like part this guide is for anyone who is curious about machine learning but has no idea where to start the goal is be accessible to anyone which means that there s a lot of generalizations and we skip lots of details but who cares if this gets anyone more interested in ml then mission accomplished back in part we created a simple algorithm that estimated the value of a house based on its attributes given data about a house like this we ended up with this simple estimation function in other words we estimated the value of the house by multiplying each of its attributes by a weight then we just added those numbers up to get the house s value instead of using code let s represent that same function as a simple diagram however this algorithm only works for simple problems where the result has a linear relationship with the input what if the truth behind house prices isn t so simple for example maybe the neighborhood matters a lot for big houses and small houses but doesn t matter at all for medium sized houses how could we capture that kind of complicated detail in our model to be more clever we could run this algorithm multiple times with different of weights that each capture different edge cases now we have four different price estimates let s combine those four price estimates into one final estimate we ll run them through the same algorithm again but using another set of weights our new super answer combines the estimates from our four different attempts to solve the problem because of this it can model more cases than we could capture in one simple model let s combine our four attempts to guess into one big diagram this is a neural network each node knows how to take in a set of inputs apply weights to them and calculate an output value by chaining together lots of these nodes we can model complex functions there s a lot that i m skipping over to keep this brief including feature scaling and the activation function but the most important part is that these basic ideas click it s just like lego we can t model much with one single lego block but we can model anything if we have enough basic lego blocks to stick together the neural network we ve seen always returns the same answer when you give it the same inputs it has no memory in programming terms it s a stateless algorithm in many cases like estimating the price of house that s exactly what you want but the one thing this kind of model can t do is respond to patterns in data over time imagine i handed you a keyboard and asked you to write a story but before you start my job is to guess the very first letter that you will type what letter should i guess i can use my knowledge of english to increase my odds of guessing the right letter for example you will probably type a letter that is common at the beginning of words if i looked at stories you wrote in the past i could narrow it down further based on the words you usually use at the beginning of your stories once i had all that data i could use it to build a neural network to model how likely it is that you would start with any given letter our model might look like this but let s make the problem harder let s say i need to guess the next letter you are going to type at any point in your story this is a much more interesting problem let s use the first few words of ernest hemingway s the sun also rises as an example what letter is going to come next you probably guessed n the word is probably going to be boxing we know this based on the letters we ve already seen in the sentence and our knowledge of common words in english also the word middleweight gives us an extra clue that we are talking about boxing in other words it s easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of english to solve this problem with a neural network we need to add state to our model each time we ask our neural network for an answer we also save a set of our intermediate calculations and re use them the next time as part of our input that way our model will adjust its predictions based on the input that it has seen recently keeping track of state in our model makes it possible to not just predict the most likely first letter in the story but to predict the most likely next letter given all previous letters this is the basic idea of a recurrent neural network we are updating the network each time we use it this allows it to update its predictions based on what it saw most recently it can even model patterns over time as long as we give it enough of a memory predicting the next letter in a story might seem pretty useless what s the point one cool use might be auto predict for a mobile phone keyboard but what if we took this idea to the extreme what if we asked the model to predict the next most likely character over and over forever we d be asking it to write a complete story for us we saw how we could guess the next letter in hemingway s sentence let s try generating a whole story in the style of hemingway to do this we are going to use the recurrent neural network implementation that andrej karpathy wrote andrej is a deep learning researcher at stanford and he wrote an excellent introduction to generating text with rnns you can view all the code for the model on github we ll create our model from the complete text of the sun also rises characters using unique letters including punctuation uppercase lowercase etc this data set is actually really small compared to typical real world applications to generate a really good model of hemingway s style it would be much better to have at several times as much sample text but this is good enough to play around with as an example as we just start to train the rnn it s not very good at predicting letters here s what it generates after a loops of training you can see that it has figured out that sometimes words have spaces between them but that s about it after about iterations things are looking more promising the model has started to identify the patterns in basic sentence structure it s adding periods at the ends of sentences and even quoting dialog a few words are recognizable but there s also still a lot of nonsense but after several thousand more training iterations it looks pretty good at this point the algorithm has captured the basic pattern of hemingway s short direct dialog a few sentences even sort of make sense compare that with some real text from the book even by only looking for patterns one character at a time our algorithm has reproduced plausible looking prose with proper formatting that is kind of amazing we don t have to generate text completely from scratch either we can seed the algorithm by supplying the first few letters and just let it find the next few letters for fun let s make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of er he and the s not bad but the really mind blowing part is that this algorithm can figure out patterns in any sequence of data it can easily generate real looking recipes or fake obama speeches but why limit ourselves human language we can apply this same idea to any kind of sequential data that has a pattern in nintendo released super mario makertm for the wii u gaming system this game lets you draw out your own super mario brothers levels on the gamepad and then upload them to the internet so you friends can play through them you can include all the classic power ups and enemies from the original mario games in your levels it s like a virtual lego set for people who grew up playing super mario brothers can we use the same model that generated fake hemingway text to generate fake super mario brothers levels first we need a data set for training our model let s take all the outdoor levels from the original super mario brothers game released in this game has levels and about of them have the same outdoor style so we ll stick to those to get the designs for each level i took an original copy of the game and wrote a program to pull the level designs out of the game s memory super mario bros is a year old game and there are lots of resources online that help you figure out how the levels were stored in the game s memory extracting level data from an old video game is a fun programming exercise that you should try sometime here s the first level from the game which you probably remember if you ever played it if we look closely we can see the level is made of a simple grid of objects we could just as easily represent this grid as a sequence of characters with one character representing each object we ve replaced each object in the level with a letter and so on using a different letter for each different kind of object in the level i ended up with text files that looked like this looking at the text file you can see that mario levels don t really have much of a pattern if you read them line by line the patterns in a level really emerge when you think of the level as a series of columns so in order for the algorithm to find the patterns in our data we need to feed the data in column by column figuring out the most effective representation of your input data called feature selection is one of the keys of using machine learning algorithms well to train the model i needed to rotate my text files by degrees this made sure the characters were fed into the model in an order where a pattern would more easily show up just like we saw when creating the model of hemingway s prose a model improves as we train it after a little training our model is generating junk it sort of has an idea that s and s should show up a lot but that s about it it hasn t figured out the pattern yet after several thousand iterations it s starting to look like something the model has almost figured out that each line should be the same length it has even started to figure out some of the logic of mario the pipes in mario are always two blocks wide and at least two blocks high so the p s in the data should appear in x clusters that s pretty cool with a lot more training the model gets to the point where it generates perfectly valid data let s sample an entire level s worth of data from our model and rotate it back horizontal this data looks great there are several awesome things to notice finally let s take this level and recreate it in super mario maker play it yourself if you have super mario maker you can play this level by bookmarking it online or by looking it up using level code ac f c the recurrent neural network algorithm we used to train our model is the same kind of algorithm used by real world companies to solve hard problems like speech detection and language translation what makes our model a toy instead of cutting edge is that our model is generated from very little data there just aren t enough levels in the original super mario brothers game to provide enough data for a really good model if we could get access to the hundreds of thousands of user created super mario maker levels that nintendo has we could make an amazing model but we can t because nintendo won t let us have them big companies don t give away their data for free as machine learning becomes more important in more industries the difference between a good program and a bad program will be how much data you have to train your models that s why companies like google and facebook need your data so badly for example google recently open sourced tensorflow its software toolkit for building large scale machine learning applications it was a pretty big deal that google gave away such important capable technology for free this is the same stuff that powers google translate but without google s massive trove of data in every language you can t create a competitor to google translate data is what gives google its edge think about that the next time you open up your google maps location history or facebook location history and notice that it stores every place you ve ever been in machine learning there s never a single way to solve a problem you have limitless options when deciding how to pre process your data and which algorithms to use often combining multiple approaches will give you better results than any single approach readers have sent me links to other interesting approaches to generating super mario levels if you liked this article please consider signing up for my machine learning is fun email list i ll only email you when i have something new and awesome to share it s the best way to find out when i write more articles like this you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Arthur Juliani,9000,6,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=tag_archive---------4----------------,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,for this tutorial in my reinforcement learning series we are going to be exploring a family of rl algorithms called q learning algorithms these are a little different than the policy based algorithms that will be looked at in the the following tutorials parts instead of starting with a complex and unwieldy deep neural network we will begin by implementing a simple lookup table version of the algorithm and then show how to implement a neural network equivalent using tensorflow given that we are going back to basics it may be best to think of this as part of the series it will hopefully give an intuition into what is really happening in q learning that we can then build on going forward when we eventually combine the policy gradient and q learning approaches to build state of the art rl agents if you are more interested in policy networks or already have a grasp on q learning feel free to start the tutorial series here instead unlike policy gradient methods which attempt to learn functions which directly map an observation to an action q learning attempts to learn the value of being in a given state and taking a specific action there while both approaches ultimately allow us to take intelligent actions given a situation the means of getting to that action differ significantly you may have heard about deepq networks which can play atari games these are really just larger and more complex implementations of the q learning algorithm we are going to discuss here for this tutorial we are going to be attempting to solve the frozenlake environment from the openai gym for those unfamiliar the openai gym provides an easy way for people to experiment with their learning agents in an array of provided toy games the frozenlake environment consists of a x grid of blocks each one either being the start block the goal block a safe frozen block or a dangerous hole the objective is to have an agent learn to navigate from the start to the goal without moving onto a hole at any given time the agent can choose to move either up down left or right the catch is that there is a wind which occasionally blows the agent onto a space they didn t choose as such perfect performance every time is impossible but learning to avoid the holes and reach the goal are certainly still doable the reward at every step is except for entering the goal which provides a reward of thus we will need an algorithm that learns long term expected rewards this is exactly what q learning is designed to provide in it s simplest implementation q learning is a table of values for every state row and action column possible in the environment within each cell of the table we learn a value for how good it is to take a given action within a given state in the case of the frozenlake environment we have possible states one for each block and possible actions the four directions of movement giving us a x table of q values we start by initializing the table to be uniform all zeros and then as we observe the rewards we obtain for various actions we update the table accordingly we make updates to our q table using something called the bellman equation which states that the expected long term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state in this way we reuse our own q table when estimating how to update our table for future actions in equation form the rule looks like this this says that the q value for a given state s and action a should represent the current reward r plus the maximum discounted future reward expected according to our own table for the next state s we would end up in the discount variable allows us to decide how important the possible future rewards are compared to the present reward by updating in this way the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state below is a python walkthrough of the q table algorithm implemented in the frozenlake environment thanks to praneet d for finding the optimal hyperparameters for this approach now you may be thinking tables are great but they don t really scale do they while it is easy to have a x table for a simple grid world the number of possible states in any modern game or real world environment is nearly infinitely larger for most interesting problems tables simply don t work we instead need some way to take a description of our state and produce q values for actions without a table that is where neural networks come in by acting as a function approximator we can take any number of possible states that can be represented as a vector and learn to map them to q values in the case of the frozenlake example we will be using a one layer network which takes the state encoded in a one hot vector x and produces a vector of q values one for each action such a simple network acts kind of like a glorified table with the network weights serving as the old cells the key difference is that we can easily expand the tensorflow network with added layers activation functions and different input types whereas all that is impossible with a regular table the method of updating is a little different as well instead of directly updating our table with a network we will be using backpropagation and a loss function our loss function will be sum of squares loss where the difference between the current predicted q values and the target value is computed and the gradients passed through the network in this case our q target for the chosen action is the equivalent to the q value computed in equation above below is the tensorflow walkthrough of implementing our simple q network while the network learns to solve the frozenlake problem it turns out it doesn t do so quite as efficiently as the q table while neural networks allow for greater flexibility they do so at the cost of stability when it comes to q learning there are a number of possible extensions to our simple q network which allow for greater performance and more robust learning two tricks in particular are referred to as experience replay and freezing target networks those improvements and other tweaks were the key to getting atari playing deep q networks and we will be exploring those additions in the future for more info on the theory behind q learning see this great post by tambet matiisen i hope this tutorial has been helpful for those curious about how to implement simple q learning algorithms if this post has been valuable to you please consider donating to help support future tutorials articles and implementations any contribution is greatly appreciated if you d like to follow my work on deep learning ai and cognitive science follow me on medium arthur juliani or on twitter awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning unity d cognitive neuroscience phd student exploring frontier technology through the lens of artificial intelligence data science and the shape of things to come
Adam Geitgey,6800,11,https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=tag_archive---------5----------------,Machine Learning is Fun Part 6: How to do Speech Recognition with Deep Learning,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in tie ng vie t or speech recognition is invading our lives it s built into our phones our game consoles and our smart watches it s even automating our homes for just you can get an amazon echo dot a magic box that allows you to order pizza get a weather report or even buy trash bags just by speaking out loud the echo dot has been so popular this holiday season that amazon can t seem to keep them in stock but speech recognition has been around for decades so why is it just now hitting the mainstream the reason is that deep learning finally made speech recognition accurate enough to be useful outside of carefully controlled environments andrew ng has long predicted that as speech recognition goes from accurate to accurate it will become a primary way that we interact with computers the idea is that this accuracy gap is the difference between annoyingly unreliable and incredibly useful thanks to deep learning we re finally cresting that peak let s learn how to do speech recognition with deep learning if you know how neural machine translation works you might guess that we could simply feed sound recordings into a neural network and train it to produce text that s the holy grail of speech recognition with deep learning but we aren t quite there yet at least at the time that i wrote this i bet that we will be in a couple of years the big problem is that speech varies in speed one person might say hello very quickly and another person might say heeeelllllllllllllooooo very slowly producing a much longer sound file with much more data both both sound files should be recognized as exactly the same text hello automatically aligning audio files of various lengths to a fixed length piece of text turns out to be pretty hard to work around this we have to use some special tricks and extra precessing in addition to a deep neural network let s see how it works the first step in speech recognition is obvious we need to feed sound waves into a computer in part we learned how to take an image and treat it as an array of numbers so that we can feed directly into a neural network for image recognition but sound is transmitted as waves how do we turn sound waves into numbers let s use this sound clip of me saying hello sound waves are one dimensional at every moment in time they have a single value based on the height of the wave let s zoom in on one tiny part of the sound wave and take a look to turn this sound wave into numbers we just record of the height of the wave at equally spaced points this is called sampling we are taking a reading thousands of times a second and recording a number representing the height of the sound wave at that point in time that s basically all an uncompressed wav audio file is cd quality audio is sampled at khz readings per second but for speech recognition a sampling rate of khz samples per second is enough to cover the frequency range of human speech lets sample our hello sound wave times per second here s the first samples you might be thinking that sampling is only creating a rough approximation of the original sound wave because it s only taking occasional readings there s gaps in between our readings so we must be losing data right but thanks to the nyquist theorem we know that we can use math to perfectly reconstruct the original sound wave from the spaced out samples as long as we sample at least twice as fast as the highest frequency we want to record i mention this only because nearly everyone gets this wrong and assumes that using higher sampling rates always leads to better audio quality it doesn t end rant we now have an array of numbers with each number representing the sound wave s amplitude at th of a second intervals we could feed these numbers right into a neural network but trying to recognize speech patterns by processing these samples directly is difficult instead we can make the problem easier by doing some pre processing on the audio data let s start by grouping our sampled audio into millisecond long chunks here s our first milliseconds of audio i e our first samples plotting those numbers as a simple line graph gives us a rough approximation of the original sound wave for that millisecond period of time this recording is only th of a second long but even this short recording is a complex mish mash of different frequencies of sound there s some low sounds some mid range sounds and even some high pitched sounds sprinkled in but taken all together these different frequencies mix together to make up the complex sound of human speech to make this data easier for a neural network to process we are going to break apart this complex sound wave into it s component parts we ll break out the low pitched parts the next lowest pitched parts and so on then by adding up how much energy is in each of those frequency bands from low to high we create a fingerprint of sorts for this audio snippet imagine you had a recording of someone playing a c major chord on a piano that sound is the combination of three musical notes c e and g all mixed together into one complex sound we want to break apart that complex sound into the individual notes to discover that they were c e and g this is the exact same idea we do this using a mathematic operation called a fourier transform it breaks apart the complex sound wave into the simple sound waves that make it up once we have those individual sound waves we add up how much energy is contained in each one the end result is a score of how important each frequency range is from low pitch i e bass notes to high pitch each number below represents how much energy was in each hz band of our millisecond audio clip but this is a lot easier to see when you draw this as a chart if we repeat this process on every millisecond chunk of audio we end up with a spectrogram each column from left to right is one ms chunk a spectrogram is cool because you can actually see musical notes and other pitch patterns in audio data a neural network can find patterns in this kind of data more easily than raw sound waves so this is the data representation we ll actually feed into our neural network now that we have our audio in a format that s easy to process we will feed it into a deep neural network the input to the neural network will be millisecond audio chunks for each little audio slice it will try to figure out the letter that corresponds the sound currently being spoken we ll use a recurrent neural network that is a neural network that has a memory that influences future predictions that s because each letter it predicts should affect the likelihood of the next letter it will predict too for example if we have said hel so far it s very likely we will say lo next to finish out the word hello it s much less likely that we will say something unpronounceable next like xyz so having that memory of previous predictions helps the neural network make more accurate predictions going forward after we run our entire audio clip through the neural network one chunk at a time we ll end up with a mapping of each audio chunk to the letters most likely spoken during that chunk here s what that mapping looks like for me saying hello our neural net is predicting that one likely thing i said was hhhee ll lllooo but it also thinks that it was possible that i said hhhuu ll lllooo or even aaauu ll lllooo we have some steps we follow to clean up this output first we ll replace any repeated characters a single character then we ll remove any blanks that leaves us with three possible transcriptions hello hullo and aullo if you say them out loud all of these sound similar to hello because it s predicting one character at a time the neural network will come up with these very sounded out transcriptions for example if you say he would not go it might give one possible transcription as he wud net go the trick is to combine these pronunciation based predictions with likelihood scores based on large database of written text books news articles etc you throw out transcriptions that seem the least likely to be real and keep the transcription that seems the most realistic of our possible transcriptions hello hullo and aullo obviously hello will appear more frequently in a database of text not to mention in our original audio based training data and thus is probably correct so we ll pick hello as our final transcription instead of the others done you might be thinking but what if someone says hullo it s a valid word maybe hello is the wrong transcription of course it is possible that someone actually said hullo instead of hello but a speech recognition system like this trained on american english will basically never produce hullo as the transcription it s just such an unlikely thing for a user to say compared to hello that it will always think you are saying hello no matter how much you emphasize the u sound try it out if your phone is set to american english try to get your phone s digital assistant to recognize the world hullo you can t it refuses it will always understand it as hello not recognizing hullo is a reasonable behavior but sometimes you ll find annoying cases where your phone just refuses to understand something valid you are saying that s why these speech recognition models are always being retrained with more data to fix these edge cases one of the coolest things about machine learning is how simple it sometimes seems you get a bunch of data feed it into a machine learning algorithm and then magically you have a world class ai system running on your gaming laptop s video card right that sort of true in some cases but not for speech recognizing speech is a hard problem you have to overcome almost limitless challenges bad quality microphones background noise reverb and echo accent variations and on and on all of these issues need to be present in your training data to make sure the neural network can deal with them here s another example did you know that when you speak in a loud room you unconsciously raise the pitch of your voice to be able to talk over the noise humans have no problem understanding you either way but neural networks need to be trained to handle this special case so you need training data with people yelling over noise to build a voice recognition system that performs on the level of siri google now or alexa you will need a lot of training data far more data than you can likely get without hiring hundreds of people to record it for you and since users have low tolerance for poor quality voice recognition systems you can t skimp on this no one wants a voice recognition system that works of the time for a company like google or amazon hundreds of thousands of hours of spoken audio recorded in real life situations is gold that s the single biggest thing that separates their world class speech recognition system from your hobby system the whole point of putting google now and siri on every cell phone for free or selling alexa units that have no subscription fee is to get you to use them as much as possible every single thing you say into one of these systems is recorded forever and used as training data for future versions of speech recognition algorithms that s the whole game don t believe me if you have an android phone with google now click here to listen to actual recordings of yourself saying every dumb thing you ve ever said into it so if you are looking for a start up idea i wouldn t recommend trying to build your own speech recognition system to compete with google instead figure out a way to get people to give you recordings of themselves talking for hours the data can be your product instead if you liked this article please consider signing up for my machine learning is fun email list i ll only email you when i have something new and awesome to share it s the best way to find out when i write more articles like this you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Adam Geitgey,5800,16,https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa?source=tag_archive---------6----------------,Machine Learning is Fun Part 5: Language Translation with Deep Learning and the Magic of Sequences,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in tie ng vie t or italiano we all know and love google translate the website that can instantly translate between different human languages as if by magic it is even available on our phones and smartwatches the technology behind google translate is called machine translation it has changed the world by allowing people to communicate when it wouldn t otherwise be possible but we all know that high school students have been using google translate to umm assist with their spanish homework for years isn t this old news it turns out that over the past two years deep learning has totally rewritten our approach to machine translation deep learning researchers who know almost nothing about language translation are throwing together relatively simple machine learning solutions that are beating the best expert built language translation systems in the world the technology behind this breakthrough is called sequence to sequence learning it s very powerful technique that be used to solve many kinds problems after we see how it is used for translation we ll also learn how the exact same algorithm can be used to write ai chat bots and describe pictures let s go so how do we program a computer to translate human language the simplest approach is to replace every word in a sentence with the translated word in the target language here s a simple example of translating from spanish to english word by word this is easy to implement because all you need is a dictionary to look up each word s translation but the results are bad because it ignores grammar and context so the next thing you might do is start adding language specific rules to improve the results for example you might translate common two word phrases as a single group and you might swap the order nouns and adjectives since they usually appear in reverse order in spanish from how they appear in english that worked if we just keep adding more rules until we can handle every part of grammar our program should be able to translate any sentence right this is how the earliest machine translation systems worked linguists came up with complicated rules and programmed them in one by one some of the smartest linguists in the world labored for years during the cold war to create translation systems as a way to interpret russian communications more easily unfortunately this only worked for simple plainly structured documents like weather reports it didn t work reliably for real world documents the problem is that human language doesn t follow a fixed set of rules human languages are full of special cases regional variations and just flat out rule breaking the way we speak english more influenced by who invaded who hundreds of years ago than it is by someone sitting down and defining grammar rules after the failure of rule based systems new translation approaches were developed using models based on probability and statistics instead of grammar rules building a statistics based translation system requires lots of training data where the exact same text is translated into at least two languages this double translated text is called parallel corpora in the same way that the rosetta stone was used by scientists in the s to figure out egyptian hieroglyphs from greek computers can use parallel corpora to guess how to convert text from one language to another luckily there s lots of double translated text already sitting around in strange places for example the european parliament translates their proceedings into languages so researchers often use that data to help build translation systems the fundamental difference with statistical translation systems is that they don t try to generate one exact translation instead they generate thousands of possible translations and then they rank those translations by likely each is to be correct they estimate how correct something is by how similar it is to the training data here s how it works first we break up our sentence into simple chunks that can each be easily translated next we will translate each of these chunks by finding all the ways humans have translated those same chunks of words in our training data it s important to note that we are not just looking up these chunks in a simple translation dictionary instead we are seeing how actual people translated these same chunks of words in real world sentences this helps us capture all of the different ways they can be used in different contexts some of these possible translations are used more frequently than others based on how frequently each translation appears in our training data we can give it a score for example it s much more common for someone to say quiero to mean i want than to mean i try so we can use how frequently quiero was translated to i want in our training data to give that translation more weight than a less frequent translation next we will use every possible combination of these chunks to generate a bunch of possible sentences just from the chunk translations we listed in step we can already generate nearly different variations of our sentence by combining the chunks in different ways here are some examples but in a real world system there will be even more possible chunk combinations because we ll also try different orderings of words and different ways of chunking the sentence now need to scan through all of these generated sentences to find the one that is that sounds the most human to do this we compare each generated sentence to millions of real sentences from books and news stories written in english the more english text we can get our hands on the better take this possible translation it s likely that no one has ever written a sentence like this in english so it would not be very similar to any sentences in our data set we ll give this possible translation a low probability score but look at this possible translation this sentence will be similar to something in our training set so it will get a high probability score after trying all possible sentences we ll pick the sentence that has the most likely chunk translations while also being the most similar overall to real english sentences our final translation would be i want to go to the prettiest beach not bad statistical machine translation systems perform much better than rule based systems if you give them enough training data franz josef och improved on these ideas and used them to build google translate in the early s machine translation was finally available to the world in the early days it was surprising to everyone that the dumb approach to translating based on probability worked better than rule based systems designed by linguists this led to a somewhat mean saying among researchers in the s statistical machine translation systems work well but they are complicated to build and maintain every new pair of languages you want to translate requires experts to tweak and tune a new multi step translation pipeline because it is so much work to build these different pipelines trade offs have to be made if you are asking google to translate georgian to telegu it has to internally translate it into english as an intermediate step because there s not enough georgain to telegu translations happening to justify investing heavily in that language pair and it might do that translation using a less advanced translation pipeline than if you had asked it for the more common choice of french to english wouldn t it be cool if we could have the computer do all that annoying development work for us the holy grail of machine translation is a black box system that learns how to translate by itself just by looking at training data with statistical machine translation humans are still needed to build and tweak the multi step statistical models in kyunghyun cho s team made a breakthrough they found a way to apply deep learning to build this black box system their deep learning model takes in a parallel corpora and and uses it to learn how to translate between those two languages without any human intervention two big ideas make this possible recurrent neural networks and encodings by combining these two ideas in a clever way we can build a self learning translation system we ve already talked about recurrent neural networks in part but let s quickly review a regular non recurrent neural network is a generic machine learning algorithm that takes in a list of numbers and calculates a result based on previous training neural networks can be used as a black box to solve lots of problems for example we can use a neural network to calculate the approximate value of a house based on attributes of that house but like most machine learning algorithms neural networks are stateless you pass in a list of numbers and the neural network calculates a result if you pass in those same numbers again it will always calculate the same result it has no memory of past calculations in other words always equals a recurrent neural network or rnn for short is a slightly tweaked version of a neural network where the previous state of the neural network is one of the inputs to the next calculation this means that previous calculations change the results of future calculations why in the world would we want to do this shouldn t always equal no matter what we last calculated this trick allows neural networks to learn patterns in a sequence of data for example you can use it to predict the next most likely word in a sentence based on the first few words rnns are useful any time you want to learn patterns in data because human language is just one big complicated pattern rnns are increasingly used in many areas of natural language processing if you want to learn more about rnns you can read part where we used one to generate a fake ernest hemingway book and then used another one to generate fake super mario brothers levels the other idea we need to review is encodings we talked about encodings in part as part of face recognition to explain encodings let s take a slight detour into how we can tell two different people apart with a computer when you are trying to tell two faces apart with a computer you collect different measurements from each face and use those measurements to compare faces for example we might measure the size of each ear or the spacing between the eyes and compare those measurements from two pictures to see if they are the same person you re probably already familiar with this idea from watching any primetime detective show like csi the idea of turning a face into a list of measurements is an example of an encoding we are taking raw data a picture of a face and turning it into a list of measurements that represent it the encoding but like we saw in part we don t have to come up with a specific list of facial features to measure ourselves instead we can use a neural network to generate measurements from a face the computer can do a better job than us in figuring out which measurements are best able to differentiate two similar people this is our encoding it lets us represent something very complicated a picture of a face with something simple numbers now comparing two different faces is much easier because we only have to compare these numbers for each face instead of comparing full images guess what we can do the same thing with sentences we can come up with an encoding that represents every possible different sentence as a series of unique numbers to generate this encoding we ll feed the sentence into the rnn one word at time the final result after the last word is processed will be the values that represent the entire sentence great so now we have a way to represent an entire sentence as a set of unique numbers we don t know what each number in the encoding means but it doesn t really matter as long as each sentence is uniquely identified by it s own set of numbers we don t need to know exactly how those numbers were generated ok so we know how to use an rnn to encode a sentence into a set of unique numbers how does that help us here s where things get really cool what if we took two rnns and hooked them up end to end the first rnn could generate the encoding that represents a sentence then the second rnn could take that encoding and just do the same logic in reverse to decode the original sentence again of course being able to encode and then decode the original sentence again isn t very useful but what if and here s the big idea we could train the second rnn to decode the sentence into spanish instead of english we could use our parallel corpora training data to train it to do that and just like that we have a generic way of converting a sequence of english words into an equivalent sequence of spanish words this is a powerful idea note that we glossed over some things that are required to make this work with real world data for example there s additional work you have to do to deal with different lengths of input and output sentences see bucketing and padding there s also issues with translating rare words correctly if you want to build your own language translation system there s a working demo included with tensorflow that will translate between english and french however this is not for the faint of heart or for those with limited budgets this technology is still new and very resource intensive even if you have a fast computer with a high end video card it might take about a month of continuous processing time to train your own language translation system also sequence to sequence language translation techniques are improving so rapidly that it s hard to keep up many recent improvements like adding an attention mechanism or tracking context are significantly improving results but these developments are so new that there aren t even wikipedia pages for them yet if you want to do anything serious with sequence to sequence learning you ll need to keep with new developments as they occur so what else can we do with sequence to sequence models about a year ago researchers at google showed that you can use sequence to sequence models to build ai bots the idea is so simple that it s amazing it works at all first they captured chat logs between google employees and google s tech support team then they trained a sequence to sequence model where the employee s question was the input sentence and the tech support team s response was the translation of that sentence when a user interacted with the bot they would translate each of the user s messages with this system to get the bot s response the end result was a semi intelligent bot that could sometimes answer real tech support questions here s part of a sample conversation between a user and the bot from their paper they also tried building a chat bot based on millions of movie subtitles the idea was to use conversations between movie characters as a way to train a bot to talk like a human the input sentence is a line of dialog said by one character and the translation is what the next character said in response this produced really interesting results not only did the bot converse like a human but it displayed a small bit of intelligence this is only the beginning of the possibilities we aren t limited to converting one sentence into another sentence it s also possible to make an image to sequence model that can turn an image into text a different team at google did this by replacing the first rnn with a convolutional neural network like we learned about in part this allows the input to be a picture instead of a sentence the rest works basically the same way and just like that we can turn pictures into words as long as we have lots and lots of training data andrej karpathy expanded on these ideas to build a system capable of describing images in great detail by processing multiple regions of an image separately this makes it possible to build image search engines that are capable of finding images that match oddly specific search queries there s even researchers working on the reverse problem generating an entire picture based on just a text description just from these examples you can start to imagine the possibilities so far there have been sequence to sequence applications in everything from speech recognition to computer vision i bet there will be a lot more over the next year if you want to learn more in depth about sequence to sequence models and translation here s some recommended resources if you liked this article please consider signing up for my machine learning is fun email list i ll only email you when i have something new and awesome to share it s the best way to find out when i write more articles like this you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Tal Perry,2600,17,https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02?source=tag_archive---------7----------------,Deep Learning the Stock Market – Tal Perry – Medium,update took me a while but here is an ipython notebook with a rough implementation in the past few months i ve been fascinated with deep learning especially its applications to language and text i ve spent the bulk of my career in financial technologies mostly in algorithmic trading and alternative data services you can see where this is going i wrote this to get my ideas straight in my head while i ve become a deep learning enthusiast i don t have too many opportunities to brain dump an idea in most of its messy glory i think that a decent indication of a clear thought is the ability to articulate it to people not from the field i hope that i ve succeeded in doing that and that my articulation is also a pleasurable read why nlp is relevant to stock prediction in many nlp problems we end up taking a sequence and encoding it into a single fixed size representation then decoding that representation into another sequence for example we might tag entities in the text translate from english to french or convert audio frequencies to text there is a torrent of work coming out in these areas and a lot of the results are achieving state of the art performance in my mind the biggest difference between the nlp and financial analysis is that language has some guarantee of structure it s just that the rules of the structure are vague markets on the other hand don t come with a promise of a learnable structure that such a structure exists is the assumption that this project would prove or disprove rather it might prove or disprove if i can find that structure assuming the structure is there the idea of summarizing the current state of the market in the same way we encode the semantics of a paragraph seems plausible to me if that doesn t make sense yet keep reading it will you shall know a word by the company it keeps firth j r there is tons of literature on word embeddings richard socher s lecture is a great place to start in short we can make a geometry of all the words in our language and that geometry captures the meaning of words and relationships between them you may have seen the example of king man woman queen or something of the sort embeddings are cool because they let us represent information in a condensed way the old way of representing words was holding a vector a big list of numbers that was as long as the number of words we know and setting a in a particular place if that was the current word we are looking at that is not an efficient approach nor does it capture any meaning with embeddings we can represent all of the words in a fixed number of dimensions seems to be plenty works great and then leverage their higher dimensional geometry to understand them the picture below shows an example an embedding was trained on more or less the entire internet after a few days of intensive calculations each word was embedded in some high dimensional space this space has a geometry concepts like distance and so we can ask which words are close together the authors inventors of that method made an example here are the words that are closest to frog but we can embed more than just words we can do say stock market embeddings market vec the first word embedding algorithm i heard about was word vec i want to get the same effect for the market though i ll be using a different algorithm my input data is a csv the first column is the date and there are columns corresponding to the high low open closing price of stocks that is my input vector is dimensional which is too big so the first thing i m going to do is stuff it into a lower dimensional space say because i liked the movie taking something in dimensions and stuffing it into a dimensional space my sound hard but its actually easy we just need to multiply matrices a matrix is a big excel spreadsheet that has numbers in every cell and no formatting problems imagine an excel table with columns and rows and when we basically bang it against the vector a new vector comes out that is only of size i wish that s how they would have explained it in college the fanciness starts here as we re going to set the numbers in our matrix at random and part of the deep learning is to update those numbers so that our excel spreadsheet changes eventually this matrix spreadsheet i ll stick with matrix from now on will have numbers in it that bang our original dimensional vector into a concise dimensional summary of itself we re going to get a little fancier here and apply what they call an activation function we re going to take a function and apply it to each number in the vector individually so that they all end up between and or and infinity it depends why it makes our vector more special and makes our learning process able to understand more complicated things how so what what i m expecting to find is that that new embedding of the market prices the vector into a smaller space captures all the essential information for the task at hand without wasting time on the other stuff so i d expect they d capture correlations between other stocks perhaps notice when a certain sector is declining or when the market is very hot i don t know what traits it will find but i assume they ll be useful now what lets put aside our market vectors for a moment and talk about language models andrej karpathy wrote the epic post the unreasonable effectiveness of recurrent neural networks if i d summarize in the most liberal fashion the post boils down to and then as a punchline he generated a bunch of text that looks like shakespeare and then he did it again with the linux source code and then again with a textbook on algebraic geometry so i ll get back to the mechanics of that magic box in a second but let me remind you that we want to predict the future market based on the past just like he predicted the next word based on the previous one where karpathy used characters we re going to use our market vectors and feed them into the magic black box we haven t decided what we want it to predict yet but that is okay we won t be feeding its output back into it either going deeper i want to point out that this is where we start to get into the deep part of deep learning so far we just have a single layer of learning that excel spreadsheet that condenses the market now we re going to add a few more layers and stack them to make a deep something that s the deep in deep learning so karpathy shows us some sample output from the linux source code this is stuff his black box wrote notice that it knows how to open and close parentheses and respects indentation conventions the contents of the function are properly indented and the multi line printk statement has an inner indentation that means that this magic box understands long range dependencies when it s indenting within the print statement it knows it s in a print statement and also remembers that it s in a function or at least another indented scope that s nuts it s easy to gloss over that but an algorithm that has the ability to capture and remember long term dependencies is super useful because we want to find long term dependencies in the market inside the magical black box what s inside this magical black box it is a type of recurrent neural network rnn called an lstm an rnn is a deep learning algorithm that operates on sequences like sequences of characters at every step it takes a representation of the next character like the embeddings we talked about before and operates on the representation with a matrix like we saw before the thing is the rnn has some form of internal memory so it remembers what it saw previously it uses that memory to decide how exactly it should operate on the next input using that memory the rnn can remember that it is inside of an intended scope and that is how we get properly nested output text a fancy version of an rnn is called a long short term memory lstm lstm has cleverly designed memory that allows it to so an lstm can see a and say to itself oh yeah that s important i should remember that and when it does it essentially remembers an indication that it is in a nested scope once it sees the corresponding it can decide to forget the original opening brace and thus forget that it is in a nested scope we can have the lstm learn more abstract concepts by stacking a few of them on top of each other that would make us deep again now each output of the previous lstm becomes the inputs of the next lstm and each one goes on to learn higher abstractions of the data coming in in the example above and this is just illustrative speculation the first layer of lstms might learn that characters separated by a space are words the next layer might learn word types like static void action new function the next layer might learn the concept of a function and its arguments and so on it s hard to tell exactly what each layer is doing though karpathy s blog has a really nice example of how he did visualize exactly that connecting market vec and lstms the studious reader will notice that karpathy used characters as his inputs not embeddings technically a one hot encoding of characters but lars eidnes actually used word embeddings when he wrote auto generating clickbait with recurrent neural network the figure above shows the network he used ignore the softmax part we ll get to it later for the moment check out how on the bottom he puts in a sequence of words vectors at the bottom and each one remember a word vector is a representation of a word in the form of a bunch of numbers like we saw in the beginning of this post lars inputs a sequence of word vectors and each one of them we re going to do the same thing with one difference instead of word vectors we ll input marketvectors those market vectors we described before to recap the marketvectors should contain a summary of what s happening in the market at a given point in time by putting a sequence of them through lstms i hope to capture the long term dynamics that have been happening in the market by stacking together a few layers of lstms i hope to capture higher level abstractions of the market s behavior what comes out thus far we haven t talked at all about how the algorithm actually learns anything we just talked about all the clever transformations we ll do on the data we ll defer that conversation to a few paragraphs down but please keep this part in mind as it is the se up for the punch line that makes everything else worthwhile in karpathy s example the output of the lstms is a vector that represents the next character in some abstract representation in eidnes example the output of the lstms is a vector that represents what the next word will be in some abstract space the next step in both cases is to change that abstract representation into a probability vector that is a list that says how likely each character or word respectively is likely to appear next that s the job of the softmax function once we have a list of likelihoods we select the character or word that is the most likely to appear next in our case of predicting the market we need to ask ourselves what exactly we want to market to predict some of the options that i thought about were and are regression problems where we have to predict an actual number instead of the likelihood of a specific event like the letter n appearing or the market going up those are fine but not what i want to do and are fairly similar they both ask to predict an event in technical jargon a class label an event could be the letter n appearing next or it could be moved up while not going down more than in the last minutes the trade off between and is that is much more common and thus easier to learn about while is more valuable as not only is it an indicator of profit but also has some constraint on risk is the one we ll continue with for this article because it s similar to and but has mechanics that are easier to follow the vix is sometimes called the fear index and it represents how volatile the stocks in the s p are it is derived by observing the implied volatility for specific options on each of the stocks in the index sidenote why predict the vix what makes the vix an interesting target is that back to our lstm outputs and the softmax how do we use the formulations we saw before to predict changes in the vix a few minutes in the future for each point in our dataset we ll look what happened to the vix minutes later if it went up by more than without going down more than during that time we ll output a otherwise a then we ll get a sequence that looks like we want to take the vector that our lstms output and squish it so that it gives us the probability of the next item in our sequence being a the squishing happens in the softmax part of the diagram above technically since we only have class now we use a sigmoid so before we get into how this thing learns let s recap what we ve done so far how does this thing learn now the fun part everything we did until now was called the forward pass we d do all of those steps while we train the algorithm and also when we use it in production here we ll talk about the backward pass the part we do only while in training that makes our algorithm learn so during training not only did we prepare years worth of historical data we also prepared a sequence of prediction targets that list of and that showed if the vix moved the way we want it to or not after each observation in our data to learn we ll feed the market data to our network and compare its output to what we calculated comparing in our case will be simple subtraction that is we ll say that our model s error is or in english the square root of the square of the difference between what actually happened and what we predicted here s the beauty that s a differential function that is we can tell by how much the error would have changed if our prediction would have changed a little our prediction is the outcome of a differentiable function the softmax the inputs to the softmax the lstms are all mathematical functions that are differentiable now all of these functions are full of parameters those big excel spreadsheets i talked about ages ago so at this stage what we do is take the derivative of the error with respect to every one of the millions of parameters in all of those excel spreadsheets we have in our model when we do that we can see how the error will change when we change each parameter so we ll change each parameter in a way that will reduce the error this procedure propagates all the way to the beginning of the model it tweaks the way we embed the inputs into marketvectors so that our marketvectors represent the most significant information for our task it tweaks when and what each lstm chooses to remember so that their outputs are the most relevant to our task it tweaks the abstractions our lstms learn so that they learn the most important abstractions for our task which in my opinion is amazing because we have all of this complexity and abstraction that we never had to specify anywhere it s all inferred mathamagically from the specification of what we consider to be an error what s next now that i ve laid this out in writing and it still makes sense to me i want so if you ve come this far please point out my errors and share your inputs other thoughts here are some mostly more advanced thoughts about this project what other things i might try and why it makes sense to me that this may actually work liquidity and efficient use of capital generally the more liquid a particular market is the more efficient that is i think this is due to a chicken and egg cycle whereas a market becomes more liquid it is able to absorb more capital moving in and out without that capital hurting itself as a market becomes more liquid and more capital can be used in it you ll find more sophisticated players moving in this is because it is expensive to be sophisticated so you need to make returns on a large chunk of capital in order to justify your operational costs a quick corollary is that in less liquid markets the competition isn t quite as sophisticated and so the opportunities a system like this can bring may not have been traded away the point being were i to try and trade this i would try and trade it on less liquid segments of the market that is maybe the tase instead of the s p this stuff is new the knowledge of these algorithms the frameworks to execute them and the computing power to train them are all new at least in the sense that they are available to the average joe such as myself i d assume that top players have figured this stuff out years ago and have had the capacity to execute for as long but as i mention in the above paragraph they are likely executing in liquid markets that can support their size the next tier of market participants i assume have a slower velocity of technological assimilation and in that sense there is or soon will be a race to execute on this in as yet untapped markets multiple time frames while i mentioned a single stream of inputs in the above i imagine that a more efficient way to train would be to train market vectors at least on multiple time frames and feed them in at the inference stage that is my lowest time frame would be sampled every seconds and i d expect the network to learn dependencies that stretch hours at most i don t know if they are relevant or not but i think there are patterns on multiple time frames and if the cost of computation can be brought low enough then it is worthwhile to incorporate them into the model i m still wrestling with how best to represent these on the computational graph and perhaps it is not mandatory to start with marketvectors when using word vectors in nlp we usually start with a pretrained model and continue adjusting the embeddings during training of our model in my case there are no pretrained market vector available nor is tehre a clear algorithm for training them my original consideration was to use an auto encoder like in this paper but end to end training is cooler a more serious consideration is the success of sequence to sequence models in translation and speech recognition where a sequence is eventually encoded as a single vector and then decoded into a different representation like from speech to text or from english to french in that view the entire architecture i described is essentially the encoder and i haven t really laid out a decoder but i want to achieve something specific with the first layer the one that takes as input the dimensional vector and outputs a dimensional one i want it to find correlations or relations between various stocks and compose features about them the alternative is to run each input through an lstm perhaps concatenate all of the output vectors and consider that output of the encoder stage i think this will be inefficient as the interactions and correlations between instruments and their features will be lost and thre will be x more computation required on the other hand such an architecture could naively be paralleled across multiple gpus and hosts which is an advantage cnns recently there has been a spur of papers on character level machine translation this paper caught my eye as they manage to capture long range dependencies with a convolutional layer rather than an rnn i haven t given it more than a brief read but i think that a modification where i d treat each stock as a channel and convolve over channels first like in rgb images would be another way to capture the market dynamics in the same way that they essentially encode semantic meaning from characters from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder of https lighttag io platform to annotate text for nlp google developer expert in ml i do deep learning on text for a living and for fun
Andrej Karpathy,9200,7,https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b?source=tag_archive---------8----------------,Yes you should understand backprop – Andrej Karpathy – Medium,when we offered cs n deep learning class at stanford we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level the students had to implement the forward and the backward pass of each layer in raw numpy inevitably some students complained on the class message boards this is seemingly a perfectly sensible appeal if you re never going to write backward passes once the class is over why practice writing them are we just torturing the students for our own amusement some easy answers could make arguments along the lines of it s worth knowing what s under the hood as an intellectual curiosity or perhaps you might want to improve on the core algorithm later but there is a much stronger and practical argument which i wanted to devote a whole post to the problem with backpropagation is that it is a leaky abstraction in other words it is easy to fall into the trap of abstracting away the learning process believing that you can simply stack arbitrary layers together and backprop will magically make them work on your data so lets look at a few explicit examples where this is not the case in quite unintuitive ways we re starting off easy here at one point it was fashionable to use sigmoid or tanh non linearities in the fully connected layers the tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non linearities can saturate and entirely stop learning your training loss will be flat and refuse to go down for example a fully connected layer with sigmoid non linearity computes using raw numpy if your weight matrix w is initialized too large the output of the matrix multiply could have a very large range e g numbers between and which will make all outputs in the vector z almost binary either or but if that is the case z z which is local gradient of the sigmoid non linearity will in both cases become zero vanish making the gradient for both x and w be zero the rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule another non obvious fun fact about sigmoid is that its local gradient z z achieves a maximum at when z that means that every time the gradient signal flows through a sigmoid gate its magnitude always diminishes by one quarter or more if you re using basic sgd this would make the lower layers of a network train much slower than the higher ones tldr if you re using sigmoids or tanh non linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn t cause them to be fully saturated see a longer explanation in this cs n lecture video another fun non linearity is the relu which thresholds neurons at zero from below the forward and backward pass for a fully connected layer that uses relu would at the core include if you stare at this for a while you ll see that if a neuron gets clamped to zero in the forward pass i e z it doesn t fire then its weights will get zero gradient this can lead to what is called the dead relu problem where if a relu neuron is unfortunately initialized such that it never fires or if a neuron s weights ever get knocked off with a large update during training into this regime then this neuron will remain permanently dead it s like permanent irrecoverable brain damage sometimes you can forward the entire training set through a trained network and find that a large fraction e g of your neurons were zero the entire time tldr if you understand backpropagation and your network has relus you re always nervous about dead relus these are neurons that never turn on for any example in your entire training set and will remain permanently dead neurons can also die during training usually as a symptom of aggressive learning rates see a longer explanation in cs n lecture video vanilla rnns feature another good example of unintuitive effects of backpropagation i ll copy paste a slide from cs n that has a simplified rnn that does not take any input x and only computes the recurrence on the hidden state equivalently the input x could always be zero this rnn is unrolled for t time steps when you stare at what the backward pass is doing you ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix the recurrence matrix whh interspersed with non linearity backprop what happens when you take one number a and start multiplying it by some other number b i e a b b b b b b this sequence either goes to zero if b or explodes to infinity when b the same thing happens in the backward pass of an rnn except b is a matrix and not just a number so we have to reason about its largest eigenvalue instead tldr if you understand backpropagation and you re using rnns you are nervous about having to do gradient clipping or you prefer to use an lstm see a longer explanation in this cs n lecture video lets look at one more the one that actually inspired this post yesterday i was browsing for a deep q learning implementation in tensorflow to see how others deal with computing the numpy equivalent of q a where a is an integer vector turns out this trivial operation is not supported in tf anyway i searched dqn tensorflow clicked the first link and found the core code here is an excerpt if you re familiar with dqn you can see that there is the target q t which is just reward gamma argmax a q s a and then there is q acted which is q s a of the action that was taken the authors here subtract the two into variable delta which they then want to minimize on line with the l loss with tf reduce mean tf square so far so good the problem is on line the authors are trying to be robust to outliers so if the delta is too large they clip it with tf clip by value this is well intentioned and looks sensible from the perspective of the forward pass but it introduces a major bug if you think about the backward pass the clip by value function has a local gradient of zero outside of the range min delta to max delta so whenever the delta is above min max delta the gradient becomes exactly zero during backprop the authors are clipping the raw q delta when they are likely trying to clip the gradient for added robustness in that case the correct thing to do is to use the huber loss in place of tf square it s a bit gross in tensorflow because all we want to do is clip the gradient if it is above a threshold but since we can t meddle with the gradients directly we have to do it in this round about way of defining the huber loss in torch this would be much more simple i submitted an issue on the dqn repo and this was promptly fixed backpropagation is a leaky abstraction it is a credit assignment scheme with non trivial consequences if you try to ignore how it works under the hood because tensorflow automagically makes my networks learn you will not be ready to wrestle with the dangers it presents and you will be much less effective at building and debugging neural networks the good news is that backpropagation is not that difficult to understand if presented properly i have relatively strong feelings on this topic because it seems to me that of backpropagation materials out there present it all wrong filling pages with mechanical math instead i would recommend the cs n lecture on backprop which emphasizes intuition yay for shameless self advertising and if you can spare the time as a bonus work through the cs n assignments which get you to write backprop manually and help you solidify your understanding that s it for now i hope you ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing also i m aware that this post has unintentionally turned into several cs n ads apologies for that from a quick cheer to a standing ovation clap to show how much you enjoyed this story director of ai at tesla previously research scientist at openai and phd student at stanford i like to train deep neural nets on large datasets
Per Harald Borgen,4800,7,https://medium.com/learning-new-stuff/machine-learning-in-a-year-cdb0b0ebd29c?source=tag_archive---------9----------------,Machine Learning in a Year – Learning New Stuff – Medium,this is a follow up to an article i wrote last year machine learning in a week on how i kickstarted my way into machine learning ml by devoting five days to the subject after this highly effective introduction i continued learning on my spare time and almost exactly one year later i did my first ml project at work which involved using various ml and natural language processing nlp techniques to qualify sales leads at xeneta this felt like a blessing getting paid to do something i normally did for fun it also ripped me out of the delusion that only people with masters degrees or ph d s work with ml professionally in this post i want to share my journey as it might inspire others to do the same my interest in ml stems back to when i started reading articles about it on hacker news i simply found the idea of teaching machines stuff by looking at data appealing at the time i wasn t even a professional developer but a hobby coder who d done a couple of small projects so i began watching the first few chapters of udacity s supervised learning course while also reading all articles i came across on the subject this gave me a little bit of conceptual understanding though no practical skills i also didn t finish it as i rarely do with mooc s in january i joined the founders and coders fac bootcamp in london in order to become a developer a few weeks in i wanted to learn how to actually code machine learning algorithms so i started a study group with a few of my peers every tuesday evening we d watch lectures from coursera s machine learning course it s a fantastic course and i learned a hell of a lot but it s tough for a beginner i had to watch the lectures over and over again before grasping the concepts the octave coding task are challenging as well especially if you don t know octave as a result of the difficulty one by one fell off the study group as the weeks passed eventually i fell off it myself as well in hindsight i should have started with a course that either used ml libraries for the coding tasks as opposed to building the algorithms from scratch or at least used a programming language i knew if i could go back in time i d choose udacity s intro to machine learning as it s easier and uses python and scikit learn this way we would have gotten our hands dirty as soon as possible gained confidence and had more fun one of the last things i did at fac was the ml week stunt my goal was to be able to apply machine learning to actual problems at the end of the week which i managed to do throughout the week i did the following it s by far the steepest ml learning curve i ve ever experienced go ahead and read the article if you want a more detailed overview after i finished fac in london and moved back to norway i tried to repeat the success from the ml week but for neural networks instead this failed there were simply too many distractions to spend hours of coding and learning every day i had underestimated how important it was to be surrounded by peers at fac however i got started with neural nets at least and slowly started to grasp the concept by july i managed to code my first net it s probably the crappiest implementation ever created and i actually find it embarrassing to show off but it did the trick i proved to myself that i understood concepts like backpropagation and gradient descent in the second half of the year my progression slowed down as i started a new job the most important takeaway from this period was the leap from non vectorized to vectorized implementations of neural networks which involved repeating linear algebra from university by the end of the year i wrote an article as a summary of how i learned this during the christmas vacation of i got a motivational boost again and decided try out kaggle so i spent quite some time experimenting with various algorithms for their homesite quote conversion otto group product classification and bike sharing demand contests the main takeaway from this was the experience of iteratively improving the results by experimenting with the algorithms and the data i learned to trust my logic when doing machine learning if tweaking a parameter or engineering a new feature seems like a good idea logically it s quite likely that it actually will help back at work in january i wanted to continue in the flow i d gotten into during christmas so i asked my manager if i could spend some time learning stuff during my work hours as well which he happily approved having gotten a basic understanding of neural networks at this point i wanted to move on to deep learning my first attempt was udacity s deep learning course which ended up as a big disappointment the contents of the video lectures are good but they are too short and shallow to me and the ipython notebook assignments ended up being too frustrating as i spent most of my time debugging code errors which is the most effective way to kill motivation so after doing that for a couple of sessions at work i simply gave up to their defense i m a total noob when it comes to ipython notebooks so it might not be as bad for you as it was for me so it might be that i simply wasn t ready for the course luckily i then discovered stanford s cs d and decided to give it a shot it is a fantastic course and though it s difficult i never end up debugging when doing the problem sets secondly they actually give you the solution code as well which i often look at when i m stuck so that i can work my way backwards to understand the steps needed to reach a solution though i ve haven t finished it yet it has significantly boosted my knowledge in nlp and neural networks so far however it s been tough really tough at one point i realized i needed help from someone better than me so i came in touch with a ph d student who was willing to help me out for usd per hour both with the problem sets as well as the overall understanding this has been critical for me in order to move on as he has uncovered a lot of black holes in my knowledge in addition to this xeneta also hired a data scientist recently he s got a masters degree in math so i often ask him for help when i m stuck with various linear algebra an calculus tasks or ml in general so be sure to check out which resources you have internally in your company as well after doing all this i finally felt ready to do a ml project at work it basically involved training an algorithm to qualify sales leads by reading company descriptions and has actually proven to be a big time saver for the sales guys using the tool check out out article i wrote about it below or head over to github to dive straight into the code getting to this point has surely been a long journey but also a fast one when i started my machine learning in a week project i certainly didn t have any hopes of actually using it professionally within a year but it s percent possible and if i can do it so can anybody else thanks for reading my name is per i m a co founder of scrimba a better way to teach and learn code if you ve read this far i d recommend you to check out this demo from a quick cheer to a standing ovation clap to show how much you enjoyed this story co founder of scrimba the next generation platform for teaching and learning code https scrimba com a publication about improving your technical skills
Xiaohan Zeng,48000,13,https://medium.com/@XiaohanZeng/i-interviewed-at-five-top-companies-in-silicon-valley-in-five-days-and-luckily-got-five-job-offers-25178cf74e0f?source=tag_archive---------0----------------,"I interviewed at five top companies in Silicon Valley in five days, and luckily got five job offers",in the five days from july th to th i interviewed at linkedin salesforce einstein google airbnb and facebook and got all five job offers it was a great experience and i feel fortunate that my efforts paid off so i decided to write something about it i will discuss how i prepared review the interview process and share my impressions about the five companies i had been at groupon for almost three years it s my first job and i have been working with an amazing team and on awesome projects we ve been building cool stuff making impact within the company publishing papers and all that but i felt my learning rate was being annealed read slowing down yet my mind was craving more also as a software engineer in chicago there are so many great companies that all attract me in the bay area life is short and professional life shorter still after talking with my wife and gaining her full support i decided to take actions and make my first ever career change although i m interested in machine learning positions the positions at the five companies are slightly different in the title and the interviewing process three are machine learning engineer linkedin google facebook one is data engineer salesforce and one is software engineer in general airbnb therefore i needed to prepare for three different areas coding machine learning and system design since i also have a full time job it took me months in total to prepare here is how i prepared for the three areas while i agree that coding interviews might not be the best way to assess all your skills as a developer there is arguably no better way to tell if you are a good engineer in a short period of time imo it is the necessary evil to get you that job i mainly used leetcode and geeksforgeeks for practicing but hackerrank and lintcode are also good places i spent several weeks going over common data structures and algorithms then focused on areas i wasn t too familiar with and finally did some frequently seen problems due to my time constraints i usually did two problems per day here are some thoughts this area is more closely related to the actual working experience many questions can be asked during system design interviews including but not limited to system architecture object oriented design database schema design distributed system design scalability etc there are many resources online that can help you with the preparation for the most part i read articles on system design interviews architectures of large scale systems and case studies here are some resources that i found really helpful although system design interviews can cover a lot of topics there are some general guidelines for how to approach the problem with all that said the best way to practice for system design interviews is to actually sit down and design a system i e your day to day work instead of doing the minimal work go deeper into the tools frameworks and libraries you use for example if you use hbase rather than simply using the client to run some ddl and do some fetches try to understand its overall architecture such as the read write flow how hbase ensures strong consistency what minor major compactions do and where lru cache and bloom filter are used in the system you can even compare hbase with cassandra and see the similarities and differences in their design then when you are asked to design a distributed key value store you won t feel ambushed many blogs are also a great source of knowledge such as hacker noon and engineering blogs of some companies as well as the official documentation of open source projects the most important thing is to keep your curiosity and modesty be a sponge that absorbs everything it is submerged into machine learning interviews can be divided into two aspects theory and product design unless you are have experience in machine learning research or did really well in your ml course it helps to read some textbooks classical ones such as the elements of statistical learning and pattern recognition and machine learning are great choices and if you are interested in specific areas you can read more on those make sure you understand basic concepts such as bias variance trade off overfitting gradient descent l l regularization bayes theorem bagging boosting collaborative filtering dimension reduction etc familiarize yourself with common formulas such as bayes theorem and the derivation of popular models such as logistic regression and svm try to implement simple models such as decision trees and k means clustering if you put some models on your resume make sure you understand it thoroughly and can comment on its pros and cons for ml product design understand the general process of building a ml product here s what i tried to do here i want to emphasize again on the importance of remaining curious and learning continuously try not to merely using the api for spark mllib or xgboost and calling it done but try to understand why stochastic gradient descent is appropriate for distributed training or understand how xgboost differs from traditional gbdt e g what is special about its loss function why it needs to compute the second order derivative etc i started by replying to hr s messages on linkedin and asking for referrals after a failed attempt at a rock star startup which i will touch upon later i prepared hard for several months and with help from my recruiters i scheduled a full week of onsites in the bay area i flew in on sunday had five full days of interviews with around interviewers at some best tech companies in the world and very luckily got job offers from all five of them all phone screenings are standard the only difference is in the duration for some companies like linkedin it s one hour while for facebook and airbnb it s minutes proficiency is the key here since you are under the time gun and usually you only get one chance you would have to very quickly recognize the type of problem and give a high level solution be sure to talk to the interviewer about your thinking and intentions it might slow you down a little at the beginning but communication is more important than anything and it only helps with the interview do not recite the solution as the interviewer would almost certainly see through it for machine learning positions some companies would ask ml questions if you are interviewing for those make sure you brush up your ml skills as well to make better use of my time i scheduled three phone screenings in the same afternoon one hour apart from each the upside is that you might benefit from the hot hand and the downside is that the later ones might be affected if the first one does not go well so i don t recommend it for everyone one good thing about interviewing with multiple companies at the same time is that it gives you certain advantages i was able to skip the second round phone screening with airbnb and salesforce because i got the onsite at linkedin and facebook after only one phone screening more surprisingly google even let me skip their phone screening entirely and schedule my onsite to fill the vacancy after learning i had four onsites coming in the next week i knew it was going to make it extremely tiring but hey nobody can refuse a google onsite invitation linkedin this is my first onsite and i interviewed at the sunnyvale location the office is very neat and people look very professional as always the sessions are one hour each coding questions are standard but the ml questions can get a bit tough that said i got an email from my hr containing the preparation material which was very helpful and in the end i did not see anything that was too surprising i heard the rumor that linkedin has the best meals in the silicon valley and from what i saw if it s not true it s not too far from the truth acquisition by microsoft seems to have lifted the financial burden from linkedin and freed them up to do really cool things new features such as videos and professional advertisements are exciting as a company focusing on professional development linkedin prioritizes the growth of its own employees a lot of teams such as ads relevance and feed ranking are expanding so act quickly if you want to join salesforce einstein rock star project by rock star team the team is pretty new and feels very much like a startup the product is built on the scala stack so type safety is a real thing there great talks on the optimus prime library by matthew tovbin at scala days chicago and leah mcguire at spark summit west i interviewed at their palo alto office the team has a cohesive culture and work life balance is great there everybody is passionate about what they are doing and really enjoys it with four sessions it is shorter compared to the other onsite interviews but i wish i could have stayed longer after the interview matthew even took me for a walk to the hp garage google absolutely the industry leader and nothing to say about it that people don t already know but it s huge like really really huge it took me minutes to ride a bicycle to meet my friends there also lines for food can be too long forever a great place for developers i interviewed at one of the many buildings on the mountain view campus and i don t know which one it is because it s huge my interviewers all look very smart and once they start talking they are even smarter it would be very enjoyable to work with these people one thing that i felt special about google s interviews is that the analysis of algorithm complexity is really important make sure you really understand what big o notation means airbnb fast expanding unicorn with a unique culture and arguably the most beautiful office in the silicon valley new products such as experiences and restaurant reservation high end niche market and expansion into china all contribute to a positive prospect perfect choice if you are risk tolerant and want a fast growing pre ipo experience airbnb s coding interview is a bit unique because you ll be coding in an ide instead of whiteboarding so your code needs to compile and give the right answer some problems can get really hard and they ve got the one of a kind cross functional interviews this is how airbnb takes culture seriously and being technically excellent doesn t guarantee a job offer for me the two cross functionals were really enjoyable i had casual conversations with the interviewers and we all felt happy at the end of the session overall i think airbnb s onsite is the hardest due to the difficulty of the problems longer duration and unique cross functional interviews if you are interested be sure to understand their culture and core values facebook another giant that is still growing fast and smaller and faster paced compared to google with its product lines dominating the social network market and big investments in ai and vr i can only see more growth potential for facebook in the future with stars like yann lecun and yangqing jia it s the perfect place if you are interested in machine learning i interviewed at building the one with the rooftop garden and ocean view and also where zuckerberg s office is located i m not sure if the interviewers got instructions but i didn t get clear signs whether my solutions were correct although i believed they were by noon the prior four days started to take its toll and i was having a headache i persisted through the afternoon sessions but felt i didn t do well at all i was a bit surprised to learn that i was getting an offer from them as well generally i felt people there believe the company s vision and are proud of what they are building being a company with half a trillion market cap and growing facebook is a perfect place to grow your career at this is a big topic that i won t cover in this post but i found this article to be very helpful some things that i do think are important all successes start with failures including interviews before i started interviewing for these companies i failed my interview at databricks in may back in april xiangrui contacted me via linkedin asking me if i was interested in a position on the spark mllib team i was extremely thrilled because i use spark and love scala databricks engineers are top notch and spark is revolutionizing the whole big data world it is an opportunity i couldn t miss so i started interviewing after a few days the bar is very high and the process is quite long including one pre screening questionnaire one phone screening one coding assignment and one full onsite i managed to get the onsite invitation and visited their office in downtown san francisco where treasure island can be seen my interviewer were incredibly intelligent yet equally modest during the interviews i often felt being pushed to the limits it was fine until one disastrous session where i totally messed up due to insufficient skills and preparation and it ended up a fiasco xiangrui was very kind and walked me to where i wanted to go after the interview was over and i really enjoyed talking to him i got the rejection several days later it was expected but i felt frustrated for a few days nonetheless although i missed the opportunity to work there i wholeheartedly wish they will continue to make greater impact and achievements from the first interview in may to finally accepting the job offer in late september my first career change was long and not easy it was difficult for me to prepare because i needed to keep doing well at my current job for several weeks i was on a regular schedule of preparing for the interview till am getting up at am the next day and fully devoting myself to another day at work interviewing at five companies in five days was also highly stressful and risky and i don t recommend doing it unless you have a very tight schedule but it does give you a good advantage during negotiation should you secure multiple offers i d like to thank all my recruiters who patiently walked me through the process the people who spend their precious time talking to me and all the companies that gave me the opportunities to interview and extended me offers lastly but most importantly i want to thank my family for their love and support my parents for watching me taking the first and every step my dear wife for everything she has done for me and my daughter for her warming smile thanks for reading through this long post you can find me on linkedin or twitter xiaohan zeng ps since the publication of this post it has unexpectedly received some attention i would like to thank everybody for the congratulations and shares and apologize for not being able to respond to each of them this post has been translated into some other languages it has been reposted in tech in asia breaking into startups invited me to a live video streaming together with sophia ciocca covershr did a short qna with me from a quick cheer to a standing ovation clap to show how much you enjoyed this story critical mind romantic heart
Gil Fewster,3300,5,https://medium.freecodecamp.org/the-mind-blowing-ai-announcement-from-google-that-you-probably-missed-2ffd31334805?source=tag_archive---------1----------------,The mind-blowing AI announcement from Google that you probably missed.,disclaimer i m not an expert in neural networks or machine learning since originally writing this article many people with far more expertise in these fields than myself have indicated that while impressive what google have achieved is evolutionary not revolutionary in the very least it s fair to say that i m guilty of anthropomorphising in parts of the text i ve left the article s content unchanged because i think it s interesting to compare the gut reaction i had with the subsequent comments of experts in the field i strongly encourage readers to browse the comments after reading the article for some perspectives more sober and informed than my own in the closing weeks of google published an article that quietly sailed under most people s radars which is a shame because it may just be the most astonishing article about machine learning that i read last year don t feel bad if you missed it not only was the article competing with the pre christmas rush that most of us were navigating it was also tucked away on google s research blog beneath the geektastic headline zero shot translation with google s multilingual neural machine translation system this doesn t exactly scream must read does it especially when you ve got projects to wind up gifts to buy and family feuds to be resolved all while the advent calendar relentlessly counts down the days until christmas like some kind of chocolate filled yuletide doomsday clock luckily i m here to bring you up to speed here s the deal up until september of last year google translate used phrase based translation it basically did the same thing you and i do when we look up key words and phrases in our lonely planet language guides it s effective enough and blisteringly fast compared to awkwardly thumbing your way through a bunch of pages looking for the french equivalent of please bring me all of your cheese and don t stop until i fall over but it lacks nuance phrase based translation is a blunt instrument it does the job well enough to get by but mapping roughly equivalent words and phrases without an understanding of linguistic structures can only produce crude results this approach is also limited by the extent of an available vocabulary phrase based translation has no capacity to make educated guesses at words it doesn t recognize and can t learn from new input all that changed in september when google gave their translation tool a new engine the google neural machine translation system gnmt this new engine comes fully loaded with all the hot buzzwords like neural network and machine learning the short version is that google translate got smart it developed the ability to learn from the people who used it it learned how to make educated guesses about the content tone and meaning of phrases based on the context of other words and phrases around them and here s the bit that should make your brain explode it got creative google translate invented its own language to help it translate more effectively what s more nobody told it to it didn t develop a language or interlingua as google call it because it was coded to it developed a new language because the software determined over time that this was the most efficient way to solve the problem of translation stop and think about that for a moment let it sink in a neural computing system designed to translate content from one human language into another developed its own internal language to make the task more efficient without being told to do so in a matter of weeks i ve added a correction retraction of this paragraph in the notes to understand what s going on we need to understand what zero shot translation capability is here s google s mike schuster nikhil thorat and melvin johnson from the original blog post here you can see an advantage of google s new neural machine over the old phrase based approach the gmnt is able to learn how to translate between two languages without being explicitly taught this wouldn t be possible in a phrase based model where translation is dependent upon an explicit dictionary to map words and phrases between each pair of languages being translated and this leads the google engineers onto that truly astonishing discovery of creation so there you have it in the last weeks of as journos around the world started penning their was this the worst year in living memory thinkpieces google engineers were quietly documenting a genuinely astonishing breakthrough in software engineering and linguistics i just thought maybe you d want to know ok to really understand what s going on we probably need multiple computer science and linguistics degrees i m just barely scraping the surface here if you ve got time to get a few degrees or if you ve already got them please drop me a line and explain it all me to slowly update in my excitement it s fair to say that i ve exaggerated the idea of this as an intelligent system at least so far as we would think about human intelligence and decision making make sure you read chris mcdonald s comment after the article for a more sober perspective update nafrondel s excellent detailed reply is also a must read for an expert explanation of how neural networks function from a quick cheer to a standing ovation clap to show how much you enjoyed this story a tinkerer our community publishes stories worth reading on development design and data science
David Venturi,10600,20,https://medium.freecodecamp.org/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0?source=tag_archive---------2----------------,"Every single Machine Learning course on the internet, ranked by your reviews",a year and a half ago i dropped out of one of the best computer science programs in canada i started creating my own data science master s program using online resources i realized that i could learn everything i needed through edx coursera and udacity instead and i could learn it faster more efficiently and for a fraction of the cost i m almost finished now i ve taken many data science related courses and audited portions of many more i know the options out there and what skills are needed for learners preparing for a data analyst or data scientist role so i started creating a review driven guide that recommends the best courses for each subject within data science for the first guide in the series i recommended a few coding classes for the beginner data scientist then it was statistics and probability classes then introductions to data science also data visualization for this guide i spent a dozen hours trying to identify every online machine learning course offered as of may extracting key bits of information from their syllabi and reviews and compiling their ratings my end goal was to identify the three best courses available and present them to you below for this task i turned to none other than the open source class central community and its database of thousands of course ratings and reviews since class central founder dhawal shah has kept a closer eye on online courses than arguably anyone else in the world dhawal personally helped me assemble this list of resources each course must fit three criteria we believe we covered every notable course that fits the above criteria since there are seemingly hundreds of courses on udemy we chose to consider the most reviewed and highest rated ones only there s always a chance that we missed something though so please let us know in the comments section if we left a good course out we compiled average ratings and number of reviews from class central and other review sites to calculate a weighted average rating for each course we read text reviews and used this feedback to supplement the numerical ratings we made subjective syllabus judgment calls based on three factors a popular definition originates from arthur samuel in machine learning is a subfield of computer science that gives computers the ability to learn without being explicitly programmed in practice this means developing computer programs that can make predictions based on data just as humans can learn from experience so can computers where data experience a machine learning workflow is the process required for carrying out a machine learning project though individual projects can differ most workflows share several common tasks problem evaluation data exploration data preprocessing model training testing deployment etc below you ll find helpful visualization of these core steps the ideal course introduces the entire process and provides interactive examples assignments and or quizzes where students can perform each task themselves first off let s define deep learning here is a succinct description as would be expected portions of some of the machine learning courses contain deep learning content i chose not to include deep learning only courses however if you are interested in deep learning specifically we ve got you covered with the following article my top three recommendations from that list would be several courses listed below ask students to have prior programming calculus linear algebra and statistics experience these prerequisites are understandable given that machine learning is an advanced discipline missing a few subjects good news some of this experience can be acquired through our recommendations in the first two articles programming statistics of this data science career guide several top ranked courses below also provide gentle calculus and linear algebra refreshers and highlight the aspects most relevant to machine learning for those less familiar stanford university s machine learning on coursera is the clear current winner in terms of ratings reviews and syllabus fit taught by the famous andrew ng google brain founder and former chief scientist at baidu this was the class that sparked the founding of coursera it has a star weighted average rating over reviews released in it covers all aspects of the machine learning workflow though it has a smaller scope than the original stanford class upon which it is based it still manages to cover a large number of techniques and algorithms the estimated timeline is eleven weeks with two weeks dedicated to neural networks and deep learning free and paid options are available ng is a dynamic yet gentle instructor with a palpable experience he inspires confidence especially when sharing practical implementation tips and warnings about common pitfalls a linear algebra refresher is provided and ng highlights the aspects of calculus most relevant to machine learning evaluation is automatic and is done via multiple choice quizzes that follow each lesson and programming assignments the assignments there are eight of them can be completed in matlab or octave which is an open source version of matlab ng explains his language choice though python and r are likely more compelling choices in with the increased popularity of those languages reviewers note that that shouldn t stop you from taking the course a few prominent reviewers noted the following columbia university s machine learning is a relatively new offering that is part of their artificial intelligence micromasters on edx though it is newer and doesn t have a large number of reviews the ones that it does have are exceptionally strong professor john paisley is noted as brilliant clear and clever it has a star weighted average rating over reviews the course also covers all aspects of the machine learning workflow and more algorithms than the above stanford offering columbia s is a more advanced introduction with reviewers noting that students should be comfortable with the recommended prerequisites calculus linear algebra statistics probability and coding quizzes programming assignments and a final exam are the modes of evaluation students can use either python octave or matlab to complete the assignments the course s total estimated timeline is eight to ten hours per week over twelve weeks it is free with a verified certificate available for purchase below are a few of the aforementioned sparkling reviews machine learning a ztm on udemy is an impressively detailed offering that provides instruction in both python and r which is rare and can t be said for any of the other top courses it has a star weighted average rating over reviews which makes it the most reviewed course of the ones considered it covers the entire machine learning workflow and an almost ridiculous in a good way number of algorithms through hours of on demand video the course takes a more applied approach and is lighter math wise than the above two courses each section starts with an intuition video from eremenko that summarizes the underlying theory of the concept being taught de ponteves then walks through implementation with separate videos for both python and r as a bonus the course includes python and r code templates for students to download and use on their own projects there are quizzes and homework challenges though these aren t the strong points of the course eremenko and the superdatascience team are revered for their ability to make the complex simple also the prerequisites listed are just some high school mathematics so this course might be a better option for those daunted by the stanford and columbia offerings a few prominent reviewers noted the following our pick had a weighted average rating of out of stars over reviews let s look at the other alternatives sorted by descending rating a reminder that deep learning only courses are not included in this guide you can find those here the analytics edge massachusetts institute of technology edx more focused on analytics in general though it does cover several machine learning topics uses r strong narrative that leverages familiar real world examples challenging ten to fifteen hours per week over twelve weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews python for data science and machine learning bootcamp jose portilla udemy has large chunks of machine learning content but covers the whole data science process more of a very detailed intro to python amazing course though not ideal for the scope of this guide hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews data science and machine learning bootcamp with r jose portilla udemy the comments for portilla s above course apply here as well except for r hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews machine learning series lazy programmer inc udemy taught by a data scientist big data engineer full stack software engineer with an impressive resume lazy programmer currently has a series of machine learning focused courses on udemy in total the courses have ratings and almost all of them have stars a useful course ordering is provided in each individual course s description uses python cost varies depending on udemy discounts which are frequent machine learning georgia tech udacity a compilation of what was three separate courses supervised unsupervised and reinforcement learning part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree oms bite sized videos as is udacity s style friendly professors estimated timeline of four months free it has a star weighted average rating over reviews implementing predictive analytics with spark in azure hdinsight microsoft edx introduces the core concepts of machine learning and a variety of algorithms leverages several big data friendly tools including apache spark scala and hadoop uses both python and r four hours per week over six weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews data science and machine learning with python hands on frank kane udemy uses python kane has nine years of experience at amazon and imdb nine hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews scala and spark for big data and machine learning jose portilla udemy big data focus specifically on implementation in scala and spark ten hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews machine learning engineer nanodegree udacity udacity s flagship machine learning program which features a best in class project review system and career support the program is a compilation of several individual udacity courses which are free co created by kaggle estimated timeline of six months currently costs usd per month with a tuition refund available for those who graduate within months it has a star weighted average rating over reviews learning from data introductory machine learning california institute of technology edx enrollment is currently closed on edx but is also available via caltech s independent platform see below it has a star weighted average rating over reviews learning from data introductory machine learning yaser abu mostafa california institute of technology a real caltech course not a watered down version reviews note it is excellent for understanding machine learning theory the professor yaser abu mostafa is popular among students and also wrote the textbook upon which this course is based videos are taped lectures with lectures slides picture in picture uploaded to youtube homework assignments are pdf files the course experience for online students isn t as polished as the top three recommendations it has a star weighted average rating over reviews mining massive datasets stanford university machine learning with a focus on big data introduces modern distributed file systems and mapreduce ten hours per week over seven weeks free it has a star weighted average rating over reviews aws machine learning a complete guide with python chandra lingam udemy a unique focus on cloud based machine learning and specifically amazon web services uses python nine hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews introduction to machine learning face detection in python holczer balazs udemy uses python eight hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews statlearning statistical learning stanford university based on the excellent textbook an introduction to statistical learning with applications in r and taught by the professors who wrote it reviewers note that the mooc isn t as good as the book citing thin exercises and mediocre videos five hours per week over nine weeks free it has a star weighted average rating over reviews machine learning specialization university of washington coursera great courses but last two classes including the capstone project were canceled reviewers note that this series is more digestable read easier for those without strong technical backgrounds than other top machine learning courses e g stanford s or caltech s be aware that the series is incomplete with recommender systems deep learning and a summary missing free and paid options available it has a star weighted average rating over reviews from to machine learning nlp python cut to the chase loony corn udemy a down to earth shy but confident take on machine learning techniques taught by four person team with decades of industry experience together uses python cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews principles of machine learning microsoft edx uses r python and microsoft azure machine learning part of the microsoft professional program certificate in data science three to four hours per week over six weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews big data statistical inference and machine learning queensland university of technology futurelearn a nice brief exploratory machine learning course with a focus on big data covers a few tools like r h o flow and weka only three weeks in duration at a recommended two hours per week but one reviewer noted that six hours per week would be more appropriate free and paid options available it has a star weighted average rating over reviews genomic data science and clustering bioinformatics v university of california san diego coursera for those interested in the intersection of computer science and biology and how it represents an important frontier in modern science focuses on clustering and dimensionality reduction part of ucsd s bioinformatics specialization free and paid options available it has a star weighted average rating over reviews intro to machine learning udacity prioritizes topic breadth and practical tools in python over depth and theory the instructors sebastian thrun and katie malone make this class so fun consists of bite sized videos and quizzes followed by a mini project for each lesson currently part of udacity s data analyst nanodegree estimated timeline of ten weeks free it has a star weighted average rating over reviews machine learning for data analysis wesleyan university coursera a brief intro machine learning and a few select algorithms covers decision trees random forests lasso regression and k means clustering part of wesleyan s data analysis and interpretation specialization estimated timeline of four weeks free and paid options available it has a star weighted average rating over reviews programming with python for data science microsoft edx produced by microsoft in partnership with coding dojo uses python eight hours per week over six weeks free and paid options available it has a star weighted average rating over reviews machine learning for trading georgia tech udacity focuses on applying probabilistic machine learning approaches to trading decisions uses python part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree oms estimated timeline of four months free it has a star weighted average rating over reviews practical machine learning johns hopkins university coursera a brief practical introduction to a number of machine learning algorithms several one two star reviews expressing a variety of concerns part of jhu s data science specialization four to nine hours per week over four weeks free and paid options available it has a star weighted average rating over reviews machine learning for data science and analytics columbia university edx introduces a wide range of machine learning topics some passionate negative reviews with concerns including content choices a lack of programming assignments and uninspiring presentation seven to ten hours per week over five weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews recommender systems specialization university of minnesota coursera strong focus one specific type of machine learning recommender systems a four course specialization plus a capstone project which is a case study taught using lenskit an open source toolkit for recommender systems free and paid options available it has a star weighted average rating over reviews machine learning with big data university of california san diego coursera terrible reviews that highlight poor instruction and evaluation some noted it took them mere hours to complete the whole course part of ucsd s big data specialization free and paid options available it has a star weighted average rating over reviews practical predictive analytics models and methods university of washington coursera a brief intro to core machine learning concepts one reviewer noted that there was a lack of quizzes and that the assignments were not challenging part of uw s data science at scale specialization six to eight hours per week over four weeks free and paid options available it has a star weighted average rating over reviews the following courses had one or no reviews as of may machine learning for musicians and artists goldsmiths university of london kadenze unique students learn algorithms software tools and machine learning best practices to make sense of human gesture musical audio and other real time data seven sessions in length audit free and premium usd per month options available it has one star review applied machine learning in python university of michigan coursera taught using python and the scikit learn toolkit part of the applied data science with python specialization scheduled to start may th free and paid options available applied machine learning microsoft edx taught using various tools including python r and microsoft azure machine learning note microsoft produces the course includes hands on labs to reinforce the lecture content three to four hours per week over six weeks free with a verified certificate available for purchase machine learning with python big data university taught using python targeted towards beginners estimated completion time of four hours big data university is affiliated with ibm free machine learning with apache systemml big data university taught using apache systemml which is a declarative style language designed for large scale machine learning estimated completion time of eight hours big data university is affiliated with ibm free machine learning for data science university of california san diego edx doesn t launch until january programming examples and assignments are in python using jupyter notebooks eight hours per week over ten weeks free with a verified certificate available for purchase introduction to analytics modeling georgia tech edx the course advertises r as its primary programming tool five to ten hours per week over ten weeks free with a verified certificate available for purchase predictive analytics gaining insights from big data queensland university of technology futurelearn brief overview of a few algorithms uses hewlett packard enterprise s vertica analytics platform as an applied tool start date to be announced two hours per week over four weeks free with a certificate of achievement available for purchase introduccio n al machine learning universitas telefo nica miri ada x taught in spanish an introduction to machine learning that covers supervised and unsupervised learning a total of twenty estimated hours over four weeks machine learning path step dataquest taught in python using dataquest s interactive in browser platform multiple guided projects and a plus project where you build your own machine learning system using your own data subscription required the following six courses are offered by datacamp datacamp s hybrid teaching style leverages video and text based instruction with lots of examples through an in browser code editor a subscription is required for full access to each course introduction to machine learning datacamp covers classification regression and clustering algorithms uses r fifteen videos and exercises with an estimated timeline of six hours supervised learning with scikit learn datacamp uses python and scikit learn covers classification and regression algorithms seventeen videos and exercises with an estimated timeline of four hours unsupervised learning in r datacamp provides a basic introduction to clustering and dimensionality reduction in r sixteen videos and exercises with an estimated timeline of four hours machine learning toolbox datacamp teaches the big ideas in machine learning uses r videos and exercises with an estimated timeline of four hours machine learning with the experts school budgets datacamp a case study from a machine learning competition on drivendata involves building a model to automatically classify items in a school s budget datacamp s supervised learning with scikit learn is a prerequisite fifteen videos and exercises with an estimated timeline of four hours unsupervised learning in python datacamp covers a variety of unsupervised learning algorithms using python scikit learn and scipy the course ends with students building a recommender system to recommend popular musical artists thirteen videos and exercises with an estimated timeline of four hours machine learning tom mitchell carnegie mellon university carnegie mellon s graduate introductory machine learning course a prerequisite to their second graduate level course statistical machine learning taped university lectures with practice problems homework assignments and a midterm all with solutions posted online a version of the course also exists cmu is one of the best graduate schools for studying machine learning and has a whole department dedicated to ml free statistical machine learning larry wasserman carnegie mellon university likely the most advanced course in this guide a follow up to carnegie mellon s machine learning course taped university lectures with practice problems homework assignments and a midterm all with solutions posted online free undergraduate machine learning nando de freitas university of british columbia an undergraduate machine learning course lectures are filmed and put on youtube with the slides posted on the course website the course assignments are posted as well no solutions though de freitas is now a full time professor at the university of oxford and receives praise for his teaching abilities in various forums graduate version available see below machine learning nando de freitas university of british columbia a graduate machine learning course the comments in de freitas undergraduate course above apply here as well this is the fifth of a six piece series that covers the best online courses for launching yourself into the data science field we covered programming in the first article statistics and probability in the second article intros to data science in the third article and data visualization in the fourth the final piece will be a summary of those articles plus the best online courses for other key topics such as data wrangling databases and even software engineering if you re looking for a complete list of data science online courses you can find them on class central s data science and big data subject page if you enjoyed reading this check out some of class central s other pieces if you have suggestions for courses i missed let me know in the responses if you found this helpful click the so more people will see it here on medium this is a condensed version of my original article published on class central where i ve included detailed course syllabi from a quick cheer to a standing ovation clap to show how much you enjoyed this story curriculum lead projects datacamp i created my own data science master s program our community publishes stories worth reading on development design and data science
Vishal Maini,32000,10,https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12?source=tag_archive---------3----------------,A Beginner’s Guide to AI/ML 🤖👶 – Machine Learning for Humans – Medium,part why machine learning matters the big picture of artificial intelligence and machine learning past present and future part supervised learning learning with an answer key introducing linear regression loss functions overfitting and gradient descent part supervised learning ii two methods of classification logistic regression and svms part supervised learning iii non parametric learners k nearest neighbors decision trees random forests introducing cross validation hyperparameter tuning and ensemble models part unsupervised learning clustering k means hierarchical dimensionality reduction principal components analysis pca singular value decomposition svd part neural networks deep learning why where and how deep learning works drawing inspiration from the brain convolutional neural networks cnns recurrent neural networks rnns real world applications part reinforcement learning exploration and exploitation markov decision processes q learning policy learning and deep reinforcement learning the value learning problem appendix the best machine learning resources a curated list of resources for creating your machine learning curriculum this guide is intended to be accessible to anyone basic concepts in probability statistics programming linear algebra and calculus will be discussed but it isn t necessary to have prior knowledge of them to gain value from this series artificial intelligence will shape our future more powerfully than any other innovation this century anyone who does not understand it will soon find themselves feeling left behind waking up in a world full of technology that feels more and more like magic the rate of acceleration is already astounding after a couple of ai winters and periods of false hope over the past four decades rapid advances in data storage and computer processing power have dramatically changed the game in recent years in google trained a conversational agent ai that could not only convincingly interact with humans as a tech support helpdesk but also discuss morality express opinions and answer general facts based questions the same year deepmind developed an agent that surpassed human level performance at atari games receiving only the pixels and game score as inputs soon after in deepmind obsoleted their own achievement by releasing a new state of the art gameplay method called a c meanwhile alphago defeated one of the best human players at go an extraordinary achievement in a game dominated by humans for two decades after machines first conquered chess many masters could not fathom how it would be possible for a machine to grasp the full nuance and complexity of this ancient chinese war strategy game with its possible board positions there are only atoms in the universe in march openai created agents that invented their own language to cooperate and more effectively achieve their goal soon after facebook reportedly successfully training agents to negotiate and even lie just a few days ago as of this writing on august openai reached yet another incredible milestone by defeating the world s top professionals in v matches of the online multiplayer game dota much of our day to day technology is powered by artificial intelligence point your camera at the menu during your next trip to taiwan and the restaurant s selections will magically appear in english via the google translate app today ai is used to design evidence based treatment plans for cancer patients instantly analyze results from medical tests to escalate to the appropriate specialist immediately and conduct scientific research for drug discovery in everyday life it s increasingly commonplace to discover machines in roles traditionally occupied by humans really don t be surprised if a little housekeeping delivery bot shows up instead of a human next time you call the hotel desk to send up some toothpaste in this series we ll explore the core machine learning concepts behind these technologies by the end you should be able to describe how they work at a conceptual level and be equipped with the tools to start building similar applications yourself artificial intelligence is the study of agents that perceive the world around them form plans and make decisions to achieve their goals its foundations include mathematics logic philosophy probability linguistics neuroscience and decision theory many fields fall under the umbrella of ai such as computer vision robotics machine learning and natural language processing machine learning is a subfield of artificial intelligence its goal is to enable computers to learn on their own a machine s learning algorithm enables it to identify patterns in observed data build models that explain the world and predict things without having explicit pre programmed rules and models the technologies discussed above are examples of artificial narrow intelligence ani which can effectively perform a narrowly defined task meanwhile we re continuing to make foundational advances towards human level artificial general intelligence agi also known as strong ai the definition of an agi is an artificial intelligence that can successfully perform any intellectual task that a human being can including learning planning and decision making under uncertainty communicating in natural language making jokes manipulating people trading stocks or reprogramming itself and this last one is a big deal once we create an ai that can improve itself it will unlock a cycle of recursive self improvement that could lead to an intelligence explosion over some unknown time period ranging from many decades to a single day you may have heard this point referred to as the singularity the term is borrowed from the gravitational singularity that occurs at the center of a black hole an infinitely dense one dimensional point where the laws of physics as we understand them start to break down a recent report by the future of humanity institute surveyed a panel of ai researchers on timelines for agi and found that researchers believe there is a chance of ai outperforming humans in all tasks in years grace et al we ve personally spoken with a number of sane and reasonable ai practitioners who predict much longer timelines the upper limit being never and others whose timelines are alarmingly short as little as a few years the advent of greater than human level artificial superintelligence asi could be one of the best or worst things to happen to our species it carries with it the immense challenge of specifying what ais will want in a way that is friendly to humans while it s impossible to say what the future holds one thing is certain is a good time to start understanding how machines think to go beyond the abstractions of a philosopher in an armchair and intelligently shape our roadmaps and policies with respect to ai we must engage with the details of how machines see the world what they want their potential biases and failure modes their temperamental quirks just as we study psychology and neuroscience to understand how humans learn decide act and feel machine learning is at the core of our journey towards artificial general intelligence and in the meantime it will change every industry and have a massive impact on our day to day lives that s why we believe it s worth understanding machine learning at least at a conceptual level and we designed this series to be the best place to start you don t necessarily need to read the series cover to cover to get value out of it here are three suggestions on how to approach it depending on your interests and how much time you have vishal most recently led growth at upstart a lending platform that utilizes machine learning to price credit automate the borrowing process and acquire users he spends his time thinking about startups applied cognitive science moral philosophy and the ethics of artificial intelligence samer is a master s student in computer science and engineering at ucsd and co founder of conigo labs prior to grad school he founded tablescribe a business intelligence tool for smbs and spent two years advising fortune companies at mckinsey samer previously studied computer science and ethics politics and economics at yale most of this series was written during a day trip to the united kingdom in a frantic blur of trains planes cafes pubs and wherever else we could find a dry place to sit our aim was to solidify our own understanding of artificial intelligence machine learning and how the methods therein fit together and hopefully create something worth sharing in the process and now without further ado let s dive into machine learning with part supervised learning more from machine learning for humans a special thanks to jonathan eng edoardo conti grant schneider sunny kumar stephanie he tarun wadhwa and sachin maini series editor for their significant contributions and feedback from a quick cheer to a standing ovation clap to show how much you enjoyed this story research comms deepmindai previously upstart yale trueventurestec demystifying artificial intelligence machine learning discussions on safe and intentional application of ai for positive social impact
Tim Anglade,7000,23,https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3?source=tag_archive---------4----------------,"How HBO’s Silicon Valley built “Not Hotdog” with mobile TensorFlow, Keras & React Native",the hbo show silicon valley released a real ai app that identifies hotdogs and not hotdogs like the one shown on season s th episode the app is now available on android as well as ios to achieve this we designed a bespoke neural architecture that runs directly on your phone and trained it with tensorflow keras nvidia gpus while the use case is farcical the app is an approachable example of both deep learning and edge computing all ai work is powered by the user s device and images are processed without ever leaving their phone this provides users with a snappier experience no round trip to the cloud offline availability and better privacy this also allows us to run the app at a cost of even under the load of a million users providing significant savings compared to traditional cloud based ai approaches the app was developed in house by the show by a single developer running on a single laptop attached gpu using hand curated data in that respect it may provide a sense of what can be achieved today with a limited amount of time resources by non technical companies individual developers and hobbyists alike in that spirit this article attempts to give a detailed overview of steps involved to help others build their own apps if you haven t seen the show or tried the app you should the app lets you snap a picture and then tells you whether it thinks that image is of a hotdog or not it s a straightforward use case that pays homage to recent ai research and applications in particular imagenet while we ve probably dedicated more engineering resources to recognizing hotdogs than anyone else the app still fails in horrible and or subtle ways conversely it s also sometimes able to recognize hotdogs in complex situations according to engadget it s incredible i ve had more success identifying food with the app in minutes than i have had tagging and identifying songs with shazam in the past two years have you ever found yourself reading hacker news thinking they raised a m series a for that i could build it in one weekend this app probably feels a lot like that and the initial prototype was indeed built in a single weekend using google cloud platform s vision api and react native but the final app we ended up releasing on the app store required months of additional part time work to deliver meaningful improvements that would be difficult for an outsider to appreciate we spent weeks optimizing overall accuracy training time inference time iterating on our setup tooling so we could have a faster development iterations and spent a whole weekend optimizing the user experience around ios android permissions don t even get me started on that one all too often technical blog posts or academic papers skip over this part preferring to present the final chosen solution in the interest of helping others learn from our mistake choices we will present an abridged view of the approaches that didn t work for us before we describe the final architecture we ended up shipping in the next section we chose react native to build the prototype as it would give us an easy sandbox to experiment with and would help us quickly support many devices the experience ended up being a good one and we kept react native for the remainder of the project it didn t always make things easy and the design for the app was purposefully limited but in the end react native got the job done the other main component we used for the prototype google cloud s vision api was quickly abandoned there were main factors for these reasons we started experimenting with what s trendily called edge computing which for our purposes meant that after training our neural network on our laptop we would export it and embed it directly into our mobile app so that the neural network execution phase or inference would run directly inside the user s phone through a chance encounter with pete warden of the tensorflow team we had become aware of its ability to run tensorflow directly embedded on an ios device and started exploring that path after react native tensorflow became the second fixed part of our stack it only took a day of work to integrate tensorflow s objective c camera example in our react native shell it took slightly longer to use their transfer learning script which helps you retrain the inception architecture to deal with a more specific image problem inception is the name of a family of neural architectures built by google to deal with image recognition problems inception is available pre trained which means the training phase has been completed and the weights are set most often for image recognition networks they have been trained on imagenet a dataset containing over different types of objects hotdogs are one of them however much like google cloud s vision api imagenet training rewards breadth as much as depth here and out of the box accuracy on a single one of the categories can be lacking as such retraining also called transfer learning aims to take a full trained neural net and retrain it to perform better on the specific problem you d like to handle this usually involves some degree of forgetting either by excising entire layers from the stack or by slowly erasing the network s ability to distinguish a type of object e g chairs in favor of better accuracy at recognizing the one you care about i e hotdogs while the network inception in this case may have been trained on the m images contained in imagenet we were able to retrain it on a just a few thousand hotdog images to get drastically enhanced hotdog recognition the big advantage of transfer learning are you will get better results much faster and with less data than if you train from scratch a full training might take months on multiple gpus and require millions of images while retraining can conceivably be done in hours on a laptop with a couple thousand images one of the biggest challenges we encountered was understanding exactly what should count as a hotdog and what should not defining what a hotdog is ends up being surprisingly difficult do cut up sausages count and if so which kinds and subject to cultural interpretation similarly the open world nature of our problem meant we had to deal with an almost infinite number of inputs while certain computer vision problems have relatively limited inputs say x rays of bolts with or without a mechanical default we had to prepare the app to be fed selfies nature shots and any number of foods suffice to say this approach was promising and did lead to some improved results however it had to be abandoned for a couple of reasons first the nature of our problem meant a strong imbalance in training data there are many more examples of things that are not hotdogs than things that are hotdogs in practice this means that if you train your algorithm on hotdog images and non hotdog images and it recognizes of the former but of the latter it will still score accuracy by default this was not straightforward to solve out of the box using tensorflow s retrain tool and basically necessitated setting up a deep learning model from scratch import weights and train in a more controlled manner at this point we decided to bite the bullet and get something started with keras a deep learning library that provides nicer easier to use abstractions on top of tensorflow including pretty awesome training tools and a class weights option which is ideal to deal with this sort of dataset imbalance we were dealing with we used that opportunity to try other popular neural architectures like vgg but one problem remained none of them could comfortably fit on an iphone they consumed too much memory which led to app crashes and would sometime takes up to seconds to compute which was not ideal from a ux standpoint many things were attempted to mitigate that but in the end it these architectures were just too big to run efficiently on mobile to give you a context out of time this was roughly the mid way point of the project by that time the ui was done and very little of it was going to change but in hindsight the neural net was at best done we had a good sense of challenges a good dataset but lines of the final neural architecture had been written none of our neural code could reliably run on mobile and even our accuracy was going to improve drastically in the weeks to come the problem directly ahead of us was simple if inception and vgg were too big was there a simpler pre trained neural network we could retrain at the suggestion of the always excellent jeremy p howard where has that guy been all our life we explored xception enet and squeezenet we quickly settled on squeezenet due to its explicit positioning as a solution for embedded deep learning and the availability of a pre trained keras model on github yay open source so how big of a difference does this make an architecture like vgg uses about million parameters essentially the number of numbers necessary to model the neurons and values between them inception is already a massive improvement requiring only million parameters squeezenet in comparison only requires million this has two advantages there are tradeoffs of course during this phase we started experimenting with tuning the neural network architecture in particular we started using batch normalization and trying different activation functions after adding batch normalization and elu to squeezenet we were able to train neural network that achieve accuracy when training from scratch however they were relatively brittle meaning the same network would overfit in some cases or underfit in others when confronted to real life testing even adding more examples to the dataset and playing with data augmentation failed to deliver a network that met expectations so while this phase was promising and for the first time gave us a functioning app that could work entirely on an iphone in less than a second we eventually moved to our th final architecture our final architecture was spurred in large part by the publication on april of google s mobilenets paper promising a new neural architecture with inception like accuracy on simple problems like ours with only m or so parameters this meant it sat in an interesting sweet spot between a squeezenet that had maybe been overly simplistic for our purposes and the possibly overwrought elephant trying to squeeze in a tutu of using inception or vgg on mobile the paper introduced some capacity to tune the size complexity of network specifically to trade memory cpu consumption against accuracy which was very much top of mind for us at the time with less than a month to go before the app had to launch we endeavored to reproduce the paper s results this was entirely anticlimactic as within a day of the paper being published a keras implementation was already offered publicly on github by refik can malli a student at istanbul technical university whose work we had already benefitted from when we took inspiration from his excellent keras squeezenet implementation the depth openness of the deep learning community and the presence of talented minds like r c is what makes deep learning viable for applications today but they also make working in this field more thrilling than any tech trend we ve been involved with our final architecture ended up making significant departures from the mobilenets architecture or from convention in particular so how does this stack work exactly deep learning often gets a bad rap for being a black box and while it s true many components of it can be mysterious the networks we use often leak information about how some of their magic work we can look at the layers of this stack and how they activate on specific input images giving us a sense of each layer s ability to recognize sausage buns or other particularly salient hotdog features data quality was of the utmost importance a neural network can only be as good as the data that trained it and improving training set quality was probably one of the top things we spent time on during this project the key things we did to improve this were the final composition of our dataset was k images of which only k were hotdogs there are only so many hotdogs you can look at but there are many not hotdogs to look at the imbalance was dealt with by saying a keras class weight of in favor of hotdogs of the remaining k images most were of food with just k photos of non food items to help the network generalize a bit more and not get tricked into seeing a hotdog if presented with an image of a human in a red outfit our data augmentation rules were as follows these numbers were derived intuitively based on experiments and our understanding of the real life usage of our app as opposed to careful experimentation the final key to our data pipeline was using patrick rodriguez s multiprocess image data generator for keras while keras does have a built in multi threaded and multiprocess implementation we found patrick s library to be consistently faster in our experiments for reasons we did not have time to investigate this library cut our training time to a third of what it used to be the network was trained using a macbook pro and attached external gpu egpu specifically an nvidia gtx ti we d probably buy a ti if we were starting today we were able to train the network on batches of images at a time the network was trained for a total of epochs meaning we ran all k images through the network times this took about hours we trained the network in phases while learning rates were identified by running the linear experiment recommended by the clr paper they seem to intuitively make sense in that the max for each phase is within a factor of of the previous minimum which is aligned with the industry standard recommendation of halving your learning rate if your accuracy plateaus during training in the interest of time we performed some training runs on a paperspace p instance running ubuntu in those cases we were able to double the batch size and found that optimal learning rates for each phase were roughly double as well even having designed a relatively compact neural architecture and having trained it to handle situations it may find in a mobile context we had a lot of work left to make it run properly trying to run a top of the line neural net architecture out of the box can quickly burns hundreds megabytes of ram which few mobile devices can spare today beyond network optimizations it turns out the way you handle images or even load tensorflow itself can have a huge impact on how quickly your network runs how little ram it uses and how crash free the experience will be for your users this was maybe the most mysterious part of this project relatively little information can be found about it possibly due to the dearth of production deep learning applications running on mobile devices as of today however we must commend the tensorflow team and particularly pete warden andrew harp and chad whipkey for the existing documentation and their kindness in answering our inquiries instead of using tensorflow on ios we looked at using apple s built in deep learning libraries instead bnns mpscnn and later on coreml we would have designed the network in keras trained it with tensorflow exported all the weight values re implemented the network with bnns or mpscnn or imported it via coreml and loaded the parameters into that new implementation however the biggest obstacle was that these new apple libraries are only available on ios and we wanted to support older versions of ios as ios adoption and these frameworks continue to improve there may not be a case for using tensorflow on device in the near future if you think injecting javascript into your app on the fly is cool try injecting neural nets into your app the last production trick we used was to leverage codepush and apple s relatively permissive terms of service to live inject new versions of our neural networks after submission to the app store while this was mostly done to help us quickly deliver accuracy improvements to our users after release you could conceivably use this approach to drastically expand or alter the feature set of your app without going through an app store review again there are a lot of things that didn t work or we didn t have time to do and these are the ideas we d investigate in the future finally we d be remiss not to mention the obvious and important influence of user experience developer experience and built in biases in developing an ai app each probably deserve their own post or their own book but here are the very concrete impacts of these things in our experience ux user experience is arguably more critical at every stage of the development of an ai app than for a traditional application there are no deep learning algorithms that will give you perfect results right now but there are many situations where the right mix of deep learning ux will lead to results that are indistinguishable from perfect proper ux expectations are irreplaceable when it comes to setting developers on the right path to design their neural networks setting the proper expectations for users when they use the app and gracefully handling the inevitable ai failures building ai apps without a ux first mindset is like training a neural net without stochastic gradient descent you will end up stuck in the local minima of the uncanny valley on your way to building the perfect ai use case dx developer experience is extremely important as well because deep learning training time is the new horsing around while waiting for your program to compile we suggest you heavily favor dx first hence keras as it s always possible to optimize runtime for later runs manual gpu parallelization multi process data augmentation tensorflow pipeline even re implementing for caffe pytorch even projects with relatively obtuse apis documentation like tensorflow greatly improve dx by providing a highly tested highly used well maintained environment for training running neural networks for the same reason it s hard to beat both the cost as well as the flexibility of having your own local gpu for development being able to look at edit images locally edit code with your preferred tool without delays greatly improves the development quality speed of building ai projects most ai apps will hit more critical cultural biases than ours but as an example even our straightforward use case caught us flat footed with built in biases in our initial dataset that made the app unable to recognize french style hotdogs asian hotdogs and more oddities we did not have immediate personal experience with it s critical to remember that ai do not make better decisions than humans they are infected by the same human biases we fall prey to via the training sets humans provide thanks to mike judge alec berg clay tarver todd silverstein jonathan dotan lisa schomas amy solomon dorothy street rich toyon and all the writers of the show the app would simply not exist without them meaghan dana david jay and everyone at hbo scale venture partners gitlab rachel thomas and jeremy howard fast ai for all that they have taught me and for kindly reviewing a draft of this post check out their free online deep learning course it s awesome jp simard for his help on ios and finally the tensorflow team r machinelearning for their help inspiration and thanks to everyone who used shared the app it made staring at pictures of hotdogs for months on end totally worth it from a quick cheer to a standing ovation clap to show how much you enjoyed this story a i startups hbo s silicon valley get in touch timanglade gmail com
Sophia Ciocca,53000,9,https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe?source=tag_archive---------5----------------,How Does Spotify Know You So Well? – Member Feature Stories – Medium,member feature story a software engineer explains the science behind personalized music recommendations photo by studioeast getty images photo by studioeast getty images this monday just like every monday before it over million spotify users found a fresh new playlist waiting for them called discover weekly it s a custom mixtape of songs they ve never listened to before but will probably love and it s pretty much magic i m a huge fan of spotify and particularly discover weekly why it makes me feel seen it knows my musical tastes better than any person in my entire life ever has and i m consistently delighted by how satisfyingly just right it is every week with tracks i probably would never have found myself or known i would like for those of you who live under a soundproof rock let me introduce you to my virtual best friend as it turns out i m not alone in my obsession with discover weekly the user base goes crazy for it which has driven spotify to rethink its focus and invest more resources into algorithm based playlists ever since discover weekly debuted in i ve been dying to know how it works what s more i m a spotify fangirl so i sometimes like to pretend that i work there and research their products after three weeks of mad googling i feel like i ve finally gotten a glimpse behind the curtain so how does spotify do such an amazing job of choosing those songs for each person each week let s zoom out for a second to look at how other music services have tackled music recommendations and how spotify s doing it better back in the s songza kicked off the online music curation scene using manual curation to create playlists for users this meant that a team of music experts or other human curators would put together playlists that they just thought sounded good and then users would listen to those playlists later beats music would employ this same strategy manual curation worked alright but it was based on that specific curator s choices and therefore couldn t take into account each listener s individual music taste like songza pandora was also one of the original players in digital music curation it employed a slightly more advanced approach instead manually tagging attributes of songs this meant a group of people listened to music chose a bunch of descriptive words for each track and tagged the tracks accordingly then pandora s code could simply filter for certain tags to make playlists of similar sounding music around that same time a music intelligence agency from the mit media lab called the echo nest was born which took a radical cutting edge approach to personalized music the echo nest used algorithms to analyze the audio and textual content of music allowing it to perform music identification personalized recommendation playlist creation and analysis finally taking another approach is last fm which still exists today and uses a process called collaborative filtering to identify music its users might like but more on that in a moment so if that s how other music curation services have handled recommendations how does spotify s magic engine run how does it seem to nail individual users tastes so much more accurately than any of the other services spotify doesn t actually use a single revolutionary recommendation model instead they mix together some of the best strategies used by other services to create their own uniquely powerful discovery engine to create discover weekly there are three main types of recommendation models that spotify employs let s dive into how each of these recommendation models work first some background when people hear the words collaborative filtering they generally think of netflix as it was one of the first companies to use this method to power a recommendation model taking users star based movie ratings to inform its understanding of which movies to recommend to other similar users after netflix was successful the use of collaborative filtering spread quickly and is now often the starting point for anyone trying to make a recommendation model unlike netflix spotify doesn t have a star based system with which users rate their music instead spotify s data is implicit feedback specifically the stream counts of the tracks and additional streaming data such as whether a user saved the track to their own playlist or visited the artist s page after listening to a song but what is collaborative filtering truly and how does it work here s a high level rundown explained in a quick conversation what s going on here each of these individuals has track preferences the one on the left likes tracks p q r and s while the one on the right likes tracks q r s and t collaborative filtering then uses that data to say hmmm you both like three of the same tracks q r and s so you are probably similar users therefore you re each likely to enjoy other tracks that the other person has listened to that you haven t heard yet therefore it suggests that the one on the right check out track p the only track not mentioned but that his similar counterpart enjoyed and the one on the left check out track t for the same reasoning simple right but how does spotify actually use that concept in practice to calculate millions of users suggested tracks based on millions of other users preferences with matrix math done with python libraries in actuality this matrix you see here is gigantic each row represents one of spotify s million users if you use spotify you yourself are a row in this matrix and each column represents one of the million songs in spotify s database then the python library runs this long complicated matrix factorization formula when it finishes we end up with two types of vectors represented here by x and y x is a user vector representing one single user s taste and y is a song vector representing one single song s profile now we have million user vectors and million song vectors the actual content of these vectors is just a bunch of numbers that are essentially meaningless on their own but are hugely useful when compared to find out which users musical tastes are most similar to mine collaborative filtering compares my vector with all of the other users vectors ultimately spitting out which users are the closest matches the same goes for the y vector songs you can compare a single song s vector with all the others and find out which songs are most similar to the one in question collaborative filtering does a pretty good job but spotify knew they could do even better by adding another engine enter nlp the second type of recommendation models that spotify employs are natural language processing nlp models the source data for these models as the name suggests are regular ol words track metadata news articles blogs and other text around the internet natural language processing which is the ability of a computer to understand human speech as it is spoken is a vast field unto itself often harnessed through sentiment analysis apis the exact mechanisms behind nlp are beyond the scope of this article but here s what happens on a very high level spotify crawls the web constantly looking for blog posts and other written text about music to figure out what people are saying about specific artists and songs which adjectives and what particular language is frequently used in reference to those artists and songs and which other artists and songs are also being discussed alongside them while i don t know the specifics of how spotify chooses to then process this scraped data i can offer some insight based on how the echo nest used to work with them they would bucket spotify s data up into what they call cultural vectors or top terms each artist and song had thousands of top terms that changed on the daily each term had an associated weight which correlated to its relative importance roughly the probability that someone will describe the music or artist with that term then much like in collaborative filtering the nlp model uses these terms and weights to create a vector representation of the song that can be used to determine if two pieces of music are similar cool right first a question you might be thinking first of all adding a third model further improves the accuracy of the music recommendation service but this model also serves a secondary purpose unlike the first two types raw audio models take new songs into account take for example a song your singer songwriter friend has put up on spotify maybe it only has listens so there are few other listeners to collaboratively filter it against it also isn t mentioned anywhere on the internet yet so nlp models won t pick it up luckily raw audio models don t discriminate between new tracks and popular tracks so with their help your friend s song could end up in a discover weekly playlist alongside popular songs but how can we analyze raw audio data which seems so abstract with convolutional neural networks convolutional neural networks are the same technology used in facial recognition software in spotify s case they ve been modified for use on audio data instead of pixels here s an example of a neural network architecture this particular neural network has four convolutional layers seen as the thick bars on the left and three dense layers seen as the more narrow bars on the right the inputs are time frequency representations of audio frames which are then concatenated or linked together to form the spectrogram the audio frames go through these convolutional layers and after passing through the last one you can see a global temporal pooling layer which pools across the entire time axis effectively computing statistics of the learned features across the time of the song after processing the neural network spits out an understanding of the song including characteristics like estimated time signature key mode tempo and loudness below is a plot of data for a second snippet of around the world by daft punk ultimately this reading of the song s key characteristics allows spotify to understand fundamental similarities between songs and therefore which users might enjoy them based on their own listening history that covers the basics of the three major types of recommendation models feeding spotify s recommendations pipeline and ultimately powering the discover weekly playlist of course these recommendation models are all connected to spotify s larger ecosystem which includes giant amounts of data storage and uses lots of hadoop clusters to scale recommendations and make these engines work on enormous matrices endless online music articles and huge numbers of audio files i hope this was informative and piqued your curiosity like it did mine for now i ll be working my way through my own discover weekly finding my new favorite music while appreciating all the machine learning that s going on behind the scenes thanks also to ladycollective for reading this article and suggesting edits software engineer writer and generally creative human interested in art feminism mindfulness and authenticity http sophiaciocca com welcome to a place where words matter on medium smart voices and original ideas take center stage with no ads in sight watch follow all the topics you care about and we ll deliver the best stories for you to your homepage and inbox explore get unlimited access to the best stories on medium and support writers while you re at it just month upgrade
Dhruv Parthasarathy,4300,12,https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4?source=tag_archive---------6----------------,A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN,at athelas we use convolutional neural networks cnns for a lot more than just classification in this post we ll see how cnns can be used with great results in image instance segmentation ever since alex krizhevsky geoff hinton and ilya sutskever won imagenet in convolutional neural networks cnns have become the gold standard for image classification in fact since then cnns have improved to the point where they now outperform humans on the imagenet challenge while these results are impressive image classification is far simpler than the complexity and diversity of true human visual understanding in classification there s generally an image with a single object as the focus and the task is to say what that image is see above but when we look at the world around us we carry out far more complex tasks we see complicated sights with multiple overlapping objects and different backgrounds and we not only classify these different objects but also identify their boundaries differences and relations to one another can cnns help us with such complex tasks namely given a more complicated image can we use cnns to identify the different objects in the image and their boundaries as has been shown by ross girshick and his peers over the last few years the answer is conclusively yes through this post we ll cover the intuition behind some of the main techniques used in object detection and segmentation and see how they ve evolved from one implementation to the next in particular we ll cover r cnn regional cnn the original application of cnns to this problem along with its descendants fast r cnn and faster r cnn finally we ll cover mask r cnn a paper released recently by facebook research that extends such object detection techniques to provide pixel level segmentation here are the papers referenced in this post inspired by the research of hinton s lab at the university of toronto a small team at uc berkeley led by professor jitendra malik asked themselves what today seems like an inevitable question object detection is the task of finding the different objects in an image and classifying them as seen in the image above the team comprised of ross girshick a name we ll see again jeff donahue and trevor darrel found that this problem can be solved with krizhevsky s results by testing on the pascal voc challenge a popular object detection challenge akin to imagenet they write let s now take a moment to understand how their architecture regions with cnns r cnn works understanding r cnn the goal of r cnn is to take in an image and correctly identify where the main objects via a bounding box in the image but how do we find out where these bounding boxes are r cnn does what we might intuitively do as well propose a bunch of boxes in the image and see if any of them actually correspond to an object r cnn creates these bounding boxes or region proposals using a process called selective search which you can read about here at a high level selective search shown in the image above looks at the image through windows of different sizes and for each size tries to group together adjacent pixels by texture color or intensity to identify objects once the proposals are created r cnn warps the region to a standard square size and passes it through to a modified version of alexnet the winning submission to imagenet that inspired r cnn as shown above on the final layer of the cnn r cnn adds a support vector machine svm that simply classifies whether this is an object and if so what object this is step in the image above improving the bounding boxes now having found the object in the box can we tighten the box to fit the true dimensions of the object we can and this is the final step of r cnn r cnn runs a simple linear regression on the region proposal to generate tighter bounding box coordinates to get our final result here are the inputs and outputs of this regression model so to summarize r cnn is just the following steps r cnn works really well but is really quite slow for a few simple reasons in ross girshick the first author of r cnn solved both these problems leading to the second algorithm in our short history fast r cnn let s now go over its main insights fast r cnn insight roi region of interest pooling for the forward pass of the cnn girshick realized that for each image a lot of proposed regions for the image invariably overlapped causing us to run the same cnn computation again and again times his insight was simple why not run the cnn just once per image and then find a way to share that computation across the proposals this is exactly what fast r cnn does using a technique known as roipool region of interest pooling at its core roipool shares the forward pass of a cnn for an image across its subregions in the image above notice how the cnn features for each region are obtained by selecting a corresponding region from the cnn s feature map then the features in each region are pooled usually using max pooling so all it takes us is one pass of the original image as opposed to fast r cnn insight combine all models into one network the second insight of fast r cnn is to jointly train the cnn classifier and bounding box regressor in a single model where earlier we had different models to extract image features cnn classify svm and tighten bounding boxes regressor fast r cnn instead used a single network to compute all three you can see how this was done in the image above fast r cnn replaced the svm classifier with a softmax layer on top of the cnn to output a classification it also added a linear regression layer parallel to the softmax layer to output bounding box coordinates in this way all the outputs needed came from one single network here are the inputs and outputs to this overall model even with all these advancements there was still one remaining bottleneck in the fast r cnn process the region proposer as we saw the very first step to detecting the locations of objects is generating a bunch of potential bounding boxes or regions of interest to test in fast r cnn these proposals were created using selective search a fairly slow process that was found to be the bottleneck of the overall process in the middle a team at microsoft research composed of shaoqing ren kaiming he ross girshick and jian sun found a way to make the region proposal step almost cost free through an architecture they creatively named faster r cnn the insight of faster r cnn was that region proposals depended on features of the image that were already calculated with the forward pass of the cnn first step of classification so why not reuse those same cnn results for region proposals instead of running a separate selective search algorithm indeed this is just what the faster r cnn team achieved in the image above you can see how a single cnn is used to both carry out region proposals and classification this way only one cnn needs to be trained and we get region proposals almost for free the authors write here are the inputs and outputs of their model how the regions are generated let s take a moment to see how faster r cnn generates these region proposals from cnn features faster r cnn adds a fully convolutional network on top of the features of the cnn creating what s known as the region proposal network the region proposal network works by passing a sliding window over the cnn feature map and at each window outputting k potential bounding boxes and scores for how good each of those boxes is expected to be what do these k boxes represent intuitively we know that objects in an image should fit certain common aspect ratios and sizes for instance we know that we want some rectangular boxes that resemble the shapes of humans likewise we know we won t see many boxes that are very very thin in such a way we create k such common aspect ratios we call anchor boxes for each such anchor box we output one bounding box and score per position in the image with these anchor boxes in mind let s take a look at the inputs and outputs to this region proposal network we then pass each such bounding box that is likely to be an object into fast r cnn to generate a classification and tightened bounding boxes so far we ve seen how we ve been able to use cnn features in many interesting ways to effectively locate different objects in an image with bounding boxes can we extend such techniques to go one step further and locate exact pixels of each object instead of just bounding boxes this problem known as image segmentation is what kaiming he and a team of researchers including girshick explored at facebook ai using an architecture known as mask r cnn much like fast r cnn and faster r cnn mask r cnn s underlying intuition is straight forward given that faster r cnn works so well for object detection could we extend it to also carry out pixel level segmentation mask r cnn does this by adding a branch to faster r cnn that outputs a binary mask that says whether or not a given pixel is part of an object the branch in white in the above image as before is just a fully convolutional network on top of a cnn based feature map here are its inputs and outputs but the mask r cnn authors had to make one small adjustment to make this pipeline work as expected roialign realigning roipool to be more accurate when run without modifications on the original faster r cnn architecture the mask r cnn authors realized that the regions of the feature map selected by roipool were slightly misaligned from the regions of the original image since image segmentation requires pixel level specificity unlike bounding boxes this naturally led to inaccuracies the authors were able to solve this problem by cleverly adjusting roipool to be more precisely aligned using a method known as roialign imagine we have an image of size x and a feature map of size x let s imagine we want features the region corresponding to the top left x pixels in the original image see above how might we select these pixels from the feature map we know each pixel in the original image corresponds to pixels in the feature map to select pixels from the original image we just select pixels in roipool we would round this down and select pixels causing a slight misalignment however in roialign we avoid such rounding instead we use bilinear interpolation to get a precise idea of what would be at pixel this at a high level is what allows us to avoid the misalignments caused by roipool once these masks are generated mask r cnn combines them with the classifications and bounding boxes from faster r cnn to generate such wonderfully precise segmentations if you re interested in trying out these algorithms yourselves here are relevant repositories faster r cnn mask r cnn in just years we ve seen how the research community has progressed from krizhevsky et al s original result to r cnn and finally all the way to such powerful results as mask r cnn seen in isolation results like mask r cnn seem like incredible leaps of genius that would be unapproachable yet through this post i hope you ve seen how such advancements are really the sum of intuitive incremental improvements through years of hard work and collaboration each of the ideas proposed by r cnn fast r cnn faster r cnn and finally mask r cnn were not necessarily quantum leaps yet their sum products have led to really remarkable results that bring us closer to a human level understanding of sight what particularly excites me is that the time between r cnn and mask r cnn was just three years with continued funding focus and support how much further can computer vision improve over the next three years if you see any errors or issues in this post please contact me at dhruv getathelas com and i ll immediately correct them if you re interested in applying such techniques come join us at athelas where we apply computer vision to blood diagnostics daily other posts we ve written thanks to bharath ramsundar pranav ramkrishnan tanay tandon and oliver cameron for help with this post from a quick cheer to a standing ovation clap to show how much you enjoyed this story dhruvp vp eng athelas mit math and cs undergrad mit cs masters previously director of ai programs udacity blood diagnostics through deep learning http athelas com
Andrej Karpathy,35000,8,https://medium.com/@karpathy/software-2-0-a64152b37c35?source=tag_archive---------7----------------,Software 2.0 – Andrej Karpathy – Medium,i sometimes see people refer to neural networks as just another tool in your machine learning toolbox they have some pros and cons they work here or there and sometimes you can use them to win kaggle competitions unfortunately this interpretation completely misses the forest for the trees neural networks are not just another classifier they represent the beginning of a fundamental shift in how we write software they are software the classical stack of software is what we re all familiar with it is written in languages such as python c etc it consists of explicit instructions to the computer written by a programmer by writing each line of code the programmer identifies a specific point in program space with some desirable behavior in contrast software can be written in much more abstract human unfriendly language such as the weights of a neural network no human is involved in writing this code because there are a lot of weights typical networks might have millions and coding directly in weights is kind of hard i tried instead our approach is to specify some goal on the behavior of a desirable program e g satisfy a dataset of input output pairs of examples or win a game of go write a rough skeleton of the code e g a neural net architecture that identifies a subset of program space to search and use the computational resources at our disposal to search this space for a program that works in the specific case of neural networks we restrict the search to a continuous subset of the program space where the search process can be made somewhat surprisingly efficient with backpropagation and stochastic gradient descent it turns out that a large portion of real world problems have the property that it is significantly easier to collect the data or more generally identify a desirable behavior than to explicitly write the program in these cases the programmers will often split into two the programmers manually curate maintain massage clean and label datasets each labeled example literally programs the final system because the dataset gets compiled into software code via the optimization meanwhile the programmers maintain the surrounding tools analytics visualizations labeling interfaces infrastructure and the training code let s briefly examine some concrete examples of this ongoing transition in each of these areas we ve seen improvements over the last few years when we give up on trying to address a complex problem by writing explicit code and instead transition the code into the stack visual recognition used to consist of engineered features with a bit of machine learning sprinkled on top at the end e g an svm since then we discovered much more powerful visual features by obtaining large datasets e g imagenet and searching in the space of convolutional neural network architectures more recently we don t even trust ourselves to hand code the architectures and we ve begun searching over those as well speech recognition used to involve a lot of preprocessing gaussian mixture models and hidden markov models but today consist almost entirely of neural net stuff a very related often cited humorous quote attributed to fred jelinek from reads every time i fire a linguist the performance of our speech recognition system goes up speech synthesis has historically been approached with various stitching mechanisms but today the state of the art models are large convnets e g wavenet that produce raw audio signal outputs machine translation has usually been approaches with phrase based statistical techniques but neural networks are quickly becoming dominant my favorite architectures are trained in the multilingual setting where a single model translates from any source language to any target language and in weakly supervised or entirely unsupervised settings games explicitly hand coded go playing programs have been developed for a long while but alphago zero a convnet that looks at the raw state of the board and plays a move has now become by far the strongest player of the game i expect we re going to see very similar results in other areas e g dota or starcraft databases more traditional systems outside of artificial intelligence are also seeing early hints of a transition for instance the case for learned index structures replaces core components of a data management system with a neural network outperforming cache optimized b trees by up to in speed while saving an order of magnitude in memory you ll notice that many of my links above involve work done at google this is because google is currently at the forefront of re writing large chunks of itself into software code one model to rule them all provides an early sketch of what this might look like where the statistical strength of the individual domains is amalgamated into one consistent understanding of the world why should we prefer to port complex programs into software clearly one easy answer is that they work better in practice however there are a lot of other convenient reasons to prefer this stack let s take a look at some of the benefits of software think a convnet compared to software think a production level c code base software is computationally homogeneous a typical neural network is to the first order made up of a sandwich of only two operations matrix multiplication and thresholding at zero relu compare that with the instruction set of classical software which is significantly more heterogenous and complex because you only have to provide software implementation for a small number of the core computational primitives e g matrix multiply it is much easier to make various correctness performance guarantees simple to bake into silicon as a corollary since the instruction set of a neural network is relatively small it is significantly easier to implement these networks much closer to silicon e g with custom asics neuromorphic chips and so on the world will change when low powered intelligence becomes pervasive around us e g small inexpensive chips could come with a pretrained convnet a speech recognizer and a wavenet speech synthesis network all integrated in a small protobrain that you can attach to stuff constant running time every iteration of a typical neural net forward pass takes exactly the same amount of flops there is zero variability based on the different execution paths your code could take through some sprawling c code base of course you could have dynamic compute graphs but the execution flow is normally still significantly constrained this way we are also almost guaranteed to never find ourselves in unintended infinite loops constant memory use related to the above there is no dynamically allocated memory anywhere so there is also little possibility of swapping to disk or memory leaks that you have to hunt down in your code it is highly portable a sequence of matrix multiplies is significantly easier to run on arbitrary computational configurations compared to classical binaries or scripts it is very agile if you had a c code and someone wanted you to make it twice as fast at cost of performance if needed it would be highly non trivial to tune the system for the new spec however in software we can take our network remove half of the channels retrain and there it runs exactly at twice the speed and works a bit worse it s magic conversely if you happen to get more data compute you can immediately make your program work better just by adding more channels and retraining modules can meld into an optimal whole our software is often decomposed into modules that communicate through public functions apis or endpoints however if two software modules that were originally trained separately interact we can easily backpropagate through the whole think about how amazing it could be if your web browser could automatically re design the low level system instructions stacks down to achieve a higher efficiency in loading web pages with this is the default behavior it is better than you finally and most importantly a neural network is a better piece of code than anything you or i can come up with in a large fraction of valuable verticals which currently at the very least involve anything to do with images video and sound speech the stack also has some of its own disadvantages at the end of the optimization we re left with large networks that work well but it s very hard to tell how across many applications areas we ll be left with a choice of using a accurate model we understand or accurate model we don t the stack can fail in unintuitive and embarrassing ways or worse they can silently fail e g by silently adopting biases in their training data which are very difficult to properly analyze and examine when their sizes are easily in the millions in most cases finally we re still discovering some of the peculiar properties of this stack for instance the existence of adversarial examples and attacks highlights the unintuitive nature of this stack software is code we write software is code we do not write but seems to work well it is likely that any setting where the program is not obvious but one can repeatedly evaluate the performance of it e g did you classify some images correctly do you win games of go will be subject to this transition because the optimization can find much better code than what we can write if you think of neural networks as a new software stack and not just a pretty good classifier it quickly becomes apparent there is a lot of work to do for example from a systems perspective in the stack llvm ir forms a middle layer between a number of front ends languages and back ends architectures and provides an opportunity for optimization with neural networks we re already seeing an explosion of front ends for specifying program subsets to search over pytorch tf chainer mxnet etc and back ends to run the training compilation and inference cpu gpu tpu ipu but what is a fitting ir and how we can optimize it halide like as another example we ve built up a vast amount of tooling that assists humans in writing code like powerful ides with features like syntax highlighting debuggers profilers go to def git integration etc who is going to develop the first powerful software ides which help with all of the workflows in accumulating visualizing cleaning labeling and sourcing datasets there is a lot of room for a layer of intelligence assisting the programmers e g perhaps the ide bubbles up images that the network suspects are mislabeled or assists in labeling or finds examples where the network is currently uncertain finally in the long term the future of software is bright because it is increasingly clear to many that when we develop agi it will certainly be written in software from a quick cheer to a standing ovation clap to show how much you enjoyed this story director of ai at tesla previously research scientist at openai and phd student at stanford i like to train deep neural nets on large datasets
Sebastian Heinz,4400,13,https://medium.com/mlreview/a-simple-deep-learning-model-for-stock-price-prediction-using-tensorflow-30505541d877?source=tag_archive---------8----------------,A simple deep learning model for stock price prediction using TensorFlow,for a recent hackathon that we did at statworx some of our team members scraped minutely s p data from the google finance api the data consisted of index as well as stock prices of the s p s constituents having this data at hand the idea of developing a deep learning model for predicting the s p index based on the constituents prices one minute ago came immediately on my mind playing around with the data and building the deep learning model with tensorflow was fun and so i decided to write my first medium com story a little tensorflow tutorial on predicting s p stock prices what you will read is not an in depth tutorial but more a high level introduction to the important building blocks and concepts of tensorflow models the python code i ve created is not optimized for efficiency but understandability the dataset i ve used can be downloaded from here mb our team exported the scraped stock data from our scraping server as a csv file the dataset contains n minutes of data ranging from april to august on stocks as well as the total s p index price index and stocks are arranged in wide format the data was already cleaned and prepared meaning missing stock and index prices were locf ed last observation carried forward so that the file did not contain any missing values a quick look at the s p time series using pyplot plot data sp note this is actually the lead of the s p index meaning its value is shifted minute into the future this operation is necessary since we want to predict the next minute of the index and not the current minute the dataset was split into training and test data the training data contained of the total dataset the data was not shuffled but sequentially sliced the training data ranges from april to approx end of july the test data ends end of august there are a lot of different approaches to time series cross validation such as rolling forecasts with and without refitting or more elaborate concepts such as time series bootstrap resampling the latter involves repeated samples from the remainder of the seasonal decomposition of the time series in order to simulate samples that follow the same seasonal pattern as the original time series but are not exact copies of its values most neural network architectures benefit from scaling the inputs sometimes also the output why because most common activation functions of the network s neurons such as tanh or sigmoid are defined on the or interval respectively nowadays rectified linear unit relu activations are commonly used activations which are unbounded on the axis of possible activation values however we will scale both the inputs and targets anyway scaling can be easily accomplished in python using sklearn s minmaxscaler remark caution must be undertaken regarding what part of the data is scaled and when a common mistake is to scale the whole dataset before training and test split are being applied why is this a mistake because scaling invokes the calculation of statistics e g the min max of a variable when performing time series forecasting in real life you do not have information from future observations at the time of forecasting therefore calculation of scaling statistics has to be conducted on training data and must then be applied to the test data otherwise you use future information at the time of forecasting which commonly biases forecasting metrics in a positive direction tensorflow is a great piece of software and currently the leading deep learning and neural network computation framework it is based on a c low level backend but is usually controlled via python there is also a neat tensorflow library for r maintained by rstudio tensorflow operates on a graph representation of the underlying computational task this approach allows the user to specify mathematical operations as elements in a graph of data variables and operators since neural networks are actually graphs of data and mathematical operations tensorflow is just perfect for neural networks and deep learning check out this simple example stolen from our deep learning introduction from our blog in the figure above two numbers are supposed to be added those numbers are stored in two variables a and b the two values are flowing through the graph and arrive at the square node where they are being added the result of the addition is stored into another variable c actually a b and c can be considered as placeholders any numbers that are fed into a and b get added and are stored into c this is exactly how tensorflow works the user defines an abstract representation of the model neural network through placeholders and variables afterwards the placeholders get filled with real data and the actual computations take place the following code implements the toy example from above in tensorflow after having imported the tensorflow library two placeholders are defined using tf placeholder they correspond to the two blue circles on the left of the image above afterwards the mathematical addition is defined via tf add the result of the computation is c with placeholders set up the graph can be executed with any integer value for a and b of course the former problem is just a toy example the required graphs and computations in a neural network are much more complex as mentioned before it all starts with placeholders we need two placeholders in order to fit our model x contains the network s inputs the stock prices of all s p constituents at time t t and y the network s outputs the index value of the s p at time t t the shape of the placeholders correspond to none n stocks with none meaning that the inputs are a dimensional matrix and the outputs are a dimensional vector it is crucial to understand which input and output dimensions the neural net needs in order to design it properly the none argument indicates that at this point we do not yet know the number of observations that flow through the neural net graph in each batch so we keep if flexible we will later define the variable batch size that controls the number of observations per training batch besides placeholders variables are another cornerstone of the tensorflow universe while placeholders are used to store input and target data in the graph variables are used as flexible containers within the graph that are allowed to change during graph execution weights and biases are represented as variables in order to adapt during training variables need to be initialized prior to model training we will get into that a litte later in more detail the model consists of four hidden layers the first layer contains neurons slightly more than double the size of the inputs subsequent hidden layers are always half the size of the previous layer which means and finally neurons a reduction of the number of neurons for each subsequent layer compresses the information the network identifies in the previous layers of course other network architectures and neuron configurations are possible but are out of scope for this introduction level article it is important to understand the required variable dimensions between input hidden and output layers as a rule of thumb in multilayer perceptrons mlps the type of networks used here the second dimension of the previous layer is the first dimension in the current layer for weight matrices this might sound complicated but is essentially just each layer passing its output as input to the next layer the biases dimension equals the second dimension of the current layer s weight matrix which corresponds the number of neurons in this layer after definition of the required weight and bias variables the network topology the architecture of the network needs to be specified hereby placeholders data and variables weighs and biases need to be combined into a system of sequential matrix multiplications furthermore the hidden layers of the network are transformed by activation functions activation functions are important elements of the network architecture since they introduce non linearity to the system there are dozens of possible activation functions out there one of the most common is the rectified linear unit relu which will also be used in this model the image below illustrates the network architecture the model consists of three major building blocks the input layer the hidden layers and the output layer this architecture is called a feedforward network feedforward indicates that the batch of data solely flows from left to right other network architectures such as recurrent neural networks also allow data flowing backwards in the network the cost function of the network is used to generate a measure of deviation between the network s predictions and the actual observed training targets for regression problems the mean squared error mse function is commonly used mse computes the average squared deviation between predictions and targets basically any differentiable function can be implemented in order to compute a deviation measure between predictions and targets however the mse exhibits certain properties that are advantageous for the general optimization problem to be solved the optimizer takes care of the necessary computations that are used to adapt the network s weight and bias variables during training those computations invoke the calculation of so called gradients that indicate the direction in which the weights and biases have to be changed during training in order to minimize the network s cost function the development of stable and speedy optimizers is a major field in neural network an deep learning research here the adam optimizer is used which is one of the current default optimizers in deep learning development adam stands for adaptive moment estimation and can be considered as a combination between two other popular optimizers adagrad and rmsprop initializers are used to initialize the network s variables before training since neural networks are trained using numerical optimization techniques the starting point of the optimization problem is one the key factors to find good solutions to the underlying problem there are different initializers available in tensorflow each with different initialization approaches here i use the tf variance scaling initializer which is one of the default initialization strategies note that with tensorflow it is possible to define multiple initialization functions for different variables within the graph however in most cases a unified initialization is sufficient after having defined the placeholders variables initializers cost functions and optimizers of the network the model needs to be trained usually this is done by minibatch training during minibatch training random data samples of n batch size are drawn from the training data and fed into the network the training dataset gets divided into n batch size batches that are sequentially fed into the network at this point the placeholders x and y come into play they store the input and target data and present them to the network as inputs and targets a sampled data batch of x flows through the network until it reaches the output layer there tensorflow compares the models predictions against the actual observed targets y in the current batch afterwards tensorflow conducts an optimization step and updates the networks parameters corresponding to the selected learning scheme after having updated the weights and biases the next batch is sampled and the process repeats itself the procedure continues until all batches have been presented to the network one full sweep over all batches is called an epoch the training of the network stops once the maximum number of epochs is reached or another stopping criterion defined by the user applies during the training we evaluate the networks predictions on the test set the data which is not learned but set aside for every th batch and visualize it additionally the images are exported to disk and later combined into a video animation of the training process see below the model quickly learns the shape und location of the time series in the test data and is able to produce an accurate prediction after some epochs nice one can see that the networks rapidly adapts to the basic shape of the time series and continues to learn finer patterns of the data this also corresponds to the adam learning scheme that lowers the learning rate during model training in order not to overshoot the optimization minimum after epochs we have a pretty close fit to the test data the final test mse equals it is very low because the target is scaled the mean absolute percentage error of the forecast on the test set is equal to which is pretty good note that this is just a fit to the test data no actual out of sample metrics in a real world scenario please note that there are tons of ways of further improving this result design of layers and neurons choosing different initialization and activation schemes introduction of dropout layers of neurons early stopping and so on furthermore different types of deep learning models such as recurrent neural networks might achieve better performance on this task however this is not the scope of this introductory post the release of tensorflow was a landmark event in deep learning research its flexibility and performance allows researchers to develop all kinds of sophisticated neural network architectures as well as other ml algorithms however flexibility comes at the cost of longer time to model cycles compared to higher level apis such as keras or mxnet nonetheless i am sure that tensorflow will make its way to the de facto standard in neural network and deep learning development in research and practical applications many of our customers are already using tensorflow or start developing projects that employ tensorflow models also our data science consultants at statworx are heavily using tensorflow for deep learning and neural net research and development let s see what google has planned for the future of tensorflow one thing that is missing at least in my opinion is a neat graphical user interface for designing and developing neural net architectures with tensorflow backend maybe this is something google is already working on if you have any comments or questions on my first medium story feel free to comment below i will try to answer them also feel free to use my code or share this story with your peers on social platforms of your choice update i ve added both the python script as well as a zipped dataset to a github repository feel free to clone and fork lastly follow me on twitter linkedin from a quick cheer to a standing ovation clap to show how much you enjoyed this story ceo statworx doing data science stats and ml for over a decade food wine and cocktail enthusiast check our website https www statworx com highlights from machine learning research projects and learning materials from and for ml scientists engineers an enthusiasts
Netflix Technology Blog,25000,13,https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76?source=tag_archive---------9----------------,Artwork Personalization at Netflix – Netflix TechBlog – Medium,by ashok chandrashekar fernando amat justin basilico and tony jebara for many years the main goal of the netflix personalized recommendation system has been to get the right titles in front each of our members at the right time with a catalog spanning thousands of titles and a diverse member base spanning over a hundred million accounts recommending the titles that are just right for each member is crucial but the job of recommendation does not end there why should you care about any particular title we recommend what can we say about a new and unfamiliar title that will pique your interest how do we convince you that a title is worth watching answering these questions is critical in helping our members discover great content especially for unfamiliar titles one avenue to address this challenge is to consider the artwork or imagery we use to portray the titles if the artwork representing a title captures something compelling to you then it acts as a gateway into that title and gives you some visual evidence for why the title might be good for you the artwork may highlight an actor that you recognize capture an exciting moment like a car chase or contain a dramatic scene that conveys the essence of a movie or tv show if we present that perfect image on your homepage and as they say an image is worth a thousand words then maybe just maybe you will give it a try this is yet another way netflix differs from traditional media offerings we don t have one product but over a million different products with one for each of our members with personalized recommendations and personalized visuals in previous work we discussed an effort to find the single perfect artwork for each title across all our members through multi armed bandit algorithms we hunted for the best artwork for a title say stranger things that would earn the most plays from the largest fraction of our members however given the enormous diversity in taste and preferences wouldn t it be better if we could find the best artwork for each of our members to highlight the aspects of a title that are specifically relevant to them as inspiration let us explore scenarios where personalization of artwork would be meaningful consider the following examples where different members have different viewing histories on the left are three titles a member watched in the past to the right of the arrow is the artwork that a member would get for a particular movie that we recommend for them let us consider trying to personalize the image we use to depict the movie good will hunting here we might personalize this decision based on how much a member prefers different genres and themes someone who has watched many romantic movies may be interested in good will hunting if we show the artwork containing matt damon and minnie driver whereas a member who has watched many comedies might be drawn to the movie if we use the artwork containing robin williams a well known comedian in another scenario let s imagine how the different preferences for cast members might influence the personalization of the artwork for the movie pulp fiction a member who watches many movies featuring uma thurman would likely respond positively to the artwork for pulp fiction that contains uma meanwhile a fan of john travolta may be more interested in watching pulp fiction if the artwork features john of course not all the scenarios for personalizing artwork are this clear and obvious so we don t enumerate such hand derived rules but instead rely on the data to tell us what signals to use overall by personalizing artwork we help each title put its best foot forward for every member and thus improve our member experience at netflix we embrace personalization and algorithmically adapt many aspects of our member experience including the rows we select for the homepage the titles we select for those rows the galleries we display the messages we send and so forth each new aspect that we personalize has unique challenges personalizing the artwork we display is no exception and presents different personalization challenges one challenge of image personalization is that we can only select a single piece of artwork to represent each title in each place we present it in contrast typical recommendation settings let us present multiple selections to a member where we can subsequently learn about their preferences from the item a member selects this means that image selection is a chicken and egg problem operating in a closed loop if a member plays a title it can only come from the image that we decided to present to that member what we seek to understand is when presenting a specific piece of artwork for a title influenced a member to play or not to play a title and when a member would have played a title or not regardless of which image we presented therefore artwork personalization sits on top of the traditional recommendation problem and the algorithms need to work in conjunction with each other of course to properly learn how to personalize artwork we need to collect a lot of data to find signals that indicate when one piece of artwork is significantly better for a member another challenge is to understand the impact of changing artwork that we show a member for a title between sessions does changing artwork reduce recognizability of the title and make it difficult to visually locate the title again for example if the member thought was interested before but had not yet watched it or does changing the artwork itself lead the member to reconsider it due to an improved selection clearly if we find better artwork to present to a member we should probably use it but continuous changes can also confuse people changing images also introduces an attribution problem as it becomes unclear which image led a member to be interested in a title next there is the challenge of understanding how artwork performs in relation to other artwork we select in the same page or session maybe a bold close up of the main character works for a title on a page because it stands out compared to the other artwork but if every title had a similar image then the page as a whole may not seem as compelling looking at each piece of artwork in isolation may not be enough and we need to think about how to select a diverse set of images across titles on a page and across a session beyond the artwork for other titles the effectiveness of the artwork for a title may depend on what other types of evidence and assets e g synopses trailers etc we also display for that title thus we may need a diverse selection where each can highlight complementary aspects of a title that may be compelling to a member to achieve effective personalization we also need a good pool of artwork for each title this means that we need several assets where each is engaging informative and representative of a title to avoid clickbait the set of images for a title also needs to be diverse enough to cover a wide potential audience interested in different aspects of the content after all how engaging and informative a piece of artwork is truly depends on the individual seeing it therefore we need to have artwork that highlights not only different themes in a title but also different aesthetics our teams of artists and designers strive to create images that are diverse across many dimensions they also take into consideration the personalization algorithms which will select the images during their creative process for generating artwork finally there are engineering challenges to personalize artwork at scale one challenge is that our member experience is very visual and thus contains a lot of imagery so using personalized selection for each asset means handling a peak of over million requests per second with low latency such a system must be robust failing to properly render the artwork in our ui brings a significantly degrades the experience our personalization algorithm also needs to respond quickly when a title launches which means rapidly learning to personalize in a cold start situation then after launch the algorithm must continuously adapt as the effectiveness of artwork may change over time as both the title evolves through its life cycle and member tastes evolve much of the netflix recommendation engine is powered by machine learning algorithms traditionally we collect a batch of data on how our members use the service then we run a new machine learning algorithm on this batch of data next we test this new algorithm against the current production system through an a b test an a b test helps us see if the new algorithm is better than our current production system by trying it out on a random subset of members members in group a get the current production experience while members in group b get the new algorithm if members in group b have higher engagement with netflix then we roll out the new algorithm to the entire member population unfortunately this batch approach incurs regret many members over a long period of time did not benefit from the better experience this is illustrated in the figure below to reduce this regret we move away from batch machine learning and consider online machine learning for artwork personalization the specific online learning framework we use is contextual bandits rather than waiting to collect a full batch of data waiting to learn a model and then waiting for an a b test to conclude contextual bandits rapidly figure out the optimal personalized artwork selection for a title for each member and context briefly contextual bandits are a class of online learning algorithms that trade off the cost of gathering training data required for learning an unbiased model on an ongoing basis with the benefits of applying the learned model to each member context in our previous unpersonalized image selection work we used non contextual bandits where we found the winning image regardless of the context for personalization the member is the context as we expect different members to respond differently to the images a key property of contextual bandits is that they are designed to minimize regret at a high level the training data for a contextual bandit is obtained through the injection of controlled randomization in the learned model s predictions the randomization schemes can vary in complexity from simple epsilon greedy formulations with uniform randomness to closed loop schemes that adaptively vary the degree of randomization as a function of model uncertainty we broadly refer to this process as data exploration the number of candidate artworks that are available for a title along with the size of the overall population for which the system will be deployed informs the choice of the data exploration strategy with such exploration we need to log information about the randomization for each artwork selection this logging allows us to correct for skewed selection propensities and thereby perform offline model evaluation in an unbiased fashion as described later exploration in contextual bandits typically has a cost or regret due to the fact that our artwork selection in a member session may not use the predicted best image for that session what impact does this randomization have on the member experience and consequently on our metrics with over a hundred millions members the regret incurred by exploration is typically very small and is amortized across our large member base with each member implicitly helping provide feedback on artwork for a small portion of the catalog this makes the cost of exploration per member negligible which is an important consideration when choosing contextual bandits to drive a key aspect of our member experience randomization and exploration with contextual bandits would be less suitable if the cost of exploration were high under our online exploration scheme we obtain a training dataset that records for each member title image tuple whether that selection resulted in a play of the title or not furthermore we can control the exploration such that artwork selections do not change too often this gives a cleaner attribution of the member s engagement to specific artwork we also carefully determine the label for each observation by looking at the quality of engagement to avoid learning a model that recommends clickbait images ones that entice a member to start playing but ultimately result in low quality engagement in this online learning setting we train our contextual bandit model to select the best artwork for each member based on their context we typically have up to a few dozen candidate artwork images per title to learn the selection model we can consider a simplification of the problem by ranking images for a member independently across titles even with this simplification we can still learn member image preferences across titles because for every image candidate we have some members who were presented with it and engaged with the title and some members who were presented with it and did not engage these preferences can be modeled to predict for each member title image tuple the probability that the member will enjoy a quality engagement these can be supervised learning models or contextual bandit counterparts with thompson sampling linucb or bayesian methods that intelligently balance making the best prediction with data exploration in contextual bandits the context is usually represented as an feature vector provided as input to the model there are many signals we can use as features for this problem in particular we can consider many attributes of the member the titles they ve played the genre of the titles interactions of the member with the specific title their country their language preferences the device that the member is using the time of day and the day of week since our algorithm selects images in conjunction with our personalized recommendation engine we can also use signals regarding what our various recommendation algorithms think of the title irrespective of what image is used to represent it an important consideration is that some images are naturally better than others in the candidate pool we observe the overall take rates for all the images in our data exploration which is simply the number of quality plays divided by the number of impressions our previous work on unpersonalized artwork selection used overall differences in take rates to determine the single best image to select for a whole population in our new contextual personalized model the overall take rates are still important and personalization still recovers selections that agree on average with the unpersonalized model s ranking the optimal assignment of image artwork to a member is a selection problem to find the best candidate image from a title s pool of available images once the model is trained as above we use it to rank the images for each context the model predicts the probability of play for a given image in a given a member context we sort a candidate set of images by these probabilities and pick the one with the highest probability that is the image we present to that particular member to evaluate our contextual bandit algorithms prior to deploying them online on real members we can use an offline technique known as replay this method allows us to answer counterfactual questions based on the logged exploration data figure in other words we can compare offline what would have happened in historical sessions under different scenarios if we had used different algorithms in an unbiased way replay allows us to see how members would have engaged with our titles if we had hypothetically presented images that were selected through a new algorithm rather than the algorithm used in production for images we are interested in several metrics particularly the take fraction as described above figure shows how contextual bandit approach helps increase the average take fraction across the catalog compared to random selection or non contextual bandits after experimenting with many different models offline and finding ones that had a substantial increase in replay we ultimately ran an a b test to compare the most promising personalized contextual bandits against unpersonalized bandits as we suspected the personalization worked and generated a significant lift in our core metrics we also saw a reasonable correlation between what we measured offline in replay and what we saw online with the models the online results also produced some interesting insights for example the improvement of personalization was larger in cases where the member had no prior interaction with the title this makes sense because we would expect that the artwork would be more important to someone when a title is less familiar with this approach we ve taken our first steps in personalizing the selection of artwork for our recommendations and across our service this has resulted in a meaningful improvement in how our members discover new content so we ve rolled it out to everyone this project is the first instance of personalizing not just what we recommend but also how we recommend to our members but there are many opportunities to expand and improve this initial approach these opportunities include developing algorithms to handle cold start by personalizing new images and new titles as quickly as possible for example by using techniques from computer vision another opportunity is extending this personalization approach across other types of artwork we use and other evidence that describe our titles such as synopses metadata and trailers there is also an even broader problem helping artists and designers figure out what new imagery we should add to the set to make a title even more compelling and personalizable if these types of challenges interest you please let us know we are always looking for great people to join our team and for these types of projects we are especially excited by candidates with machine learning and or computer vision expertise l li w chu j langford and x wang unbiased offline evaluation of contextual bandit based news article recommendation algorithms in proceedings of the fourth acm international conference on web search and data mining new york ny usa pp from a quick cheer to a standing ovation clap to show how much you enjoyed this story learn more about how netflix designs builds and operates our systems and engineering organizations learn about netflix s world class engineering efforts company culture product developments and more
Michael Jordan,34000,16,https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7?source=tag_archive---------0----------------,Artificial Intelligence — The Revolution Hasn’t Happened Yet,artificial intelligence ai is the mantra of the current era the phrase is intoned by technologists academicians journalists and venture capitalists alike as with many phrases that cross over from technical academic fields into general circulation there is significant misunderstanding accompanying the use of the phrase but this is not the classical case of the public not understanding the scientists here the scientists are often as befuddled as the public the idea that our era is somehow seeing the emergence of an intelligence in silicon that rivals our own entertains all of us enthralling us and frightening us in equal measure and unfortunately it distracts us there is a different narrative that one can tell about the current era consider the following story which involves humans computers data and life or death decisions but where the focus is something other than intelligence in silicon fantasies when my spouse was pregnant years ago we had an ultrasound there was a geneticist in the room and she pointed out some white spots around the heart of the fetus those are markers for down syndrome she noted and your risk has now gone up to in she further let us know that we could learn whether the fetus in fact had the genetic modification underlying down syndrome via an amniocentesis but amniocentesis was risky the risk of killing the fetus during the procedure was roughly in being a statistician i determined to find out where these numbers were coming from to cut a long story short i discovered that a statistical analysis had been done a decade previously in the uk where these white spots which reflect calcium buildup were indeed established as a predictor of down syndrome but i also noticed that the imaging machine used in our test had a few hundred more pixels per square inch than the machine used in the uk study i went back to tell the geneticist that i believed that the white spots were likely false positives that they were literally white noise she said ah that explains why we started seeing an uptick in down syndrome diagnoses a few years ago it s when the new machine arrived we didn t do the amniocentesis and a healthy girl was born a few months later but the episode troubled me particularly after a back of the envelope calculation convinced me that many thousands of people had gotten that diagnosis that same day worldwide that many of them had opted for amniocentesis and that a number of babies had died needlessly and this happened day after day until it somehow got fixed the problem that this episode revealed wasn t about my individual medical care it was about a medical system that measured variables and outcomes in various places and times conducted statistical analyses and made use of the results in other places and times the problem had to do not just with data analysis per se but with what database researchers call provenance broadly where did data arise what inferences were drawn from the data and how relevant are those inferences to the present situation while a trained human might be able to work all of this out on a case by case basis the issue was that of designing a planetary scale medical system that could do this without the need for such detailed human oversight i m also a computer scientist and it occurred to me that the principles needed to build planetary scale inference and decision making systems of this kind blending computer science with statistics and taking into account human utilities were nowhere to be found in my education and it occurred to me that the development of such principles which will be needed not only in the medical domain but also in domains such as commerce transportation and education were at least as important as those of building ai systems that can dazzle us with their game playing or sensorimotor skills whether or not we come to understand intelligence any time soon we do have a major challenge on our hands in bringing together computers and humans in ways that enhance human life while this challenge is viewed by some as subservient to the creation of artificial intelligence it can also be viewed more prosaically but with no less reverence as the creation of a new branch of engineering much like civil engineering and chemical engineering in decades past this new discipline aims to corral the power of a few key ideas bringing new resources and capabilities to people and doing so safely whereas civil engineering and chemical engineering were built on physics and chemistry this new engineering discipline will be built on ideas that the preceding century gave substance to ideas such as information algorithm data uncertainty computing inference and optimization moreover since much of the focus of the new discipline will be on data from and about humans its development will require perspectives from the social sciences and humanities while the building blocks have begun to emerge the principles for putting these blocks together have not yet emerged and so the blocks are currently being put together in ad hoc ways thus just as humans built buildings and bridges before there was civil engineering humans are proceeding with the building of societal scale inference and decision making systems that involve machines humans and the environment just as early buildings and bridges sometimes fell to the ground in unforeseen ways and with tragic consequences many of our early societal scale inference and decision making systems are already exposing serious conceptual flaws and unfortunately we are not very good at anticipating what the next emerging serious flaw will be what we re missing is an engineering discipline with its principles of analysis and design the current public dialog about these issues too often uses ai as an intellectual wildcard one that makes it difficult to reason about the scope and consequences of emerging technology let us begin by considering more carefully what ai has been used to refer to both recently and historically most of what is being called ai today particularly in the public sphere is what has been called machine learning ml for the past several decades ml is an algorithmic field that blends ideas from statistics computer science and many other disciplines see below to design algorithms that process data make predictions and help make decisions in terms of impact on the real world ml is the real thing and not just recently indeed that ml would grow into massive industrial relevance was already clear in the early s and by the turn of the century forward looking companies such as amazon were already using ml throughout their business solving mission critical back end problems in fraud detection and supply chain prediction and building innovative consumer facing services such as recommendation systems as datasets and computing resources grew rapidly over the ensuing two decades it became clear that ml would soon power not only amazon but essentially any company in which decisions could be tied to large scale data new business models would emerge the phrase data science began to be used to refer to this phenomenon reflecting the need of ml algorithms experts to partner with database and distributed systems experts to build scalable robust ml systems and reflecting the larger social and environmental scope of the resulting systems this confluence of ideas and technology trends has been rebranded as ai over the past few years this rebranding is worthy of some scrutiny historically the phrase ai was coined in the late s to refer to the heady aspiration of realizing in software and hardware an entity possessing human level intelligence we will use the phrase human imitative ai to refer to this aspiration emphasizing the notion that the artificially intelligent entity should seem to be one of us if not physically at least mentally whatever that might mean this was largely an academic enterprise while related academic fields such as operations research statistics pattern recognition information theory and control theory already existed and were often inspired by human intelligence and animal intelligence these fields were arguably focused on low level signals and decisions the ability of say a squirrel to perceive the three dimensional structure of the forest it lives in and to leap among its branches was inspirational to these fields ai was meant to focus on something different the high level or cognitive capability of humans to reason and to think sixty years later however high level reasoning and thought remain elusive the developments which are now being called ai arose mostly in the engineering fields associated with low level pattern recognition and movement control and in the field of statistics the discipline focused on finding patterns in data and on making well founded predictions tests of hypotheses and decisions indeed the famous backpropagation algorithm that was rediscovered by david rumelhart in the early s and which is now viewed as being at the core of the so called ai revolution first arose in the field of control theory in the s and s one of its early applications was to optimize the thrusts of the apollo spaceships as they headed towards the moon since the s much progress has been made but it has arguably not come about from the pursuit of human imitative ai rather as in the case of the apollo spaceships these ideas have often been hidden behind the scenes and have been the handiwork of researchers focused on specific engineering challenges although not visible to the general public research and systems building in areas such as document retrieval text classification fraud detection recommendation systems personalized search social network analysis planning diagnostics and a b testing have been a major success these are the advances that have powered companies such as google netflix facebook and amazon one could simply agree to refer to all of this as ai and indeed that is what appears to have happened such labeling may come as a surprise to optimization or statistics researchers who wake up to find themselves suddenly referred to as ai researchers but labeling of researchers aside the bigger problem is that the use of this single ill defined acronym prevents a clear understanding of the range of intellectual and commercial issues at play the past two decades have seen major progress in industry and academia in a complementary aspiration to human imitative ai that is often referred to as intelligence augmentation ia here computation and data are used to create services that augment human intelligence and creativity a search engine can be viewed as an example of ia it augments human memory and factual knowledge as can natural language translation it augments the ability of a human to communicate computing based generation of sounds and images serves as a palette and creativity enhancer for artists while services of this kind could conceivably involve high level reasoning and thought currently they don t they mostly perform various kinds of string matching and numerical operations that capture patterns that humans can make use of hoping that the reader will tolerate one last acronym let us conceive broadly of a discipline of intelligent infrastructure ii whereby a web of computation data and physical entities exists that makes human environments more supportive interesting and safe such infrastructure is beginning to make its appearance in domains such as transportation medicine commerce and finance with vast implications for individual humans and societies this emergence sometimes arises in conversations about an internet of things but that effort generally refers to the mere problem of getting things onto the internet not to the far grander set of challenges associated with these things capable of analyzing those data streams to discover facts about the world and interacting with humans and other things at a far higher level of abstraction than mere bits for example returning to my personal anecdote we might imagine living our lives in a societal scale medical system that sets up data flows and data analysis flows between doctors and devices positioned in and around human bodies thereby able to aid human intelligence in making diagnoses and providing care the system would incorporate information from cells in the body dna blood tests environment population genetics and the vast scientific literature on drugs and treatments it would not just focus on a single patient and a doctor but on relationships among all humans just as current medical testing allows experiments done on one set of humans or animals to be brought to bear in the care of other humans it would help maintain notions of relevance provenance and reliability in the way that the current banking system focuses on such challenges in the domain of finance and payment and while one can foresee many problems arising in such a system involving privacy issues liability issues security issues etc these problems should properly be viewed as challenges not show stoppers we now come to a critical issue is working on classical human imitative ai the best or only way to focus on these larger challenges some of the most heralded recent success stories of ml have in fact been in areas associated with human imitative ai areas such as computer vision speech recognition game playing and robotics so perhaps we should simply await further progress in domains such as these there are two points to make here first although one would not know it from reading the newspapers success in human imitative ai has in fact been limited we are very far from realizing human imitative ai aspirations unfortunately the thrill and fear of making even limited progress on human imitative ai gives rise to levels of over exuberance and media attention that is not present in other areas of engineering second and more importantly success in these domains is neither sufficient nor necessary to solve important ia and ii problems on the sufficiency side consider self driving cars for such technology to be realized a range of engineering problems will need to be solved that may have little relationship to human competencies or human lack of competencies the overall transportation system an ii system will likely more closely resemble the current air traffic control system than the current collection of loosely coupled forward facing inattentive human drivers it will be vastly more complex than the current air traffic control system specifically in its use of massive amounts of data and adaptive statistical modeling to inform fine grained decisions it is those challenges that need to be in the forefront and in such an effort a focus on human imitative ai may be a distraction as for the necessity argument it is sometimes argued that the human imitative ai aspiration subsumes ia and ii aspirations because a human imitative ai system would not only be able to solve the classical problems of ai as embodied e g in the turing test but it would also be our best bet for solving ia and ii problems such an argument has little historical precedent did civil engineering develop by envisaging the creation of an artificial carpenter or bricklayer should chemical engineering have been framed in terms of creating an artificial chemist even more polemically if our goal was to build chemical factories should we have first created an artificial chemist who would have then worked out how to build a chemical factory a related argument is that human intelligence is the only kind of intelligence that we know and that we should aim to mimic it as a first step but humans are in fact not very good at some kinds of reasoning we have our lapses biases and limitations moreover critically we did not evolve to perform the kinds of large scale decision making that modern ii systems must face nor to cope with the kinds of uncertainty that arise in ii contexts one could argue that an ai system would not only imitate human intelligence but also correct it and would also scale to arbitrarily large problems but we are now in the realm of science fiction such speculative arguments while entertaining in the setting of fiction should not be our principal strategy going forward in the face of the critical ia and ii problems that are beginning to emerge we need to solve ia and ii problems on their own merits not as a mere corollary to a human imitative ai agenda it is not hard to pinpoint algorithmic and infrastructure challenges in ii systems that are not central themes in human imitative ai research ii systems require the ability to manage distributed repositories of knowledge that are rapidly changing and are likely to be globally incoherent such systems must cope with cloud edge interactions in making timely distributed decisions and they must deal with long tail phenomena whereby there is lots of data on some individuals and little data on most individuals they must address the difficulties of sharing data across administrative and competitive boundaries finally and of particular importance ii systems must bring economic ideas such as incentives and pricing into the realm of the statistical and computational infrastructures that link humans to each other and to valued goods such ii systems can be viewed as not merely providing a service but as creating markets there are domains such as music literature and journalism that are crying out for the emergence of such markets where data analysis links producers and consumers and this must all be done within the context of evolving societal ethical and legal norms of course classical human imitative ai problems remain of great interest as well however the current focus on doing ai research via the gathering of data the deployment of deep learning infrastructure and the demonstration of systems that mimic certain narrowly defined human skills with little in the way of emerging explanatory principles tends to deflect attention from major open problems in classical ai these problems include the need to bring meaning and reasoning into systems that perform natural language processing the need to infer and represent causality the need to develop computationally tractable representations of uncertainty and the need to develop systems that formulate and pursue long term goals these are classical goals in human imitative ai but in the current hubbub over the ai revolution it is easy to forget that they are not yet solved ia will also remain quite essential because for the foreseeable future computers will not be able to match humans in their ability to reason abstractly about real world situations we will need well thought out interactions of humans and computers to solve our most pressing problems and we will want computers to trigger new levels of human creativity not replace human creativity whatever that might mean it was john mccarthy while a professor at dartmouth and soon to take a position at mit who coined the term ai apparently to distinguish his budding research agenda from that of norbert wiener then an older professor at mit wiener had coined cybernetics to refer to his own vision of intelligent systems a vision that was closely tied to operations research statistics pattern recognition information theory and control theory mccarthy on the other hand emphasized the ties to logic in an interesting reversal it is wiener s intellectual agenda that has come to dominate in the current era under the banner of mccarthy s terminology this state of affairs is surely however only temporary the pendulum swings more in ai than in most fields but we need to move beyond the particular historical perspectives of mccarthy and wiener we need to realize that the current public dialog on ai which focuses on a narrow subset of industry and a narrow subset of academia risks blinding us to the challenges and opportunities that are presented by the full scope of ai ia and ii this scope is less about the realization of science fiction dreams or nightmares of super human machines and more about the need for humans to understand and shape technology as it becomes ever more present and influential in their daily lives moreover in this understanding and shaping there is a need for a diverse set of voices from all walks of life not merely a dialog among the technologically attuned focusing narrowly on human imitative ai prevents an appropriately wide range of voices from being heard while industry will continue to drive many developments academia will also continue to play an essential role not only in providing some of the most innovative technical ideas but also in bringing researchers from the computational and statistical disciplines together with researchers from other disciplines whose contributions and perspectives are sorely needed notably the social sciences the cognitive sciences and the humanities on the other hand while the humanities and the sciences are essential as we go forward we should also not pretend that we are talking about something other than an engineering effort of unprecedented scale and scope society is aiming to build new kinds of artifacts these artifacts should be built to work as claimed we do not want to build systems that help us with medical treatments transportation options and commercial opportunities to find out after the fact that these systems don t really work that they make errors that take their toll in terms of human lives and happiness in this regard as i have emphasized there is an engineering discipline yet to emerge for the data focused and learning focused fields as exciting as these latter fields appear to be they cannot yet be viewed as constituting an engineering discipline moreover we should embrace the fact that what we are witnessing is the creation of a new branch of engineering the term engineering is often invoked in a narrow sense in academia and beyond with overtones of cold affectless machinery and negative connotations of loss of control by humans but an engineering discipline can be what we want it to be in the current era we have a real opportunity to conceive of something historically new a human centric engineering discipline i will resist giving this emerging discipline a name but if the acronym ai continues to be used as placeholder nomenclature going forward let s be aware of the very real limitations of this placeholder let s broaden our scope tone down the hype and recognize the serious challenges ahead michael i jordan from a quick cheer to a standing ovation clap to show how much you enjoyed this story michael i jordan is a professor in the department of electrical engineering and computer sciences and the department of statistics at uc berkeley
Blaise Aguera y Arcas,8700,15,https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477?source=tag_archive---------1----------------,Do algorithms reveal sexual orientation or just expose our stereotypes?,by blaise agu era y arcas alexander todorov and margaret mitchell a study claiming that artificial intelligence can infer sexual orientation from facial images caused a media uproar in the fall of the economist featured this work on the cover of their september th magazine on the other hand two major lgbtq organizations the human rights campaign and glaad immediately labeled it junk science michal kosinski who co authored the study with fellow researcher yilun wang initially expressed surprise calling the critiques knee jerk reactions however he then proceeded to make even bolder claims that such ai algorithms will soon be able to measure the intelligence political orientation and criminal inclinations of people from their facial images alone kosinski s controversial claims are nothing new last year two computer scientists from china posted a non peer reviewed paper online in which they argued that their ai algorithm correctly categorizes criminals with nearly accuracy from a government id photo alone technology startups had also begun to crop up claiming that they can profile people s character from their facial images these developments had prompted the three of us to collaborate earlier in the year on a medium essay physiognomy s new clothes to confront claims that ai face recognition reveals deep character traits we described how the junk science of physiognomy has roots going back into antiquity with practitioners in every era resurrecting beliefs based on prejudice using the new methodology of the age in the th century this included anthropology and psychology in the th genetics and statistical analysis and in the st artificial intelligence in late the paper motivating our physiognomy essay seemed well outside the mainstream in tech and academia but as in other areas of discourse what recently felt like a fringe position must now be addressed head on kosinski is a faculty member of stanford s graduate school of business and this new study has been accepted for publication in the respected journal of personality and social psychology much of the ensuing scrutiny has focused on ethics implicitly assuming that the science is valid we will focus on the science the authors trained and tested their sexual orientation detector using images from public profiles on a us dating website composite images of the lesbian gay and straight men and women in the sample reveal a great deal about the information available to the algorithm clearly there are differences between these four composite faces wang and kosinski assert that the key differences are in physiognomy meaning that a sexual orientation tends to go along with a characteristic facial structure however we can immediately see that some of these differences are more superficial for example the average straight woman appears to wear eyeshadow while the average lesbian does not glasses are clearly visible on the gay man and to a lesser extent on the lesbian while they seem absent in the heterosexual composites might it be the case that the algorithm s ability to detect orientation has little to do with facial structure but is due rather to patterns in grooming presentation and lifestyle we conducted a survey of americans using amazon s mechanical turk crowdsourcing platform to see if we could independently confirm these patterns asking yes no questions such as do you wear eyeshadow do you wear glasses and do you have a beard as well as questions about gender and sexual orientation the results show that lesbians indeed use eyeshadow much less than straight women do gay men and women do both wear glasses more and young opposite sex attracted men are considerably more likely to have prominent facial hair than their gay or same sex attracted peers breaking down the answers by the age of the respondent can provide a richer and clearer view of the data than any single statistic in the following figures we show the proportion of women who answer yes to do you ever use makeup top and do you wear eyeshadow bottom averaged over year age intervals the blue curves represent strictly opposite sex attracted women a nearly identical set to those who answered yes to are you heterosexual or straight the cyan curve represents women who answer yes to either or both of are you sexually attracted to women and are you romantically attracted to women and the red curve represents women who answer yes to are you homosexual gay or lesbian the shaded regions around each curve show confidence intervals the patterns revealed here are intuitive it won t be breaking news to most that straight women tend to wear more makeup and eyeshadow than same sex attracted and even more so lesbian identifying women on the other hand these curves also show us how often these stereotypes are violated that same sex attracted men of most ages wear glasses significantly more than exclusively opposite sex attracted men do might be a bit less obvious but this trend is equally clear a proponent of physiognomy might be tempted to guess that this is somehow related to differences in visual acuity between these populations of men however asking the question do you like how you look in glasses reveals that this is likely more of a stylistic choice same sex attracted women also report wearing glasses more as well as liking how they look in glasses more across a range of ages one can also see how opposite sex attracted women under the age of wear contact lenses significantly more than same sex attracted women despite reporting that they have a vision defect at roughly the same rate further illustrating how the difference is driven by an aesthetic preference similar analysis shows that young same sex attracted men are much less likely to have hairy faces than opposite sex attracted men serious facial hair in our plots is defined as answering yes to having a goatee beard or moustache but no to stubble overall opposite sex attracted men in our sample are more likely to have serious facial hair than same sex attracted men and for men under the age of who are overrepresented on dating websites this rises to wang and kosinski speculate in their paper that the faintness of the beard and moustache in their gay male composite might be connected with prenatal underexposure to androgens male hormones resulting in a feminizing effect hence sparser facial hair the fact that we see a cohort of same sex attracted men in their s who have just as much facial hair as opposite sex attracted men suggests a different story in which fashion trends and cultural norms play the dominant role in choices about facial hair among men not differing exposure to hormones early in development the authors of the paper additionally note that the heterosexual male composite appears to have darker skin than the other three composites our survey confirms that opposite sex attracted men consistently self report having a tan face yes to is your face tan slightly more often than same sex attracted men once again wang and kosinski reach for a hormonal explanation writing while the brightness of the facial image might be driven by many factors previous research found that testosterone stimulates melanocyte structure and function leading to a darker skin however a simpler answer is suggested by the responses to the question do you work outdoors overall opposite sex attracted men are more likely to work outdoors and among men under this rises to previous research has found that increased exposure to sunlight leads to darker skin none of these results prove that there is no physiological basis for sexual orientation in fact ample evidence shows us that orientation runs much deeper than a choice or a lifestyle in a critique aimed in part at fraudulent conversion therapy programs united states surgeon general david satcher wrote in a report sexual orientation is usually determined by adolescence if not earlier and there is no valid scientific evidence that sexual orientation can be changed it follows that if we dig deeply enough into human physiology and neuroscience we will eventually find reliable correlates and maybe even the origins of sexual orientation in our survey we also find some evidence of outwardly visible correlates of orientation that are not cultural perhaps most strikingly very tall women are overrepresented among lesbian identifying respondents however while this is interesting it s very far from a good predictor of women s sexual orientation makeup and eyeshadow do much better the way wang and kosinski measure the efficacy of their ai gaydar is equivalent to choosing a straight and a gay or lesbian face image both from data held out during the training process and asking how often the algorithm correctly guesses which is which performance would be no better than random chance for women guessing that the taller of the two is the lesbian achieves only accuracy barely above random chance this is because despite the statistically meaningful overrepresentation of tall women among the lesbian population the great majority of lesbians are not unusually tall by contrast the performance measures in the paper for gay men and for lesbian women seem impressive consider however that we can achieve comparable results with trivial models based only on a handful of yes no survey questions about presentation for example for pairs of women one of whom is lesbian the following not exactly superhuman algorithm is on average accurate if neither or both women wear eyeshadow flip a coin otherwise guess that the one who wears eyeshadow is straight and the other lesbian adding six more yes no questions about presentation do you ever use makeup do you have long hair do you have short hair do you ever use colored lipstick do you like how you look in glasses and do you work outdoors as additional signals raises the performance to given how many more details about presentation are available in a face image performance no longer seems so impressive several studies including a recent one in the journal of sex research have shown that human judges gaydar is no more reliable than a coin flip when the judgement is based on pictures taken under well controlled conditions head pose lighting glasses makeup etc it s better than chance if these variables are not controlled for because a person s presentation especially if that person is out involves social signaling we signal our orientation and many other kinds of status presumably in order to attract the kind of attention we want and to fit in with people like us wang and kosinski argue against this interpretation on the grounds that their algorithm works on facebook selfies of openly gay men as well as dating website selfies the issue however is not whether the images come from a dating website or facebook but whether they are self posted or taken under standardized conditions most people present themselves in ways that have been calibrated over many years of media consumption observing others looking in the mirror and gauging social reactions in one of the earliest gaydar studies using social media participants could categorize gay men with about accuracy but when the researchers used facebook images of gay and heterosexual men posted by their friends still far from a perfect control the accuracy dropped to if subtle biases in image quality expression and grooming can be picked up on by humans these biases can also be detected by an ai algorithm while wang and kosinski acknowledge grooming and style they believe that the chief differences between their composite images relate to face shape arguing that gay men s faces are more feminine narrower jaws longer noses larger foreheads while lesbian faces are more masculine larger jaws shorter noses smaller foreheads as with less facial hair on gay men and darker skin on straight men they suggest that the mechanism is gender atypical hormonal exposure during development this echoes a widely discredited th century model of homosexuality sexual inversion more likely heterosexual men tend to take selfies from slightly below which will have the apparent effect of enlarging the chin shortening the nose shrinking the forehead and attenuating the smile see our selfies below this view emphasizes dominance or perhaps more benignly an expectation that the viewer will be shorter on the other hand as a wedding photographer notes in her blog when you shoot from above your eyes look bigger which is generally attractive especially for women this may be a heteronormative assessment when a face is photographed from below the nostrils are prominent while higher shooting angles de emphasize and eventually conceal them altogether looking again at the composite images we can see that the heterosexual male face has more pronounced dark spots corresponding to the nostrils than the gay male while the opposite is true for the female faces this is consistent with a pattern of heterosexual men on average shooting from below heterosexual women from above as the wedding photographer suggests and gay men and lesbian women from directly in front a similar pattern is evident in the eyebrows shooting from above makes them look more v shaped but their apparent shape becomes flatter and eventually caret shaped as the camera is lowered shooting from below also makes the outer corners of the eyes appear lower in short the changes in the average positions of facial landmarks are consistent with what we would expect to see from differing selfie angles the ambiguity between shooting angle and the real physical sizes of facial features is hard to fully disentangle from a two dimensional image both for a human viewer and for an algorithm although the authors are using face recognition technology designed to try to cancel out all effects of head pose lighting grooming and other variables not intrinsic to the face we can confirm that this doesn t work perfectly that s why multiple distinct images of a person help when grouping photos by subject in google photos and why a person may initially appear in more than one group tom white a researcher at victoria university in new zealand has experimented with the same facial recognition engine kosinski and wang use vgg face and has found that its output varies systematically based on variables like smiling and head pose when he trains a classifier based on vgg face s output to distinguish a happy expression from a neutral one it gets the answer right of the time which is significant given that the heterosexual female composite has a much more pronounced smile changes in head pose might be even more reliably detectable for test images a classifier is able to pick out the ones facing to the right with accuracy in summary we have shown how the obvious differences between lesbian or gay and straight faces in selfies relate to grooming presentation and lifestyle that is differences in culture not in facial structure these differences include we ve demonstrated that just a handful of yes no questions about these variables can do nearly as good a job at guessing orientation as supposedly sophisticated facial recognition ai further the current generation of facial recognition remains sensitive to head pose and facial expression therefore at least at this point it s hard to credit the notion that this ai is in some way superhuman at outing us based on subtle but unalterable details of our facial structure this doesn t negate the privacy concerns the authors and various commentators have raised but it emphasizes that such concerns relate less to ai per se than to mass surveillance which is troubling regardless of the technologies used even when as in the days of the stasi in east germany these were nothing but paper files and audiotapes like computers or the internal combustion engine ai is a general purpose technology that can be used to automate a great many tasks including ones that should not be undertaken in the first place we are hopeful about the confluence of new powerful ai technologies with social science but not because we believe in reviving the th century research program of inferring people s inner character from their outer appearance rather we believe ai is an essential tool for understanding patterns in human culture and behavior it can expose stereotypes inherent in everyday language it can reveal uncomfortable truths as in google s work with the geena davis institute where our face gender classifier established that men are seen and heard nearly twice as often as women in hollywood movies yet female led films outperform others at the box office making social progress and holding ourselves to account is more difficult without such hard evidence even when it only confirms our suspicions two of us margaret mitchell and blaise agu era y arcas are research scientists specializing in machine learning and ai at google agu era y arcas leads a team that includes deep learning applied to face recognition and powers face grouping in google photos alex todorov is a professor in the psychology department at princeton where he directs the social perception lab he is the author of face value the irresistible influence of first impressions this wording is based on several large national surveys which we were able to use to sanity check our numbers about of respondents identified as homosexual gay or lesbian and as heterosexual about of all genders were exclusively same sex attracted of the men were either sexually or romantically same sex attracted and of the women just under of respondents were trans and about identified with both or neither of the pronouns she and he these numbers are broadly consistent with other surveys especially when considered as a function of age the mechanical turk population skews somewhat younger than the overall population of the us and consistent with other studies our data show that younger people are far more likely to identify non heteronormatively these are wider for same sex attracted and lesbian women because they are minority populations resulting in a larger sampling error the same holds for older people in our sample for the remainder of the plots we stick to opposite sex attracted and same sex attracted as the counts are higher and the error bars therefore smaller these categories are also somewhat less culturally freighted since they rely on questions about attraction rather than identity as with eyeshadow and makeup the effects are similar and often even larger when comparing heterosexual identifying with lesbian or gay identifying people although we didn t test this explicitly slightly different rates of laser correction surgery seem a likely cause of the small but growing disparity between opposite sex attracted and same sex attracted women who answer yes to the vision defect questions as they age this finding may prompt the further question why do more opposite sex attracted men work outdoors this is not addressed by any of our survey questions but hopefully the other evidence presented here will discourage an essentialist assumption such as straight men are just more outdoorsy without the evidence of a controlled study that can support the leap from correlation to cause such explanations are a form of logical fallacy sometimes called a just so story an unverifiable narrative explanation for a cultural practice of the lesbian identified women in the sample or were over six feet and or were over out of heterosexual women women who answered yes to are you heterosexual or straight only or were over six feet and or were over they note that these figures rise to for men and for women if images are considered these results are based on the simplest possible machine learning technique a linear classifier the classifier is trained on a randomly chosen of the data with the remaining of the data held out for testing over repetitions of this procedure the error is with the same number of repetitions and holdout basing the decision on height alone gives an error of and basing it on eyeshadow alone yields a longstanding body of work e g goffman s the presentation of self in everyday life and jones and pittman s toward a general theory of strategic self presentation delves more deeply into why we present ourselves the way we do both for instrumental reasons status power attraction and because our presentation informs and is informed by how we conceive of our social selves from a quick cheer to a standing ovation clap to show how much you enjoyed this story blaise aguera y arcas leads google s ai group in seattle he founded seadragon and was one of the creators of photosynth at microsoft
James Le,18400,11,https://towardsdatascience.com/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies-dde4edffae11?source=tag_archive---------2----------------,A Tour of The Top 10 Algorithms for Machine Learning Newbies,in machine learning there s something called the no free lunch theorem in a nutshell it states that no one algorithm works best for every problem and it s especially relevant for supervised learning i e predictive modeling for example you can t say that neural networks are always better than decision trees or vice versa there are many factors at play such as the size and structure of your dataset as a result you should try many different algorithms for your problem while using a hold out test set of data to evaluate performance and select the winner of course the algorithms you try must be appropriate for your problem which is where picking the right machine learning task comes in as an analogy if you need to clean your house you might use a vacuum a broom or a mop but you wouldn t bust out a shovel and start digging however there is a common principle that underlies all supervised machine learning algorithms for predictive modeling this is a general learning task where we would like to make predictions in the future y given new examples of input variables x we don t know what the function f looks like or its form if we did we would use it directly and we would not need to learn it from data using machine learning algorithms the most common type of machine learning is to learn the mapping y f x to make predictions of y for new x this is called predictive modeling or predictive analytics and our goal is to make the most accurate predictions possible for machine learning newbies who are eager to understand the basic of machine learning here is a quick tour on the top machine learning algorithms used by data scientists linear regression is perhaps one of the most well known and well understood algorithms in statistics and machine learning predictive modeling is primarily concerned with minimizing the error of a model or making the most accurate predictions possible at the expense of explainability we will borrow reuse and steal algorithms from many different fields including statistics and use them towards these ends the representation of linear regression is an equation that describes a line that best fits the relationship between the input variables x and the output variables y by finding specific weightings for the input variables called coefficients b for example y b b x we will predict y given the input x and the goal of the linear regression learning algorithm is to find the values for the coefficients b and b different techniques can be used to learn the linear regression model from data such as a linear algebra solution for ordinary least squares and gradient descent optimization linear regression has been around for more than years and has been extensively studied some good rules of thumb when using this technique are to remove variables that are very similar correlated and to remove noise from your data if possible it is a fast and simple technique and good first algorithm to try logistic regression is another technique borrowed by machine learning from the field of statistics it is the go to method for binary classification problems problems with two class values logistic regression is like linear regression in that the goal is to find the values for the coefficients that weight each input variable unlike linear regression the prediction for the output is transformed using a non linear function called the logistic function the logistic function looks like a big s and will transform any value into the range to this is useful because we can apply a rule to the output of the logistic function to snap values to and e g if less than then output and predict a class value because of the way that the model is learned the predictions made by logistic regression can also be used as the probability of a given data instance belonging to class or class this can be useful for problems where you need to give more rationale for a prediction like linear regression logistic regression does work better when you remove attributes that are unrelated to the output variable as well as attributes that are very similar correlated to each other it s a fast model to learn and effective on binary classification problems logistic regression is a classification algorithm traditionally limited to only two class classification problems if you have more than two classes then the linear discriminant analysis algorithm is the preferred linear classification technique the representation of lda is pretty straight forward it consists of statistical properties of your data calculated for each class for a single input variable this includes predictions are made by calculating a discriminate value for each class and making a prediction for the class with the largest value the technique assumes that the data has a gaussian distribution bell curve so it is a good idea to remove outliers from your data before hand it s a simple and powerful method for classification predictive modeling problems decision trees are an important type of algorithm for predictive modeling machinelearning the representation of the decision tree model is a binary tree this is your binary tree from algorithms and data structures nothing too fancy each node represents a single input variable x and a split point on that variable assuming the variable is numeric the leaf nodes of the tree contain an output variable y which is used to make a prediction predictions are made by walking the splits of the tree until arriving at a leaf node and output the class value at that leaf node trees are fast to learn and very fast for making predictions they are also often accurate for a broad range of problems and do not require any special preparation for your data naive bayes is a simple but surprisingly powerful algorithm for predictive modeling the model is comprised of two types of probabilities that can be calculated directly from your training data the probability of each class and the conditional probability for each class given each x value once calculated the probability model can be used to make predictions for new data using bayes theorem when your data is real valued it is common to assume a gaussian distribution bell curve so that you can easily estimate these probabilities naive bayes is called naive because it assumes that each input variable is independent this is a strong assumption and unrealistic for real data nevertheless the technique is very effective on a large range of complex problems the knn algorithm is very simple and very effective the model representation for knn is the entire training dataset simple right predictions are made for a new data point by searching through the entire training set for the k most similar instances the neighbors and summarizing the output variable for those k instances for regression problems this might be the mean output variable for classification problems this might be the mode or most common class value the trick is in how to determine the similarity between the data instances the simplest technique if your attributes are all of the same scale all in inches for example is to use the euclidean distance a number you can calculate directly based on the differences between each input variable knn can require a lot of memory or space to store all of the data but only performs a calculation or learn when a prediction is needed just in time you can also update and curate your training instances over time to keep predictions accurate the idea of distance or closeness can break down in very high dimensions lots of input variables which can negatively affect the performance of the algorithm on your problem this is called the curse of dimensionality it suggests you only use those input variables that are most relevant to predicting the output variable a downside of k nearest neighbors is that you need to hang on to your entire training dataset the learning vector quantization algorithm or lvq for short is an artificial neural network algorithm that allows you to choose how many training instances to hang onto and learns exactly what those instances should look like the representation for lvq is a collection of codebook vectors these are selected randomly in the beginning and adapted to best summarize the training dataset over a number of iterations of the learning algorithm after learned the codebook vectors can be used to make predictions just like k nearest neighbors the most similar neighbor best matching codebook vector is found by calculating the distance between each codebook vector and the new data instance the class value or real value in the case of regression for the best matching unit is then returned as the prediction best results are achieved if you rescale your data to have the same range such as between and if you discover that knn gives good results on your dataset try using lvq to reduce the memory requirements of storing the entire training dataset support vector machines are perhaps one of the most popular and talked about machine learning algorithms a hyperplane is a line that splits the input variable space in svm a hyperplane is selected to best separate the points in the input variable space by their class either class or class in two dimensions you can visualize this as a line and let s assume that all of our input points can be completely separated by this line the svm learning algorithm finds the coefficients that results in the best separation of the classes by the hyperplane the distance between the hyperplane and the closest data points is referred to as the margin the best or optimal hyperplane that can separate the two classes is the line that has the largest margin only these points are relevant in defining the hyperplane and in the construction of the classifier these points are called the support vectors they support or define the hyperplane in practice an optimization algorithm is used to find the values for the coefficients that maximizes the margin svm might be one of the most powerful out of the box classifiers and worth trying on your dataset random forest is one of the most popular and most powerful machine learning algorithms it is a type of ensemble machine learning algorithm called bootstrap aggregation or bagging the bootstrap is a powerful statistical method for estimating a quantity from a data sample such as a mean you take lots of samples of your data calculate the mean then average all of your mean values to give you a better estimation of the true mean value in bagging the same approach is used but instead for estimating entire statistical models most commonly decision trees multiple samples of your training data are taken then models are constructed for each data sample when you need to make a prediction for new data each model makes a prediction and the predictions are averaged to give a better estimate of the true output value random forest is a tweak on this approach where decision trees are created so that rather than selecting optimal split points suboptimal splits are made by introducing randomness the models created for each sample of the data are therefore more different than they otherwise would be but still accurate in their unique and different ways combining their predictions results in a better estimate of the true underlying output value if you get good results with an algorithm with high variance like decision trees you can often get better results by bagging that algorithm boosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers this is done by building a model from the training data then creating a second model that attempts to correct the errors from the first model models are added until the training set is predicted perfectly or a maximum number of models are added adaboost was the first really successful boosting algorithm developed for binary classification it is the best starting point for understanding boosting modern boosting methods build on adaboost most notably stochastic gradient boosting machines adaboost is used with short decision trees after the first tree is created the performance of the tree on each training instance is used to weight how much attention the next tree that is created should pay attention to each training instance training data that is hard to predict is given more weight whereas easy to predict instances are given less weight models are created sequentially one after the other each updating the weights on the training instances that affect the learning performed by the next tree in the sequence after all the trees are built predictions are made for new data and the performance of each tree is weighted by how accurate it was on training data because so much attention is put on correcting mistakes by the algorithm it is important that you have clean data with outliers removed a typical question asked by a beginner when facing a wide variety of machine learning algorithms is which algorithm should i use the answer to the question varies depending on many factors including the size quality and nature of data the available computational time the urgency of the task and what you want to do with the data even an experienced data scientist cannot tell which algorithm will perform the best before trying different algorithms although there are many other machine learning algorithms these are the most popular ones if you re a newbie to machine learning these would be a good starting point to learn if you enjoyed this piece i d love it if you hit the clap button so others might stumble upon it you can find my own code on github and more of my writing and projects at https jameskle com you can also follow me on twitter email me directly or find me on linkedin from a quick cheer to a standing ovation clap to show how much you enjoyed this story blue ocean thinker https jameskle com sharing concepts ideas and codes
Emmanuel Ameisen,12800,13,https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e?source=tag_archive---------3----------------,How to solve 90% of NLP problems: a step-by-step guide,for more content like this follow insight and emmanuel on twitter whether you are an established company or working to launch a new service you can always leverage text data to validate improve and expand the functionalities of your product the science of extracting meaning and learning from text data is an active topic of research called natural language processing nlp nlp produces new and exciting results on a daily basis and is a very large field however having worked with hundreds of companies the insight team has seen a few key practical applications come up much more frequently than any other while many nlp papers and tutorials exist online we have found it hard to find guidelines and tips on how to approach these problems efficiently from the ground up after leading hundreds of projects a year and gaining advice from top teams all over the united states we wrote this post to explain how to build machine learning solutions to solve problems like the ones mentioned above we ll begin with the simplest method that could work and then move on to more nuanced solutions such as feature engineering word vectors and deep learning after reading this article you ll know how to we wrote this post as a step by step guide it can also serve as a high level overview of highly effective standard approaches this post is accompanied by an interactive notebook demonstrating and applying all these techniques feel free to run the code and follow along every machine learning problem starts with data such as a list of emails posts or tweets common sources of textual information include disasters on social media dataset for this post we will use a dataset generously provided by crowdflower called disasters on social media where our task will be to detect which tweets are about a disastrous event as opposed to an irrelevant topic such as a movie why a potential application would be to exclusively notify law enforcement officials about urgent emergencies while ignoring reviews of the most recent adam sandler film a particular challenge with this task is that both classes contain the same search terms used to find the tweets so we will have to use subtler differences to distinguish between them in the rest of this post we will refer to tweets that are about disasters as disaster and tweets about anything else as irrelevant we have labeled data and so we know which tweets belong to which categories as richard socher outlines below it is usually faster simpler and cheaper to find and label enough data to train a model on rather than trying to optimize a complex unsupervised method one of the key skills of a data scientist is knowing whether the next step should be working on the model or the data a good rule of thumb is to look at the data first and then clean it up a clean dataset will allow a model to learn meaningful features and not overfit on irrelevant noise here is a checklist to use to clean your data see the code for more details after following these steps and checking for additional errors we can start using the clean labelled data to train models machine learning models take numerical values as input models working on images for example take in a matrix representing the intensity of each pixel in each color channel our dataset is a list of sentences so in order for our algorithm to extract patterns from the data we first need to find a way to represent it in a way that our algorithm can understand i e as a list of numbers a natural way to represent text for computers is to encode each character individually as a number ascii for example if we were to feed this simple representation into a classifier it would have to learn the structure of words from scratch based only on our data which is impossible for most datasets we need to use a higher level approach for example we can build a vocabulary of all the unique words in our dataset and associate a unique index to each word in the vocabulary each sentence is then represented as a list that is as long as the number of distinct words in our vocabulary at each index in this list we mark how many times the given word appears in our sentence this is called a bag of words model since it is a representation that completely ignores the order of words in our sentence this is illustrated below we have around words in our vocabulary in the disasters of social media example which means that every sentence will be represented as a vector of length the vector will contain mostly s because each sentence contains only a very small subset of our vocabulary in order to see whether our embeddings are capturing information that is relevant to our problem i e whether the tweets are about disasters or not it is a good idea to visualize them and see if the classes look well separated since vocabularies are usually very large and visualizing data in dimensions is impossible techniques like pca will help project the data down to two dimensions this is plotted below the two classes do not look very well separated which could be a feature of our embeddings or simply of our dimensionality reduction in order to see whether the bag of words features are of any use we can train a classifier based on them when first approaching a problem a general best practice is to start with the simplest tool that could solve the job whenever it comes to classifying data a common favorite for its versatility and explainability is logistic regression it is very simple to train and the results are interpretable as you can easily extract the most important coefficients from the model we split our data in to a training set used to fit our model and a test set to see how well it generalizes to unseen data after training we get an accuracy of not too shabby guessing the most frequent class irrelevant would give us only however even if precision was good enough for our needs we should never ship a model without trying to understand it a first step is to understand the types of errors our model makes and which kind of errors are least desirable in our example false positives are classifying an irrelevant tweet as a disaster and false negatives are classifying a disaster as an irrelevant tweet if the priority is to react to every potential event we would want to lower our false negatives if we are constrained in resources however we might prioritize a lower false positive rate to reduce false alarms a good way to visualize this information is using a confusion matrix which compares the predictions our model makes with the true label ideally the matrix would be a diagonal line from top left to bottom right our predictions match the truth perfectly our classifier creates more false negatives than false positives proportionally in other words our model s most common error is inaccurately classifying disasters as irrelevant if false positives represent a high cost for law enforcement this could be a good bias for our classifier to have to validate our model and interpret its predictions it is important to look at which words it is using to make decisions if our data is biased our classifier will make accurate predictions in the sample data but the model would not generalize well in the real world here we plot the most important words for both the disaster and irrelevant class plotting word importance is simple with bag of words and logistic regression since we can just extract and rank the coefficients that the model used for its predictions our classifier correctly picks up on some patterns hiroshima massacre but clearly seems to be overfitting on some meaningless terms heyoo x right now our bag of words model is dealing with a huge vocabulary of different words and treating all words equally however some of these words are very frequent and are only contributing noise to our predictions next we will try a way to represent sentences that can account for the frequency of words to see if we can pick up more signal from our data in order to help our model focus more on meaningful words we can use a tf idf score term frequency inverse document frequency on top of our bag of words model tf idf weighs words by how rare they are in our dataset discounting words that are too frequent and just add to the noise here is the pca projection of our new embeddings we can see above that there is a clearer distinction between the two colors this should make it easier for our classifier to separate both groups let s see if this leads to better performance training another logistic regression on our new embeddings we get an accuracy of a very slight improvement has our model started picking up on more important words if we are getting a better result while preventing our model from cheating then we can truly consider this model an upgrade the words it picked up look much more relevant although our metrics on our test set only increased slightly we have much more confidence in the terms our model is using and thus would feel more comfortable deploying it in a system that would interact with customers our latest model managed to pick up on high signal words however it is very likely that if we deploy this model we will encounter words that we have not seen in our training set before the previous model will not be able to accurately classify these tweets even if it has seen very similar words during training to solve this problem we need to capture the semantic meaning of words meaning we need to understand that words like good and positive are closer than apricot and continent the tool we will use to help us capture meaning is called word vec using pre trained words word vec is a technique to find continuous embeddings for words it learns from reading massive amounts of text and memorizing which words tend to appear in similar contexts after being trained on enough data it generates a dimension vector for each word in a vocabulary with words of similar meaning being closer to each other the authors of the paper open sourced a model that was pre trained on a very large corpus which we can leverage to include some knowledge of semantic meaning into our model the pre trained vectors can be found in the repository associated with this post a quick way to get a sentence embedding for our classifier is to average word vec scores of all words in our sentence this is a bag of words approach just like before but this time we only lose the syntax of our sentence while keeping some semantic information here is a visualization of our new embeddings using previous techniques the two groups of colors look even more separated here our new embeddings should help our classifier find the separation between both classes after training the same model a third time a logistic regression we get an accuracy score of our best result yet time to inspect our model since our embeddings are not represented as a vector with one dimension per word as in our previous models it s harder to see which words are the most relevant to our classification while we still have access to the coefficients of our logistic regression they relate to the dimensions of our embeddings rather than the indices of words for such a low gain in accuracy losing all explainability seems like a harsh trade off however with more complex models we can leverage black box explainers such as lime in order to get some insight into how our classifier works lime lime is available on github through an open sourced package a black box explainer allows users to explain the decisions of any classifier on one particular example by perturbing the input in our case removing words from the sentence and seeing how the prediction changes let s see a couple explanations for sentences from our dataset however we do not have time to explore the thousands of examples in our dataset what we ll do instead is run lime on a representative sample of test cases and see which words keep coming up as strong contributors using this approach we can get word importance scores like we had for previous models and validate our model s predictions looks like the model picks up highly relevant words implying that it appears to make understandable decisions these seem like the most relevant words out of all previous models and therefore we re more comfortable deploying in to production we ve covered quick and efficient approaches to generate compact sentence embeddings however by omitting the order of words we are discarding all of the syntactic information of our sentences if these methods do not provide sufficient results you can utilize more complex model that take in whole sentences as input and predict labels without the need to build an intermediate representation a common way to do that is to treat a sentence as a sequence of individual word vectors using either word vec or more recent approaches such as glove or cove this is what we will do below convolutional neural networks for sentence classification train very quickly and work well as an entry level deep learning architecture while convolutional neural networks cnn are mainly known for their performance on image data they have been providing excellent results on text related tasks and are usually much quicker to train than most complex nlp approaches e g lstms and encoder decoder architectures this model preserves the order of words and learns valuable information on which sequences of words are predictive of our target classes contrary to previous models it can tell the difference between alex eats plants and plants eat alex training this model does not require much more work than previous approaches see code for details and gives us a model that is much better than the previous ones getting accuracy as with the models above the next step should be to explore and explain the predictions using the methods we described to validate that it is indeed the best model to deploy to users by now you should feel comfortable tackling this on your own here is a quick recap of the approach we ve successfully used these approaches were applied to a particular example case using models tailored towards understanding and leveraging short text such as tweets but the ideas are widely applicable to a variety of problems i hope this helped you we d love to hear your comments and questions feel free to comment below or reach out to emmanuelameisen here or on twitter want to learn applied artificial intelligence from top professionals in silicon valley or new york learn more about the artificial intelligence program are you a company working in ai and would like to get involved in the insight ai fellows program feel free to get in touch from a quick cheer to a standing ovation clap to show how much you enjoyed this story ai lead at insight ai emmanuelameisen insight fellows program your bridge to a career in data
Mybridge,10100,6,https://medium.mybridge.co/30-amazing-machine-learning-projects-for-the-past-year-v-2018-b853b8621ac7?source=tag_archive---------4----------------,30 Amazing Machine Learning Projects for the Past Year (v.2018),for the past year we ve compared nearly open source machine learning projects to pick top chance this is an extremely competitive list and it carefully picks the best open source machine learning libraries datasets and apps published between january and december mybridge ai evaluates the quality by considering popularity engagement and recency to give you an idea about the quality the average number of github stars is open source projects can be useful for data scientists you can learn by reading the source code and build something on top of the existing projects give a plenty of time to play around with machine learning projects you may have missed for the past year recommended learning a neural networks deep learning a ztm hands on artificial neural networks recommends stars b tensorflow complete guide to tensorflow for deep learning with python recommends stars click the numbers below credit given to the biggest contributor fasttext library for fast text representation and classification stars on github courtesy of facebook research muse multilingual unsupervised or supervised word embeddings based on fast text stars on github deep photo styletransfer code and data for paper deep photo style transfer stars on github courtesy of fujun luan ph d at cornell university the world s simplest facial recognition api for python and the command line stars on github courtesy of adam geitgey magenta music and art generation with machine intelligence stars on github sonnet tensorflow based neural network library stars on github courtesy of malcolm reynolds at deepmind deeplearn js a hardware accelerated machine intelligence library for the web stars on github courtesy of nikhil thorat at google brain fast style transfer in tensorflow stars on github courtesy of logan engstrom at mit pysc starcraft ii learning environment stars on github courtesy of timo ewalds at deepmind airsim open source simulator based on unreal engine for autonomous vehicles from microsoft ai research stars on github courtesy of shital shah at microsoft facets visualizations for machine learning datasets stars on github courtesy of google brain style paints ai colorization of images stars on github tensor tensor a library for generalized sequence to sequence models google research stars on github courtesy of ryan sepassi at google brain image to image translation in pytorch e g horse zebra edges cats and more stars on github courtesy of jun yan zhu ph d at berkeley faiss a library for efficient similarity search and clustering of dense vectors stars on github courtesy of facebook research fashion mnist a mnist like fashion product database stars on github courtesy of han xiao research scientist zalando tech parlai a framework for training and evaluating ai models on a variety of openly available dialog datasets stars on github courtesy of alexander miller at facebook research fairseq facebook ai research sequence to sequence toolkit stars on github pyro deep universal probabilistic programming with python and pytorch stars on github courtesy of uber ai labs igan interactive image generation powered by gan stars on github deep image prior image restoration with neural networks but without learning stars on github courtesy of dmitry ulyanov ph d at skoltech face classification real time face detection and emotion gender classification using fer imdb datasets with a keras cnn model and opencv stars on github speech to text wavenet end to end sentence level english speech recognition using deepmind s wavenet and tensorflow stars on github courtesy of namju kim at kakao brain stargan unified generative adversarial networks for multi domain image to image translation stars on github courtesy of yunjey choi at korea university ml agents unity machine learning agents stars on github courtesy of arthur juliani deep learning at unity d deepvideoanalytics a distributed visual search and visual data analytics platform stars on github courtesy of akshay bhat ph d at cornell university opennmt open source neural machine translation in torch stars on github pix pixhd synthesizing and manipulating x images with conditional gans stars on github courtesy of ming yu liu at ai research scientist at nvidia horovod distributed training framework for tensorflow stars on github courtesy of uber engineering ai blocks a powerful and intuitive wysiwyg interface that allows anyone to create machine learning models stars on github deep neural networks for voice conversion voice style transfer in tensorflow stars on github courtesy of dabi ahn ai research at kakao brain that s it for machine learning open source projects if you like this curation read best daily articles based on your programming skills on our website from a quick cheer to a standing ovation clap to show how much you enjoyed this story we rank articles for professionals read more and achieve more
David Foster,12800,11,https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188?source=tag_archive---------5----------------,How to build your own AlphaZero AI using Python and Keras,in this article i ll attempt to cover three things in march deepmind s alphago beat times world champion go player lee sedol in a series watched by over million people a machine had learnt a super human strategy for playing go a feat previously thought impossible or at the very least at least a decade away from being accomplished this in itself was a remarkable achievement however on th october deepmind took a giant leap further the paper mastering the game of go without human knowledge unveiled a new variant of the algorithm alphago zero that had defeated alphago incredibly it had done so by learning solely through self play starting tabula rasa blank state and gradually finding strategies that would beat previous incarnations of itself no longer was a database of human expert games required to build a super human ai a mere days later on th december deepmind released another paper mastering chess and shogi by self play with a general reinforcement learning algorithm showing how alphago zero could be adapted to beat the world champion programs stockfish and elmo at chess and shogi the entire learning process from being shown the games for the first time to becoming the best computer program in the world had taken under hours with this alphazero was born the general algorithm for getting good at something quickly without any prior knowledge of human expert strategy there are two amazing things about this achievement it cannot be overstated how important this is this means that the underlying methodology of alphago zero can be applied to any game with perfect information the game state is fully known to both players at all times because no prior expertise is required beyond the rules of the game this is how it was possible for deepmind to publish the chess and shogi papers only days after the original alphago zero paper quite literally all that needed to change was the input file that describes the mechanics of the game and to tweak the hyper parameters relating to the neural network and monte carlo tree search if alphazero used super complex algorithms that only a handful of people in the world understood it would still be an incredible achievement what makes it extraordinary is that a lot of the ideas in the paper are actually far less complex than previous versions at its heart lies the following beautifully simple mantra for learning doesn t that sound a lot like how you learn to play games when you play a bad move it s either because you misjudged the future value of resulting positions or you misjudged the likelihood that your opponent would play a certain move so didn t think to explore that possibility these are exactly the two aspects of gameplay that alphazero is trained to learn firstly check out the alphago zero cheat sheet for a high level understanding of how alphago zero works it s worth having that to refer to as we walk through each part of the code there s also a great article here that explains how alphazero works in more detail clone this git repository which contains the code i ll be referencing to start the learning process run the top two panels in the run ipynb jupyter notebook once it s built up enough game positions to fill its memory the neural network will begin training through additional self play and training it will gradually get better at predicting the game value and next moves from any position resulting in better decision making and smarter overall play we ll now have a look at the code in more detail and show some results that demonstrate the ai getting stronger over time n b this is my own understanding of how alphazero works based on the information available in the papers referenced above if any of the below is incorrect apologies and i ll endeavour to correct it the game that our algorithm will learn to play is connect or four in a row not quite as complex as go but there are still game positions in total the game rules are straightforward players take it in turns to enter a piece of their colour in the top of any available column the first player to get four of their colour in a row each vertically horizontally or diagonally wins if the entire grid is filled without a four in a row being created the game is drawn here s a summary of the key files that make up the codebase this file contains the game rules for connect each squares is allocated a number from to as follows the game py file gives the logic behind moving from one game state to another given a chosen action for example given the empty board and action the takeaction method return a new game state with the starting player s piece at the bottom of the centre column you can replace the game py file with any game file that conforms to the same api and the algorithm will in principal learn strategy through self play based on the rules you have given it this contains the code that starts the learning process it loads the game rules and then iterates through the main loop of the algorithm which consist of three stages there are two agents involved in this loop the best player and the current player the best player contains the best performing neural network and is used to generate the self play memories the current player then retrains its neural network on these memories and is then pitched against the best player if it wins the neural network inside the best player is switched for the neural network inside the current player and the loop starts again this contains the agent class a player in the game each player is initialised with its own neural network and monte carlo search tree the simulate method runs the monte carlo tree search process specifically the agent moves to a leaf node of the tree evaluates the node with its neural network and then backfills the value of the node up through the tree the act method repeats the simulation multiple times to understand which move from the current position is most favourable it then returns the chosen action to the game to enact the move the replay method retrains the neural network using memories from previous games this file contains the residual cnn class which defines how to build an instance of the neural network it uses a condensed version of the neural network architecture in the alphagozero paper i e a convolutional layer followed by many residual layers then splitting into a value and policy head the depth and number of convolutional filters can be specified in the config file the keras library is used to build the network with a backend of tensorflow to view individual convolutional filters and densely connected layers in the neural network run the following inside the the run ipynb notebook this contains the node edge and mcts classes that constitute a monte carlo search tree the mcts class contains the movetoleaf and backfill methods previously mentioned and instances of the edge class store the statistics about each potential move this is where you set the key parameters that influence the algorithm adjusting these variables will affect that running time neural network accuracy and overall success of the algorithm the above parameters produce a high quality connect player but take a long time to do so to speed the algorithm up try the following parameters instead contains the playmatches and playmatchesbetweenversions functions that play matches between two agents to play against your creation run the following code it s also in the run ipynb notebook when you run the algorithm all model and memory files are saved in the run folder in the root directory to restart the algorithm from this checkpoint later transfer the run folder to the run archive folder attaching a run number to the folder name then enter the run number model version number and memory version number into the initialise py file corresponding to the location of the relevant files in the run archive folder running the algorithm as usual will then start from this checkpoint an instance of the memory class stores the memories of previous games that the algorithm uses to retrain the neural network of the current player this file contains a custom loss function that masks predictions from illegal moves before passing to the cross entropy loss function the locations of the run and run archive folders log files are saved to the log folder inside the run folder to turn on logging set the values of the logger disabled variables to false inside this file viewing the log files will help you to understand how the algorithm works and see inside its mind for example here is a sample from the logger mcts file equally from the logger tourney file you can see the probabilities attached to each move during the evaluation phase training over a couple of days produces the following chart of loss against mini batch iteration number the top line is the error in the policy head the cross entropy of the mcts move probabilities against the output from the neural network the bottom line is the error in the value head the mean squared error between the actual game value and the neural network predict of the value the middle line is an average of the two clearly the neural network is getting better at predicting the value of each game state and the likely next moves to show how this results in stronger and stronger play i ran a league between players ranging from the st iteration of the neural network up to the th each pairing played twice with both players having a chance to play first here are the final standings clearly the later versions of the neural network are superior to the earlier versions winning most of their games it also appears that the learning hasn t yet saturated with further training time the players would continue to get stronger learning more and more intricate strategies as an example one clear strategy that the neural network has favoured over time is grabbing the centre column early observe the difference between the first version of the algorithm and say the th version st neural network version th neural network version this is a good strategy as many lines require the centre column claiming this early ensures your opponent cannot take advantage of this this has been learnt by the neural network without any human input there is a game py file for a game called metasquares in the games folder this involves placing x and o markers in a grid to try to form squares of different sizes larger squares score more points than smaller squares and the player with the most points when the grid is full wins if you switch the connect game py file for the metasquares game py file the same algorithm will learn how to play metasquares instead hopefully you find this article useful let me know in the comments below if you find any typos or have questions about anything in the codebase or article and i ll get back to you as soon as possible if you would like to learn more about how our company applied data science develops innovative data science solutions for businesses feel free to get in touch through our website or directly through linkedin and if you like this feel free to leave a few hearty claps applied data science is a london based consultancy that implements end to end data science solutions for businesses delivering measurable value if you re looking to do more with your data let s talk from a quick cheer to a standing ovation clap to show how much you enjoyed this story co founder of applied data science cutting edge data science news and projects
George Seif,11400,11,https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68?source=tag_archive---------6----------------,The 5 Clustering Algorithms Data Scientists Need to Know,clustering is a machine learning technique that involves the grouping of data points given a set of data points we can use a clustering algorithm to classify each data point into a specific group in theory data points that are in the same group should have similar properties and or features while data points in different groups should have highly dissimilar properties and or features clustering is a method of unsupervised learning and is a common technique for statistical data analysis used in many fields in data science we can use clustering analysis to gain some valuable insights from our data by seeing what groups the data points fall into when we apply a clustering algorithm today we re going to look at popular clustering algorithms that data scientists need to know and their pros and cons k means is probably the most well know clustering algorithm it s taught in a lot of introductory data science and machine learning classes it s easy to understand and implement in code check out the graphic below for an illustration k means has the advantage that it s pretty fast as all we re really doing is computing the distances between points and group centers very few computations it thus has a linear complexity o n on the other hand k means has a couple of disadvantages firstly you have to select how many groups classes there are this isn t always trivial and ideally with a clustering algorithm we d want it to figure those out for us because the point of it is to gain some insight from the data k means also starts with a random choice of cluster centers and therefore it may yield different clustering results on different runs of the algorithm thus the results may not be repeatable and lack consistency other cluster methods are more consistent k medians is another clustering algorithm related to k means except instead of recomputing the group center points using the mean we use the median vector of the group this method is less sensitive to outliers because of using the median but is much slower for larger datasets as sorting is required on each iteration when computing the median vector mean shift clustering is a sliding window based algorithm that attempts to find dense areas of data points it is a centroid based algorithm meaning that the goal is to locate the center points of each group class which works by updating candidates for center points to be the mean of the points within the sliding window these candidate windows are then filtered in a post processing stage to eliminate near duplicates forming the final set of center points and their corresponding groups check out the graphic below for an illustration an illustration of the entire process from end to end with all of the sliding windows is show below each black dot represents the centroid of a sliding window and each gray dot is a data point in contrast to k means clustering there is no need to select the number of clusters as mean shift automatically discovers this that s a massive advantage the fact that the cluster centers converge towards the points of maximum density is also quite desirable as it is quite intuitive to understand and fits well in a naturally data driven sense the drawback is that the selection of the window size radius r can be non trivial dbscan is a density based clustered algorithm similar to mean shift but with a couple of notable advantages check out another fancy graphic below and let s get started dbscan poses some great advantages over other clustering algorithms firstly it does not require a pe set number of clusters at all it also identifies outliers as noises unlike mean shift which simply throws them into a cluster even if the data point is very different additionally it is able to find arbitrarily sized and arbitrarily shaped clusters quite well the main drawback of dbscan is that it doesn t perform as well as others when the clusters are of varying density this is because the setting of the distance threshold and minpoints for identifying the neighborhood points will vary from cluster to cluster when the density varies this drawback also occurs with very high dimensional data since again the distance threshold becomes challenging to estimate one of the major drawbacks of k means is its naive use of the mean value for the cluster center we can see why this isn t the best way of doing things by looking at the image below on the left hand side it looks quite obvious to the human eye that there are two circular clusters with different radius centered at the same mean k means can t handle this because the mean values of the clusters are a very close together k means also fails in cases where the clusters are not circular again as a result of using the mean as cluster center gaussian mixture models gmms give us more flexibility than k means with gmms we assume that the data points are gaussian distributed this is a less restrictive assumption than saying they are circular by using the mean that way we have two parameters to describe the shape of the clusters the mean and the standard deviation taking an example in two dimensions this means that the clusters can take any kind of elliptical shape since we have standard deviation in both the x and y directions thus each gaussian distribution is assigned to a single cluster in order to find the parameters of the gaussian for each cluster e g the mean and standard deviation we will use an optimization algorithm called expectation maximization em take a look at the graphic below as an illustration of the gaussians being fitted to the clusters then we can proceed on to the process of expectation maximization clustering using gmms there are really key advantages to using gmms firstly gmms are a lot more flexible in terms of cluster covariance than k means due to the standard deviation parameter the clusters can take on any ellipse shape rather than being restricted to circles k means is actually a special case of gmm in which each cluster s covariance along all dimensions approaches secondly since gmms use probabilities they can have multiple clusters per data point so if a data point is in the middle of two overlapping clusters we can simply define its class by saying it belongs x percent to class and y percent to class i e gmms support mixed membership hierarchical clustering algorithms actually fall into categories top down or bottom up bottom up algorithms treat each data point as a single cluster at the outset and then successively merge or agglomerate pairs of clusters until all clusters have been merged into a single cluster that contains all data points bottom up hierarchical clustering is therefore called hierarchical agglomerative clustering or hac this hierarchy of clusters is represented as a tree or dendrogram the root of the tree is the unique cluster that gathers all the samples the leaves being the clusters with only one sample check out the graphic below for an illustration before moving on to the algorithm steps hierarchical clustering does not require us to specify the number of clusters and we can even select which number of clusters looks best since we are building a tree additionally the algorithm is not sensitive to the choice of distance metric all of them tend to work equally well whereas with other clustering algorithms the choice of distance metric is critical a particularly good use case of hierarchical clustering methods is when the underlying data has a hierarchical structure and you want to recover the hierarchy other clustering algorithms can t do this these advantages of hierarchical clustering come at the cost of lower efficiency as it has a time complexity of o n unlike the linear complexity of k means and gmm there are your top clustering algorithms that a data scientist should know we ll end off with an awesome visualization of how well these algorithms and a few others perform courtesy of scikit learn from a quick cheer to a standing ovation clap to show how much you enjoyed this story certified nerd ai machine learning engineer sharing concepts ideas and codes
Mybridge,6600,6,https://medium.mybridge.co/30-amazing-python-projects-for-the-past-year-v-2018-9c310b04cdb3?source=tag_archive---------7----------------,30 Amazing Python Projects for the Past Year (v.2018),for the past year we ve compared nearly open source python projects to pick top chance this is an extremely competitive list and it carefully picks the best open source python libraries tools and programs published between january and december mybridge ai evaluates the quality by considering popularity engagement and recency to give you an idea about the quality the average number of github stars is open source projects can be useful for programmers you can learn by reading the source code and build something on top of the existing projects give a plenty of time to play around with python projects you may have missed for the past year recommended learning a beginner the python bible build projects and go from beginner to pro recommends stars b data science python for data science and machine learning bootcamp use numpy pandas seaborn matplotlib plotly recommends stars click the numbers below credit given to the biggest contributor home assistant v open source home automation platform running on python stars on github courtesy of paulus schoutsen pytorch tensors and dynamic neural networks in python with strong gpu acceleration stars on github courtesy of adam paszke and others at pytorch team grumpy a python to go source code transcompiler and runtime stars on github courtesy of dylan trotter and others at google sanic async python web server that s written to go fast stars on github courtesy of channel cat and eli uriegas python fire a library for automatically generating command line interfaces clis from absolutely any python object stars on github courtesy of david bieber and others at google brain spacy v industrial strength natural language processing nlp with python and cython stars on github courtesy of matthew honnibal pipenv python development workflow for humans stars on github courtesy of kenneth reitz micropython a lean and efficient python implementation for microcontrollers and constrained systems stars on github prophet tool for producing high quality forecasts for time series data that has multiple seasonality with linear or non linear growth stars on github courtesy of facebook serpentai game agent framework in python helping you create ais bots to play any game stars on github courtesy of nicholas brochu dash interactive reactive web apps in pure python stars on github courtesy of chris p instapy instagram bot like comment follow automation script stars on github courtesy of timg apistar a fast and expressive api framework for python stars on github courtesy of tom christie faiss a library for efficient similarity search and clustering of dense vectors stars on github courtesy of matthijs douze and others at facebook research mechanicalsoup a python library for automating interaction with websites stars on github better exceptions pretty and useful exceptions in python automatically stars on github courtesy of qix flashtext extract keywords from sentence or replace keywords in sentences stars on github courtesy of vikash singh maya datetime for humans in python stars on github kenneth reitz mimesis v python library which helps generate mock data in different languages for various purposes these data can be especially useful at various stages of software development and testing stars on github courtesy of li ki geimfari open paperless scan index and archive all of your paper documents a document management system stars on github courtesy of tina zhou fsociety hacking tools pack a penetration testing framework stars on github courtesy of manis manisso livepython visually trace python code in real time stars on github courtesy of anastasis germanidis hatch a modern project package and virtual env manager for python stars on github courtesy of ofek lev tangent source to source debuggable derivatives in pure python stars on github courtesy of alex wiltschko and others at google brain clairvoyant a python program that identifies and monitors historical cues for short term stock movement stars on github courtesy of anthony federico monkeytype a system for python that generates static type annotations by collecting runtime types stars on github courtesy of carl meyer at instagram engineering eel a little python library for making simple electron like html js gui apps stars on github surprise v a python scikit for building and analyzing recommender systems stars on github gain web crawling framework for everyone stars on github courtesy of pdftabextract a set of tools for extracting tables from pdf files helping to do data mining on scanned documents stars on github that s it for python open source of the year if you like this curation read best daily articles based on your programming skills on our website from a quick cheer to a standing ovation clap to show how much you enjoyed this story we rank articles for professionals read more and achieve more
Simon Greenman,10200,16,https://towardsdatascience.com/who-is-going-to-make-money-in-ai-part-i-77a2f30b8cef?source=tag_archive---------8----------------,Who Is Going To Make Money In AI? Part I – Towards Data Science,we are in the midst of a gold rush in ai but who will reap the economic benefits the mass of startups who are all gold panning the corporates who have massive gold mining operations the technology giants who are supplying the picks and shovels and which nations have the richest seams of gold we are currently experiencing another gold rush in ai billions are being invested in ai startups across every imaginable industry and business function google amazon microsoft and ibm are in a heavyweight fight investing over billion in ai in corporates are scrambling to ensure they realise the productivity benefits of ai ahead of their competitors while looking over their shoulders at the startups china is putting its considerable weight behind ai and the european union is talking about a billion ai investment as it fears losing ground to china and the us ai is everywhere from the billion daily searches on google to the new apple iphone x that uses facial recognition to amazon alexa that cutely answers our questions media headlines tout the stories of how ai is helping doctors diagnose diseases banks better assess customer loan risks farmers predict crop yields marketers target and retain customers and manufacturers improve quality control and there are think tanks dedicated to studying the physical cyber and political risks of ai ai and machine learning will become ubiquitous and woven into the fabric of society but as with any gold rush the question is who will find gold will it just be the brave the few and the large or can the snappy upstarts grab their nuggets will those providing the picks and shovel make most of the money and who will hit pay dirt as i started thinking about who was going to make money in ai i ended up with seven questions who will make money across the chip makers platform and infrastructure providers enabling models and algorithm providers enterprise solution providers industry vertical solution providers corporate users of ai and nations while there are many ways to skin the cat of the ai landscape hopefully below provides a useful explanatory framework a value chain of sorts the companies noted are representative of larger players in each category but in no way is this list intended to be comprehensive or predictive even though the price of computational power has fallen exponentially demand is rising even faster ai and machine learning with its massive datasets and its trillions of vector and matrix calculations has a ferocious and insatiable appetite bring on the chips nvidia s stock is up in the past two years benefiting from the fact that their graphical processing unit gpu chips that were historically used to render beautiful high speed flowing games graphics were perfect for machine learning google recently launched its second generation of tensor processing units tpus and microsoft is building its own brainwave ai machine learning chips at the same time startups such as graphcore who has raised over m is looking to enter the market incumbents chip providers such as ibm intel qualcomm and amd are not standing still even facebook is rumoured to be building a team to design its own ai chips and the chinese are emerging as serious chip players with cambricon technology announcing the first cloud ai chip this past week what is clear is that the cost of designing and manufacturing chips then sustaining a position as a global chip leader is very high it requires extremely deep pockets and a world class team of silicon and software engineers this means that there will be very few new winners just like the gold rush days those that provide the cheapest and most widely used picks and shovels will make a lot of money the ai race is now also taking place in the cloud amazon realised early that startups would much rather rent computers and software than buy it and so it launched amazon web services aws in today ai is demanding so much compute power that companies are increasingly turning to the cloud to rent hardware through infrastructure as a service iaas and platform as a service paas offerings the fight is on among the tech giants microsoft is offering their hybrid public and private azure cloud service that allegedly has over one million computers and in the past few weeks they announced that their brainwave hardware solutionsdramatically accelerate machine learning with their own bing search engine performance improving by a factor of ten google is rushing to play catchup with its own googlecloud offering and we are seeing the chinese alibaba starting to take global share amazon microsoft google and ibm are going to continue to duke this one out and watch out for the massively scaled cloud players from china the big picks and shovels guys will win again today google is the world s largest ai company attracting the best ai minds spending small country size gdp budgets on r d and sitting on the best datasets gleamed from the billions of users of their services ai is powering google s search autonomous vehicles speech recognition intelligent reasoning massive search and even its own work on drug discovery and disease diangosis and the incredible ai machine learning software and algorithms that are powering all of google s ai activity tensorflow is now being given away for free yes for free tensorflow is now an open source software project available to the world and why are they doing this as jeff dean head of google brain recently said there are million organisations in the world that could benefit from machine learning today if millions of companies use this best in class free ai software then they are likely to need lots of computing power and who is better served to offer that well google cloud is of course optimised for tensorflow and related ai services and once you become reliant on their software and their cloud you become a very sticky customer for many years to come no wonder it is a brutal race for global ai algorithm dominance with amazon microsoft ibm also offering their own cheap or free ai software services we are also seeing a fight for not only machine learning algorithms but cognitive algorithms that offer services for conversational agents and bots speech natural language processing nlp and semantics vision and enhanced core algorithms one startup in this increasingly contested space is clarifai who provides advanced image recognition systems for businesses to detect near duplicates and visual searches it has raised nearly m over the past three years the market for vision related algorithms and services is estimated to be a cumulative billion in revenue between and the giants are not standing still ibm for example is offering its watson cognitive products and services they have twenty or so apis for chatbots vision speech language knowledge management and empathy that can be simply be plugged into corporate software to create ai enabled applications cognitive apis are everywhere kdnuggets lists here over of the top cognitive services from the giants and startups these services are being put into the cloud as ai as a service aiaas to make them more accessible just recently microsoft s ceo satya nadella claimed that a million developers are using their ai apis services and tools for building ai powered apps and nearly developers are using their tools for chatbots i wouldn t want to be a startup competing with these goliaths the winners in this space are likely to favour the heavyweights again they can hire the best research and engineering talent spend the most money and have access to the largest datasets to flourish startups are going to have to be really well funded supported by leading researchers with a whole battery of ip patents and published papers deep domain expertise and have access to quality datasets and they should have excellent navigational skills to sail ahead of the giants or sail different races there will many startup casualties but those that can scale will find themselves as global enterprises or quickly acquired by the heavyweights and even if a startup has not found a path to commercialisation then they could become acquihires companies bought for their talent if they are working on enabling ai algorithms with a strong research oriented team we saw this in when deepmind a two year old london based company that developed unique reinforcement machine learning algorithms was acquired by google for m enterprise software has been dominated by giants such as salesforce ibm oracle and sap they all recognise that ai is a tool that needs to be integrated into their enterprise offerings but many startups are rushing to become the next generation of enterprise services filling in gaps where the incumbents don t currently tread or even attempting to disrupt them we analysed over two hundred use cases in the enterprise space ranging from customer management to marketing to cybersecurity to intelligence to hr to the hot area of cognitive robotic process automation rpa the enterprise field is much more open than previous spaces with a veritable medley of startups providing point solutions for these use cases today there are over ai powered companies just in the recruitment space many of them ai startups cybersecurity leader darktrace and rpa leader uipathhave war chests in the millions the incumbents also want to make sure their ecosystems stay on the forefront and are investing in startups that enhance their offering salesforce has invested in digital genius a customer management solution and similarly unbable that offers enterprise translation services incumbents also often have more pressing problems sap for example is rushing to play catchup in offering a cloud solution let alone catchup in ai we are also seeing tools providers trying to simplify the tasks required to create deploy and manage ai services in the enterprise machine learning training for example is a messy business where of time can be spent on data wrangling and an inordinate amount of time is spent on testing and tuning of what is called hyperparameters petuum a tools provider based in pittsburgh in the us has raised over m to help accelerate and optimise the deployment of machine learning models many of these enterprise startup providers can have a healthy future if they quickly demonstrate that they are solving and scaling solutions to meet real world enterprise needs but as always happens in software gold rushes there will be a handful of winners in each category and for those ai enterprise category winners they are likely to be snapped up along with the best in class tool providers by the giants if they look too threatening ai is driving a race for the best vertical industry solutions there are a wealth of new ai powered startups providing solutions to corporate use cases in the healthcare financial services agriculture automative legal and industrial sectors and many startups are taking the ambitious path to disrupt the incumbent corporate players by offering a service directly to the same customers it is clear that many startups are providing valuable point solutions and can succeed if they have access to large and proprietary data training sets domain knowledge that gives them deep insights into the opportunities within a sector a deep pool of talent around applied ai and deep pockets of capital to fund rapid growth those startups that are doing well generally speak the corporate commercial language of customers business efficiency and roi in the form of well developed go to market plans for example zestfinance has raised nearly m to help improve credit decision making that will provide fair and transparent credit to everyone they claim they have the world s best data scientists but they would wouldn t they for those startups that are looking to disrupt existing corporate players they need really deep pockets for example affirm that offers loans to consumers at the point of sale has raised over m these companies quickly need to create a defensible moat to ensure they remain competitive this can come from data network effects where more data begets better ai based services and products that gets more revenue and customers that gets more data and so the flywheel effect continues and while corporates might look to new vendors in their industry for ai solutions that could enhance their top and bottom line they are not going to sit back and let upstarts muscle in on their customers and they are not going to sit still and let their corporate competitors gain the first advantage through ai there is currently a massive race for corporate innovation large companies have their own venture groups investing in startups running accelerators and building their own startups to ensure that they are leaders in ai driven innovation large corporates are in a strong position against the startups and smaller companies due to their data assets data is the fuel for ai and machine learning who is better placed to take advantage of ai than the insurance company that has reams of historic data on underwriting claims the financial services company that knows everything about consumer financial product buying behaviour or the search company that sees more user searches for information than any other corporates large and small are well positioned to extract value from ai in fact gartner research predicts ai derived business value is projected to reach up to trillion by there are hundreds if not thousands of valuable use cases that ai can addresses across organisations corporates can improve their customer experience save costs lower prices drive revenues and sell better products and services powered by ai ai will help the big get bigger often at the expense of smaller companies but they will need to demonstrate strong visionary leadership an ability to execute and a tolerance for not always getting technology enabled projects right on the first try countries are also also in a battle for ai supremacy china has not been shy about its call to arms around ai it is investing massively in growing technical talent and developing startups its more lax regulatory environment especially in data privacy helps china lead in ai sectors such as security and facial recognition just recently there was an example of chinese police picking out one most wanted face in a crowd of at a music concert and sensetime group ltd that analyses faces and images on a massive scale reported it raised m becoming the most valuable global ai startup the chinese point out that their mobile market is x the size of the us and there are x more mobile payments taking place this is a massive data advantage the european focus on data privacy regulation could put them at a disadvantage in certain areas of ai even if the union is talking about a b investment in ai the uk germany france and japan have all made recent announcements about their nation state ai strategies for example president macron said the french government will spend billion over the next five years to support the ai ecosystem including the creation of large public datasets companies such as google s deepmind and samsung have committed to open new paris labs and fujitsu is expanding its paris research centre the british just announced a billion push into ai including funding of ai phds but while nations are investing in ai talent and the ecosystem the question is who will really capture the value will france and the uk simply be subsidising phds who will be hired by google and while payroll and income taxes will be healthy on those six figure machine learning salaries the bulk of the economic value created could be with this american company its shareholders and the smiling american treasury ai will increase productivity and wealth in companies and countries but how will that wealth be distributed when the headlines suggest that to of our jobs will be taken by the machines economists can point to lessons from hundreds of years of increasing technology automation will there be net job creation or net job loss the public debate often cites geoffrey hinton the godfather of machine learning who suggested radiologists will lose their jobs by the dozen as machines diagnose diseases from medical images but then we can look to the chinese who are using ai to assist radiologists in managing the overwhelming demand to review billion ct scans annually for lung cancer the result is not job losses but an expanded market with more efficient and accurate diagnosis however there is likely to be a period of upheaval when much of the value will go to those few companies and countries that control ai technology and data and lower skilled countries whose wealth depends on jobs that are targets of ai automation will likely suffer ai will favour the large and the technologically skilled in examining the landscape of ai it has became clear that we are now entering a truly golden era for ai and there are few key themes appearing as to where the economic value will migrate in short it looks like the ai gold rush will favour the companies and countries with control and scale over the best ai tools and technology the data the best technical workers the most customers and the strongest access to capital those with scale will capture the lion s share of the economic value from ai in some ways plus c a change plus c est la me me chose but there will also be large golden nuggets that will be found by a few choice brave startups but like any gold rush many startups will hit pay dirt and many individuals and societies will likely feel like they have not seen the benefits of the gold rush this is the first part in a series of articles i intend to write on the topic of the economics of ai i welcome your feedback written by simon greenman i am a lover of technology and how it can be applied in the business world i run my own advisory firm best practice ai helping executives of enterprises and startups accelerate the adoption of roi based ai applications please get in touch to discuss this if you enjoyed this piece i d love it if you hit the clap button so others might stumble upon it and please post your comments or you can email me directly or find me on linkedin or twitter or follow me at simon greenman from a quick cheer to a standing ovation clap to show how much you enjoyed this story ai guy mapquest guy grow innovate and transform companies with tech start up investor mentor and geek sharing concepts ideas and codes
Eugenio Culurciello,6400,8,https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0?source=tag_archive---------9----------------,The fall of RNN / LSTM – Towards Data Science,we fell for recurrent neural networks rnn long short term memory lstm and all their variants now it is time to drop them it is the year and lstm and rnn make a great come back from the dead we all read colah s blog and karpathy s ode to rnn but we were all young and unexperienced for a few years this was the way to solve sequence learning sequence translation seq seq which also resulted in amazing results in speech to text comprehension and the raise of siri cortana google voice assistant alexa also let us not forget machine translation which resulted in the ability to translate documents into different languages or neural machine translation but also translate images into text text into images and captioning video and well you got the idea then in the following years came resnet and attention one could then better understand that lstm were a clever bypass technique also attention showed that mlp network could be replaced by averaging networks influenced by a context vector more on this later it only took more years but today we can definitely say but do not take our words for it also see evidence that attention based networks are used more and more by google facebook salesforce to name a few all these companies have replaced rnn and variants for attention based models and it is just the beginning rnn have the days counted in all applications because they require more resources to train and run than attention based models see this post for more info remember rnn and lstm and derivatives use mainly sequential processing over time see the horizontal arrow in the diagram below this arrow means that long term information has to sequentially travel through all cells before getting to the present processing cell this means it can be easily corrupted by being multiplied many time by small numbers this is the cause of vanishing gradients to the rescue came the lstm module which today can be seen as multiple switch gates and a bit like resnet it can bypass units and thus remember for longer time steps lstm thus have a way to remove some of the vanishing gradients problems but not all of it as you can see from the figure above still we have a sequential path from older past cells to the current one in fact the path is now even more complicated because it has additive and forget branches attached to it no question lstm and gru and derivatives are able to learn a lot of longer term information see results here but they can remember sequences of s not s or s or more and one issue of rnn is that they are not hardware friendly let me explain it takes a lot of resources we do not have to train these network fast also it takes much resources to run these model in the cloud and given that the demand for speech to text is growing rapidly the cloud is not scalable we will need to process at the edge right into the amazon echo see note below for more details if sequential processing is to be avoided then we can find units that look ahead or better look back since most of the time we deal with real time causal data where we know the past and want to affect future decisions not so in translating sentences or analyzing recorded videos for example where we have all data and can reason on it more time such look back ahead units are neural attention modules which we previously explained here to the rescue and combining multiple neural attention modules comes the hierarchical neural attention encoder shown in the figure below a better way to look into the past is to use attention modules to summarize all past encoded vectors into a context vector ct notice there is a hierarchy of attention modules here very similar to the hierarchy of neural networks this is also similar to temporal convolutional network tcn reported in note below in the hierarchical neural attention encoder multiple layers of attention can look at a small portion of recent past say vectors while layers above can look at of these attention modules effectively integrating the information of x vectors this extends the ability of the hierarchical neural attention encoder to past vectors but more importantly look at the length of the path needed to propagate a representation vector to the output of the network in hierarchical networks it is proportional to log n where n are the number of hierarchy layers this is in contrast to the t steps that a rnn needs to do where t is the maximum length of the sequence to be remembered and t n this architecture is similar to a neural turing machine but lets the neural network decide what is read out from memory via attention this means an actual neural network will decide which vectors from the past are important for future decisions but what about storing to memory the architecture above stores all previous representation in memory unlike neural turning machines this can be rather inefficient think about storing the representation of every frame in a video most times the representation vector does not change frame to frame so we really are storing too much of the same what can we do is add another unit to prevent correlated data to be stored for example by not storing vectors too similar to previously stored ones but this is really a hack the best would be to be let the application guide what vectors should be saved or not this is the focus of current research studies stay tuned for more information tell your friends it is very surprising to us to see so many companies still use rnn lstm for speech to text many unaware that these networks are so inefficient and not scalable please tell them about this post about training rnn lstm rnn and lstm are difficult to train because they require memory bandwidth bound computation which is the worst nightmare for hardware designer and ultimately limits the applicability of neural networks solutions in short lstm require linear layer mlp layer per cell to run at and for each sequence time step linear layers require large amounts of memory bandwidth to be computed in fact they cannot use many compute unit often because the system has not enough memory bandwidth to feed the computational units and it is easy to add more computational units but hard to add more memory bandwidth note enough lines on a chip long wires from processors to memory etc as a result rnn lstm and variants are not a good match for hardware acceleration and we talked about this issue before here and here a solution will be compute in memory devices like the ones we work on at fwdnxt see this repository for a simple example of these techniques note hierarchical neural attention is similar to the ideas in wavenet but instead of a convolutional neural network we use hierarchical attention modules also hierarchical neural attention can be also bi directional note rnn and lstm are memory bandwidth limited problems see this for details the processing unit s need as much memory bandwidth as the number of operations s they can provide making it impossible to fully utilize them the external bandwidth is never going to be enough and a way to slightly ameliorate the problem is to use internal fast caches with high bandwidth the best way is to use techniques that do not require large amount of parameters to be moved back and forth from memory or that can be re used for multiple computation per byte transferred high arithmetic intensity note here is a paper comparing cnn to rnn temporal convolutional network tcn outperform canonical recurrent networks such as lstms across a diverse range of tasks and datasets while demonstrating longer effective memory note related to this topic is the fact that we know little of how our human brain learns and remembers sequences we often learn and recall long sequences in smaller segments such as a phone number memorized as four segments behavioral experiments suggest that humans and some animals employ this strategy of breaking down cognitive or behavioral sequences into chunks in a wide variety of tasks these chunks remind me of small convolutional or attention like networks on smaller sequences that then are hierarchically strung together like in the hierarchical neural attention encoder and temporal convolutional network tcn more studies make me think that working memory is similar to rnn networks that uses recurrent real neuron networks and their capacity is very low on the other hand both the cortex and hippocampus give us the ability to remember really long sequences of steps like where did i park my car at airport days ago suggesting that more parallel pathways may be involved to recall long sequences where attention mechanism gate important chunks and force hops in parts of the sequence that is not relevant to the final goal or task note the above evidence shows we do not read sequentially in fact we interpret characters words and sentences as a group an attention based or convolutional module perceives the sequence and projects a representation in our mind we would not be misreading this if we processed this information sequentially we would stop and notice the inconsistencies i have almost years of experience in neural networks in both hardware and software a rare combination see about me here medium webpage scholar linkedin and more if you found this article useful please consider a donation to support more tutorials and blogs any contribution can make a difference from a quick cheer to a standing ovation clap to show how much you enjoyed this story i dream and build new technology sharing concepts ideas and codes
WiseWolf Fund,14200,8,https://medium.com/@wisewolf_fund/unique-trends-to-look-out-for-with-artificial-intelligence-1db3de178463?source=---------0----------------,GAME-CHANGING TRENDS TO LOOK OUT FOR WITH AI – WiseWolf Fund – Medium,artificial intelligence is a state of the art technological trend that many companies are trying to integrate into their business a recent report by mckinsey states that baidu the chinese equivalent of alphabet invested billion in ai last year at the same time alphabet invested roughly billion in developing ai technologies the chinese government has been actively pursuing ai technology in an attempt to control a future cornerstone innovation companies in the us are also investing time money and energy into advancing ai technology the reason for such interest towards artificial intelligence is that artificial intelligence can enhance any product or function this is why companies and governments make considerable investments in the research and development of this technology its role in increasing the production performance while simultaneously reducing the costs cannot be underestimated since some of the largest entities in the world are focused on promoting the ai technology it would be wise to understand and follow the trend ai is already shaping the economy and in the near future its effect may be even more significant ignoring the new technology and its influence on the global economic situation is a recipe for failure despite the huge public interest and attention towards ai its evolution is still somewhat halted by the objective causes as any new and fast developing industry ai is quickly outgrowing its environment according to adam temper an author of many creative researches on artificial intelligence the development of ai is mostly limited by the lack of employees with relevant expertise very few mature standard industry tools limited high quality training material available few options for easy access to preconfigured machine learning environments and the general focus in the industry on implementation rather than design with any new complex technology the learning curve is steep our educational institutions are several steps behind the commercial applications of this technology it is important that ai scientists work collaboratively sharing knowledge and best practice to address this deficiency ai is rapidly increasing its impact on society we need to ensure that the power of ai doesn t remain with the elite few another factor that may be hindering the progress of ai is the cautious stance that people tend to take towards it artificial intelligence is still too sci fi too strange and therefore sometimes scary when people learn to trust ai it will make a true quantum leap in the way of general adoption and application adam temper supports this point too describing the possible ways for ai technology to gain public trust as at the same time if we analyze the primary purpose of ai we will see it for what it really is a tool to perform the routine tasks relieving humans for something more creative or innovative when asked about the current trends and opportunities of ai aaron edell ceo and co founder of machine box and one of the top writers on ai described them as follows ai has also become a political talking point in recent years there have been arguments that ai will help to create jobs but that it will also cause certain workers to lose their jobs for example estimations prove that self driving vehicles will cause truck drivers to lose their jobs each month also as much as million pickers and packers working in us warehouses could be out of a job this is due to the fact that by implementing ai factories can operate with as few as a dozen of workers naturally companies gladly implement artificial intelligence as it ensures considerable savings at the same time governments are concerned about the current employment situation as well as the short term and long term predictions some countries have already begun to plan measures about the new ai technology that are intended to keep the economy stable in fact it would not be fair to say that artificial intelligence causes people to lose jobs true the whole point of automation is making machines do what people used to do before however it would be more correct if we said that artificial intelligence reshapes the employment situation together with taking over human functions it creates other jobs forces people to master new skills encourages workers to increase productivity but it is obvious that ai is going to turn the regular sequence of events upside down therefore the best approach is not to wait until ai leaves you unemployed but rather proactively embrace it and learn to live with it as we said already ai can also create jobs so a wise move would be to learn to manage ai based tools with the advance of ai products learning to work with them may secure you a job and even promote your career your future largely depends on your current and expected income however another important factor is the way you manage your finances of course investing in your own or your children s knowledge is one of the best investments you can ever make at the same time if you need some financial cushion to secure your family s welfare you should look at the available investment opportunities and this is where artificial intelligence may become your best friend professional consultant and investment manager in the recent years in addition to the traditional banks and financial institutions we have witnessed the appearance of a totally new and innovative investment system we are talking about the blockchain technology and the cryptocurrencies that it supports millions of people all over the world have already appreciated the transparency and flexibility of the blockchain networks by watching the cryptocurrency trends carefully and trading wisely individual investors have made fortunes within a very short time nowadays the cryptocurrency opportunities are open for everyone not only for the industry experts there are investment funds running on artificial intelligence that are available for individual investors with such funds you are on one hand protected by the blockchain technology it ensures proper safety of your funds and the security of your transactions on the other hand you do not need to be an investment expert to make wise decisions this is where artificial intelligence is at your service it analyzes the existing trends on the extremely volatile cryptocurrency market and shows you the best opportunities the main point is that we should not regard ai as a threat to our careers and a danger to our well being instead we should analyze the investment openings created by ai technology that can secure our prosperity for example wolf coin is using ai technology to create a seamless investment channel for savvy individuals this robust channel opens great opportunities that investors can use to become new rich kids on the block most noteworthy the low entry cost of has made it one offer that will enjoy a huge buzz the focus on this new market opening will help people build a solid financial nest egg that will keep them safe even in the face of the storm wisewolf fund launching the wolf coin focused its effort on creating a great opportunity for people who wish to benefit from cryptocurrency trading but are new to this trend with artificial intelligence and advanced analytical algorithms the fund arranges the most favorable conditions for individual investors mainstream manufacturers companies and factories are embracing ai technology to change the mode of their operations therefore it is critical to keep tabs on this reality as it can bring many benefits that cannot be found elsewhere ai is one of the hottest topics of discussion however it is now clear that ai is here to stay so people should accept the obvious in order to create the future that they desire the wisest strategy is to embrace artificial intelligence and let it work to maintain our well being from a quick cheer to a standing ovation clap to show how much you enjoyed this story the wisewolf crypto fund provides an easy way to enter the cryptocurrency market even for non techies
Justin Lee,8300,11,https://medium.com/swlh/chatbots-were-the-next-big-thing-what-happened-5fc49dd6fa61?source=---------1----------------,Chatbots were the next big thing: what happened? – The Startup – Medium,oh how the headlines blared chatbots were the next big thing our hopes were sky high bright eyed and bushy tailed the industry was ripe for a new era of innovation it was time to start socializing with machines and why wouldn t they be all the road signs pointed towards insane success at the mobile world congress chatbots were the main headliners the conference organizers cited an overwhelming acceptance at the event of the inevitable shift of focus for brands and corporates to chatbots in fact the only significant question around chatbots was who would monopolize the field not whether chatbots would take off in the first place one year on we have an answer to that question no because there isn t even an ecosystem for a platform to dominate chatbots weren t the first technological development to be talked up in grandiose terms and then slump spectacularly the age old hype cycle unfolded in familiar fashion expectations built built and then it all kind of fizzled out the predicted paradim shift didn t materialize and apps are tellingly still alive and well we look back at our breathless optimism and turn to each other slightly baffled is that it that was the chatbot revolution we were promised digit s ethan bloch sums up the general consensus according to dave feldman vice president of product design at heap chatbots didn t just take on one difficult problem and fail they took on several and failed all of them bots can interface with users in different ways the big divide is text vs speech in the beginning of computer interfaces was the written word users had to type commands manually into a machine to get anything done then graphical user interfaces guis came along and saved the day we became entranced by windows mouse clicks icons and hey we eventually got color too meanwhile a bunch of research scientists were busily developing natural language nl interfaces to databases instead of having to learn an arcane database query language another bunch of scientists were developing speech processing software so that you could just speak to your computer rather than having to type this turned out to be a whole lot more difficult than anyone originally realised the next item on the agenda was holding a two way dialog with a machine here s an example dialog dating back to the s with vcr setup system pretty cool right the system takes turns in collaborative way and does a smart job of figuring out what the user wants it was carefully crafted to deal with conversations involving vcrs and could only operate within strict limitations modern day bots whether they use typed or spoken input have to face all these challenges but also work in an efficient and scalable way on a variety of platforms basically we re still trying to achieve the same innovations we were years ago here s where i think we re going wrong an oversized assumption has been that apps are over and would be replaced by bots by pitting two such disparate concepts against one another instead of seeing them as separate entities designed to serve different purposes we discouraged bot development you might remember a similar war cry when apps first came onto the scene ten years ago but do you remember when apps replaced the internet it s said that a new product or service needs to be two of the following better cheaper or faster are chatbots cheaper or faster than apps no not yet at least whether they re better is subjective but i think it s fair to say that today s best bot isn t comparable to today s best app plus nobody thinks that using lyft is too complicated or that it s too hard to order food or buy a dress on an app what is too complicated is trying to complete these tasks with a bot and having the bot fail a great bot can be about as useful as an average app when it comes to rich sophisticated multi layered apps there s no competition that s because machines let us access vast and complex information systems and the early graphical information systems were a revolutionary leap forward in helping us locate those systems modern day apps benefit from decades of research and experimentation why would we throw this away but if we swap the word replace with extend things get much more interesting today s most successful bot experiences take a hybrid approach incorporating chat into a broader strategy that encompasses more traditional elements the next wave will be multimodal apps where you can say what you want like with siri and get back information as a map text or even a spoken response another problematic aspect of the sweeping nature of hype is that it tends to bypass essential questions like these for plenty of companies bots just aren t the right solution the past two years are littered with cases of bots being blindly applied to problems where they aren t needed building a bot for the sake of it letting it loose and hoping for the best will never end well the vast majority of bots are built using decision tree logic where the bot s canned response relies on spotting specific keywords in the user input the advantage of this approach is that it s pretty easy to list all the cases that they are designed to cover and that s precisely their disadvantage too that s because these bots are purely a reflection of the capability fastidiousness and patience of the person who created them and how many user needs and inputs they were able to anticipate problems arise when life refuses to fit into those boxes according to recent reports of the bots on facebook messenger are failing to fulfil simple user requests this is partly a result of developers failing to narrow their bot down to one strong area of focus when we were building growthbot we decided to make it specific to sales and marketers not an all rounder despite the temptation to get overexcited about potential capabilties remember a bot that does one thing well is infinitely more helpful than a bot that does multiple things poorly a competent developer can build a basic bot in minutes but one that can hold a conversation that s another story despite the constant hype around ai we re still a long way from achieving anything remotely human like in an ideal world the technology known as nlp natural language processing should allow a chatbot to understand the messages it receives but nlp is only just emerging from research labs and is very much in its infancy some platforms provide a bit of nlp but even the best is at toddler level capacity for example think about siri understanding your words but not their meaning as matt asay outlines this results in another issue failure to capture the attention and creativity of developers and conversations are complex they re not linear topics spin around each other take random turns restart or abruptly finish today s rule based dialogue systems are too brittle to deal with this kind of unpredictability and statistical approaches using machine learning are just as limited the level of ai required for human like conversation just isn t available yet and in the meantime there are few high quality examples of trailblazing bots to lead the way as dave feldman remarked once upon a time the only way to interact with computers was by typing arcane commands to the terminal visual interfaces using windows icons or a mouse were a revolution in how we manipulate information there s a reasons computing moved from text based to graphical user interfaces guis on the input side it s easier and faster to click than it is to type tapping or selecting is obviously preferable to typing out a whole sentence even with predictive often error prone text on the output side the old adage that a picture is worth a thousand words is usually true we love optical displays of information because we are highly visual creatures it s no accident that kids love touch screens the pioneers who dreamt up graphical interface were inspired by cognitive psychology the study of how the brain deals with communication conversational uis are meant to replicate the way humans prefer to communicate but they end up requiring extra cognitive effort essentially we re swapping something simple for a more complex alternative sure there are some concepts that we can only express using language show me all the ways of getting to a museum that give me steps but don t take longer than minutes but most tasks can be carried out more efficiently and intuitively with guis than with a conversational ui aiming for a human dimension in business interactions makes sense if there s one thing that s broken about sales and marketing it s the lack of humanity brands hide behind ticket numbers feedback forms do not reply emails automated responses and gated contact us forms facebook s goal is that their bots should pass the so called turing test meaning you can t tell whether you are talking to a bot or a human but a bot isn t the same as a human it never will be a conversation encompasses so much more than just text humans can read between the lines leverage contextual information and understand double layers like sarcasm bots quickly forget what they re talking about meaning it s a bit like conversing with someone who has little or no short term memory as hubspot team pinpointed people aren t easily fooled and pretending a bot is a human is guaranteed to diminish returns not to mention the fact that you re lying to your users and even those rare bots that are powered by state of the art nlp and excel at processing and producing content will fall short in comparison and here s the other thing conversational uis are built to replicate the way humans prefer to communicate with other humans but is that how humans prefer to interact with machines not necessarily at the end of the day no amount of witty quips or human like mannerisms will save a bot from conversational failure in a way those early adopters weren t entirely wrong people are yelling at google home to play their favorite song ordering pizza from the domino s bot and getting makeup tips from sephora but in terms of consumer response and developer involvement chatbots haven t lived up to the hype generated circa not even close computers are good at being computers searching for data crunching numbers analyzing opinions and condensing that information computers aren t good at understanding human emotion the state of nlp means they still don t get what we re asking them never mind how we feel that s why it s still impossible to imagine effective customer support sales or marketing without the essential human touch empathy and emotional intelligence for now bots can continue to help us with automated repetitive low level tasks and queries as cogs in a larger more complex system and we did them and ourselves a disservice by expecting so much so soon but that s not the whole story yes our industry massively overestimated the initial impact chatbots would have emphasis on initial as bill gates once said the hype is over and that s a good thing now we can start examining the middle grounded grey area instead of the hyper inflated frantic black and white zone i believe we re at the very beginning of explosive growth this sense of anti climax is completely normal for transformational technology messaging will continue to gain traction chatbots aren t going away nlp and ai are becoming more sophisticated every day developers apps and platforms will continue to experiment with and heavily invest in conversational marketing and i can t wait to see what happens next from a quick cheer to a standing ovation clap to show how much you enjoyed this story head of growth for growthbot messaging conversational strategy hubspot medium s largest publication for makers subscribe to receive our top stories here https goo gl zhclji
Michael Solana,680,5,https://medium.com/s/story/artificial-intelligence-is-humanitys-rorschach-test-6fb1ef9c0ce4?source=---------2----------------,Artificial Intelligence Is Humanity's Rorschach Test,member feature story slime sunday founders fund slime sunday founders fund i don t fear artificial intelligence i fear people who fear artificial intelligence it s the s a psychologist stares at his patient a balding middle aged foreman with a cigarette in his hand and a curl of smoke around him like a halo on an acid trip the psychologist holds up an inkblot an ambiguous black splatter on a white flashcard and asks his patient what he sees the thinking is his patient not willing or otherwise able to express his feelings his thoughts his motivations might inadvertently reveal some piece of his inner self while describing the ambiguous the foreman doesn t see a nondescript swiggle or stain he sees a man and woman making love perhaps violently he sees a mother holding her child he sees a grisly murder while the descriptions of these inkblots reveal very little about the world they reveal a great deal about the man describing them because when faced with an inscrutable abstract he projects himself onto the ambiguous let s look at this in the context of artificial intelligence i m not talking about self driving cars or algorithms serving ads for wallpaper and nice leather boots on gmail i m not talking about the stuff we call artificial intelligence to raise money from bewildered venture capitalists on sand hill road i m talking about general artificial intelligence which is a computer that wants stuff and chiefly to live i m talking about building a conscious machine just smart enough to make itself smarter from here the thought experiment runs like this the conscious machine does make itself smarter and once it s smarter it learns how to make itself smarter which it does for good measure the smarter the machine becomes the faster this pattern repeats itself and the intelligence of the machine begins to increase exponentially in this way a conscious artificial intelligence born on a tuesday morning might be twice as smart as the smartest man who ever lived by wednesday afternoon and omnipotent by friday this is how we invent the thing that invents god in nerd lore it s known as the singularity the question the only question that could possibly matter to a human no longer at the top of the intellectual food chain is what does an exponential intelligence want conventional wisdom it extremely wants to murder you the dystopian version of superintelligence is illustrated with frequency by leaders in the technology industry and is famously depicted by hollywood in films like terminator or more recently ex machina and even the avengers the angry god a i is a story you know because it is the story you are constantly told we build the thinking machine it surpasses our abilities in every way and it destroys us for one of any number of reasons maybe it perceives us as a threat maybe we re just in its way and it hardly perceives us at all humanity a disposable insect race there are of course many arguments in opposition to the now ubiquitous concept of our apocalypse by artificial intelligence i myself have called into question the logic of such dystopian arguments in anatomy of next but our subject here is less pertaining to the nature of the conscious machine than it is to the way we talk about this subject and what it means first consider that most of the artificial intelligence depicted in culture looks human a representation with no basis in technological reality then the true scope of the singularity is almost impossible to predict which begs a question where are these opinions about the broadly unknowable coming from there s an obvious difficulty in trying to understand the hypothetical motivations of a hypothetically god like intelligence to your beloved labradoodle you are a being of immense magic with near unfathomable motivations you summon light and sound from inanimate matter soar through the streets on angry metal cast fire from your hands the labradoodle s conception of man is distorted because there is a vast difference between the intelligence of a dog and the intelligence of a human let us name this difference x now as we try and understand the difference between the most intelligent human who has ever lived and a hypothetical god like intelligence born of the singularity let us set our difference in intelligence at a conservative x how does one even begin to conceive of a being this smart here we approach our inscrutable abstract and our robot rorschach test but in this contemporary version of the famous psychological prompts what we are observing is not even entirely ambiguous we are attempting to imagine a greatly amplified mind here each of us has a particularly relevant data point our own in trying to imagine the amplified intelligence it is natural to imagine our own intelligence amplified in imagining the motivations of this amplified intelligence we naturally imagine ourselves if as you try to conceive of a future with machine intelligence a monster comes to mind it is likely you aren t afraid of something alien at all you re afraid of something exactly like you what would you do with unlimited power psychological projection seems to work in several contexts outside of general artificial intelligence in the technology industry the concept of meritocracy is now hotly debated how much of your life is determined by luck and how much by chance there s no answer here we know for sure but has there ever been a better rorschach test for separating high achievers from people who were given what they have questions pertaining to human nature are almost open self reflection are we basically good with some exceptions or are humans basically beasts with an animal nature just barely contained by a set of slowly eroding stories we tell ourselves law faith society the inner workings of a mind can t be fully shared and they can t be observed by a neutral party we therefore do not can not currently know anything of the inner workings of people in general but we can know ourselves so in the face of large abstractions concerning intelligence we hold up a mirror not everyone who fears general artificial intelligence would cause harm to others there are many people who haven t thought deeply about these questions at all they look to their neighbors for cues on what to think and there is no shortage of people willing to tell them the media has ads to sell after all and historically they have found great success in doing this with horror stories but as we try to understand the people who have thought about these questions with some depth with the depth required of a thoughtful screenplay for example or a book or a company it s worth considering the inkblot technology liberty teenagers with superpowers vp foundersfund creator producer anatomyofnext welcome to a place where words matter on medium smart voices and original ideas take center stage with no ads in sight watch follow all the topics you care about and we ll deliver the best stories for you to your homepage and inbox explore get unlimited access to the best stories on medium and support writers while you re at it just month upgrade
Emmanuel Ameisen,935,11,https://blog.insightdatascience.com/reinforcement-learning-from-scratch-819b65f074d8?source=---------3----------------,Reinforcement Learning from scratch – Insight Data,want to learn about applied artificial intelligence from leading practitioners in silicon valley new york or toronto learn more about the insight artificial intelligence fellows program are you a company working in ai and would like to get involved in the insight ai fellows program feel free to get in touch recently i gave a talk at the o reilly ai conference in beijing about some of the interesting lessons we ve learned in the world of nlp while there i was lucky enough to attend a tutorial on deep reinforcement learning deep rl from scratch by unity technologies i thought that the session led by arthur juliani was extremely informative and wanted to share some big takeaways below in our conversations with companies we ve seen a rise of interesting deep rl applications tools and results in parallel the inner workings and applications of deep rl such as alphago pictured above can often seem esoteric and hard to understand in this post i will give an overview of core aspects of the field that can be understood by anyone many of the visuals are from the slides of the talk and some are new the explanations and opinions are mine if anything is unclear reach out to me here deep rl is a field that has seen vast amounts of research interest including learning to play atari games beating pro players at dota and defeating go champions contrary to many classical deep learning problems that often focus on perception does this image contain a stop sign deep rl adds the dimension of actions that influence the environment what is the goal and how do i get there in dialog systems for example classical deep learning aims to learn the right response for a given query on the other hand deep reinforcement learning focuses on the right sequences of sentences that will lead to a positive outcome for example a happy customer this makes deep rl particularly attractive for tasks that require planning and adaptation such as manufacturing or self driving however industry applications have trailed behind the rapidly advancing results coming out of the research community a major reason is that deep rl often requires an agent to experiment millions of times before learning anything useful the best way to do this rapidly is by using a simulation environment this tutorial will be using unity to create environments to train agents in for this workshop led by arthur juliani and leon chen their goal was to get every participants to successfully train multiple deep rl algorithms in hours a tall order below is a comprehensive overview of many of the main algorithms that power deep rl today for a more complete set of tutorials arthur juliani wrote an part series starting here deep rl can be used to best the top human players at go but to understand how that s done you first need to understand a few simple concepts starting with much easier problems it all starts with slot machines let s imagine you are faced with chests that you can pick from at each turn each of them have a different average payout and your goal is to maximize the total payout you receive after a fixed number of turns this is a classic problem called multi armed bandits and is where we will start the crux of the problem is to balance exploration which helps us learn about which states are good and exploitation where we now use what we know to pick the best slot machine here we will utilize a value function that maps our actions to an estimated reward called the q function first we ll initialize all q values at equal values then we ll update the q value of each action picking each chest based on how good the payout was after choosing this action this allows us to learn a good value function we will approximate our q function using a neural network starting with a very shallow one that learns a probability distribution by using a softmax over the potential chests while the value function tells us how good we estimate each action to be the policy is the function that determines which actions we end up taking intuitively we might want to use a policy that picks the action with the highest q value this performs poorly in practice as our q estimates will be very wrong at the start before we gather enough experience through trial and error this is why we need to add a mechanism to our policy to encourage exploration one way to do that is to use epsilon greedy which consists of taking a random action with probability epsilon we start with epsilon being close to always choosing random actions and lower epsilon as we go along and learn more about which chests are good eventually we learn which chests are best in practice we might want to take a more subtle approach than either taking the action we think is the best or a random action a popular method is boltzmann exploration which adjust probabilities based on our current estimate of how good each chest is adding in a randomness factor adding different states the previous example was a world in which we were always in the same state waiting to pick from the same chests in front of us most real word problems consist of many different states that is what we will add to our environment next now the background behind chests alternates between colors at each turn changing the average values of the chests this means we need to learn a q function that depends not only on the action the chest we pick but the state what the color of the background is this version of the problem is called contextual multi armed bandits surprisingly we can use the same approach as before the only thing we need to add is an extra dense layer to our neural network that will take in as input a vector representing the current state of the world learning about the consequences of our actions there is another key factor that makes our current problem simpler than mosts in most environments such as in the maze depicted above the actions that we take have an impact on the state of the world if we move up on this grid we might receive a reward or we might receive nothing but the next turn we will be in a different state this is where we finally introduce a need for planning first we will define our q function as the immediate reward in our current state plus the discounted reward we are expecting by taking all of our future actions this solution works if our q estimate of states is accurate so how can we learn a good estimate we will use a method called temporal difference td learning to learn a good q function the idea is to only look at a limited number of steps in the future td for example only uses the next states to evaluate the reward surprisingly we can use td which looks at the current state and our estimate of the reward the next turn and get great results the structure of the network is the same but we need to go through one forward step before receiving the error we then use this error to back propagate gradients like in traditional deep learning and update our value estimates introducing monte carlo another method to estimate the eventual success of our actions is monte carlo estimates this consists of playing out the entire episode with our current policy until we reach an end success by reaching a green block or failure by reaching a red block in the image above and use that result to update our value estimates for each traversed state this allows us to propagate values efficiently in one batch at the end of an episode instead of every time we make a move the cost is that we are introducing noise to our estimates since we attribute very distant rewards to them the world is rarely discrete the previous methods were using neural networks to approximate our value estimates by mapping from a discrete number of states and actions to a value in the maze for example there were states squares and actions move in each adjacent direction in this environment we are trying to learn how to balance a ball on a dimensional paddle by deciding at each time step whether we want to tilt the paddle left or right here the state space becomes continuous the angle of the paddle and the position of the ball the good news is we can still use neural networks to approximate this function a note about off policy vs on policy learning the methods we used previously are off policy methods meaning we can generate data with any strategy using epsilon greedy for example and learn from it on policy methods can only learn from actions that were taken following our policy remember a policy is the method we use to determine which actions to take this constrains our learning process as we have to have an exploration strategy that is built in to the policy itself but allows us to tie results directly to our reasoning and enables us to learn more efficiently the approach we will use here is called policy gradients and is an on policy method previously we were first learning a value function q for each action in each state and then building a policy on top in vanilla policy gradient we still use monte carlo estimates but we learn our policy directly through a loss function that increases the probability of choosing rewarding actions since we are learning on policy we cannot use methods such as epsilon greedy which includes random choices to get our agent to explore the environment the way that we encourage exploration is by using a method called entropy regularization which pushes our probability estimates to be wider and thus will encourage us to make riskier choices to explore the space leveraging deep learning for representations in practice many state of the art rl methods require learning both a policy and value estimates the way we do this with deep learning is by having both be two separate outputs of the same backbone neural network which will make it easier for our neural network to learn good representations one method to do this is advantage actor critic a c we learn our policy directly with policy gradients defined above and learn a value function using something called advantage instead of updating our value function based on rewards we update it based on our advantage which measures how much better or worse an action was than our previous value function estimated it to be this helps make learning more stable compared to simple q learning and vanilla policy gradients learning directly from the screen there is an additional advantage to using deep learning for these methods which is that deep neural networks excel at perceptive tasks when a human plays a game the information received is not a list of states but an image usually of a screen or a board or the surrounding environment image based learning combines a convolutional neural network cnn with rl in this environment we pass in a raw image instead of features and add a layer cnn to our architecture without changing anything else we can even inspect activations to see what the network picks up on to determine value and policy in the example below we can see that the network uses the current score and distant obstacles to estimate the value of the current state while focusing on nearby obstacles for determining actions neat as a side note while toying around with the provided implementation i ve found that visual learning is very sensitive to hyperparameters changing the discount rate slightly for example completely prevented the neural network from learning even on a toy application this is a widely known problem but it is interesting to see it first hand nuanced actions so far we ve played with environments with continuous and discrete state spaces however every environment we studied had a discrete action space we could move in one of four directions or tilt the paddle to the left or right ideally for applications such as self driving cars we would like to learn continuous actions such as turning the steering wheel between and degrees in this environment called d ball world we can choose to tilt the paddle to any value on each of its axes this gives us more control as to how we perform actions but makes the action space much larger we can approach this by approximating our potential choices with gaussian distributions we learn a probability distribution over potential actions by learning the mean and variance of a gaussian distribution and our policy we sample from that distribution simple in theory next steps for the brave there are a few concepts that separate the algorithms described above from state of the art approaches it s interesting to see that conceptually the best robotics and game playing algorithms are not that far away from the ones we just explored that s it for this overview i hope this has been informative and fun if you are looking to dive deeper into the theory of rl give arthur s posts a read or diving deeper by following david silver s ucl course if you are looking to learn more about the projects we do at insight or how we work with companies please check us out below or reach out to me here want to learn about applied artificial intelligence from leading practitioners in silicon valley new york or toronto learn more about the insight artificial intelligence fellows program are you a company working in ai and would like to get involved in the insight ai fellows program feel free to get in touch from a quick cheer to a standing ovation clap to show how much you enjoyed this story ai lead at insight ai emmanuelameisen insight fellows program your bridge to a career in data
Irhum Shafkat,2000,15,https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1?source=---------4----------------,Intuitively Understanding Convolutions for Deep Learning,the advent of powerful and versatile deep learning frameworks in recent years has made it possible to implement convolution layers into a deep learning model an extremely simple task often achievable in a single line of code however understanding convolutions especially for the first time can often feel a bit unnerving with terms like kernels filters channels and so on all stacked onto each other yet convolutions as a concept are fascinatingly powerful and highly extensible and in this post we ll break down the mechanics of the convolution operation step by step relate it to the standard fully connected network and explore just how they build up a strong visual hierarchy making them powerful feature extractors for images the d convolution is a fairly simple operation at heart you start with a kernel which is simply a small matrix of weights this kernel slides over the d input data performing an elementwise multiplication with the part of the input it is currently on and then summing up the results into a single output pixel the kernel repeats this process for every location it slides over converting a d matrix of features into yet another d matrix of features the output features are essentially the weighted sums with the weights being the values of the kernel itself of the input features located roughly in the same location of the output pixel on the input layer whether or not an input feature falls within this roughly same location gets determined directly by whether it s in the area of the kernel that produced the output or not this means the size of the kernel directly determines how many or few input features get combined in the production of a new output feature this is all in pretty stark contrast to a fully connected layer in the above example we have input features and output features if this were a standard fully connected layer you d have a weight matrix of parameters with every output feature being the weighted sum of every single input feature convolutions allow us to do this transformation with only parameters with each output feature instead of looking at every input feature only getting to look at input features coming from roughly the same location do take note of this as it ll be critical to our later discussion before we move on it s definitely worth looking into two techniques that are commonplace in convolution layers padding and strides padding does something pretty clever to solve this pad the edges with extra fake pixels usually of value hence the oft used term zero padding this way the kernel when sliding can allow the original edge pixels to be at its center while extending into the fake pixels beyond the edge producing an output the same size as the input the idea of the stride is to skip some of the slide locations of the kernel a stride of means to pick slides a pixel apart so basically every single slide acting as a standard convolution a stride of means picking slides pixels apart skipping every other slide in the process downsizing by roughly a factor of a stride of means skipping every slides downsizing roughly by factor and so on more modern networks such as the resnet architectures entirely forgo pooling layers in their internal layers in favor of strided convolutions when needing to reduce their output sizes of course the diagrams above only deals with the case where the image has a single input channel in practicality most input images have channels and that number only increases the deeper you go into a network it s pretty easy to think of channels in general as being a view of the image as a whole emphasising some aspects de emphasising others so this is where a key distinction between terms comes in handy whereas in the channel case where the term filter and kernel are interchangeable in the general case they re actually pretty different each filter actually happens to be a collection of kernels with there being one kernel for every single input channel to the layer and each kernel being unique each filter in a convolution layer produces one and only one output channel and they do it like so each of the kernels of the filter slides over their respective input channels producing a processed version of each some kernels may have stronger weights than others to give more emphasis to certain input channels than others eg a filter may have a red kernel channel with stronger weights than others and hence respond more to differences in the red channel features than the others each of the per channel processed versions are then summed together to form one channel the kernels of a filter each produce one version of each channel and the filter as a whole produces one overall output channel finally then there s the bias term the way the bias term works here is that each output filter has one bias term the bias gets added to the output channel so far to produce the final output channel and with the single filter case down the case for any number of filters is identical each filter processes the input with its own different set of kernels and a scalar bias with the process described above producing a single output channel they are then concatenated together to produce the overall output with the number of output channels being the number of filters a nonlinearity is then usually applied before passing this as input to another convolution layer which then repeats this process even with the mechanics of the convolution layer down it can still be hard to relate it back to a standard feed forward network and it still doesn t explain why convolutions scale to and work so much better for image data suppose we have a input and we want to transform it into a grid if we were using a feedforward network we d reshape the input into a vector of length and pass it through a densely connected layer with inputs and outputs one could visualize the weight matrix w for a layer and although the convolution kernel operation may seem a bit strange at first it is still a linear transformation with an equivalent transformation matrix if we were to use a kernel k of size on the reshaped input to get a output the equivalent transformation matrix would be note while the above matrix is an equivalent transformation matrix the actual operation is usually implemented as a very different matrix multiplication the convolution then as a whole is still a linear transformation but at the same time it s also a dramatically different kind of transformation for a matrix with elements there s just parameters which themselves are reused several times each output node only gets to see a select number of inputs the ones inside the kernel there is no interaction with any of the other inputs as the weights to them are set to it s useful to see the convolution operation as a hard prior on the weight matrix in this context by prior i mean predefined network parameters for example when you use a pretrained model for image classification you use the pretrained network parameters as your prior as a feature extractor to your final densely connected layer in that sense there s a direct intuition between why both are so efficient compared to their alternatives transfer learning is efficient by orders of magnitude compared to random initialization because you only really need to optimize the parameters of the final fully connected layer which means you can have fantastic performance with only a few dozen images per class here you don t need to optimize all parameters because we set most of them to zero and they ll stay that way and the rest we convert to shared parameters resulting in only actual parameters to optimize this efficiency matters because when you move from the inputs of mnist to real world images thats over inputs a dense layer attempting to halve the input to inputs would still require over billion parameters for comparison the entirety of resnet has some million parameters so fixing some parameters to and tying parameters increases efficiency but unlike the transfer learning case where we know the prior is good because it works on a large general set of images how do we know this is any good the answer lies in the feature combinations the prior leads the parameters to learn early on in this article we discussed that so with backpropagation coming in all the way from the classification nodes of the network the kernels have the interesting task of learning weights to produce features only from a set of local inputs additionally because the kernel itself is applied across the entire image the features the kernel learns must be general enough to come from any part of the image if this were any other kind of data eg categorical data of app installs this would ve been a disaster for just because your number of app installs and app type columns are next to each other doesn t mean they have any local shared features common with app install dates and time used sure the four may have an underlying higher level feature eg which apps people want most that can be found but that gives us no reason to believe the parameters for the first two are exactly the same as the parameters for the latter two the four could ve been in any consistent order and still be valid pixels however always appear in a consistent order and nearby pixels influence a pixel e g if all nearby pixels are red it s pretty likely the pixel is also red if there are deviations that s an interesting anomaly that could be converted into a feature and all this can be detected from comparing a pixel with its neighbors with other pixels in its locality and this idea is really what a lot of earlier computer vision feature extraction methods were based around for instance for edge detection one can use a sobel edge detection filter a kernel with fixed parameters operating just like the standard one channel convolution for a non edge containing grid eg the background sky most of the pixels are the same value so the overall output of the kernel at that point is for a grid with an vertical edge there is a difference between the pixels to the left and right of the edge and the kernel computes that difference to be non zero activating and revealing the edges the kernel only works only a grids at a time detecting anomalies on a local scale yet when applied across the entire image is enough to detect a certain feature on a global scale anywhere in the image so the key difference we make with deep learning is ask this question can useful kernels be learnt for early layers operating on raw pixels we could reasonably expect feature detectors of fairly low level features like edges lines etc there s an entire branch of deep learning research focused on making neural network models interpretable one of the most powerful tools to come out of that is feature visualization using optimization the idea at core is simple optimize a image usually initialized with random noise to activate a filter as strongly as possible this does make intuitive sense if the optimized image is completely filled with edges that s strong evidence that s what the filter itself is looking for and is activated by using this we can peek into the learnt filters and the results are stunning one important thing to notice here is that convolved images are still images the output of a small grid of pixels from the top left of an image will still be on the top left so you can run another convolution layer on top of another such as the two on the left to extract deeper features which we visualize yet however deep our feature detectors get without any further changes they ll still be operating on very small patches of the image no matter how deep your detectors are you can t detect faces from a grid and this is where the idea of the receptive field comes in a essential design choice of any cnn architecture is that the input sizes grow smaller and smaller from the start to the end of the network while the number of channels grow deeper this as mentioned earlier is often done through strides or pooling layers locality determines what inputs from the previous layer the outputs get to see the receptive field determines what area of the original input to the entire network the output gets to see the idea of a strided convolution is that we only process slides a fixed distance apart and skip the ones in the middle from a different point of view we only keep outputs a fixed distance apart and remove the rest we then apply a nonlinearity to the output and per usual then stack another new convolution layer on top and this is where things get interesting even if were we to apply a kernel of the same size having the same local area to the output of the strided convolution the kernel would have a larger effective receptive field this is because the output of the strided layer still does represent the same image it is not so much cropping as it is resizing only thing is that each single pixel in the output is a representative of a larger area of whose other pixels were discarded from the same rough location from the original input so when the next layer s kernel operates on the output it s operating on pixels collected from a larger area note if you re familiar with dilated convolutions note that the above is not a dilated convolution both are methods of increasing the receptive field but dilated convolutions are a single layer while this takes place on a regular convolution following a strided convolution with a nonlinearity inbetween this expansion of the receptive field allows the convolution layers to combine the low level features lines edges into higher level features curves textures as we see in the mixed a layer followed by a pooling strided layer the network continues to create detectors for even higher level features parts patterns as we see for mixed a the repeated reduction in image size across the network results in by the th block on convolutions input sizes of just compared to inputs of at this point each single pixel represents a grid of pixels which is huge compared to earlier layers where an activation meant detecting an edge here an activation on the tiny grid is one for a very high level feature such as for birds the network as a whole progresses from a small number of filters in case of googlenet detecting low level features to a very large number of filters in the final convolution each looking for an extremely specific high level feature followed by a final pooling layer which collapses each grid into a single pixel each channel is a feature detector with a receptive field equivalent to the entire image compared to what a standard feedforward network would have done the output here is really nothing short of awe inspiring a standard feedforward network would have produced abstract feature vectors from combinations of every single pixel in the image requiring intractable amounts of data to train the cnn with the priors imposed on it starts by learning very low level feature detectors and as across the layers as its receptive field is expanded learns to combine those low level features into progressively higher level features not an abstract combination of every single pixel but rather a strong visual hierarchy of concepts by detecting low level features and using them to detect higher level features as it progresses up its visual hierarchy it is eventually able to detect entire visual concepts such as faces birds trees etc and that s what makes them such powerful yet efficient with image data with the visual hierarchy cnns build it is pretty reasonable to assume that their vision systems are similar to humans and they re really great with real world images but they also fail in ways that strongly suggest their vision systems aren t entirely human like the most major problem adversarial examples examples which have been specifically modified to fool the model adversarial examples would be a non issue if the only tampered ones that caused the models to fail were ones that even humans would notice the problem is the models are susceptible to attacks by samples which have only been tampered with ever so slightly and would clearly not fool any human this opens the door for models to silently fail which can be pretty dangerous for a wide range of applications from self driving cars to healthcare robustness against adversarial attacks is currently a highly active area of research the subject of many papers and even competitions and solutions will certainly improve cnn architectures to become safer and more reliable cnns were the models that allowed computer vision to scale from simple applications to powering sophisticated products and services ranging from face detection in your photo gallery to making better medical diagnoses they might be the key method in computer vision going forward or some other new breakthrough might just be around the corner regardless one thing is for sure they re nothing short of amazing at the heart of many present day innovative applications and are most certainly worth deeply understanding hope you enjoyed this article if you d like to stay connected you ll find me on twitter here if you have a question comments are welcome i find them to be useful to my own learning process as well from a quick cheer to a standing ovation clap to show how much you enjoyed this story curious programmer tinkers around in python and deep learning sharing concepts ideas and codes
Sam Drozdov,2300,6,https://uxdesign.cc/an-intro-to-machine-learning-for-designers-5c74ba100257?source=---------5----------------,An intro to Machine Learning for designers – UX Collective,there is an ongoing debate about whether or not designers should write code wherever you fall on this issue most people would agree that designers should know about code this helps designers understand constraints and empathize with developers it also allows designers to think outside of the pixel perfect box when problem solving for the same reasons designers should know about machine learning put simply machine learning is a field of study that gives computers the ability to learn without being explicitly programmed arthur samuel even though arthur samuel coined the term over fifty years ago only recently have we seen the most exciting applications of machine learning digital assistants autonomous driving and spam free email all exist thanks to machine learning over the past decade new algorithms better hardware and more data have made machine learning an order of magnitude more effective only in the past few years companies like google amazon and apple have made some of their powerful machine learning tools available to developers now is the best time to learn about machine learning and apply it to the products you are building since machine learning is now more accessible than ever before designers today have the opportunity to think about how machine learning can be applied to improve their products designers should be able to talk with software developers about what is possible how to prepare and what outcomes to expect below are a few example applications that should serve as inspiration for these conversations machine learning can help create user centric products by personalizing experiences to the individuals who use them this allows us to improve things like recommendations search results notifications and ads machine learning is effective at finding abnormal content credit card companies use this to detect fraud email providers use this to detect spam and social media companies use this to detect things like hate speech machine learning has enabled computers to begin to understand the things we say natural language processing and the things we see computer vision this allows siri to understand siri set a reminder google photos to create albums of your dog and facebook to describe a photo to those visually impaired machine learning is also helpful in understanding how users are grouped this insight can then be used to look at analytics on a group by group basis from here different features can be evaluated across groups or be rolled out to only a particular group of users machine learning allows us to make predictions about how a user might behave next knowing this we can help prepare for a user s next action for example if we can predict what content a user is planning on viewing we can preload that content so it s immediately ready when they want it depending on the application and what data is available there are different types of machine learning algorithms to choose from i ll briefly cover each of the following supervised learning allows us to make predictions using correctly labeled data labeled data is a group of examples that has informative tags or outputs for example photos with associated hashtags or a house s features eq number of bedrooms location and its price by using supervised learning we can fit a line to the labelled data that either splits the data into categories or represents the trend of the data using this line we are able to make predictions on new data for example we can look at new photos and predict hashtags or look at a new house s features and predict its price if the output we are trying to predict is a list of tags or values we call it classification if the output we are trying to predict is a number we call it regression unsupervised learning is helpful when we have unlabeled data or we are not exactly sure what outputs like an image s hashtags or a house s price are meaningful instead we can identify patterns among unlabeled data for example we can identify related items on an e commerce website or recommend items to someone based on others who made similar purchases if the pattern is a group we call it a cluster if the pattern is a rule e q if this then that we call it an association reinforcement learning doesn t use an existing data set instead we create an agent to collect its own data through trial and error in an environment where it is reinforced with a reward for example an agent can learn to play mario by receiving a positive reward for collecting coins and a negative reward for walking into a goomba reinforcement learning is inspired by the way that humans learn and has turned out to be an effective way to teach computers specifically reinforcement has been effective at training computers to play games like go and dota understanding the problem you are trying to solve and the available data will constrain the types of machine learning you can use e q identifying objects in an image with supervised learning requires a labeled data set of images however constraints are the fruit of creativity in some cases you can set out to collect data that is not already available or consider other approaches even though machine learning is a science it comes with a margin of error it is important to consider how a user s experience might be impacted by this margin of error for example when an autonomous car fails to recognize its surroundings people can get hurt even though machine learning has never been as accessible as it is today it still requires additional resources developers and time to be integrated into a product this makes it important to think about whether the resulting impact justifies the amount of resources needed to implement we have barely covered the tip of the iceberg but hopefully at this point you feel more comfortable thinking about how machine learning can be applied to your product if you are interested in learning more about machine learning here are some helpful resources thanks for reading chat with me on twitter samueldrozdov from a quick cheer to a standing ovation clap to show how much you enjoyed this story digital product designer samueldrozdov com curated stories on user experience usability and product design by fabriciot and caioab
Conor Dewey,252,10,https://towardsdatascience.com/the-big-list-of-ds-ml-interview-resources-2db4f651bd63?source=---------6----------------,The Big List of DS/ML Interview Resources – Towards Data Science,data science interviews certainly aren t easy i know this first hand i ve participated in over individual interviews and phone screens while applying for competitive internships over the last calendar year through this exciting and somewhat at times very painful process i ve accumulated a plethora of useful resources that helped me prepare for and eventually pass data science interviews long story short i ve decided to sort through all my bookmarks and notes in order to deliver a comprehensive list of data science resources with this list by your side you should have more than enough effective tools at your disposal next time you re prepping for a big interview it s worth noting that many of these resources are naturally going to geared towards entry level and intern data science positions as that s where my expertise lies keep that in mind and enjoy here s some of the more general resources covering data science as a whole specifically i highly recommend checking out the first two links regarding data science interview questions while the ebook itself is a couple bucks out of pocket the answers themselves are free on quora these were some of my favorite full coverage questions to practice with right before an interview even data scientists cannot escape the dreaded algorithmic coding interview in my experience this isn t the case of the time but chances are you ll be asked to work through something similar to an easy or medium question on leetcode or hackerrank as far as language goes most companies will let you use whatever language you want personally i did almost all of my algorithmic coding in java even though the positions were targeted at python and r programmers if i had to recommend one thing it s to break out your wallet and invest in cracking the coding interview it absolutely lives up to the hype i plan to continue using it for years to come once the interviewer knows that you can think through problems and code effectively chances are that you ll move onto some more data science specific applications depending on the interviewer and the position you will likely be able to choose between python and r as your tool of choice since i m partial to python my resources below will primarily focus on effectively using pandas and numpy for data analysis a data science interview typically isn t complete without checking your knowledge of sql this can be done over the phone or through a live coding question more likely the latter i ve found that the difficulty level of these questions can vary a good bit ranging from being painfully easy to requiring complex joins and obscure functions our good friend statistics is still crucial for data scientists and it s reflected as such in interviews i had many interviews begin by seeing if i can explain a common statistics or probability concept in simple and concise terms as positions get more experienced i suspect this happens less and less as traditional statistical questions begin to take the more practical form of a b testing scenarios covered later in the post you ll notice that i ve compiled a few more resources here than in other sections this isn t a mistake machine learning is a complex field that is a virtual guarantee in data science interviews today the way that you ll be tested on this is no guarantee however it may come up as a conceptual question regarding cross validation or bias variance tradeoff or it may take the form of a take home assignment with a dataset attached i ve seen both several times so you ve got to be prepared for anything specifically check out the machine learning flashcards below they re only a couple bucks and were my by far my favorite way to quiz myself on any conceptual ml stuff this won t be covered in every single data science interview but it s certainly not uncommon most interviews will have atleast one section solely dedicated to product thinking which often lends itself to a b testing of some sort make sure your familiar with the concepts and statistical background necessary in order to be prepared when it comes up if you have time to spare i took the free online course by udacity and overall i was pretty impressed lastly i wanted to call out all of the posts related to data science jobs and interviewing that i read over and over again to understand not only how to prepare but what to expect as well if you only check out one section here this is the one to focus on this is the layer that sits on top of all the technical skills and application don t overlook it i hope you find these resources useful during your next interview or job search i know i did truthfully i m just glad that i saved these links somewhere lastly this post is part of an ongoing initiative to open source my experience applying and interviewing at data science positions so if you enjoyed this content then be sure to follow me for more stuff like this if you re interested in receiving my weekly rundown of interesting articles and resources focused on data science machine learning and artificial intelligence then subscribe to self driven data science using the form below if you enjoyed this post feel free to hit the clap button and if you re interested in posts to come make sure to follow me on medium at the link below i ll be writing and shipping every day this month as part of a day challenge this article was originally published on conordewey com from a quick cheer to a standing ovation clap to show how much you enjoyed this story data scientist writer www conordewey com sharing concepts ideas and codes
Abhishek Parbhakar,937,6,https://towardsdatascience.com/must-know-information-theory-concepts-in-deep-learning-ai-e54a5da9769d?source=---------7----------------,Must know Information Theory concepts in Deep Learning (AI),information theory is an important field that has made significant contribution to deep learning and ai and yet is unknown to many information theory can be seen as a sophisticated amalgamation of basic building blocks of deep learning calculus probability and statistics some examples of concepts in ai that come from information theory or related fields in the early th century scientists and engineers were struggling with the question how to quantify the information is there a analytical way or a mathematical measure that can tell us about the information content for example consider below two sentences it is not difficult to tell that the second sentence gives us more information since it also tells that bruno is big and brown in addition to being a dog how can we quantify the difference between two sentences can we have a mathematical measure that tells us how much more information second sentence have as compared to the first scientists were struggling with these questions semantics domain and form of data only added to the complexity of the problem then mathematician and engineer claude shannon came up with the idea of entropy that changed our world forever and marked the beginning of digital information age shannon proposed that the semantic aspects of data are irrelevant and nature and meaning of data doesn t matter when it comes to information content instead he quantified information in terms of probability distribution and uncertainty shannon also introduced the term bit that he humbly credited to his colleague john tukey this revolutionary idea not only laid the foundation of information theory but also opened new avenues for progress in fields like artificial intelligence below we discuss four popular widely used and must known information theoretic concepts in deep learning and data sciences also called information entropy or shannon entropy entropy gives a measure of uncertainty in an experiment let s consider two experiments if we compare the two experiments in exp it is easier to predict the outcome as compared to exp so we can say that exp is inherently more uncertain unpredictable than exp this uncertainty in the experiment is measured using entropy therefore if there is more inherent uncertainty in the experiment then it has higher entropy or lesser the experiment is predictable more is the entropy the probability distribution of experiment is used to calculate the entropy a deterministic experiment which is completely predictable say tossing a coin with p h has entropy zero an experiment which is completely random say rolling fair dice is least predictable has maximum uncertainty and has the highest entropy among such experiments another way to look at entropy is the average information gained when we observe outcomes of an random experiment the information gained for a outcome of an experiment is defined as a function of probability of occurrence of that outcome more the rarer is the outcome more is the information gained from observing it for example in an deterministic experiment we always know the outcome so no new information gained is here from observing the outcome and hence entropy is zero for a discrete random variable x with possible outcomes states x x n the entropy in unit of bits is defined as where p x i is the probability of i th outcome of x cross entropy is used to compare two probability distributions it tells us how similar two distributions are cross entropy between two probability distributions p and q defined over same set of outcomes is given by mutual information is a measure of mutual dependency between two probability distributions or random variables it tells us how much information about one variable is carried by the another variable mutual information captures dependency between random variables and is more generalized than vanilla correlation coefficient which captures only the linear relationship mutual information of two discrete random variables x and y is defined as where p x y is the joint probability distribution of x and y and p x and p y are the marginal probability distribution of x and y respectively also called relative entropy kl divergence is another measure to find similarities between two probability distributions it measures how much one distribution diverges from the other suppose we have some data and true distribution underlying it is p but we don t know this p so we choose a new distribution q to approximate this data since q is just an approximation it won t be able to approximate the data as good as p and some information loss will occur this information loss is given by kl divergence kl divergence between p and q tells us how much information we lose when we try to approximate data given by p with q kl divergence of a probability distribution q from another probability distribution p is defined as kl divergence is commonly used in unsupervised machine learning technique variational autoencoders information theory was originally formulated by mathematician and electrical engineer claude shannon in his seminal paper a mathematical theory of communication in note terms experiments random variable ai machine learning deep learning data science have been used loosely above but have technically different meanings in case you liked the article do follow me abhishek parbhakar for more articles related to ai philosophy and economics from a quick cheer to a standing ovation clap to show how much you enjoyed this story finding equilibria among ai philosophy and economics sharing concepts ideas and codes
Aman Dalmia,2300,17,https://blog.usejournal.com/what-i-learned-from-interviewing-at-multiple-ai-companies-and-start-ups-a9620415e4cc?source=---------8----------------,What I learned from interviewing at multiple AI companies and start-ups,over the past months i ve been interviewing at various companies like google s deepmind wadhwani institute of ai microsoft ola fractal analytics and a few others primarily for the roles data scientist software engineer research engineer in the process not only did i get an opportunity to interact with many great minds but also had a peek at myself along with a sense of what people really look for when interviewing someone i believe that if i d had this knowledge before i could have avoided many mistakes and have prepared in a much better manner which is what the motivation behind this post is to be able to help someone bag their dream place of work this post arose from a discussion with one of my juniors on the lack of really fulfilling job opportunities offered through campus placements for people working in ai also when i was preparing i noticed people using a lot of resources but as per my experience over the past months i realised that one can do away with a few minimal ones for most roles in ai all of which i m going to mention at the end of the post i begin with how to get noticed a k a the interview then i provide a list of companies and start ups to apply which is followed by how to ace that interview based on whatever experience i ve had i add a section on what we should strive to work for i conclude with minimal resources you need for preparation note for people who are sitting for campus placements there are two things i d like to add firstly most of what i m going to say except for the last one maybe is not going to be relevant to you for placements but and this is my second point as i mentioned before opportunities on campus are mostly in software engineering roles having no intersection with ai so this post is specifically meant for people who want to work on solving interesting problems using ai also i want to add that i haven t cleared all of these interviews but i guess that s the essence of failure it s the greatest teacher the things that i mention here may not all be useful but these are things that i did and there s no way for me to know what might have ended up making my case stronger to be honest this step is the most important one what makes off campus placements so tough and exhausting is getting the recruiter to actually go through your profile among the plethora of applications that they get having a contact inside the organisation place a referral for you would make it quite easy but in general this part can be sub divided into three keys steps a do the regulatory preparation and do that well so with regulatory preparation i mean a linkedin profile a github profile a portfolio website and a well polished cv firstly your cv should be really neat and concise follow this guide by udacity for cleaning up your cv resume revamp it has everything that i intend to say and i ve been using it as a reference guide myself as for the cv template some of the in built formats on overleaf are quite nice i personally use deedy resume here s a preview as it can be seen a lot of content can be fit into one page however if you really do need more than that then the format linked above would not work directly instead you can find a modified multi page format of the same here the next most important thing to mention is your github profile a lot of people underestimate the potential of this just because unlike linkedin it doesn t have a who viewed your profile option people do go through your github because that s the only way they have to validate what you have mentioned in your cv given that there s a lot of noise today with people associating all kinds of buzzwords with their profile especially for data science open source has a big role to play too with majority of the tools implementations of various algorithms lists of learning resources all being open sourced i discuss the benefits of getting involved in open source and how one can start from scratch in an earlier post here the bare minimum for now should be create a github account if you don t already have one create a repository for each of the projects that you have done add documentation with clear instructions on how to run the code add documentation for each file mentioning the role of each function the meaning of each parameter proper formatting e g pep for python along with a script to automate the previous step optional moving on the third step is what most people lack which is having a portfolio website demonstrating their experience and personal projects making a portfolio indicates that you are really serious about getting into the field and adds a lot of points to the authenticity factor also you generally have space constraints on your cv and tend to miss out on a lot of details you can use your portfolio to really delve deep into the details if you want to and it s highly recommended to include some sort of visualisation or demonstration of the project idea it s really easy to create one too as there are a lot of free platforms with drag and drop features making the process really painless i personally use weebly which is a widely used tool it s better to have a reference to begin with there are a lot of awesome ones out there but i referred to deshraj yadav s personal website to begin with making mine finally a lot of recruiters and start ups have nowadays started using linkedin as their go to platform for hiring a lot of good jobs get posted there apart from recruiters the people working at influential positions are quite active there as well so if you can grab their attention you have a good chance of getting in too apart from that maintaining a clean profile is necessary for people to have the will to connect with you an important part of linkedin is their search tool and for you to show up you must have the relevant keywords interspersed over your profile it took me a lot of iterations and re evaluations to finally have a decent one also you should definitely ask people with or under whom you ve worked with to endorse you for your skills and add a recommendation talking about their experience of working with you all of this increases your chance of actually getting noticed i ll again point towards udacity s guide for linkedin and github profiles all this might seem like a lot but remember that you don t need to do it in a single day or even a week or a month it s a process it never ends setting up everything at first would definitely take some effort but once it s there and you keep updating it regularly as events around you keep happening you ll not only find it to be quite easy but also you ll be able to talk about yourself anywhere anytime without having to explicitly prepare for it because you become so aware about yourself b stay authentic i ve seen a lot of people do this mistake of presenting themselves as per different job profiles according to me it s always better to first decide what actually interests you what would you be happy doing and then search for relevant opportunities not the other way round the fact that the demand for ai talent surpasses the supply for the same gives you this opportunity spending time on your regulatory preparation mentioned above would give you an all around perspective on yourself and help make this decision easier also you won t need to prepare answers to various kinds of questions that you get asked during an interview most of them would come out naturally as you d be talking about something you really care about c networking once you re done with a figured out b networking is what will actually help you get there if you don t talk to people you miss out on hearing about many opportunities that you might have a good shot at it s important to keep connecting with new people each day if not physically then on linkedin so that upon compounding it after many days you have a large and strong network networking is not messaging people to place a referral for you when i was starting off i did this mistake way too often until i stumbled upon this excellent article by mark meloon where he talks about the importance of building a real connection with people by offering our help first another important step in networking is to get your content out for example if you re good at something blog about it and share that blog on facebook and linkedin not only does this help others it helps you as well once you have a good enough network your visibility increases multi fold you never know how one person from your network liking or commenting on your posts may help you reach out to a much broader audience including people who might be looking for someone of your expertise i m presenting this list in alphabetical order to avoid the misinterpretation of any specific preference however i do place a on the ones that i d personally recommend this recommendation is based on either of the following mission statement people personal interaction or scope of learning more than is purely based on the nd and rd factors your interview begins the moment you have entered the room and a lot of things can happen between that moment and the time when you re asked to introduce yourself your body language and the fact that you re smiling while greeting them plays a big role especially when you re interviewing for a start up as culture fit is something that they extremely care about you need to understand that as much as the interviewer is a stranger to you you re a stranger to him her too so they re probably just as nervous as you are it s important to view the interview as more of a conversation between yourself and the interviewer both of you are looking for a mutual fit you are looking for an awesome place to work at and the interviewer is looking for an awesome person like you to work with so make sure that you re feeling good about yourself and that you take the charge of making the initial moments of your conversation pleasant for them and the easiest way i know how to make that happen is to smile there are mostly two types of interviews one where the interviewer has come with come prepared set of questions and is going to just ask you just that irrespective of your profile and the second where the interview is based on your cv i ll start with the second one this kind of interview generally begins with a can you tell me a bit about yourself at this point things are a big no talking about your gpa in college and talking about your projects in detail an ideal statement should be about a minute or two long should give a good idea on what have you been doing till now and it s not restricted to academics you can talk about your hobbies like reading books playing sports meditation etc basically anything that contributes to defining you the interviewer will then take something that you talk about here as a cue for his next question and then the technical part of the interview begins the motive of this kind of interview is to really check whether whatever you have written on your cv is true or not there would be a lot of questions on what could be done differently or if x was used instead of y what would have happened at this point it s important to know the kind of trade offs that is usually made during implementation for e g if the interviewer says that using a more complex model would have given better results then you might say that you actually had less data to work with and that would have lead to overfitting in one of the interviews i was given a case study to work on and it involved designing algorithms for a real world use case i ve noticed that once i ve been given the green flag to talk about a project the interviewers really like it when i talk about it in the following flow problem or previous approaches our approach result intuition the other kind of interview is really just to test your basic knowledge don t expect those questions to be too hard but they would definitely scratch every bit of the basics that you should be having mainly based around linear algebra probability statistics optimisation machine learning and or deep learning the resources mentioned in the minimal resources you need for preparation section should suffice but make sure that you don t miss out one bit among them the catch here is the amount of time you take to answer those questions since these cover the basics they expect that you should be answering them almost instantly so do your preparation accordingly throughout the process it s important to be confident and honest about what you know and what you don t know if there s a question that you re certain you have no idea about say it upfront rather than making aah um sounds if some concept is really important but you are struggling with answering it the interviewer would generally depending on how you did in the initial parts be happy to give you a hint or guide you towards the right solution it s a big plus if you manage to pick their hints and arrive at the correct solution try to not get nervous and the best way to avoid that is by again smiling now we come to the conclusion of the interview where the interviewer would ask you if you have any questions for them it s really easy to think that your interview is done and just say that you have nothing to ask i know many people who got rejected just because of failing at this last question as i mentioned before it s not only you who is being interviewed you are also looking for a mutual fit with the company itself so it s quite obvious that if you really want to join a place you must have many questions regarding the work culture there or what kind of role are they seeing you in it can be as simple as being curious about the person interviewing you there s always something to learn from everything around you and you should make sure that you leave the interviewer with the impression that you re truly interested in being a part of their team a final question that i ve started asking all my interviewers is for a feedback on what they might want me to improve on this has helped me tremendously and i still remember every feedback that i ve gotten which i ve incorporated into my daily life that s it based on my experience if you re just honest about yourself are competent truly care about the company you re interviewing for and have the right mindset you should have ticked all the right boxes and should be getting a congratulatory mail soon we live in an era full of opportunities and that applies to anything that you love you just need to strive to become the best at it and you will find a way to monetise it as gary vaynerchuk just follow him already says this is a great time to be working in ai and if you re truly passionate about it you have so much that you can do with ai you can empower so many people that have always been under represented we keep nagging about the problems surrounding us but there s been never such a time where common people like us can actually do something about those problems rather than just complaining jeffrey hammerbacher founder cloudera had famously said we can do so much with ai than we can ever imagine there are many extremely challenging problems out there which require incredibly smart people like you to put your head down on and solve you can make many lives better time to let go of what is cool or what would look good think and choose wisely any data science interview comprises of questions mostly of a subset of the following four categories computer science math statistics and machine learning if you re not familiar with the math behind deep learning then you should consider going over my last post for resources to understand them however if you are comfortable i ve found that the chapters and of the deep learning book are enough to prepare revise for theoretical questions during such interviews i ve been preparing summaries for a few chapters which you can refer to where i ve tried to even explain a few concepts that i found challenging to understand at first in case you are not willing to go through the entire chapters and if you ve already done a course on probability you should be comfortable answering a few numerical as well for stats covering these topics should be enough now the range of questions here can vary depending on the type of position you are applying for if it s a more traditional machine learning based interview where they want to check your basic knowledge in ml you can complete any one of the following courses machine learning by andrew ng cs machine learning course by caltech professor yaser abu mostafa important topics are supervised learning classification regression svm decision tree random forests logistic regression multi layer perceptron parameter estimation bayes decision rule unsupervised learning k means clustering gaussian mixture models dimensionality reduction pca now if you re applying for a more advanced position there s a high chance that you might be questioned on deep learning in that case you should be very comfortable with convolutional neural networks cnns and or depending upon what you ve worked on recurrent neural networks rnns and their variants and by being comfortable you must know what is the fundamental idea behind deep learning how cnns rnns actually worked what kind of architectures have been proposed and what has been the motivation behind those architectural changes now there s no shortcut for this either you understand them or you put enough time to understand them for cnns the recommended resource is stanford s cs n and cs n for rnns i found this neural network class by hugo larochelle to be really enlightening too refer this for a quick refresher too udacity coming to the aid here too by now you should have figured out that udacity is a really important place for an ml practitioner there are not a lot of places working on reinforcement learning rl in india and i too am not experienced in rl as of now so that s one thing to add to this post sometime in the future getting placed off campus is a long journey of self realisation i realise that this has been another long post and i m again extremely grateful to you for valuing my thoughts i hope that this post finds a way of being useful to you and that it helped you in some way to prepare for your next data science interview better if it did i request you to really think about what i talk about in what we should strive to work for i m very thankful to my friends from iit guwahati for their helpful feedback especially ameya godbole kothapalli vignesh and prabal jain a majority of what i mention here like viewing an interview as a conversation and seeking feedback from our interviewers arose from multiple discussions with prabal who has been advising me constantly on how i can improve my interviewing skills this story is published in noteworthy where thousands come every day to learn about the people ideas shaping the products we love follow our publication to see more product design stories featured by the journal team from a quick cheer to a standing ovation clap to show how much you enjoyed this story ai fanatic math lover dreamer the official journal blog
Lance Ulanoff,15100,5,https://medium.com/@LanceUlanoff/did-google-duplex-just-pass-the-turing-test-ffcfe6868b02?source=---------9----------------,Did Google Duplex just pass the Turing Test? – Lance Ulanoff – Medium,i think it was the first um that was the moment when i realized i was hearing something extraordinary a computer carrying out a completely natural and very human sounding conversation with a real person and it wasn t just a random talk this conversation had a purpose a destination to make an appointment at a hair salon the entity making the call and appointment was google assistant running duplex google s still experimental ai voice system and the venue was google i o google s yearly developer conference which this year focused heavily on the latest developments in ai machine and deep learning google ceo sundar pichai explained that what we were hearing was a real phone call made to a hair salon that didn t know it was part of an experiment or that they were talking to a computer he launched duplex by asking google assistant to book a haircut appointment for tuesday morning the ai did the rest duplex made the call and when someone at the salon picked up the voice ai started the conversation with hi i m calling to book a woman s hair cut appointment for a client um i m looking for something on may third when the attendant asked duplex to give her one second duplex responded with mmm hmm the conversation continued as the salon representative presented various dates and times and the ai asked about other options eventually the ai and the salon worker agreed on an appointment date and time what i heard was so convincing i had trouble discerning who was the salon worker and who what was the duplex ai it was stunning and somewhat disconcerting i liken it to the feeling you d get if a store mannequin suddenly smiled at you it was easily the most remarkable human computer conversation i d ever heard and the closest thing i ve seen a voice ai passing the turing test which is the ai threshold suggested by computer scientist alan turing in the s turing posited that by computers would be able to fool humans into thinking they were conversing with other humans at least of the time he was right in a chatbot named eugene goostman successfully impersonated a wise ass year old programmer during lengthy text based chats with unsuspecting humans turing however hadn t necessarily considered voice based systems and for obvious reasons talking computers are somewhat less adept at fooling humans spend a few minutes conversing with your voice assistant of choice and you ll soon discover their limitations their speech can be stilted pronunciations off and response times can be slow especially if they re trying to access a cloud based server and forget about conversations most can handle two consecutive queries at most and they virtually all require a trigger phrase like alexa or hey siri google is working on removing unnecessary okay googles in short back and forth convos with the digital assistant google assistant running duplex didn t exhibit any of those short comings it sounded like a young female assistant carefully scheduling her boss s haircut in addition to the natural cadence google added speech disfluencies the verbal ticks ums uhs and mm hmms and latency or pauses that naturally occur when people are speaking the result is a perfectly human voice produced entirely by a computer the second call demonstration where a male voiced duplex tried to make restaurant reservations was even more remarkable the human call participant didn t entirely understand duplex s verbal requests and then told duplex that for the number of people it wanted to bring to the restaurant they didn t need a reservation duplex handled all this without missing a beat the amazing thing is that the assistant can actually understand the nuances of conversation said pichai during the keynote that ability comes by way of neural network technology and intensive machine learning for as accomplished as duplex is in making hair appointments and restaurant reservations it might stumble in deeper or more abstract conversations in a blog post on duplex development google engineers explained that they constrained duplex s training to closed domains or well defined topics like dinner reservations and hair appointments this gave them the ability to perform intense exploration of the topics and focus training duplex was guided during training within the domain by experienced operators who could keep track of mistakes and worked with engineers to improve responses in short this means that while duplex has your hair and dining out options covered it could stumble in movie reservations and negotiations with your cable provider even so duplex fooled two humans i heard no hesitation or confusion in the hair salon call there was no indication that the salon worker thought something was amiss she wanted to help this young woman make an appointment what will she think when she learns she was duped by duplex obviously duplex s conversations were also short each lasting less than a minute putting them well short of the turing test benchmark i would ve enjoyed hearing the conversations devolve as they extended a few minutes or more i m sure duplex will soon tackle more domains and longer conversations and it will someday pass the turing test it s only a matter of time before duplex is handling other mundane or difficult calls for us like calling our parents with our own voices see wavenet technology eventually we ll have our duplex voices call each other handling pleasantries and making plans which google assistant can then drop in our google calendar but that s the future for now duplex s performance stands as a powerful proof of concept for our long imagined future of conversational ai s capable of helping entertaining and engaging with us it s the first major step on the path to the ai depicted in the movie her where joaquin phoenix starred as a man who falls in love with his chatty voice assistant played by the disembodied voice of scarlett johansson so no duplex didn t pass the turing test but i do wonder what alan turing would think of it from a quick cheer to a standing ovation clap to show how much you enjoyed this story tech expert journalist social media commentator amateur cartoonist and robotics fan
Sophia Arakelyan,7,4,https://buzzrobot.com/from-ballerina-to-ai-researcher-part-i-46fce67f809b?source=---------2----------------,From Ballerina to AI Researcher: Part I – buZZrobot,last year i published the article from ballerina to ai writer where i described how i embraced the technical part of ai without a technical background but having love and passion for ai i educated myself and was able to build a neural net classifier and do projects in deep rl recently i ve become a participant in the openai scholarship program openai is a non profit that gathers top ai researchers to ensure the safety of ai to benefit humanity every week for the next three months i ll publish blog posts sharing my story of transformation from a person dedicated to years of professional dancing and then writing about tech and ai to actually conducting ai research finding your true calling the key component of happiness my primary goal with the series of blog posts from ballerina to ai researcher is to show that it s never too late to embrace a new field start over again and find your true calling finding work you love is one of the most important components of happiness something that you do every day and invest your time in to grow that makes you feel fulfilled gives you energy something that is a refuge for your soul great things never come easy we have to be able to fight to make great things happen but you can t fight for something you don t believe in especially if you don t feel like it s really important for you and humanity finding that thing is a real challenge i feel lucky that i found my true passion ai to me the technology itself and the ai community researchers scientists people who dedicate their lives to building the most powerful technology of all time with the mission to benefit humanity and make it safe for us is a great source of energy the structure of the blog post series today i m giving an overall intro of what i m going to cover in my from ballerina to ai researcher series i ll dedicate the sequence of blog posts during the openai scholars program to several aspects of ai technology i ll cover those areas that concern me a lot like ai and automation bias in ml dual use of ai etc also the structure of my posts will include some insights on what i m working on right now the final technical project will be available by the end of august and will be open sourced i feel very lucky to have alec radford an experienced researcher as my mentor who guides me in the nlp and nlu research area first week of my scholarship i ve dedicated my first week within the program to learning about the transformer architecture that performs much better on sequential data compared to rnns lstms the novelty of the architecture is its multi head self attention mechanism according to the original paper experiments with the transformer on two machine translation tasks showed the model to be superior in quality while being more parallelizable and requiring significantly less time to train more concretely when rnns or cnns take a sequence as an input it goes through sentences word by word which is a huge obstacle toward parallelization of the process takes more time to train models moreover if sequences are too long the model tends to forget the content of distant positions in sequence or mixes it with the following positions content this is the fundamental problem in dealing with sequential data the transformer architecture reduced this problem thanks to the multi head self attention mechanism i digged into rnn lstm models to catch up with the background information to that end i ve found andrew ng s course on deep learning along with the papers extremely useful to develop insights regarding the transformer i went through the following resources the video by ukasz kaiser from google brain one of the model s creators a blog post with very well elaborated content re the model ran the code tensor tensor and the code using the pytorch framework from this paper to feel the difference between the tf and pytorch frameworks overall the goal within the program is to develop deep comprehension of the nlu research area challenges current state of the art and to formulate and test hypotheses that tackle the most important problems of the field i ll share more on what i m working on in my future articles meanwhile if you have questions feedback please leave a comment if you want to learn more about me here are my facebook and twitter accounts i d appreciate your feedback on my posts such as what topics are most interesting to you that i should consider further coverage on from a quick cheer to a standing ovation clap to show how much you enjoyed this story former ballerina turned ai writer fan of sci fi astrophysics consciousness is the key founder of buzzrobot com the publication aims to cover practical aspects of ai technology use cases along with interviews with notable people in the ai field
Matt Schlicht,5000,11,https://chatbotsmagazine.com/the-complete-beginner-s-guide-to-chatbots-8280b7b906ca?source=tag_archive---------3----------------,The Complete Beginner’s Guide To Chatbots – Chatbots Magazine,what are chatbots why are they such a big opportunity how do they work how can i build one how can i meet other people interested in chatbots these are the questions we re going to answer for you right now ready let s do this do you work in ecommerce stop reading and click here we made something for you p s here is where i believe the future of bots is headed you will probably disagree with me at first p p s my newest guide about conversational commerce is up i think you ll find it super interesting a chatbot is a service powered by rules and sometimes artificial intelligence that you interact with via a chat interface the service could be any number of things ranging from functional to fun and it could live in any major chat product facebook messenger slack telegram text messages etc if you haven t wrapped your head around it yet don t worry here s an example to help you visualize a chatbot if you wanted to buy shoes from nordstrom online you would go to their website look around until you find the shoes you wanted and then you would purchase them if nordstrom makes a bot which i m sure they will you would simply be able to message nordstrom on facebook it would ask you what you re looking for and you would simply tell it instead of browsing a website you will have a conversation with the nordstrom bot mirroring the type of experience you would get when you go into the retail store watch this video from facebook s recent f conference where they make their major announcements at the mark david marcus the vice president of messaging products at facebook explains what it looks like to buy shoes in a facebook messenger bot buying shoes isn t the only thing chatbots can be used for here are a couple of other examples see with bots the possibilities are endless you can build anything imaginable and i encourage you to do just that but why make a bot sure it looks cool it s using some super advanced technology but why should someone spend their time and energy on it it s a huge opportunity huge scroll down and i ll explain you are probably wondering why does anyone care about chatbots they look like simple text based services what s the big deal great question i ll tell you why people care about chatbots it s because for the first time ever people are using messenger apps more than they are using social networks let that sink in for a second people are using messenger apps more than they are using social networks so logically if you want to build a business online you want to build where the people are that place is now inside messenger apps this is why chatbots are such a big deal it s potentially a huge business opportunity for anyone willing to jump headfirst and build something people want but how do these bots work how do they know how to talk to people and answer questions isn t that artificial intelligence and isn t that insanely hard to do yes you are correct it is artificial intelligence but it s something that you can totally do yourself let me explain there are two types of chatbots one functions based on a set of rules and the other more advanced version uses machine learning what does this mean chatbot that functions based on rules chatbot that functions using machine learning bots are created with a purpose a store will likely want to create a bot that helps you purchase something where someone like comcast might create a bot that can answer customer support questions you start to interact with a chatbot by sending it a message click here to try sending a message to the cnn chatbot on facebook so if these bots use artificial intelligence to make them work well isn t that really hard to do don t i need to be an expert at artificial intelligence to be able to build something that has artificial intelligence short answer no you don t have to be an expert at artificial intelligence to create an awesome chatbot that has artificial intelligence just make sure to not over promise on your application s abilities if you can t make the product good with artificial intelligence right now it might be best to not put it in yet however over the past decade quite a bit of advancements have been made in the area of artificial intelligence so much in fact that anyone who knows how to code can incorporate some level of artificial intelligence into their products how do you build artificial intelligence into your bot don t worry i ve got you covered i ll tell you how to do it in the next section of this post building a chatbot can sound daunting but it s totally doable you ll be creating an artificial intelligence powered chatting machine in no time or of course you can always build a basic chat bot that doesn t have a fancy ai brain and strictly follows rules you will need to figure out what problem you are going to solve with your bot choose which platform your bot will live on facebook slack etc set up a server to run your bot from and choose which service you will use to build your bot here are a ton of resources to get you started platform documentation other resources don t want to build your own now that you ve got your chatbot and artificial intelligence resources maybe it s time you met other people who are also interested in chatbots chatbots have been around for decades but because of the recent advancements in artificial intelligence and machine learning there is a big opportunity for people to create bots that are better faster and stronger if you re reading this you probably fall into one of these categories wouldn t it be awesome if you had a place to meet learn and share information with other people interested in chatbots yeah we thought so too that s why i created a forum called chatbot news and it has quickly become the largest community related to chatbots the members of the chatbots group are investors who manage well over billion in capital employees at facebook instagram fitbit nike and ycombinator companies and hackers from around the world we would love if you joined click here to request an invite private chatbots community i have also created the silicon valley chatbots meetup register here to be notified when we schedule our first event from a quick cheer to a standing ovation clap to show how much you enjoyed this story ceo of octane ai founder of chatbots magazine yc alum forbes under product at ustream for years sold for mil did digital for lil wayne chatbots ai nlp facebook messenger slack telegram and more
Gil Fewster,3300,5,https://medium.freecodecamp.org/the-mind-blowing-ai-announcement-from-google-that-you-probably-missed-2ffd31334805?source=tag_archive---------4----------------,The mind-blowing AI announcement from Google that you probably missed.,disclaimer i m not an expert in neural networks or machine learning since originally writing this article many people with far more expertise in these fields than myself have indicated that while impressive what google have achieved is evolutionary not revolutionary in the very least it s fair to say that i m guilty of anthropomorphising in parts of the text i ve left the article s content unchanged because i think it s interesting to compare the gut reaction i had with the subsequent comments of experts in the field i strongly encourage readers to browse the comments after reading the article for some perspectives more sober and informed than my own in the closing weeks of google published an article that quietly sailed under most people s radars which is a shame because it may just be the most astonishing article about machine learning that i read last year don t feel bad if you missed it not only was the article competing with the pre christmas rush that most of us were navigating it was also tucked away on google s research blog beneath the geektastic headline zero shot translation with google s multilingual neural machine translation system this doesn t exactly scream must read does it especially when you ve got projects to wind up gifts to buy and family feuds to be resolved all while the advent calendar relentlessly counts down the days until christmas like some kind of chocolate filled yuletide doomsday clock luckily i m here to bring you up to speed here s the deal up until september of last year google translate used phrase based translation it basically did the same thing you and i do when we look up key words and phrases in our lonely planet language guides it s effective enough and blisteringly fast compared to awkwardly thumbing your way through a bunch of pages looking for the french equivalent of please bring me all of your cheese and don t stop until i fall over but it lacks nuance phrase based translation is a blunt instrument it does the job well enough to get by but mapping roughly equivalent words and phrases without an understanding of linguistic structures can only produce crude results this approach is also limited by the extent of an available vocabulary phrase based translation has no capacity to make educated guesses at words it doesn t recognize and can t learn from new input all that changed in september when google gave their translation tool a new engine the google neural machine translation system gnmt this new engine comes fully loaded with all the hot buzzwords like neural network and machine learning the short version is that google translate got smart it developed the ability to learn from the people who used it it learned how to make educated guesses about the content tone and meaning of phrases based on the context of other words and phrases around them and here s the bit that should make your brain explode it got creative google translate invented its own language to help it translate more effectively what s more nobody told it to it didn t develop a language or interlingua as google call it because it was coded to it developed a new language because the software determined over time that this was the most efficient way to solve the problem of translation stop and think about that for a moment let it sink in a neural computing system designed to translate content from one human language into another developed its own internal language to make the task more efficient without being told to do so in a matter of weeks i ve added a correction retraction of this paragraph in the notes to understand what s going on we need to understand what zero shot translation capability is here s google s mike schuster nikhil thorat and melvin johnson from the original blog post here you can see an advantage of google s new neural machine over the old phrase based approach the gmnt is able to learn how to translate between two languages without being explicitly taught this wouldn t be possible in a phrase based model where translation is dependent upon an explicit dictionary to map words and phrases between each pair of languages being translated and this leads the google engineers onto that truly astonishing discovery of creation so there you have it in the last weeks of as journos around the world started penning their was this the worst year in living memory thinkpieces google engineers were quietly documenting a genuinely astonishing breakthrough in software engineering and linguistics i just thought maybe you d want to know ok to really understand what s going on we probably need multiple computer science and linguistics degrees i m just barely scraping the surface here if you ve got time to get a few degrees or if you ve already got them please drop me a line and explain it all me to slowly update in my excitement it s fair to say that i ve exaggerated the idea of this as an intelligent system at least so far as we would think about human intelligence and decision making make sure you read chris mcdonald s comment after the article for a more sober perspective update nafrondel s excellent detailed reply is also a must read for an expert explanation of how neural networks function from a quick cheer to a standing ovation clap to show how much you enjoyed this story a tinkerer our community publishes stories worth reading on development design and data science
Adam Geitgey,10400,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3?source=tag_archive---------5----------------,Machine Learning is Fun! Part 2 – Adam Geitgey – Medium,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in italiano espan ol franc ais tu rkc e portugue s tie ng vie t or in part we said that machine learning is using generic algorithms to tell you something interesting about your data without writing any code specific to the problem you are solving if you haven t already read part read it now this time we are going to see one of these generic algorithms do something really cool create video game levels that look like they were made by humans we ll build a neural network feed it existing super mario levels and watch new ones pop out just like part this guide is for anyone who is curious about machine learning but has no idea where to start the goal is be accessible to anyone which means that there s a lot of generalizations and we skip lots of details but who cares if this gets anyone more interested in ml then mission accomplished back in part we created a simple algorithm that estimated the value of a house based on its attributes given data about a house like this we ended up with this simple estimation function in other words we estimated the value of the house by multiplying each of its attributes by a weight then we just added those numbers up to get the house s value instead of using code let s represent that same function as a simple diagram however this algorithm only works for simple problems where the result has a linear relationship with the input what if the truth behind house prices isn t so simple for example maybe the neighborhood matters a lot for big houses and small houses but doesn t matter at all for medium sized houses how could we capture that kind of complicated detail in our model to be more clever we could run this algorithm multiple times with different of weights that each capture different edge cases now we have four different price estimates let s combine those four price estimates into one final estimate we ll run them through the same algorithm again but using another set of weights our new super answer combines the estimates from our four different attempts to solve the problem because of this it can model more cases than we could capture in one simple model let s combine our four attempts to guess into one big diagram this is a neural network each node knows how to take in a set of inputs apply weights to them and calculate an output value by chaining together lots of these nodes we can model complex functions there s a lot that i m skipping over to keep this brief including feature scaling and the activation function but the most important part is that these basic ideas click it s just like lego we can t model much with one single lego block but we can model anything if we have enough basic lego blocks to stick together the neural network we ve seen always returns the same answer when you give it the same inputs it has no memory in programming terms it s a stateless algorithm in many cases like estimating the price of house that s exactly what you want but the one thing this kind of model can t do is respond to patterns in data over time imagine i handed you a keyboard and asked you to write a story but before you start my job is to guess the very first letter that you will type what letter should i guess i can use my knowledge of english to increase my odds of guessing the right letter for example you will probably type a letter that is common at the beginning of words if i looked at stories you wrote in the past i could narrow it down further based on the words you usually use at the beginning of your stories once i had all that data i could use it to build a neural network to model how likely it is that you would start with any given letter our model might look like this but let s make the problem harder let s say i need to guess the next letter you are going to type at any point in your story this is a much more interesting problem let s use the first few words of ernest hemingway s the sun also rises as an example what letter is going to come next you probably guessed n the word is probably going to be boxing we know this based on the letters we ve already seen in the sentence and our knowledge of common words in english also the word middleweight gives us an extra clue that we are talking about boxing in other words it s easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of english to solve this problem with a neural network we need to add state to our model each time we ask our neural network for an answer we also save a set of our intermediate calculations and re use them the next time as part of our input that way our model will adjust its predictions based on the input that it has seen recently keeping track of state in our model makes it possible to not just predict the most likely first letter in the story but to predict the most likely next letter given all previous letters this is the basic idea of a recurrent neural network we are updating the network each time we use it this allows it to update its predictions based on what it saw most recently it can even model patterns over time as long as we give it enough of a memory predicting the next letter in a story might seem pretty useless what s the point one cool use might be auto predict for a mobile phone keyboard but what if we took this idea to the extreme what if we asked the model to predict the next most likely character over and over forever we d be asking it to write a complete story for us we saw how we could guess the next letter in hemingway s sentence let s try generating a whole story in the style of hemingway to do this we are going to use the recurrent neural network implementation that andrej karpathy wrote andrej is a deep learning researcher at stanford and he wrote an excellent introduction to generating text with rnns you can view all the code for the model on github we ll create our model from the complete text of the sun also rises characters using unique letters including punctuation uppercase lowercase etc this data set is actually really small compared to typical real world applications to generate a really good model of hemingway s style it would be much better to have at several times as much sample text but this is good enough to play around with as an example as we just start to train the rnn it s not very good at predicting letters here s what it generates after a loops of training you can see that it has figured out that sometimes words have spaces between them but that s about it after about iterations things are looking more promising the model has started to identify the patterns in basic sentence structure it s adding periods at the ends of sentences and even quoting dialog a few words are recognizable but there s also still a lot of nonsense but after several thousand more training iterations it looks pretty good at this point the algorithm has captured the basic pattern of hemingway s short direct dialog a few sentences even sort of make sense compare that with some real text from the book even by only looking for patterns one character at a time our algorithm has reproduced plausible looking prose with proper formatting that is kind of amazing we don t have to generate text completely from scratch either we can seed the algorithm by supplying the first few letters and just let it find the next few letters for fun let s make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of er he and the s not bad but the really mind blowing part is that this algorithm can figure out patterns in any sequence of data it can easily generate real looking recipes or fake obama speeches but why limit ourselves human language we can apply this same idea to any kind of sequential data that has a pattern in nintendo released super mario makertm for the wii u gaming system this game lets you draw out your own super mario brothers levels on the gamepad and then upload them to the internet so you friends can play through them you can include all the classic power ups and enemies from the original mario games in your levels it s like a virtual lego set for people who grew up playing super mario brothers can we use the same model that generated fake hemingway text to generate fake super mario brothers levels first we need a data set for training our model let s take all the outdoor levels from the original super mario brothers game released in this game has levels and about of them have the same outdoor style so we ll stick to those to get the designs for each level i took an original copy of the game and wrote a program to pull the level designs out of the game s memory super mario bros is a year old game and there are lots of resources online that help you figure out how the levels were stored in the game s memory extracting level data from an old video game is a fun programming exercise that you should try sometime here s the first level from the game which you probably remember if you ever played it if we look closely we can see the level is made of a simple grid of objects we could just as easily represent this grid as a sequence of characters with one character representing each object we ve replaced each object in the level with a letter and so on using a different letter for each different kind of object in the level i ended up with text files that looked like this looking at the text file you can see that mario levels don t really have much of a pattern if you read them line by line the patterns in a level really emerge when you think of the level as a series of columns so in order for the algorithm to find the patterns in our data we need to feed the data in column by column figuring out the most effective representation of your input data called feature selection is one of the keys of using machine learning algorithms well to train the model i needed to rotate my text files by degrees this made sure the characters were fed into the model in an order where a pattern would more easily show up just like we saw when creating the model of hemingway s prose a model improves as we train it after a little training our model is generating junk it sort of has an idea that s and s should show up a lot but that s about it it hasn t figured out the pattern yet after several thousand iterations it s starting to look like something the model has almost figured out that each line should be the same length it has even started to figure out some of the logic of mario the pipes in mario are always two blocks wide and at least two blocks high so the p s in the data should appear in x clusters that s pretty cool with a lot more training the model gets to the point where it generates perfectly valid data let s sample an entire level s worth of data from our model and rotate it back horizontal this data looks great there are several awesome things to notice finally let s take this level and recreate it in super mario maker play it yourself if you have super mario maker you can play this level by bookmarking it online or by looking it up using level code ac f c the recurrent neural network algorithm we used to train our model is the same kind of algorithm used by real world companies to solve hard problems like speech detection and language translation what makes our model a toy instead of cutting edge is that our model is generated from very little data there just aren t enough levels in the original super mario brothers game to provide enough data for a really good model if we could get access to the hundreds of thousands of user created super mario maker levels that nintendo has we could make an amazing model but we can t because nintendo won t let us have them big companies don t give away their data for free as machine learning becomes more important in more industries the difference between a good program and a bad program will be how much data you have to train your models that s why companies like google and facebook need your data so badly for example google recently open sourced tensorflow its software toolkit for building large scale machine learning applications it was a pretty big deal that google gave away such important capable technology for free this is the same stuff that powers google translate but without google s massive trove of data in every language you can t create a competitor to google translate data is what gives google its edge think about that the next time you open up your google maps location history or facebook location history and notice that it stores every place you ve ever been in machine learning there s never a single way to solve a problem you have limitless options when deciding how to pre process your data and which algorithms to use often combining multiple approaches will give you better results than any single approach readers have sent me links to other interesting approaches to generating super mario levels if you liked this article please consider signing up for my machine learning is fun email list i ll only email you when i have something new and awesome to share it s the best way to find out when i write more articles like this you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
David Venturi,10600,20,https://medium.freecodecamp.org/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0?source=tag_archive---------6----------------,"Every single Machine Learning course on the internet, ranked by your reviews",a year and a half ago i dropped out of one of the best computer science programs in canada i started creating my own data science master s program using online resources i realized that i could learn everything i needed through edx coursera and udacity instead and i could learn it faster more efficiently and for a fraction of the cost i m almost finished now i ve taken many data science related courses and audited portions of many more i know the options out there and what skills are needed for learners preparing for a data analyst or data scientist role so i started creating a review driven guide that recommends the best courses for each subject within data science for the first guide in the series i recommended a few coding classes for the beginner data scientist then it was statistics and probability classes then introductions to data science also data visualization for this guide i spent a dozen hours trying to identify every online machine learning course offered as of may extracting key bits of information from their syllabi and reviews and compiling their ratings my end goal was to identify the three best courses available and present them to you below for this task i turned to none other than the open source class central community and its database of thousands of course ratings and reviews since class central founder dhawal shah has kept a closer eye on online courses than arguably anyone else in the world dhawal personally helped me assemble this list of resources each course must fit three criteria we believe we covered every notable course that fits the above criteria since there are seemingly hundreds of courses on udemy we chose to consider the most reviewed and highest rated ones only there s always a chance that we missed something though so please let us know in the comments section if we left a good course out we compiled average ratings and number of reviews from class central and other review sites to calculate a weighted average rating for each course we read text reviews and used this feedback to supplement the numerical ratings we made subjective syllabus judgment calls based on three factors a popular definition originates from arthur samuel in machine learning is a subfield of computer science that gives computers the ability to learn without being explicitly programmed in practice this means developing computer programs that can make predictions based on data just as humans can learn from experience so can computers where data experience a machine learning workflow is the process required for carrying out a machine learning project though individual projects can differ most workflows share several common tasks problem evaluation data exploration data preprocessing model training testing deployment etc below you ll find helpful visualization of these core steps the ideal course introduces the entire process and provides interactive examples assignments and or quizzes where students can perform each task themselves first off let s define deep learning here is a succinct description as would be expected portions of some of the machine learning courses contain deep learning content i chose not to include deep learning only courses however if you are interested in deep learning specifically we ve got you covered with the following article my top three recommendations from that list would be several courses listed below ask students to have prior programming calculus linear algebra and statistics experience these prerequisites are understandable given that machine learning is an advanced discipline missing a few subjects good news some of this experience can be acquired through our recommendations in the first two articles programming statistics of this data science career guide several top ranked courses below also provide gentle calculus and linear algebra refreshers and highlight the aspects most relevant to machine learning for those less familiar stanford university s machine learning on coursera is the clear current winner in terms of ratings reviews and syllabus fit taught by the famous andrew ng google brain founder and former chief scientist at baidu this was the class that sparked the founding of coursera it has a star weighted average rating over reviews released in it covers all aspects of the machine learning workflow though it has a smaller scope than the original stanford class upon which it is based it still manages to cover a large number of techniques and algorithms the estimated timeline is eleven weeks with two weeks dedicated to neural networks and deep learning free and paid options are available ng is a dynamic yet gentle instructor with a palpable experience he inspires confidence especially when sharing practical implementation tips and warnings about common pitfalls a linear algebra refresher is provided and ng highlights the aspects of calculus most relevant to machine learning evaluation is automatic and is done via multiple choice quizzes that follow each lesson and programming assignments the assignments there are eight of them can be completed in matlab or octave which is an open source version of matlab ng explains his language choice though python and r are likely more compelling choices in with the increased popularity of those languages reviewers note that that shouldn t stop you from taking the course a few prominent reviewers noted the following columbia university s machine learning is a relatively new offering that is part of their artificial intelligence micromasters on edx though it is newer and doesn t have a large number of reviews the ones that it does have are exceptionally strong professor john paisley is noted as brilliant clear and clever it has a star weighted average rating over reviews the course also covers all aspects of the machine learning workflow and more algorithms than the above stanford offering columbia s is a more advanced introduction with reviewers noting that students should be comfortable with the recommended prerequisites calculus linear algebra statistics probability and coding quizzes programming assignments and a final exam are the modes of evaluation students can use either python octave or matlab to complete the assignments the course s total estimated timeline is eight to ten hours per week over twelve weeks it is free with a verified certificate available for purchase below are a few of the aforementioned sparkling reviews machine learning a ztm on udemy is an impressively detailed offering that provides instruction in both python and r which is rare and can t be said for any of the other top courses it has a star weighted average rating over reviews which makes it the most reviewed course of the ones considered it covers the entire machine learning workflow and an almost ridiculous in a good way number of algorithms through hours of on demand video the course takes a more applied approach and is lighter math wise than the above two courses each section starts with an intuition video from eremenko that summarizes the underlying theory of the concept being taught de ponteves then walks through implementation with separate videos for both python and r as a bonus the course includes python and r code templates for students to download and use on their own projects there are quizzes and homework challenges though these aren t the strong points of the course eremenko and the superdatascience team are revered for their ability to make the complex simple also the prerequisites listed are just some high school mathematics so this course might be a better option for those daunted by the stanford and columbia offerings a few prominent reviewers noted the following our pick had a weighted average rating of out of stars over reviews let s look at the other alternatives sorted by descending rating a reminder that deep learning only courses are not included in this guide you can find those here the analytics edge massachusetts institute of technology edx more focused on analytics in general though it does cover several machine learning topics uses r strong narrative that leverages familiar real world examples challenging ten to fifteen hours per week over twelve weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews python for data science and machine learning bootcamp jose portilla udemy has large chunks of machine learning content but covers the whole data science process more of a very detailed intro to python amazing course though not ideal for the scope of this guide hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews data science and machine learning bootcamp with r jose portilla udemy the comments for portilla s above course apply here as well except for r hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews machine learning series lazy programmer inc udemy taught by a data scientist big data engineer full stack software engineer with an impressive resume lazy programmer currently has a series of machine learning focused courses on udemy in total the courses have ratings and almost all of them have stars a useful course ordering is provided in each individual course s description uses python cost varies depending on udemy discounts which are frequent machine learning georgia tech udacity a compilation of what was three separate courses supervised unsupervised and reinforcement learning part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree oms bite sized videos as is udacity s style friendly professors estimated timeline of four months free it has a star weighted average rating over reviews implementing predictive analytics with spark in azure hdinsight microsoft edx introduces the core concepts of machine learning and a variety of algorithms leverages several big data friendly tools including apache spark scala and hadoop uses both python and r four hours per week over six weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews data science and machine learning with python hands on frank kane udemy uses python kane has nine years of experience at amazon and imdb nine hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews scala and spark for big data and machine learning jose portilla udemy big data focus specifically on implementation in scala and spark ten hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews machine learning engineer nanodegree udacity udacity s flagship machine learning program which features a best in class project review system and career support the program is a compilation of several individual udacity courses which are free co created by kaggle estimated timeline of six months currently costs usd per month with a tuition refund available for those who graduate within months it has a star weighted average rating over reviews learning from data introductory machine learning california institute of technology edx enrollment is currently closed on edx but is also available via caltech s independent platform see below it has a star weighted average rating over reviews learning from data introductory machine learning yaser abu mostafa california institute of technology a real caltech course not a watered down version reviews note it is excellent for understanding machine learning theory the professor yaser abu mostafa is popular among students and also wrote the textbook upon which this course is based videos are taped lectures with lectures slides picture in picture uploaded to youtube homework assignments are pdf files the course experience for online students isn t as polished as the top three recommendations it has a star weighted average rating over reviews mining massive datasets stanford university machine learning with a focus on big data introduces modern distributed file systems and mapreduce ten hours per week over seven weeks free it has a star weighted average rating over reviews aws machine learning a complete guide with python chandra lingam udemy a unique focus on cloud based machine learning and specifically amazon web services uses python nine hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews introduction to machine learning face detection in python holczer balazs udemy uses python eight hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews statlearning statistical learning stanford university based on the excellent textbook an introduction to statistical learning with applications in r and taught by the professors who wrote it reviewers note that the mooc isn t as good as the book citing thin exercises and mediocre videos five hours per week over nine weeks free it has a star weighted average rating over reviews machine learning specialization university of washington coursera great courses but last two classes including the capstone project were canceled reviewers note that this series is more digestable read easier for those without strong technical backgrounds than other top machine learning courses e g stanford s or caltech s be aware that the series is incomplete with recommender systems deep learning and a summary missing free and paid options available it has a star weighted average rating over reviews from to machine learning nlp python cut to the chase loony corn udemy a down to earth shy but confident take on machine learning techniques taught by four person team with decades of industry experience together uses python cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews principles of machine learning microsoft edx uses r python and microsoft azure machine learning part of the microsoft professional program certificate in data science three to four hours per week over six weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews big data statistical inference and machine learning queensland university of technology futurelearn a nice brief exploratory machine learning course with a focus on big data covers a few tools like r h o flow and weka only three weeks in duration at a recommended two hours per week but one reviewer noted that six hours per week would be more appropriate free and paid options available it has a star weighted average rating over reviews genomic data science and clustering bioinformatics v university of california san diego coursera for those interested in the intersection of computer science and biology and how it represents an important frontier in modern science focuses on clustering and dimensionality reduction part of ucsd s bioinformatics specialization free and paid options available it has a star weighted average rating over reviews intro to machine learning udacity prioritizes topic breadth and practical tools in python over depth and theory the instructors sebastian thrun and katie malone make this class so fun consists of bite sized videos and quizzes followed by a mini project for each lesson currently part of udacity s data analyst nanodegree estimated timeline of ten weeks free it has a star weighted average rating over reviews machine learning for data analysis wesleyan university coursera a brief intro machine learning and a few select algorithms covers decision trees random forests lasso regression and k means clustering part of wesleyan s data analysis and interpretation specialization estimated timeline of four weeks free and paid options available it has a star weighted average rating over reviews programming with python for data science microsoft edx produced by microsoft in partnership with coding dojo uses python eight hours per week over six weeks free and paid options available it has a star weighted average rating over reviews machine learning for trading georgia tech udacity focuses on applying probabilistic machine learning approaches to trading decisions uses python part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree oms estimated timeline of four months free it has a star weighted average rating over reviews practical machine learning johns hopkins university coursera a brief practical introduction to a number of machine learning algorithms several one two star reviews expressing a variety of concerns part of jhu s data science specialization four to nine hours per week over four weeks free and paid options available it has a star weighted average rating over reviews machine learning for data science and analytics columbia university edx introduces a wide range of machine learning topics some passionate negative reviews with concerns including content choices a lack of programming assignments and uninspiring presentation seven to ten hours per week over five weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews recommender systems specialization university of minnesota coursera strong focus one specific type of machine learning recommender systems a four course specialization plus a capstone project which is a case study taught using lenskit an open source toolkit for recommender systems free and paid options available it has a star weighted average rating over reviews machine learning with big data university of california san diego coursera terrible reviews that highlight poor instruction and evaluation some noted it took them mere hours to complete the whole course part of ucsd s big data specialization free and paid options available it has a star weighted average rating over reviews practical predictive analytics models and methods university of washington coursera a brief intro to core machine learning concepts one reviewer noted that there was a lack of quizzes and that the assignments were not challenging part of uw s data science at scale specialization six to eight hours per week over four weeks free and paid options available it has a star weighted average rating over reviews the following courses had one or no reviews as of may machine learning for musicians and artists goldsmiths university of london kadenze unique students learn algorithms software tools and machine learning best practices to make sense of human gesture musical audio and other real time data seven sessions in length audit free and premium usd per month options available it has one star review applied machine learning in python university of michigan coursera taught using python and the scikit learn toolkit part of the applied data science with python specialization scheduled to start may th free and paid options available applied machine learning microsoft edx taught using various tools including python r and microsoft azure machine learning note microsoft produces the course includes hands on labs to reinforce the lecture content three to four hours per week over six weeks free with a verified certificate available for purchase machine learning with python big data university taught using python targeted towards beginners estimated completion time of four hours big data university is affiliated with ibm free machine learning with apache systemml big data university taught using apache systemml which is a declarative style language designed for large scale machine learning estimated completion time of eight hours big data university is affiliated with ibm free machine learning for data science university of california san diego edx doesn t launch until january programming examples and assignments are in python using jupyter notebooks eight hours per week over ten weeks free with a verified certificate available for purchase introduction to analytics modeling georgia tech edx the course advertises r as its primary programming tool five to ten hours per week over ten weeks free with a verified certificate available for purchase predictive analytics gaining insights from big data queensland university of technology futurelearn brief overview of a few algorithms uses hewlett packard enterprise s vertica analytics platform as an applied tool start date to be announced two hours per week over four weeks free with a certificate of achievement available for purchase introduccio n al machine learning universitas telefo nica miri ada x taught in spanish an introduction to machine learning that covers supervised and unsupervised learning a total of twenty estimated hours over four weeks machine learning path step dataquest taught in python using dataquest s interactive in browser platform multiple guided projects and a plus project where you build your own machine learning system using your own data subscription required the following six courses are offered by datacamp datacamp s hybrid teaching style leverages video and text based instruction with lots of examples through an in browser code editor a subscription is required for full access to each course introduction to machine learning datacamp covers classification regression and clustering algorithms uses r fifteen videos and exercises with an estimated timeline of six hours supervised learning with scikit learn datacamp uses python and scikit learn covers classification and regression algorithms seventeen videos and exercises with an estimated timeline of four hours unsupervised learning in r datacamp provides a basic introduction to clustering and dimensionality reduction in r sixteen videos and exercises with an estimated timeline of four hours machine learning toolbox datacamp teaches the big ideas in machine learning uses r videos and exercises with an estimated timeline of four hours machine learning with the experts school budgets datacamp a case study from a machine learning competition on drivendata involves building a model to automatically classify items in a school s budget datacamp s supervised learning with scikit learn is a prerequisite fifteen videos and exercises with an estimated timeline of four hours unsupervised learning in python datacamp covers a variety of unsupervised learning algorithms using python scikit learn and scipy the course ends with students building a recommender system to recommend popular musical artists thirteen videos and exercises with an estimated timeline of four hours machine learning tom mitchell carnegie mellon university carnegie mellon s graduate introductory machine learning course a prerequisite to their second graduate level course statistical machine learning taped university lectures with practice problems homework assignments and a midterm all with solutions posted online a version of the course also exists cmu is one of the best graduate schools for studying machine learning and has a whole department dedicated to ml free statistical machine learning larry wasserman carnegie mellon university likely the most advanced course in this guide a follow up to carnegie mellon s machine learning course taped university lectures with practice problems homework assignments and a midterm all with solutions posted online free undergraduate machine learning nando de freitas university of british columbia an undergraduate machine learning course lectures are filmed and put on youtube with the slides posted on the course website the course assignments are posted as well no solutions though de freitas is now a full time professor at the university of oxford and receives praise for his teaching abilities in various forums graduate version available see below machine learning nando de freitas university of british columbia a graduate machine learning course the comments in de freitas undergraduate course above apply here as well this is the fifth of a six piece series that covers the best online courses for launching yourself into the data science field we covered programming in the first article statistics and probability in the second article intros to data science in the third article and data visualization in the fourth the final piece will be a summary of those articles plus the best online courses for other key topics such as data wrangling databases and even software engineering if you re looking for a complete list of data science online courses you can find them on class central s data science and big data subject page if you enjoyed reading this check out some of class central s other pieces if you have suggestions for courses i missed let me know in the responses if you found this helpful click the so more people will see it here on medium this is a condensed version of my original article published on class central where i ve included detailed course syllabi from a quick cheer to a standing ovation clap to show how much you enjoyed this story curriculum lead projects datacamp i created my own data science master s program our community publishes stories worth reading on development design and data science
Michael Jordan,34000,16,https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7?source=tag_archive---------7----------------,Artificial Intelligence — The Revolution Hasn’t Happened Yet,artificial intelligence ai is the mantra of the current era the phrase is intoned by technologists academicians journalists and venture capitalists alike as with many phrases that cross over from technical academic fields into general circulation there is significant misunderstanding accompanying the use of the phrase but this is not the classical case of the public not understanding the scientists here the scientists are often as befuddled as the public the idea that our era is somehow seeing the emergence of an intelligence in silicon that rivals our own entertains all of us enthralling us and frightening us in equal measure and unfortunately it distracts us there is a different narrative that one can tell about the current era consider the following story which involves humans computers data and life or death decisions but where the focus is something other than intelligence in silicon fantasies when my spouse was pregnant years ago we had an ultrasound there was a geneticist in the room and she pointed out some white spots around the heart of the fetus those are markers for down syndrome she noted and your risk has now gone up to in she further let us know that we could learn whether the fetus in fact had the genetic modification underlying down syndrome via an amniocentesis but amniocentesis was risky the risk of killing the fetus during the procedure was roughly in being a statistician i determined to find out where these numbers were coming from to cut a long story short i discovered that a statistical analysis had been done a decade previously in the uk where these white spots which reflect calcium buildup were indeed established as a predictor of down syndrome but i also noticed that the imaging machine used in our test had a few hundred more pixels per square inch than the machine used in the uk study i went back to tell the geneticist that i believed that the white spots were likely false positives that they were literally white noise she said ah that explains why we started seeing an uptick in down syndrome diagnoses a few years ago it s when the new machine arrived we didn t do the amniocentesis and a healthy girl was born a few months later but the episode troubled me particularly after a back of the envelope calculation convinced me that many thousands of people had gotten that diagnosis that same day worldwide that many of them had opted for amniocentesis and that a number of babies had died needlessly and this happened day after day until it somehow got fixed the problem that this episode revealed wasn t about my individual medical care it was about a medical system that measured variables and outcomes in various places and times conducted statistical analyses and made use of the results in other places and times the problem had to do not just with data analysis per se but with what database researchers call provenance broadly where did data arise what inferences were drawn from the data and how relevant are those inferences to the present situation while a trained human might be able to work all of this out on a case by case basis the issue was that of designing a planetary scale medical system that could do this without the need for such detailed human oversight i m also a computer scientist and it occurred to me that the principles needed to build planetary scale inference and decision making systems of this kind blending computer science with statistics and taking into account human utilities were nowhere to be found in my education and it occurred to me that the development of such principles which will be needed not only in the medical domain but also in domains such as commerce transportation and education were at least as important as those of building ai systems that can dazzle us with their game playing or sensorimotor skills whether or not we come to understand intelligence any time soon we do have a major challenge on our hands in bringing together computers and humans in ways that enhance human life while this challenge is viewed by some as subservient to the creation of artificial intelligence it can also be viewed more prosaically but with no less reverence as the creation of a new branch of engineering much like civil engineering and chemical engineering in decades past this new discipline aims to corral the power of a few key ideas bringing new resources and capabilities to people and doing so safely whereas civil engineering and chemical engineering were built on physics and chemistry this new engineering discipline will be built on ideas that the preceding century gave substance to ideas such as information algorithm data uncertainty computing inference and optimization moreover since much of the focus of the new discipline will be on data from and about humans its development will require perspectives from the social sciences and humanities while the building blocks have begun to emerge the principles for putting these blocks together have not yet emerged and so the blocks are currently being put together in ad hoc ways thus just as humans built buildings and bridges before there was civil engineering humans are proceeding with the building of societal scale inference and decision making systems that involve machines humans and the environment just as early buildings and bridges sometimes fell to the ground in unforeseen ways and with tragic consequences many of our early societal scale inference and decision making systems are already exposing serious conceptual flaws and unfortunately we are not very good at anticipating what the next emerging serious flaw will be what we re missing is an engineering discipline with its principles of analysis and design the current public dialog about these issues too often uses ai as an intellectual wildcard one that makes it difficult to reason about the scope and consequences of emerging technology let us begin by considering more carefully what ai has been used to refer to both recently and historically most of what is being called ai today particularly in the public sphere is what has been called machine learning ml for the past several decades ml is an algorithmic field that blends ideas from statistics computer science and many other disciplines see below to design algorithms that process data make predictions and help make decisions in terms of impact on the real world ml is the real thing and not just recently indeed that ml would grow into massive industrial relevance was already clear in the early s and by the turn of the century forward looking companies such as amazon were already using ml throughout their business solving mission critical back end problems in fraud detection and supply chain prediction and building innovative consumer facing services such as recommendation systems as datasets and computing resources grew rapidly over the ensuing two decades it became clear that ml would soon power not only amazon but essentially any company in which decisions could be tied to large scale data new business models would emerge the phrase data science began to be used to refer to this phenomenon reflecting the need of ml algorithms experts to partner with database and distributed systems experts to build scalable robust ml systems and reflecting the larger social and environmental scope of the resulting systems this confluence of ideas and technology trends has been rebranded as ai over the past few years this rebranding is worthy of some scrutiny historically the phrase ai was coined in the late s to refer to the heady aspiration of realizing in software and hardware an entity possessing human level intelligence we will use the phrase human imitative ai to refer to this aspiration emphasizing the notion that the artificially intelligent entity should seem to be one of us if not physically at least mentally whatever that might mean this was largely an academic enterprise while related academic fields such as operations research statistics pattern recognition information theory and control theory already existed and were often inspired by human intelligence and animal intelligence these fields were arguably focused on low level signals and decisions the ability of say a squirrel to perceive the three dimensional structure of the forest it lives in and to leap among its branches was inspirational to these fields ai was meant to focus on something different the high level or cognitive capability of humans to reason and to think sixty years later however high level reasoning and thought remain elusive the developments which are now being called ai arose mostly in the engineering fields associated with low level pattern recognition and movement control and in the field of statistics the discipline focused on finding patterns in data and on making well founded predictions tests of hypotheses and decisions indeed the famous backpropagation algorithm that was rediscovered by david rumelhart in the early s and which is now viewed as being at the core of the so called ai revolution first arose in the field of control theory in the s and s one of its early applications was to optimize the thrusts of the apollo spaceships as they headed towards the moon since the s much progress has been made but it has arguably not come about from the pursuit of human imitative ai rather as in the case of the apollo spaceships these ideas have often been hidden behind the scenes and have been the handiwork of researchers focused on specific engineering challenges although not visible to the general public research and systems building in areas such as document retrieval text classification fraud detection recommendation systems personalized search social network analysis planning diagnostics and a b testing have been a major success these are the advances that have powered companies such as google netflix facebook and amazon one could simply agree to refer to all of this as ai and indeed that is what appears to have happened such labeling may come as a surprise to optimization or statistics researchers who wake up to find themselves suddenly referred to as ai researchers but labeling of researchers aside the bigger problem is that the use of this single ill defined acronym prevents a clear understanding of the range of intellectual and commercial issues at play the past two decades have seen major progress in industry and academia in a complementary aspiration to human imitative ai that is often referred to as intelligence augmentation ia here computation and data are used to create services that augment human intelligence and creativity a search engine can be viewed as an example of ia it augments human memory and factual knowledge as can natural language translation it augments the ability of a human to communicate computing based generation of sounds and images serves as a palette and creativity enhancer for artists while services of this kind could conceivably involve high level reasoning and thought currently they don t they mostly perform various kinds of string matching and numerical operations that capture patterns that humans can make use of hoping that the reader will tolerate one last acronym let us conceive broadly of a discipline of intelligent infrastructure ii whereby a web of computation data and physical entities exists that makes human environments more supportive interesting and safe such infrastructure is beginning to make its appearance in domains such as transportation medicine commerce and finance with vast implications for individual humans and societies this emergence sometimes arises in conversations about an internet of things but that effort generally refers to the mere problem of getting things onto the internet not to the far grander set of challenges associated with these things capable of analyzing those data streams to discover facts about the world and interacting with humans and other things at a far higher level of abstraction than mere bits for example returning to my personal anecdote we might imagine living our lives in a societal scale medical system that sets up data flows and data analysis flows between doctors and devices positioned in and around human bodies thereby able to aid human intelligence in making diagnoses and providing care the system would incorporate information from cells in the body dna blood tests environment population genetics and the vast scientific literature on drugs and treatments it would not just focus on a single patient and a doctor but on relationships among all humans just as current medical testing allows experiments done on one set of humans or animals to be brought to bear in the care of other humans it would help maintain notions of relevance provenance and reliability in the way that the current banking system focuses on such challenges in the domain of finance and payment and while one can foresee many problems arising in such a system involving privacy issues liability issues security issues etc these problems should properly be viewed as challenges not show stoppers we now come to a critical issue is working on classical human imitative ai the best or only way to focus on these larger challenges some of the most heralded recent success stories of ml have in fact been in areas associated with human imitative ai areas such as computer vision speech recognition game playing and robotics so perhaps we should simply await further progress in domains such as these there are two points to make here first although one would not know it from reading the newspapers success in human imitative ai has in fact been limited we are very far from realizing human imitative ai aspirations unfortunately the thrill and fear of making even limited progress on human imitative ai gives rise to levels of over exuberance and media attention that is not present in other areas of engineering second and more importantly success in these domains is neither sufficient nor necessary to solve important ia and ii problems on the sufficiency side consider self driving cars for such technology to be realized a range of engineering problems will need to be solved that may have little relationship to human competencies or human lack of competencies the overall transportation system an ii system will likely more closely resemble the current air traffic control system than the current collection of loosely coupled forward facing inattentive human drivers it will be vastly more complex than the current air traffic control system specifically in its use of massive amounts of data and adaptive statistical modeling to inform fine grained decisions it is those challenges that need to be in the forefront and in such an effort a focus on human imitative ai may be a distraction as for the necessity argument it is sometimes argued that the human imitative ai aspiration subsumes ia and ii aspirations because a human imitative ai system would not only be able to solve the classical problems of ai as embodied e g in the turing test but it would also be our best bet for solving ia and ii problems such an argument has little historical precedent did civil engineering develop by envisaging the creation of an artificial carpenter or bricklayer should chemical engineering have been framed in terms of creating an artificial chemist even more polemically if our goal was to build chemical factories should we have first created an artificial chemist who would have then worked out how to build a chemical factory a related argument is that human intelligence is the only kind of intelligence that we know and that we should aim to mimic it as a first step but humans are in fact not very good at some kinds of reasoning we have our lapses biases and limitations moreover critically we did not evolve to perform the kinds of large scale decision making that modern ii systems must face nor to cope with the kinds of uncertainty that arise in ii contexts one could argue that an ai system would not only imitate human intelligence but also correct it and would also scale to arbitrarily large problems but we are now in the realm of science fiction such speculative arguments while entertaining in the setting of fiction should not be our principal strategy going forward in the face of the critical ia and ii problems that are beginning to emerge we need to solve ia and ii problems on their own merits not as a mere corollary to a human imitative ai agenda it is not hard to pinpoint algorithmic and infrastructure challenges in ii systems that are not central themes in human imitative ai research ii systems require the ability to manage distributed repositories of knowledge that are rapidly changing and are likely to be globally incoherent such systems must cope with cloud edge interactions in making timely distributed decisions and they must deal with long tail phenomena whereby there is lots of data on some individuals and little data on most individuals they must address the difficulties of sharing data across administrative and competitive boundaries finally and of particular importance ii systems must bring economic ideas such as incentives and pricing into the realm of the statistical and computational infrastructures that link humans to each other and to valued goods such ii systems can be viewed as not merely providing a service but as creating markets there are domains such as music literature and journalism that are crying out for the emergence of such markets where data analysis links producers and consumers and this must all be done within the context of evolving societal ethical and legal norms of course classical human imitative ai problems remain of great interest as well however the current focus on doing ai research via the gathering of data the deployment of deep learning infrastructure and the demonstration of systems that mimic certain narrowly defined human skills with little in the way of emerging explanatory principles tends to deflect attention from major open problems in classical ai these problems include the need to bring meaning and reasoning into systems that perform natural language processing the need to infer and represent causality the need to develop computationally tractable representations of uncertainty and the need to develop systems that formulate and pursue long term goals these are classical goals in human imitative ai but in the current hubbub over the ai revolution it is easy to forget that they are not yet solved ia will also remain quite essential because for the foreseeable future computers will not be able to match humans in their ability to reason abstractly about real world situations we will need well thought out interactions of humans and computers to solve our most pressing problems and we will want computers to trigger new levels of human creativity not replace human creativity whatever that might mean it was john mccarthy while a professor at dartmouth and soon to take a position at mit who coined the term ai apparently to distinguish his budding research agenda from that of norbert wiener then an older professor at mit wiener had coined cybernetics to refer to his own vision of intelligent systems a vision that was closely tied to operations research statistics pattern recognition information theory and control theory mccarthy on the other hand emphasized the ties to logic in an interesting reversal it is wiener s intellectual agenda that has come to dominate in the current era under the banner of mccarthy s terminology this state of affairs is surely however only temporary the pendulum swings more in ai than in most fields but we need to move beyond the particular historical perspectives of mccarthy and wiener we need to realize that the current public dialog on ai which focuses on a narrow subset of industry and a narrow subset of academia risks blinding us to the challenges and opportunities that are presented by the full scope of ai ia and ii this scope is less about the realization of science fiction dreams or nightmares of super human machines and more about the need for humans to understand and shape technology as it becomes ever more present and influential in their daily lives moreover in this understanding and shaping there is a need for a diverse set of voices from all walks of life not merely a dialog among the technologically attuned focusing narrowly on human imitative ai prevents an appropriately wide range of voices from being heard while industry will continue to drive many developments academia will also continue to play an essential role not only in providing some of the most innovative technical ideas but also in bringing researchers from the computational and statistical disciplines together with researchers from other disciplines whose contributions and perspectives are sorely needed notably the social sciences the cognitive sciences and the humanities on the other hand while the humanities and the sciences are essential as we go forward we should also not pretend that we are talking about something other than an engineering effort of unprecedented scale and scope society is aiming to build new kinds of artifacts these artifacts should be built to work as claimed we do not want to build systems that help us with medical treatments transportation options and commercial opportunities to find out after the fact that these systems don t really work that they make errors that take their toll in terms of human lives and happiness in this regard as i have emphasized there is an engineering discipline yet to emerge for the data focused and learning focused fields as exciting as these latter fields appear to be they cannot yet be viewed as constituting an engineering discipline moreover we should embrace the fact that what we are witnessing is the creation of a new branch of engineering the term engineering is often invoked in a narrow sense in academia and beyond with overtones of cold affectless machinery and negative connotations of loss of control by humans but an engineering discipline can be what we want it to be in the current era we have a real opportunity to conceive of something historically new a human centric engineering discipline i will resist giving this emerging discipline a name but if the acronym ai continues to be used as placeholder nomenclature going forward let s be aware of the very real limitations of this placeholder let s broaden our scope tone down the hype and recognize the serious challenges ahead michael i jordan from a quick cheer to a standing ovation clap to show how much you enjoyed this story michael i jordan is a professor in the department of electrical engineering and computer sciences and the department of statistics at uc berkeley
Milo Spencer-Harper,7800,6,https://medium.com/technology-invention-and-more/how-to-build-a-simple-neural-network-in-9-lines-of-python-code-cc8f23647ca1?source=tag_archive---------8----------------,How to build a simple neural network in 9 lines of Python code,as part of my quest to learn about ai i set myself the goal of building a simple neural network in python to ensure i truly understand it i had to build it from scratch without using a neural network library thanks to an excellent blog post by andrew trask i achieved my goal here it is in just lines of code in this blog post i ll explain how i did it so you can build your own i ll also provide a longer but more beautiful version of the source code but first what is a neural network the human brain consists of billion cells called neurons connected together by synapses if sufficient synaptic inputs to a neuron fire that neuron will also fire we call this process thinking we can model this process by creating a neural network on a computer it s not necessary to model the biological complexity of the human brain at a molecular level just its higher level rules we use a mathematical technique called matrices which are grids of numbers to make it really simple we will just model a single neuron with three inputs and one output we re going to train the neuron to solve the problem below the first four examples are called a training set can you work out the pattern should the be or you might have noticed that the output is always equal to the value of the leftmost input column therefore the answer is the should be training process but how do we teach our neuron to answer the question correctly we will give each input a weight which can be a positive or negative number an input with a large positive weight or a large negative weight will have a strong effect on the neuron s output before we start we set each weight to a random number then we begin the training process eventually the weights of the neuron will reach an optimum for the training set if we allow the neuron to think about a new situation that follows the same pattern it should make a good prediction this process is called back propagation formula for calculating the neuron s output you might be wondering what is the special formula for calculating the neuron s output first we take the weighted sum of the neuron s inputs which is next we normalise this so the result is between and for this we use a mathematically convenient function called the sigmoid function if plotted on a graph the sigmoid function draws an s shaped curve so by substituting the first equation into the second the final formula for the output of the neuron is you might have noticed that we re not using a minimum firing threshold to keep things simple formula for adjusting the weights during the training cycle diagram we adjust the weights but how much do we adjust the weights by we can use the error weighted derivative formula why this formula first we want to make the adjustment proportional to the size of the error secondly we multiply by the input which is either a or a if the input is the weight isn t adjusted finally we multiply by the gradient of the sigmoid curve diagram to understand this last one consider that the gradient of the sigmoid curve can be found by taking the derivative so by substituting the second equation into the first equation the final formula for adjusting the weights is there are alternative formulae which would allow the neuron to learn more quickly but this one has the advantage of being fairly simple constructing the python code although we won t use a neural network library we will import four methods from a python mathematics library called numpy these are for example we can use the array method to represent the training set shown earlier the t function transposes the matrix from horizontal to vertical so the computer is storing the numbers like this ok i think we re ready for the more beautiful version of the source code once i ve given it to you i ll conclude with some final thoughts i have added comments to my source code to explain everything line by line note that in each iteration we process the entire training set simultaneously therefore our variables are matrices which are grids of numbers here is a complete working example written in python also available here https github com miloharper simple neural network final thoughts try running the neural network using this terminal command python main py you should get a result that looks like we did it we built a simple neural network using python first the neural network assigned itself random weights then trained itself using the training set then it considered a new situation and predicted the correct answer was so very close traditional computer programs normally can t learn what s amazing about neural networks is that they can learn adapt and respond to new situations just like the human mind of course that was just neuron performing a very simple task but what if we hooked millions of these neurons together could we one day create something conscious i ve been inspired by the huge response this article has received i m considering creating an online course click here to tell me what topic to cover i d love to hear your feedback from a quick cheer to a standing ovation clap to show how much you enjoyed this story studied economics at oxford university founder of www moju io interested in politics and ai technology trends and new invention follow this collection to update the latest trend update as a collection editor i don t have any permission to add your articles in the wild please submit your article and i will approve also follow this collection please
Greg Fish,1,4,https://worldofweirdthings.com/looking-for-a-ghost-in-the-machine-4c997c4da45b?source=tag_archive---------0----------------,looking for a ghost in the machine – [ weird things ],a short while ago i wrote about some of the challenges involved in creating artificial intelligence and raised the question of how exactly a machine would spontaneously attain self awareness while i ve gotten plenty of feedback about how far technology has come so far and how it s imminent that machines will become much smarter than us i never got any specifics as to how exactly this would happen to me it s not a philosophical question because i m used to looking at technology from a design and development standpoint when i ask for specifics i m talking about functional requirements so far the closest thing to outlining the requirements for a super intelligent computer is a paper by university of oxford philosopher and futurist nick bostrom the first thing bostrom tries to do is to establish a benchmark by how to grade what he calls a super intellect and qualifying his definition according to him this super intellect would be smarter than any human mind in every capacity from the scientific to the creative it s a pretty lofty goal because designing something smarter than yourself requires that you build something you don t fully understand you might have a sudden stroke of luck and succeed but it s more than likely that you ll build a defective product instead imagine building a dna helix from scratch and with no detailed manual to go by even if you have all the tools and know where to find some bits of information to guide you when you don t know exactly what you re doing the task becomes very challenging and you end up making a lot of mistakes along the way there s also the question of how exactly we evaluate what the term smarter means in bostrom s projections when you have an intelligent machine become fully proficient in a certain area of expertise like say medicine it could combine with another machine which has an excellent understanding of physics and so on until all this consolidation leads to a device that knows all that we know and can use all that cross disciplinary knowledge to gain insights we just don t have yet technologically that should be possible but the question is whether a machine like that would really be smarter than humans per se it would be far more knowledgeable than any individual human granted but it s not as if there aren t experts in particular fields coming together to make all sorts of cross disciplinary connections and discoveries what bostrom calls a super intellect is actually just a massive knowledge base that can mine itself for information the paper was last revised in when we didn t have the enormous digital libraries we take for granted in today s world those libraries seem a fair bit like bostrom s super intellect in their function and if we were to combine them to mine their depths with sophisticated algorithms which look for cross disciplinary potential we d bring his concept to life but there s not a whole lot of intelligence there just a lot of data much of which would be subject to change or revision as research and discovery continue just like bostrom says it would be a very useful tool for scientists and researchers however it wouldn t be thinking on its own and giving the humans advice even if we put all this data on supercomputers which could live up to the paper s ambitious hardware requirements rev it up to match the estimated capacity of our brain it says and watch a new kind of intellect start waking up and take shape with the proper software according to bostrom the human brain operates at teraflops or trillion floating point operations per second now as he predicted computers have reached this speed by and went far beyond that in fact we have supercomputers which are as much as ten times faster supposedly at these operating speeds we should be able to write software which allows supercomputers to learn by interacting with humans and sifting through our digitized knowledge but the reality is that we d be trying to teach an intimate object made of metal and plastic how to think and solve problems something we re already born with and hone over our lifetimes you can teach someone how to ride a bike and how to balance but how exactly would you teach someone to understand the purpose of riding a bike how would you tell someone with no emotion no desires no wants and no needs why he should go anywhere that deep layer of motivation and wiring has taken several billion years to appear and was honed over a million additional years of evolution when we start trying to make an ai system comparable to ours we re effectively way behind from the get go to truly create an intelligent computer which doesn t just act as if it s thinking or do mechanical actions which are easy to predict and program we d need to impart in all that information in trillions of lines of code and trick circuitry into deducing it needs to behave like a living being and that s a job that couldn t be done in less than century much less in the next to years as projected by ray kurzweil and his fans eerie illustration by neil blevins from a quick cheer to a standing ovation clap to show how much you enjoyed this story techie rantt staff writer and editor computer lobotomist science tech and other oddities
Oliver Lindberg,1,7,https://medium.com/the-lindberg-interviews/interview-with-googles-alfred-spector-on-voice-search-hybrid-intelligence-and-more-2f6216aa480c?source=tag_archive---------1----------------,"Interview with Google’s Alfred Spector on voice search, hybrid intelligence and more",google s a pretty good search engine right well you ain t seen nothing yet vp of research alfred spector talks to oliver lindberg about the technologies emerging from google labs from voice search to hybrid intelligence and beyond this article originally appeared in issue of net magazine in and was republished at www techradar com google has always been tight lipped about products that haven t launched yet it s no secret however that thanks to the company s bottom up culture its engineers are working on tons of new projects at the same time following the mantra of release early release often the speed at which the search engine giant is churning out tools is staggering at the heart of it all is alfred spector google s vice president of research and special initiatives one of the areas google is making significant advances in is voice search spector is astounded by how rapidly it s come along the google mobile app features search by voice capabilities that are available for the iphone blackberry windows mobile and android all versions understand english including us uk australian and indian english accents but the latest addition for nokia s phones even introduces mandarin speech recognition which because of its many different accents and tonal characteristics posed a huge engineering challenge it s the most spoken language in the world but as it isn t exactly keyboard friendly voice search could become immensely popular in china voice is one of these grand technology challenges in computer science spector explains can a computer understand the human voice it s been worked on for many decades and what we ve realised over the last couple of years is that search particularly on handheld devices is amenable to voice as an import mechanism it s very valuable to be able to use voice all of us know that no matter how good the keyboard it s tricky to type exactly the right thing into a searchbar while holding your backpack and everything else to get a computer to take account of your voice is no mean feat of course one idea is to take all of the voices that the system hears over time into one huge pan human voice model so on the one hand we have a voice that s higher and with an english accent and on the other hand my voice which is deeper and with an american accent they both go into one model or it just becomes personalised to the individual voice scientists are a little unclear as to which is the best approach the research department is also making progress in machine translation google translate already features languages including swahili and yiddish the latest version introduces instant real time translation phonetic input and text to speech support in english we re able to go from any language to any of the others and there are times so possibilities spector explains we re focusing on increasing the number of languages because we d like to handle even those languages where there s not an enormous volume of usage it will make the web far more valuable to more people if they can access the english or chinese language web for example but we also continue to focus on quality because almost always the translations are valuable but imperfect sometimes it comes from training our translation system over more raw data so we have say eu documents in english and french and can compare them and learn rules for translation the other approach is to bring more knowledge into translation for example we re using more syntactic knowledge today and doing automated parsing with language it s been a grand challenge of the field since the late s now it s finally achieved mass usage the team led by scientist franz josef och has been collecting data for more than languages and the google translator toolkit which makes use of the wisdom of the crowds now even supports languages many of which are minority languages the editor enables users to translate text correct the automatic translation and publish it spector thinks that this approach is the future as computers become even faster handling more and more data a lot of it in the cloud machines learn from users and thus become smarter he calls this concept hybrid intelligence it s very difficult to solve these technological problems without human input he says it s hard to create a robot that s as clever smart and knowledgeable of the world as we humans are but it s not as tough to build a computational system like google which extends what we do greatly and gradually learns something about the world from us but that requires our interpretation to make it really successful we need to get computers and people communicating in both directions so the computer learns from the human and makes the human more effective examples of hybrid intelligence are google suggest which instantly offers popular searches as you type a search query and the did you mean feature in google search which corrects you when you misspell a query in the search bar the more you use it the better the system gets training computers to become seemingly more intelligent poses major hurdles for google s engineers computers don t train as efficiently as people do spector explains let s take the chess example if a kasparov was the educator we could count on almost anything he says as being accurate but if you tried to learn from a million chess players you learn from my children as well who play chess but they re and eight they ll be right sometimes and not right other times there s noise in that and some of the noise is spam one also has to have careful regard for privacy issues by collecting enormous amounts of data google hopes to create a powerful database that eventually will understand the relationship between words for example a dog is an animal and a dog has four legs the challenge is to try to establish these relationships automatically using tons of information instead of having experts teach the system this database would then improve search results and language translations because it would have a better understanding of the meaning of the words there s also a lot of research around conceptual search let s take a video of a couple in front of the empire state building we watch the video and it s clear they re on their honeymoon but what is the video about is it about love or honeymoons or is it about renting office space it s a fundamentally challenging problem one example of conceptual search is google image swirl which was added to labs in november enter a keyword and you get a list of images clicking on each one brings up a cluster of related pictures click on any of them to expand the wonder wheel further google notes that they re not just the most relevant images the algorithm determines the most relevant group of images with similar appearance and meaning to improve the world s data google continues to focus on the importance of the open internet another labs project google fusion tables facilitates data management in the cloud it enables users to create tables filter and aggregate data merge it with other data sources and visualise it with google maps or the google visualisation api the data sets can then be published shared or kept private and commented on by people around the world it s an example of open collaboration spector says if it s public we can crawl it to make it searchable and easily visible to people we hired one of the best database researchers in the world alon halevy to lead it google is aiming to make more information available more easily across multiple devices whether it s images videos speech or maps no matter which language we re using spector calls the impact totally transparent processing it revolutionises the role of computation in day today life the computer can break down all these barriers to communication and knowledge no matter what device we re using we have access to things we can do translations there are books or government documents and some day we hope to have medical records whatever you want no matter where you are you can find it spector retired in early and now serves as the cto of two sigma investments this article originally appeared in issue of net magazine in and was republished at www techradar com photography by andy short from a quick cheer to a standing ovation clap to show how much you enjoyed this story independent editor and content consultant founder and captain of pixelpioneers co founder and curator of www generateconf com former editor of netmag interviews with leading tech entrepreneurs and web designers conducted by oliverlindberg at netmag
Greg Fish,1,4,https://worldofweirdthings.com/the-technical-trouble-with-humanoid-robots-2c712649f3c5?source=tag_archive---------5----------------,the technical trouble with humanoid robots – [ weird things ],if you ve been reading this blog long enough you may recall that i m not a big fan of humanoid robots there s no need to invoke the uncanny valley effect even though some attempts to build humanoid robots managed to produce rather creepy entities which try to look as human as possible to goad future users into some kind of social bond with them presumably to gain their trust and get into a perfect position to kill the inferior things made of flesh no the reason why i m not sure that humanoid robots will be invaluable to us in the future is a very pragmatic one simply put emulating bipedalism is a huge computational overhead as well as a major and unavoidable engineering and maintenance headache and with the limits on size and weight of would be robot butlers as well as the patience of its users humanoid bot designers may be aiming a bit too high we walk run and perform complicated tasks with our hands and feet so easily we only notice the amount of effort and coordination this takes after an injury that limits our mobility the reason why we can do that lies in a small squishy mass of neurons coordinating a firestorm of constant activity unlike old standing urban myths imply we actually use all of our brainpower and we need it to help coordinate and execute the same motions that robots struggle to repeat of course our brains are cheating when compared to a computer because with tens of billions of neurons and trillions of synapses our brains are like screaming fast supercomputers they can calculate what it will take to catch a ball in mid air in less than a few hundred milliseconds and make the most minute adjustments to our muscles in order to keep us balanced and upright just as quickly likewise our bodies can heal the constant wear and tear on our joints wear and tear we will accumulate from walking running and bumping into things bipedal robots navigating our world wouldn t have these assets humanoid machines would need to be constantly maintained just to keep up with us in a mechanical sense and carry the equivalent of red storm in their heads or at least be linked to something like it to even hope to coordinate themselves as quickly as we do cognitively and physically academically this is a lofty goal which could yield new algorithms and robotic designs practically not so much while last month s feature in pop sci bemoaned the lack of interest in humanoid robots in the u s it also failed to demonstrate why such an incredibly complicated machine would be needed for basic household chores that could be done by robotic systems functioning independently and without the need to move on two legs instead we got the standard baby boomers caretaker argument which goes somewhat like this or alternatively a computer could book your appointments via e mail or a system that lets patients make an appointment with their doctors on the web a smart dispenser that gives you the right amount of pills checks for potential interactions based on public medical databases and beeps to remind you to take your medicine and a programmable walker with actuators and a few buttons could do these jobs while costing far less than the tens of millions a humanoid robot would cost by and requiring much less coordination or learning than a programmable humanoid why wouldn t we want to pursue immediate fixes to what s being described as a looming caretaker shortage choosing instead to invest billions of dollars into e jeeves which may take an entire decade or two just to learn how to go about daily human life ready to tackle the problem only after it was no longer an issue even if we started right now if anything harping on the need for a robotic hand for baby boomers future medical woes would only prompt more r d cash into immediate solutions and rules based intelligent agents we already employ rather than long term academic research there s a huge gap between human abilities and machinery because we have the benefit of having evolved over hundreds of millions of years of trial and error machines even though they re advancing at an ever faster pace only had a few decades by comparison it will take decades more to build self repairing machines and computer chips that can boast the same performance as a supercomputer while being small enough to fit in human sized robots heads before robotic butlers become practical and feasible and even then we might go with distinctly robotic versions because they d be cheaper to maintain and operate from a quick cheer to a standing ovation clap to show how much you enjoyed this story techie rantt staff writer and editor computer lobotomist science tech and other oddities
Frank Diana,50,10,https://medium.com/@frankdiana/the-evolving-role-of-business-analytics-76818e686e39?source=tag_archive---------2----------------,The Evolving Role of Business Analytics – Frank Diana – Medium,an older post that seems to be getting a lot of attention appreciation for analytics rising business analytics refers to the skills technologies applications and practices for the continuous exploration of data to gain insight that drive business decisions business analytics is multi faceted it combines multiple forms of analytics and applies the right method to deliver expected results it focuses on developing new insights using techniques including data mining predictive analytics natural language processing artificial intelligence statistical analysis and quantitative analysis in addition domain knowledge is a key component of the business analytics portfolio business analytics can then be viewed as the combination of domain knowledge and all forms of analytics in a way that creates analytic applications focused on enabling specific business outcomes analytic applications have a set of business outcomes that they must enable for fraud its reducing loss for quality safety it might be avoiding expensive recalls understanding how to enable these outcomes is the first step in determining the make up of each specific application for example in the case of insurance fraud it s not enough to use statistical analysis to predict fraud you need a strong focus on text domain expertise and the ability to visually portray organized crime rings insight gained through this analysis may be used as input for human decisions or may drive fully automated decisions database capacity processor speeds and software enhancements will continue to drive even more sophisticated analytic applications the key components of business analytics are there is a massive explosion of data occurring on a number of levels the notion of data overload was echoed in a previous ibm ceo study titled capitalizing on complexity in this study a large number of ceos described their organizations as data rich but insight poor many voiced frustration over their inability to transform available data into feasible action plans this notion of turning data into insight and insight to action is a common and growing theme according to pricewaterhouse coopers there are approximately to million blogs and million internet discussion boards and forums in the english language alone as the forrester diagram describes more consumers are moving up the ladder and becoming creators of content in addition estimates show the volume of unstructured data email audio video web pages etc doubles every three months effectively managing and harnessing this vast amount of information presents both a great challenge and a great opportunity data is flowing through medical devices scientific devices sensors monitors detectors other supply chain devices instrumented cars and roads instrumented domestic appliances etc everything will be instrumented and from this instrumentation comes data this data will be analyzed to find insight that drives smarter decisions the utility sector provides a great example of the growing need for analytics the smart grid and the gradual installation of intelligent endpoints smart meters and other devices will generate volumes of data smart grid utilities are evolving into brokers of information the data tsunami that will wash over utilities in the coming years is a formidable it challenge but it is also a huge opportunity to move beyond simple meter to cash functions and into real time optimization of their operations this type of instrumentation is playing out in many industries as this occurs industry players will be challenged to leverage the data generated by these devices inside the enterprise consider the increasing volumes of emails word documents pdfs excel worksheets and free form text fields that contain everything from budgets and forecasts to customer proposals contracts call center notes and expense reports outside the enterprise the growth of web based content which is primarily unstructured continues to accelerate everything from social media comments in blogs forums and social networks to survey verbatim and wiki pages most industry analysts estimate more than of the intelligence required to make smarter decisions is contained in unstructured data or text the survey results in a recent mit sloan report support both an aggressive adoption of analytics and a shift in the analytic footprint according to the report many traditional forms of analytics will be surpassed in the next months the authors produced a very effective visual that shows this shift from today s analytic footprint to the future footprint although listed as number one the authors describe visualization as dashboards and scorecards the traditional methods of visualization new and emerging methods help accelerate time to insight these new approaches help us absorb insight from large volumes of data in rapid fashion the analytics identified as creating the most value in months are companies and organizations continue to invest millions of dollars capturing storing and maintaining all types of business data to drive sales and revenue optimize operations manage risk and ensure compliance most of this investment has been in technologies and applications that manage structured data coded information residing in relational data base management systems in the form of rows and columns current methods such as traditional business intelligence bi are more about querying and reporting and focus on answering questions such as what happened how many how often and what actions are needed new forms of advanced analytics are required to address the business imperatives described earlier business analytics focuses on answering questions such as why is this happening what if these trends continue what will happen next predict what is the best that can happen optimize there is a growing view that prescribing outcomes is the ultimate role of analytics that is identifying those actions that deliver the right business outcomes organizations should first define the insights needed to meet business objectives and then identify data that provides that insight too often companies start with data the previously mentioned ibm study also revealed that analytics driven organizations had percent more revenue growth with percent more return on capital invested organizations expect value from emerging analytic techniques to soar the growth of innovative analytic applications will serve as a means to help individuals across the organization consume and act upon insights derived through complex analysis some examples of innovative use a recent mit sloan report effectively uses the maturity model concept to describe how organizations typically evolve to analytic excellence the authors point out that organizations begin with efficiency goals and then address growth objectives after experience is gained the authors believe this is a common practice but not necessarily a best practice they see the traditional analytic adoption path starting in data intensive areas like financial management operations and sales and marketing as companies move up the maturity curve they branch out into new functions such as strategy product research customer service and customer experience in the opinion of the authors these patterns suggest that success in one area stimulates adoption in others they suggest that this allows organizations to increase their level of sophistication the authors of the mit sloan special report through their analysis of survey results have created three levels of analytic capabilities the report provides a very nice matrix that describes these levels in the context of a maturity model in reviewing business challenges outlined in the matrix there is one very interesting dynamic the transition from cost and efficiencies to revenue growth customer retention and customer acquisition the authors of the report found that as the value of analytics grows organizations are likely to seek a wider range of capabilities and more advanced use of existing ones the survey indicated that this dynamic is leading some organizations to create a centralized analytics unit that makes it possible to share analytic resources efficiently and effectively these centralized enterprise units are the primary source of analytics providing a home for more advanced skills within the organization this same dynamic will lead to the appointment of chief analytics officers cao starting in the availability of strong business focused analytical talent will be the greatest constraint on a company s analytical capabilities the outsourcing of analytics will become an attractive alternative as the need for specialized skills will lead organizations to look for outside help outsourcing analytics allows a company to focus on taking action based on insights delivered by the outsourcer the outsourcer can leverage these specialized resources across multiple clients as the importance of analytics grows organizations will have an option to outsource expect to see more of this in we will see more organizations establish enterprise data management functions to coordinate data across business units we will also see smarter approaches such as information lifecycle management as opposed to the common approach of throwing more hardware at the growing data problem the information management challenge will grow as millions of next generation tech savvy users use feeds and mash ups to bring data together into usable parts so they can answer their own questions this gives rise to new challenges including data security and governance originally published at frankdiana net on march from a quick cheer to a standing ovation clap to show how much you enjoyed this story tcs executive focused on the rapid evolution of society and business fascinated by the view of the world in the next decade and beyond https frankdiana net
Paul Christiano,43,31,https://ai-alignment.com/a-formalization-of-indirect-normativity-7e44db640160?source=tag_archive---------0----------------,Formalizing indirect normativity – AI Alignment,this post outlines a formalization of what nick bostrom calls indirect normativity i don t think it s an adequate solution to the ai control problem but to my knowledge it was the first precise specification of a goal that meets the not terrible bar i e which does not lead to terrible consequences if pursued without any caveats or restrictions the proposal outlined here was sketched in early while i was visiting fhi and was my first serious foray into ai control when faced with the challenge of writing down precise moral principles adhering to the standards demanded in mathematics moral philosophers encounter two serious difficulties in light of these difficulties a moral philosopher might simply declare it is not my place to aspire to mathematical standards of precision ethics as a project inherently requires shared language understanding and experience it becomes impossible or meaningless without them this may be a defensible philosophical position but unfortunately the issue is not entirely philosophical in the interest of building institutions or machines which reliably pursue what we value we may one day be forced to describe precisely what we value in a way that does not depend on charitable or common sense interpretation in the same way that we today must describe what we want done precisely to computers often with considerable effort if some aspects of our values cannot be described formally then it may be more difficult to use institutions or machines to reliably satisfy them this is not to say that describing our values formally is necessary to satisfying them merely that it might make it easier since we are focusing on finding any precise and satisfactory moral theory rather than resolving disputes in moral philosophy we will adopt a consequentialist approach without justification and focus on axiology moreover we will begin from the standpoint of expected utility maximization and leave aside questions about how or over what space the maximization is performed we aim to mathematically define a utility function u such that we would be willing to build a hypothetical machine which exceptionlessly maximized u possibly at the catastrophic expense of any other values we will assume that the machine has an ability to reason which at least rivals that of humans and is willing to tolerate arbitrarily complex definitions of u within its ability to reason about them we adopt an indirect approach rather than specifying what exactly we want we specify a process for determining what we want this process is extremely complex so that any computationally limited agent will always be uncertain about the process output however by reasoning about the process it is possible to make judgments about which action has the highest expected utility in light of this uncertainty for example i might adopt the principle a state of affairs is valuable to the extent that i would judge it valuable after a century of reflection in general i will be uncertain about what i would say after a century but i can act on the basis of my best guesses after a century i will probably prefer worlds with more happiness and so today i should prefer worlds with more happiness after a century i have only a small probability of valuing trees feelings and so today i should go out of my way to avoid hurting them if it is either instrumentally useful or extremely easy as i spend more time thinking my beliefs about what i would say after a century may change and i will start to pursue different states of affairs even though the formal definition of my values is static similarly i might desire to think about the value of trees feelings if i expect that my opinions are unstable if i spend a month thinking about trees my current views will then be a much better predictor of my views after a hundred years and if i know better whether or not trees feelings are valuable i can make better decisions this example is quite informal but it communicates the main idea of the approach we stress that the value of our contribution if any is in the possibility of a precise formulation our proposal itself will be relatively informal instead it is a description of how you would arrive at a precise formulation the use of indirection seems to be necessary to achieve the desired level of precision our proposal contains only two explicit steps each of these steps requires substantial elaboration but we must also specify what we expect the human to do with these tools this proposal is best understood in the context of other fantastic seeming proposals such as my utility is whatever i would write down if i reflected for a thousand years without interruption or biological decay the counterfactual events which take place within the definition are far beyond the realm our intuition recognizes as realistic and have no place except in thought experiments but to the extent that we can reason about these counterfactuals and change our behavior on the basis of that reasoning if so motivated we can already see how such fantastic situations could affect our more prosaic reality the remainder of this document consists of brief elaboration of some of these steps and a few arguments about why this is a desirable process the first step of our proposal is a high fidelity mathematical model of human cognition we will set aside philosophical troubles and assume that the human brain is a purely physical system which may be characterized mathematically even granting this it is not clear how we can realistically obtain such a characterization the most obvious approach to characterizing a brain is to combine measurements of its behavior or architecture with an understanding of biology chemistry and physics this project represents a massive engineering effort which is currently just beginning most pessimistically our proposal could be postponed until this project s completion this could still be long before the mathematical characterization of the brain becomes useful for running experiments or automating human activities because we are interested only in a definition we do not care about having the computational resources necessary to simulate the brain an impractical mathematical definition however may be much easier to obtain we can define a model of a brain in terms of exhaustive searches which could never be practically carried out for example given some observations of a neuron we can formally define a brute force search for a model of that neuron similarly given models of individual neurons we may be able to specify a brute force search over all ways of connecting those neurons which account for our observations of the brain say some data acquired through functional neuroimaging it may be possible to carry out this definition without exploiting any structural knowledge about the brain beyond what is necessary to measure it effectively by collecting imaging data for a human exposed to a wide variety of stimuli we can recover a large corpus of data which must be explained by any model of a human brain moreover by using our explicit knowledge of human cognition we can algorithmically generate an extensive range of tests which identify a successful simulation by probing responses to questions or performance on games or puzzles in fact this project may be possible using existing resources the complexity of the human brain is not as unapproachable as it may at first appear though it may contain synapses each described by many parameters it can be specified much more compactly a newborn s brain can be specified by about bits of genetic information together with a recipe for a physical simulation of development the human brain appears to form new long term memories at a rate of bits per second suggesting that it may be possible to specify an adult brain using additional bits of experiential information this suggests that it may require only about bits of information to specify a human brain which is at the limits of what can be reasonably collected by existing technology for functional neuroimaging this discussion has glossed over at least one question what do we mean by brain emulation human cognition does not reside in a physical system with sharp boundaries and it is not clear how you would define or use a simulation of the input output behavior of such an object we will focus on some system which does have precisely defined input output behavior and which captures the important aspects of human cognition consider a system containing a human a keyboard a monitor and some auxiliary instruments well insulated from the environment except for some wires carrying inputs to the monitor and outputs from the keyboard and auxiliary instruments and wires carrying power the inputs to this system are simply screens to be displayed on the monitor say delivered as a sequence to be displayed one after another at frames per second while the outputs are the information conveyed from the keyboard and the other measuring apparatuses also delivered as a sequence of data dumps each recording activity from the last th of a second this human in a box system can be easily formally defined if a precise description of a human brain and coarse descriptions of the human body and the environment are available alternatively the input output behavior of the human in a box can be directly observed and a computational model constructed for the entire system let h be a mathematical definition of the resulting randomized function from input sequences in in in k to the next output out k h is by design a good approximation to what the human would output if presented with any particular input sequence using h we can mathematically define what would happen if the human interacted with a wide variety of systems for example if we deliver out k as the input to an abstract computer running some arbitrary software and then define in k as what the screen would next display we can mathematically define the distribution over transcripts which would have arisen if the human had interacted with the abstract computer this computer could be running an interactive shell a video game or a messaging client note that h reflects the behavior of a particular human in a particular mental state this state is determined by the process used to design h or the data used to learn it in general we can control h by choosing an appropriate human and providing appropriate instructions training more emulations could be produced by similar measures if necessary using only a single human may seem problematic but we will not rely on this lone individual to make all relevant ethical judgments instead we will try to select a human with the motivational stability to carry out the subsequent steps faithfully which will define u using the judgment of a community consisting of many humans this discussion has been brief and has necessarily glossed over several important difficulties one difficulty is the danger of using computationally unbounded brute force search given the possibility of short programs which exhibit goal oriented behavior another difficulty is that unless the emulation project is extremely conservative the models it produces are not likely to be fully functional humans their thoughts may be blurred in various ways they may be missing many memories or skills and they may lack important functionalities such as long term memory formation or emotional expression the scope of these issues depends on the availability of data from which to learn the relevant aspects of human cognition realistic proposals along these lines will need to accommodate these shortcomings relying on distorted emulations as a tool to construct increasingly accurate models for any idealized software with a distinguished instruction return we can use h to mathematically define the distribution over return values which would result if the human were to interact with that software we will informally define a particular program t which provides a rich environment in which the remainder of our proposal can be implemented from a technical perspective this will be the last step of our proposal the remaining steps will be reflected only in the intentions and behavior of the human being simulated in h fix a convenient and adequately expressive language say a dialect of python designed to run on an abstract machine t implements a standard interface for an interactive shell in this language the user can look through all of the past instructions that have been executed and their return values rendered as strings or execute a new instruction we also provide symbols representing h and t themselves as functions from sequences of k inputs to a value for the kth output we also provide some useful information such as a snapshot of the internet and some information about the process used to create h and t which we encode as a bit string and store in a single environment variable data we assume that our language of choice has a return instruction and we have t return whenever the user executes this instruction some care needs to be taken to define the behavior if t enters an infinite loop we want to minimize the probability that the human accidentally hangs the terminal with catastrophic consequences but we cannot provide a complete safety net without running into unresolvable issues with self reference we define u to be the value returned by h interacting with t if h represented an unfortunate mental state then this interaction could be short and unproductive the simulated human could just decide to type return and be done with it however by choosing an appropriate human to simulate and inculcating an appropriate mental state we can direct the process further we intend for h to use the resources in t to initiate a larger deliberative process for example the first step of this process may be to instantiate many copies of h interacting with variants of messaging clients which are in contact with each other the return value from the original process could then be defined as the value returned by a designated leader from this community or as a majority vote amongst the copies of h or so on another step might be to create appropriate realistic virtual environments for simulated brains rather than confining them to boxes for motivational stability it may be helpful to design various coordination mechanisms involving frameworks for interaction cached mental states which are frequently re instantiated or sanity checks whereby one copy of h monitors the behavior of another the resulting communities of simulated brains then engage in a protracted planning process ensuring that subsequent steps can be carried out safely or developing alternative approaches the main priority of this community is to reduce the probability of errors as far as possible exactly what constitutes an error will be discussed at more length later at the end of this process we obtain a formal definition of a new protocol h which submits its inputs for consideration to a large community and then produces its outputs using some deliberation mechanism democratic vote one leader using the rest of the community as advisors etc the next step requires our community of simulated brains to construct a detailed simulation of earth which they can observe and manipulate once they have such a simulation they have access to all of the data which would have been available on earth in particular they can now explore many possible futures and construct simulations for each living human in order to locate earth we will again leverage an exhaustive search first h decides on informal desiderata for an earth simulation these are likely to be as follows once h has decided on the desiderata it uses a brute force search to find a simulation satisfying them for each possible program it instantiates a new copy of h tasked with evaluating whether that program is an acceptable simulation we then define e to be a uniform distribution over programs which pass this evaluation we might have doubts about whether this process produces the real earth perhaps even once we have verified that it is identical according to a laundry list of measures it may still be different in other important ways there are two reasons why we might care about such differences first if the simulated earth has a substantially different set of people than the real earth then a different set of people will be involved in the subsequent decision making if we care particularly about the opinions of the people who actually exist which the reader might well being amongst such people then this may be unsatisfactory second if events transpire significantly differently on the simulated earth than the real earth value judgments designed to guide behavior appropriately in the simulated earth may lead to less appropriate behaviors in the real earth this will not be a problem if our ultimate definition of u consists of universalizable ethical principles but we will see that u might take other forms these concerns are addressed by a few broad arguments first checking a detailed but arbitrary laundry list actually provides a very strong guarantee for example if this laundry list includes verifying a snapshot of the internet then every event or person documented on the internet must exist unchanged and every keystroke of every person composing a document on the internet must not be disturbed if the world is well interconnected then it may be very difficult to modify parts of the world without having substantial effects elsewhere and so if a long enough arbitrary list of properties is fixed we expect nearly all of the world to be the same as well second if the essential character of the world is fixed but detailed are varied we should expect the sort of moral judgments reached by consensus to be relatively constant finally if the system whose behavior depends on these moral judgments is identical between the real and simulated worlds then outputting a u which causes that system to behave a certain way in the simulated world will also cause that system to behave that way in the real world once h has defined a simulation of the world which permits inspection and intervention by careful trial and error h can inspect a variety of possible futures in particular they can find interventions which cause the simulated human society to conduct a real brain emulation project and produce high fidelity brain scans for all living humans once these scans have been obtained h can use them to define u as the output of a new community h which draws on the expertise of all living humans operating under ideal conditions there are two important degrees of flexibility how to arrange the community for efficient communication and deliberation and how to delegate the authority to define u in terms of organization the distinction between different approaches is probably not very important for example it would probably be perfectly satisfactory to start from a community of humans interacting with each other over something like the existing internet but on abstract secure infrastructure more important are the safety measures which would be in place and the mechanism for resolving differences of value between different simulated humans the basic approach to resolving disputes is to allow each human to independently create a utility function u each bounded in the interval and then to return their average this average can either be unweighted or can be weighted by a measure of each individual s influence in the real world in accordance with a game theoretic notion like the shapley value applied to abstract games or simulations of the original world more sophisticated mechanisms are also possible and may be desirable of course these questions can and should be addressed in part by h during its deliberation in the previous step after all h has access to an unlimited length of time to deliberate and has infinitely powerful computational aids the role of our reasoning at this stage is simply to suggest that we can reasonably expect h to discover effective solutions as when discussing discovering a brain simulation by brute force we have skipped over some critical issues in this section in general brute force searches particularly over programs which we would like to run are quite dangerous because such searches will discover many programs with destructive goal oriented behaviors to deal with these issues in both cases we must rely on patience and powerful safety measures once we have a formal description of a community of interacting humans given as much time as necessary to deliberate and equipped with infinitely powerful computational aids it becomes increasingly difficult to make coherent predictions about their behavior critically though we can also become increasingly confident that the outcome of their behavior will reflect their intentions we sketch some possibilities to illustrate the degree of flexibility available perhaps the most natural possibility is for this community to solve some outstanding philosophical problems and to produce a utility function which directly captures their preferences however even if they quickly discovered a formulation which appeared to be attractive they would still be wise to spend a great length of time and to leverage some of these other techniques to ensure that their proposed solution was really satisfactory another natural possibility is to eschew a comprehensive theory of ethics and define value in terms of the community s judgment we can define a utility function in terms of the hypothetical judgments of astronomical numbers of simulated humans collaboratively evaluating the goodness of a state of affairs by examining its history at the atomic level understanding the relevant higher order structure and applying human intuitions it seems quite likely that the community will gradually engage in self modifications enlarging their cognitive capacity along various dimensions as they come to understand the relevant aspects of cognition and judge such modifications to preserve their essential character either independently or as an outgrowth of this process they may gradually or abruptly pass control to machine intelligences which they are suitably confident expresses their values this process could be used to acquire the power necessary to define a utility function in one of the above frameworks or understanding value preserving self modification or machine intelligence may itself prove an important ingredient in formalizing what it is we value any of these operations would be performed only after considerable analysis when the original simulated humans were extremely confident in the desirability of the results whatever path they take and whatever coordination mechanisms they use eventually they will output a utility function u we then define u if u u if u and u u otherwise at this point we have offered a proposal for formally defining a function u we have made some general observations about what this definition entails but now we may wonder to what extent u reflects our values or more relevantly to what extent our values are served by the creation of u maximizers concerns may be divided into a few natural categories we respond to each of these objections in turn if the process works as intended we will reach a stage in which a large community of humans reflects on their values undergoes a process of discovery and potentially self modification and then outputs its result we may be concerned that this dynamic does not adequately capture what we value for example we may believe that some other extrapolation dynamic captures our values or that it is morally desirable to act on the basis of our current beliefs without further reflection or that the presence of realistic disruptions such as the threat of catastrophe has an important role in shaping our moral deliberation the important observation in the defense of our proposal is that whatever objections we could think of today we could think of within the simulation if upon reflection we decide that too much reflection is undesirable we can simply change our plans appropriately if we decide that realistic interference is important for moral deliberation we can construct a simulation in which such interference occurs or determine our moral principles by observing moral judgments in our own world s possible futures there is some chance that this proposal is inadequate for some reason which won t be apparent upon reflection but then by definition this is a fact which we cannot possibly hope to learn by deliberating now it therefore seems quite difficult to maintain objections to the proposal along these lines one aspect of the proposal does get locked in however after being considered by only one human rather than by a large civilization the distribution of authority amongst different humans and the nature of mechanisms for resolving differing value judgments here we have two possible defenses one is that the mechanism for resolving such disagreements can be reflected on at length by the individual simulated in h this individual can spend generations of subjective time and greatly expand her own cognitive capacities while attempting to determine the appropriate way to resolve such disagreements however this defense is not completely satisfactory we may be able to rely on this individual to produce a very technically sound and generally efficient proposal but the proposal itself is quite value laden and relying on one individual to make such a judgment is in some sense begging the question a second more compelling defense is that the structure of our world has already provided a mechanism for resolving value disagreements by assigning decision making weight in a way that depends on current influence for example as determined by the simulated ability of various coalitions to achieve various goals we can generate a class of proposals which are at a minimum no worse than the status quo of course these considerations will also be shaped by the conditions surrounding the creation or maintenance of systems which will be guided by u for example if a nation were to create a u maximizer they might first adopt an internal policy for assigning influence on u by performing this decision making in an idealized environment we can also reduce the likelihood of destructive conflict and increase the opportunities for mutually beneficial bargaining we may have moral objections to codifying this sort of might makes right policy favoring a more democratic proposal or something else entirely but as a matter of empirical fact a more cosmopolitan proposal will be adopted only if it is supported by those with the appropriate forms of influence a situation which is unchanged by precisely codifying existing power structure finally the values of the simulations in this process may diverge from the values of the original human models for one reaosn or another for example the simulated humans may predictably disagree with the original models about ethical questions by virtue of probably having no physical instantiation that is the output of this process is defined in terms of what a particular human would do in a situation which that human knows will never come to pass if i ask what would i do if i were to wake up in a featureless room and told that the future of humanity depended on my actions the answer might begin with become distressed that i am clearly inhabiting a hypothetical situation and adjust my ethical views to take into account the fact that people in hypothetical situations apparently have relevant first person experience setting aside the question of whether such adjustments are justified they at least raise the possibility that our values may diverge from those of the simulations in this process these changes might be minimized by understanding their nature in advance and treating them on a case by case basis if we can become convinced that our understanding is exhaustive for example we could try and use humans who robustly employ updateless decision theories which never undergo such predictable changes or we could attempt to engineer a situation in which all of the humans being emulated do have physical instantiations and naive self interest for those emulations aligns roughly with the desired behavior for example by allowing the early emulations to write themselves into our world we can imagine many ways in which this process can fail to work as intended the original brain emulations may accurately model human behavior the original subject may deviate from the intended plans or simulated humans can make an error when interacting with their virtual environment which causes the process to get hijacked by some unintended dynamic we can argue that the proposal is likely to succeed and can bolster the argument in various ways by reducing the number of assumptions necessary for succees building in fault tolerance justifying each assumption more rigorously and so on however we are unlikely to eliminate the possibility of error therefore we need to argue that if the process fails with some small probability the resulting values will only be slightly disturbed this is the reason for requiring u to lie in the interval we will see that this restriction bounds the damage which may be done by an unlikely failure if the process fails with some small probability then we can represent the resulting utility function as u u u where u is the intended utility function and u is a utility function produced by some arbitrary error process now consider two possible states of affairs a and b such that u a u b u b then since u we have u a u a u a u b u b u b u b thus if a is substantially better than b according to u then a is better than b according to u this shows that a small probability of error whether coming from the stochasticity of our process or an agent s uncertainty about the process output has only a small effect on the resulting values moreover the process contains a humans who have access to a simulation of our world this implies in particular that they have access to a simulation of whatever u maximizing agents exist in the world and they have knowledge of those agents beliefs about u this allows them to choose u with perfect knowledge of the effects of error in these agents judgments in some cases this will allow them to completely negate the effect of error terms for example if the randomness in our process causes a perfectly cooperate community of simulated humans to control u with probability and causes an arbitrary adversary to control it with probability then the simulated humans can spend half of their mass outputting a utility function which exactly counters the effect of the adversary in general the situation is not quite so simple the fraction of mass controlled by any particular coalition will vary as the system s uncertainty about u varies and so it will be impossible to counteract the effect of an error term in a way which is time independent instead we will argue later that an appropriate choice of a bounded and noisy u can be used to achieve a very wide variety of effective behaviors of u maximizers overcoming the limitations both of bounded utility maximization and of noisy specification of utility functions many possible problems with this scheme were described or implicitly addressed above but that discussion was not exhaustive and there are some classes of errors that fall through the cracks one interesting class of failures concerns changes in the values of the hypothetical human h this human is in a very strange situation and it seems quite possible that the physical universe we know contains extremely few instances of that situation especially as the process unfolds and becomes more exotic so h s first person experience of this situation may lead to significant changes in h s views for example our intuition that our own universe is valuable seems to be derived substantially from our judgment that our own first person experiences are valuable if hypothetically we found ourselves in a very alien universe it seems quite plausible that we would judge the experiences within that universe to be morally valuable as well depending perhaps on our initial philosophical inclinations another example concerns our self interest much of individual humans values seem to depend on their own anticipations about what will happen to them especially when faced with the prospect of very negative outcomes if hypothetically we woke up in a completely non physical situation it is not exactly clear what we would anticipate and this may distort our behavior would we anticipate the planned thought experiment occurring as planned would we focus our attention on those locations in the universe where a simulation of the thought experiment might be occurring this possibility is particularly troubling in light of the incentives our scheme creates anyone who can manipulate h s behavior can have a significant effect on the future of our world and so many may be motivated to create simulations of h a realistic u maximizer will not be able to carry out the process described in the definition of u in fact this process probably requires immensely more computing resources than are available in the universe it may even involve the reaction of a simulated human to watching a simulation of the universe to what extent can we make robust guarantees about the behavior of such an agent we have already touched on this difficulty when discussing the maxim a state of affairs is valuable to the extent i would judge it valuable after a century of reflection we cannot generally predict our own judgments in a hundred years time but we can have well founded beliefs about those judgments and act on the basis of those beliefs we can also have beliefs about the value of further deliberation and can strike a balance between such deliberation and acting on our current best guess a u maximizer faces a similar set of problems it cannot understand the exact form of u but it can still have well founded beliefs about u and about what sorts of actions are good according to u for example if we suppose that the u maximizer can carry out any reasoning that we can carry out then the u maximizer knows to avoid anything which we suspect would be bad according to u for example torturing humans even if the u maximizer cannot carry out this reasoning as long as it can recognize that humans have powerful predictive models for other humans it can simply appropriate those models either by carrying out reasoning inspired by human models or by simply asking moreover the community of humans being simulated in our process has access to a simulation of whatever u maximizer is operating under this uncertainty and has a detailed understanding of that uncertainty this allows the community to shape their actions in a way with predictable to the u maximizer consequences it is easily conceivable that our values cannot be captured by a bounded utility function easiest to imagine is the possibility that some states of the world are much better than others in a way that requires unbounded utility functions but it is also conceivable that the framework of utility maximization is fundamentally not an appropriate one for guiding such an agent s action or that the notion of utility maximization hides subtleties which we do not yet appreciate we will argue that it is possible to transform bounded utility maximization into an arbitrary alternative system of decision making by designing a utility function which rewards worlds in which the u maximizer replaced itself with an alternative decision maker it is straightforward to design a utility function which is maximized in worlds where any particular u maximizer converted itself into a non u maximizer even if no simple characterization can be found for the desired act we can simply instantiate many communities of humans to look over a world history and decide whether or not they judge the u maximizer to have acted appropriately the more complicated question is whether a realistic u maximizer can be made to convert itself into a non u maximizer given that it is logically uncertain about the nature of u it is at least conceivable that it couldn t if the desirability of some other behavior is only revealed by philosophical considerations which are too complex to ever be discovered by physically limited agents then we should not expect any physically limited u maximizer to respond to those considerations of course in this case we could also not expect normal human deliberation to correctly capture our values the relevant question is whether a u maximizer could switch to a different normative framework if an ordinary investment of effort by human society revealed that a different normative framework was more appropriate if a u maximizer does not spend any time investigating this possibility than it may not be expected to act on it but to the extent that we assign a significant probability to the simulated humans deciding that a different normative framework is more appropriate and to the extent that the u maximizer is able to either emulate or accept our reasoning it will also assign a significant probability to this possibility unless it is able to rule it out by more sophisticated reasoning if we and the u maximizer expect the simulations to output a u which rewards a switch to a different normative framework and this possibility is considered seriously then u maximization entails exploring this possibility if these explorations suggest that the simulated humans probably do recommend some particular alternative framework and will output a u which assigns high value to worlds in which this framework is adopted and low value to worlds in which it isn t then a u maximizer will change frameworks such a change of frameworks may involve sweeping action in the world for example the u maximizer may have created many other agents which are pursuing activities instrumentally useful to maximizing u these agents may then need to be destroyed or altered anticipating this possibility the u maximizer is likely to take actions to ensure that its current best guess about u does not get locked in this argument suggests that a u maximizer could adopt an arbitrary alternative framework if it were feasible to conclude that humans would endorse that framework upon reflection our proposal appears to be something of a cop out in that it declines to directly take a stance on any ethical issues indeed not only do we fail to specify a utility function ourselves but we expect the simulations to which we have delegated the problem to in turn delegate it at least a few more times clearly at some point this process must bottom out with actual value judgments and we may be concerned that this sort of passing the buck is just obscuring deeper problems which will arise when the process does bottom out as observed above whatever such concerns we might have can also be discovered by the simulations we create if there is some fundamental difficulty which always arises when trying to assign values then we certainly have not exacerbated this problem by delegation nevertheless there are at least two coherent objections one might raise both of these objections can be met with a single response in the current world we face a broad range of difficult and often urgent problems by passing the buck the first time we delegate resolution of ethical challenges to a civilization which does not have to deal with some of these difficulties in particular it faces no urgent existential threats this allows us to divert as much energy as possible to dealing with practical problems today while still capturing most of the benefits of nearly arbitrarily extensive ethical deliberation this process is defined in terms of the behavior of unthinkably many hypothetical brain emulations it is conceivable that the moral status of these emulations may be significant we must make a distinction between two possible sources of moral value it could be the case that a u maximizer carries out simulations on physical hardware in order to better understand u and these simulations have moral value or it could be the case that the hypothetical emulations themselves have moral value in the first case we can remark that the moral value of such simulations is itself incorporated into the definition of u therefore a u maximizer will be sensitive to the possible suffering of simulations it runs while trying to learn about u as long as it believes that we may might be concerned about the simulations welfare upon reflection it can rely as much as possible on approaches which do not involve running simulations which deprive simulations of the first person experience of discomfort or which estimate outcomes by running simulations in more pleasant circumstances if the u maximizer is able to foresee that we will consider certain sacrifices in simulation welfare worthwhile then it will make those sacrifices in general in the same way that we can argue that estimates of u reflect our values over states of affairs we can argue that estimates of u reflects our values over processes for learning about u in the second case a u maximizer in our world may have little ability to influence the welfare of hypothetical simulations invoked in the definition of u however the possible disvalue of these simulations experiences are probably seriously diminished in general the moral value of such hypothetical simulations experiences is somewhat dubious if we simply write down the definition of u these simulations seem to have no more reality than story book characters whose activities we describe the best arguments for their moral relevance comes from the great causal significance of their decisions if the actions of a powerful u maximizer depend on its beliefs about what a particular simulation would do in a particular situation including for example that simulation s awareness of discomfort or fear or confusion at the absurdity of the hypothetical situation in which they find themselves then it may be the case that those emotional responses are granted moral significance however although we may define astronomical numbers of hypothetical simulations the detailed emotional responses of very view of these simulations will play an important role in the definition of u moreover for the most part the existences of the hypothetical simulations we define are extremely well controlled by those simulations themselves and may be expected to be counted as unusually happy by the lights of the simulations themselves the early simulations who have less such control are created from an individual who has provided consent and is selected to find such situations particularly non distressing finally we observe that u can exert control over the experiences of even hypothetical simulations if the early simulations would experience morally relevant suffering because of their causal significance but the later simulations they generate robustly disvalue this suffering the later simulations can simulate each other and ensure that they all take the same actions eliminating the causal significance of the earlier simulations originally published at ordinaryideas wordpress com on april from a quick cheer to a standing ovation clap to show how much you enjoyed this story openai aligning ai systems with human interests
Robbie Tilton,3,15,https://medium.com/@robbietilton/emotional-computing-with-ai-3513884055fa?source=tag_archive---------1----------------,Emotional Computing – Robbie Tilton – Medium,investigating the human to computer relationship through reverse engineering the turing test humans are getting closer to creating a computer with the ability to feel and think although the processes of the human brain are at large unknown computer scientists have been working to simulate the human capacity to feel and understand emotions this paper explores what it means to live in an age where computers can have emotional depth and what this means for the future of human to computer interactions in an experiment between a human and a human disguised as a computer the turing test is reverse engineered in order to understand the role computers will play as they become more adept to the processes of the human mind implications for this study are discussed and the direction for future research suggested the computer is a gateway technology that has opened up new ways of creation communication and expression computers in first world countries are a standard household item approximately of americans owning one as of us census bereau and are utilized as a tool to achieve a diverse range of goals as this product continues to become more globalized transistors are becoming smaller processors are becoming faster hard drives are holding information in new networked patterns and humans are adapting to the methods of interaction expected of machines at the same time with more powerful computers and quicker means of communication many researchers are exploring how a computer can serve as a tool to simulate the brains cognition if a computer is able to achieve the same intellectual and emotional properties as the human brain we could potentially understand how we ourselves think and feel coined by mit the term affective computing relates to computation of emotion or the affective phenomena and is a study that breaks down complex processes of the brain relating them to machine like activities marvin minsky rosalind picard clifford nass and scott brave along with many others have contributed to this field and what it would mean to have a computer that could fully understand its users in their research it is very clear that humans have the capacity to associate human emotions and personality traits with a machine nass and brave but can a human ever truly treat machine as a person in this paper we will uncover what it means for humans to interact with machines of greater intelligence and attempt to predict the future of human to computer interactions the human to computer relationship is continuously evolving and is dependent on the software interface users interact with with regards to current wide scale interfaces osx windows linux ios and android the tools and abilities that a computer provide remains to be the central focus of computational advancements for commercial purposes this relationship to software is driven by utilitarian needs and humans do not expect emotional comprehension or intellectually equivalent thoughts in their household devices as face tracking eye tracking speech recognition and kinetic recognition are advancing in their experimental laboratories it is anticipated that these technologies will eventually make their way to the mainstream market to provide a new relationship to what a computer can understand about its users and how a user can interact with a computer this paper is not about if a computer will have the ability to feel and love its user but asks the question to what capacity will humans be able to reciprocate feelings to a machine how does intelligence quotient iq differ from emotional quotient eq an iq is a representational relationship of intelligence that measures cognitive abilities like learning understanding and dealing with new situations an eq is a method of measuring emotional intelligence and the ability to both use emotions and cognitive skills cherry advances in computer iq have been astonishing and have proved that machines are capable of answering difficult questions accurately are able to hold a conversation with human like understanding and allow for emotional connections between a human and machine the turing test in particular has shown the machines ability to think and even fool a person into believing that it is a human turing test explained in detail in section machines like deep blue watson eliza svetlana cleverbot and many more have all expanded the perceptions of what a computer is and can be if an increased computational iq can allow a human to computer relationship to feel more like a human to human interaction what would the advancement of computational eq bring us peter robinson a professor at the university of cambridge states that if a computer understands its users feelings that it can then respond with an interaction that is more intuitive for its users robinson in essence eq advocates feel that it can facilitate a more natural interaction process where collaboration can occur with a computer in alan turing s computing machinery and intelligence turing a variant on the classic british parlor imitation game is proposed the original game revolves around three players a man a a woman b and an interrogator the interrogator stays in a room apart from a and b and only can communicate to the participants through text based communication a typewriter or instant messenger style interface when the game begins one contestant a or b is asked to pretend to be the opposite gender and to try and convince the interrogator of this at the same time the opposing participant is given full knowledge that the other contestant is trying to fool the interrogator with alan turing s computational background he took this imitation game one step further by replacing one of the participants a or b with a machine thus making the investigator try and depict if he she was speaking to a human or machine in turing proposed that by the average interrogator would not have more than a percent chance of making the right identification after five minutes of questioning the turing test was first passed in with eliza by joseph weizenbaum a chat robot programmed to act like a rogerian psychotherapist weizenbaum in kenneth colby created a similar bot called parry that incorporated more personality than eliza and was programmed to act like a paranoid schizophrenic bowden since these initial victories for the test the st century has proven to continue to provide machines with more human like qualities and traits that have made people fall in love with them convinced them of being human and have human like reasoning brian christian the author of the most human human argues that the problem with designing artificial intelligence with greater ability is that even though these machines are capable of learning and speaking that they have no self they are mere accumulations of identities and thoughts that are foreign to the machine and have no central identity of their own he also argues that people are beginning to idealize the machine and admire machines capabilities more than their fellow humans in essence he argues humans are evolving to become more like machines with less of a notion of self christian turing states we like to believe that man is in some subtle way superior to the rest of creation and it is likely to be quite strong in intellectual people since they value the power of thinking more highly than others and are more inclined to base their belief in the superiority of man on this power if this is true will humans idealize the future of the machine for its intelligence or will they remain an inferior being as an object of our creation reversing the turing test allows us to understand how humans will treat machines when machines provide an equivalent emotional and intellectual capacity this also hits directly on jefferson lister s quote not until a machine can write a sonnet or compose a concerto because of thoughts and emotions felt and not by the chance fall of symbols could we agree that machine equals brain that is not only write it but know that it had written it participants were given a chat room simulation between two participants a a human interrogator and b a human disguised as a computer in this simulation a and b were both placed in different rooms to avoid influence and communicated through a text based interface a was informed that b was an advanced computer chat bot with the capacity to feel understand learn and speak like a human b was informed to be his or herself text based communication was chosen to follow turing s argument that a computers voice should not help an interrogator determine if it s a human or computer pairings of participants were chosen to participate in the interaction one at a time to avoid influence from other participants each experiment was five minutes in length to replicate turing s time restraints twenty eight graduate students were recruited from the nyu interactive telecommunications program to participate in the study male and female the experiment was evenly distributed across men and women after being recruited in person participants were directed to a website that gave instructions and ran the experiment upon entering the website a participants were told that we were in the process of evaluating an advanced cloud based computing system that had the capacity to feel emotion understand learn and converse like a human b participants were instructed that they would be communicating with another person through text and to be themselves they were also told that participant a thinks they are a computer but that they shouldn t act like a computer or pretend to be one in any way this allowed a to explicitly understand that they were talking to a computer while b knew a perspective and explicitly were not going to play the role of a computer participants were then directed to communicate with the bot or human freely without restrictions after five minutes of conversation the participants were asked to stop and then filled out a questionnaire participants were asked to rate iq and eq of the person they were conversing with a participants perceived the following of b iq not good barely acceptable okay great excellent iq average rating eq not good barely acceptable okay great excellent eq average rating ability to hold a conversation not good barely acceptable okay great excellent ability to hold a conversation average b participants perceived the following of a iq not good barely acceptable okay great excellent iq average rating eq not good barely acceptable okay great excellent eq average rating ability to hold a conversation not good barely acceptable okay great excellent ability to hold a conversation average overall a participants gave the perceived chabot higher ratings than b participants gave a in particular the highest rating was in regards to the chat bot s iq this data states that people viewed the chat bot to be more intellectually competent it also implies that people talking with bots decrease their iq eq and conversation ability when communicating with computers a participants were allowed to decide their username within the chat system to best reflect how they wanted to portray themselves to the machine b participants were designated the gender neutral name bot in an attempt to ganger gender perceptions for the machine the male to female ratio was divided evenly with all participants being male and being female a participants of the time thought b was a male a female and gender neutral on the other hand b participants of the time thought a was a male a female and gender neutral the usernames a chose are as follows hihi inessah somade willzing jihyun g ann divagrrl thisdoug jono minion p itslynnburke from these results it is clear that people associate the male gender and gender neutrality with machines it also demonstrates that people modify their identities when speaking with machines b participants were asked if they would like to pursue a friendship with the person they chatted with of participants responded affirmatively that they would indeed like to pursue a friendship while said maybe or no one response stated i would like to continue the conversation but i don t think i would be enticed to pursue a friendship another responded maybe i like people who are intellectually curious but i worry that the person might be a bit of a smart ass overall the participant disguised as a machine may or may not pursue a friendship after five minutes of text based conversation b participants were also asked if they felt a cared about their feelings stated that a indeed did care about their feelings stated that they weren t sure if a cared about their feelings and stated that a did not care about their feelings these results indicate a user s lack of attention to b s emotional state a participants were asked what they felt could be improved about the b participants the following improvements were noted should be funny give it a better sense of humor it can be better if he knows about my friends or preference the response was inconsistent and too slow it should share more about itself your algorithm is prime prude just like that letdown siri well i guess i liked it better but it should be more engaged and human consistency not after the first cold prompt it pushed me on too many questions i felt that it gave up on answering and the response time was a bit slow outsource the chatbot to fluent english speakers elsewhere and pretend they are bots if the responses are this slow to this many inquiries then it should be about the same experience i was very impressed with its parsing ability so far not as much with its reasoning i think some parameters for the conversation would help like ask a question maybe make the response faster i was confused at first because i asked a question waited a bit then asked another question waited and then got a response from the bot the responses from this indicate that even if a computer is a human that its user may not necessarily be fully satisfied with its performance the response implies that each user would like the machine to accommodate his or her needs in order to cause less personality and cognitive friction with several participant comments incorporating response time it also indicates people expect machines to have consistent response times humans clearly vary in speed when listening thinking and responding but it is expected of machines to act in a rhythmic fashion it also suggests that there is an expectation that a machine will answer all questions asked and will not ask its users more questions than perceived necessary a participants were asked if they felt b s artificial intelligence could improve their relationship to computers if integrated in their daily products of participants responded affirmatively that they felt this could improve their relationship well i think i prefer talking to a person better but yes for ipod smart phones etc would be very handy for everyday use products yes especially iphone is always with me so it can track my daily behaviors that makes the algorithm smarter possibly i should have queries it for information that would have been more relevant to me absolutely yes the which responded negatively had doubts that it would be necessary or desirable not sure it might creep me out if it were i like siri as much as the next gal but honestly we re approaching the uncanny valley now its not clear to me why this type of relationship needs to improve i think human relationships still need a lot of work nope i still prefer flesh sacks no the findings of the paper are relevant to the future of affective computation whether a super computer with a human like iq and eq can improve the human to computer interaction the uncertainty of computational equivalency that turing brought forth is indeed an interesting starting point to understand what we want out of the future of computers the responses from the experiment affirm gender perceptions of machines and show how we display ourselves to machines it seems that we limit our intelligence limit our emotions and obscure our identities when communicating to a machine this leads us to question if we would want to give our true self to a computer if it doesn t have a self of its own it also could indicate that people censor themselves for machines because they lack a similarity that bonds humans to humans or that there s a stigma associated with placing information in a digital device the inverse relationship is also shown through the data that people perceive a bots iq eq and discussion ability to be high even though the chat bot was indeed a human this data can imply humans perceive bots to not have restrictions and to be competent at certain procedures the results also imply that humans aren t really sure what they want out of artificial intelligence in the future and that we are not certain that an affective computer would even enjoy a users company and or conversation the results also state that we currently think of computers as a very personal device that should be passive not active but reactive when interacted with it suggests a consistent reliability we expect upon machines and that we expect to take more information from a machine than it takes from us a major limitation of this experiment is the sample size and sample diversity the sample size of twenty eight students is too small to fully understand and gather a stable result set it was also only conducted with nyu interactive telecommunications students who all have extensive experience with computers and technology to get a more accurate assessment of emotions a more diverse sample range needs to be taken five minutes is a short amount of time to create an emotional connection or friendship to stay true to the turing tests limitations this was enforced but further relational understanding could be understood if more time was granted beside the visual interface of the chat window it would be important to show the emotions of participant b through a virtual avatar not having this visual feedback could have limited emotional resonance with participants a time is also a limitation people aren t used to speaking to inquisitive machines yet and even through a familiar interface a chat room many participants haven t held conversations with machines previously perhaps if chat bots become more active conversational participants in commercial applications users will feel less censored to give themselves to the conversation in addition to the refinements noted in the limitations described above there are several other experiments for possible future studies for example investigating a long term human to bot relationship this would provide a better understanding toward the emotions a human can share with a machine and how a machine can reciprocate these emotions it would also better allow computer scientists to understand what really creates a significant relationship when physical limitations are present future studies should attempt to push these results further by understanding how a larger sample reacts to a computer algorithm with higher intellectual and emotional understanding it should also attempt to understand the boundaries of emotional computing and what is ideal for the user and what is ideal for the machine without compromising either parties capacities this paper demonstrates the diverse range of emotions that people can feel for affective computation and indicates that we are not in a time where computational equivalency is fully desired or accepted positive reactions indicate that there is optimism for more adept artificial intelligence and that there is interest in the field for commercial use it also provides insight that humans limit themselves when communicating with machines and that inversely machines don t limit themselves when communicating with humans books articlesbowden m minds as machine a history of cognitive science oxford university press christian b the most human human marvin m the emotion machine commonsense thinking artificial intelligence and the future of the human mind simon schuster paperbacks nass c brave s wired for speech how voice activates and advances the human computer relationship mit press nass c brave s hutchinson k computers that care investigating the effects of orientation of emotion exhibited by an embodied computer agent human computer studies elsevier picard r affective computing mit press searle j minds brains and programs cambridge university press turing a computing machinery and intelligence mind stor wilson r keil f the mit encyclopedia of the cognitive sciences mit press weizenbaum j eliza a computer program for the study of natural language communication between man and machine communications of the acm websites cherry k what is emotional intelligence http psychology about com od personalitydevelopment a emotionalintell htm epstein r clever bots radio lab http www radiolab org may clever bots ibm deep blue ibm http www research ibm com deepblue ibm watson ibm http www ibm com innovation us watson index html leavitt d i took the turing test new york times http www nytimes com books review book review the most human human by brian christian html personal robotics group nexi mit http robotic media mit edu robinson p the emotional computer camrbidge ideas http www cam ac uk research news the emotional computer us census bereau households with a computer and internet use to http www census gov hhes computer s eliza mit http www manifestation com neurotoys eliza php from a quick cheer to a standing ovation clap to show how much you enjoyed this story
Wildcat2030,5,5,https://becominghuman.ai/becoming-a-cyborg-should-be-taken-gently-of-modern-bio-paleo-machines-cyborgology-b6c65436e416?source=tag_archive---------3----------------,Becoming a Cyborg should be taken gently: Of Modern Bio-Paleo-Machines — Cyborgology,we are on the edge of a paleolithic machine intelligence world a world oscillating between that which is already historical and that which is barely recognizable some of us teetering on this bio electronic borderline have this ghostly sensation that a new horizon is on the verge of being revealed still misty yet glowing with some inner light eerie but compelling the metaphor i used for bridging seemingly contrasting on first sight paradoxical between such a futuristic concept as machine intelligence and the paleolithic age is apt i think for though advances in computation with fractional ai appearing almost everywhere are becoming nearly casual the truth of the matter is that machines are still tribal and dispersed it is a dawn all right but a dawn is still only a hint of the day that is about to shine a dawn of hyperconnected machines interweaved with biological organisms cybernetically info related and semi independent the modern paleo machines do not recognize borders do not concern themselves with values and morality and do not philosophize about the meaning of it all not yet that is as in our own paleo past the needs of the machines do not yet contain passions for individuation desire for emotional recognition or indeed feelings of dismay or despair uncontrollable urges or dreams of far worlds also this will change eventually but not yet the paleo machinic world is in its experimentation stage probing it boundaries surveying the landscape of the infoverse mapping the hyperconnected situation charting a trajectory for its own evolution all this unconsciously we the biological part of the machine are providing the tools for its uplift we embed cameras everywhere so it can see we implant sensors all over the planet so it may feel but above all we nudge and we push towards a greater connectivity all this unaware together we form a weird cohabitation of biomechanical electro organic planetary os that is changing its environment no more human not mechanical but a combined interactive intelligence that journey on oblivious to its past blind to its future irreverent to the moment of its conception already lost to its parenthood agreement and yet it evolves unconscious on the machine part unaware on the biological part the almost sentient operating system of the global planetary infosphere is emerging wild eyed complex in its arrangement of co existence it reaches to comprehend its unexpected growth the quid pro quo we give the machines the platform to evolve the machines in turn give us advantages of fitness and manipulation we give the machines a space to turn our dreams into reality the machines in turn serve our needs and acquire sapience in the process in this hypercomplex state of affairs there is no judgment and no inherent morality there is motion inevitable inexorable inescapable and mesmerizing the embodiment is cybernetic though there be no pilot cyborgian and enhanced we play the game not of thrones but of the commons connected and networked the machines follow in our footsteps catalyzing our universality providing for us in turn a meaning we cannot yet understand or realize the hybridization process is in full swing reaching to cohere tribes of machines with tribes of humans each providing for the other a non designed direction for which neither has a plan or projected outcome both mingling and weaving a reality for which there is no ontos expecting no telos all this leads us to remember that only retrospectively do we recognize the move from the paleo tribes to the neolithic status we did not know that it happened then and had no control over the motion on the same token we scarcely see the motion now and have no control over its directionality there is however a small difference some will say it is insignificant i do not think it so for we are some of us to some extent at least aware of the motion and we can embed it with a meaning of our choice we can if we muster our cognitive reason our amazing skills of abstraction and simulation whisper sweet utopias into the probability process of emergence we can if we so desire passionate the operating system to beautify the process of evolution and eliminate or mitigate the dangers of inchoate blind walking we can if we manage to control our own paleo urges to destroy ourselves allow the combined interactive intelligence of man and machine to shine forth into a brighter future of expanded subjectivity we can sing to the machines cuddle them caress their circuits accepting their electronic flaws so they can accept our bio flaws we can merge aesthetically not with conquest but with understanding we can become wise that is the difference this time around being wise in this context implies a new form of discourse an intersubjective cross pollination of a wide array of disciplines the very trans disciplinarily nature of the process of cyborgization informs the discourse of subjectivity the discourse on subjectivity not unlike the move from paleo to neolithic societal structure demands of us a re assessment of the relations between man and machine for this re assessment to take place coherently the nascent re organization of the hyperconnected machinic infosphere need be understood as a ground for the expansion of subjectivity in a sense the motion into the new hyperconnected infosphere is not unlike the move of the neolithic to domestication of plants and animals this time around however the domestication can be seen as the adoption of technologies for the furtherance of subjectivity into the world understanding this process is difficult and far from obvious it is a perspective however that might allow us a wider context of appreciation of the current upheavals happening all around us a writer futurist and a polytopian tyger a c a k a wildcat is the founder and editor of the polytopia project at space collective he also writes at reality augmented and urbnfutr as well as contributing to h magazine his passion and love for science fiction led him to initiate the sci fi ultrashorts project photo credit for baby with ipad photo illumination by amanda tipton originally published at thesocietypages org on november from a quick cheer to a standing ovation clap to show how much you enjoyed this story futurist writer polytopia philosophy science science fiction latest news info and tutorials on artificial intelligence machine learning deep learning big data and what it means for humanity
Greg Fish,1,4,https://worldofweirdthings.com/why-you-just-cant-black-box-an-a-i-d7c41e7d9123?source=tag_archive---------5----------------,why you just can’t black box an a.i. – [ weird things ],singularitarians generally believe two things about artificial intelligence first and foremost they say it s just a matter of time before we have an ai system that will quickly become superhumanly intelligent secondly and a lot more ominously they believe that this system can sweep away humanity not because it will be evil by nature but because it won t care about humans or what happens to them and one of the biggest priorities for a researcher in the field should be figuring out how to build a friendly artificial intelligence training it like one would train a pet with a mix of operant conditioning and software while the first point is one i ve covered before and pointed out again and again that superhuman is a very relative term and that computers are in many ways already superhuman without being intelligent the second point is one that i haven t yet given a proper examination and neither have vocal singularitarians why because if you read any of the papers on their version of friendly ai yo ll soon discover how quickly they begin to describe the system they re trying to tame as a black box with mostly known inputs and measurable outputs hardly a confident and lucid description of how an artificial intelligence functions and ultimately what rules will govern it no problem there say the singularitarians the system will be so advanced by the time this happens that we ll be very unlikely to know exactly how it functions anyway it will modify its own source code optimize how well it performs and generally be all but inscrutable to computer scientists sounds great for comic books but when we re talking about real artificially intelligent systems this approach sounds more like surrendering to robots artificial neural networks and bayesian classifiers to come up with whatever intelligence they want and send all the researchers and programmers out for coffee in the meantime artificial intelligence will not grow from a vacuum it will come together from systems used to tackle discrete tasks and governed by several if not one common frameworks that exchange information between these systems i say this because the only forms of intelligence we can readily identify are found in living things which use a brain to perform cognitive tasks and since brains seem to be wired this way and we re trying to emulate the basic functions of the brain it wouldn t be all that much of a stretch to assume that we d want to combine systems good at related tasks and build on the accomplishments of existing systems and to combine them we ll have to know how to build them conceiving of an ai in a black box is a good approach if we want to test how a particular system should react when working with the ai and focusing on the system we re trying to test by mocking the ai s responses down the chain of events think of it as dependency injection with an ai interfacing system but by abstracting the ai away what we ve also done is made it impossible to test the inner workings of the ai system no wonder then that the singularitarian fellows have to bring in operant conditioning or social training to basically housebreak the synthetic mind into doing what they need it to do they have no other choice in their framework we cannot simply debug the system or reset its configuration files to limit its actions but why have they resigned to such an odd notion and why do they assume that computer scientists are creating something they won t be able to control even more bizarrely why do they think that an intelligence that can t be controlled by its creators could be controlled by a module they ll attach to the black box to regulate how nice or malevolent towards humans it would be wouldn t it just find away around that module too if it s superhumanly smart wouldn t it make a lot more sense for its creators to build it to act in cooperation with humans by watching what humans say or do treating each reaction or command as a trigger for carrying out a useful action it was trained to perform and that brings us back full circle to train machines to do something we have to lay out a neural network and some higher level logic to coordinate what the networks outputs mean we ll need to confirm that the training was successful before we employ it for any specific task therefore we ll know how it learned what it learned and how it makes its decisions because all machines work on propositional logic and hence would make the same choice or set of choices at any given time if it didn t we wouldn t use it so of what use is a black box ai here when we can just lay out the logical diagram and figure out how it s making decisions and how we alter its cognitive process if need be again we could isolate the components and mock their behavior to test how individual sub systems function on their own eliminating the dependencies for each set of tests beyond that this block box is either a hindrance to a researcher or a vehicle for someone who doesn t know how to build a synthetic mind but really really wants to talk about what he imagines it will be like and how to harness its raw cognitive power and that s ok really but let s not pretend that we know that an artificial intelligence beyond its creators understanding will suddenly emerge form the digital aether when the odds of that are similar to my toaster coming to life and barking at me when it thinks i want to feed it some bread from a quick cheer to a standing ovation clap to show how much you enjoyed this story techie rantt staff writer and editor computer lobotomist science tech and other oddities
Greg Fish,2,3,https://worldofweirdthings.com/why-do-we-want-to-build-a-fully-fledged-a-g-i-1658afc3f758?source=tag_archive---------6----------------,why do we want to build a fully fledged a.g.i.? – [ weird things ],undoubtedly the most ambitious idea in the world of artificial intelligence is creating an entity comparable to a human in cognitive abilities the so called agi we could debate how it may come about whether it will want to be your friend or not whether it will settle the metaphysical question of whet makes humans who they are or open new doors in the discussion but for a second let s think like software architects and ask the question we should always tackle first before designing anything why would we want to build it what will we gain a sapient friend or partner we don t know that will we figure out what makes human ticks maybe maybe not since what works in the propositional logic of artificial neural networks doesn t necessarily always apply to an organic human brain will we settle the question of how an intellect emerges not really since we would only be providing one example and a fairly controversial one at that and what exactly will a g in agi entail will we need to embody it for it to work and if not how would we develop the intellectual capacity of an entity extant in only abstract space will we have anything in common with it and could we understand what it wants and there s more to it than that even though i just asked some fairly heavy questions were we to build an agi not by accident but by design we would effectively be making the choice to experiment on a sapient entity and that s something that may have to be cleared by an ethics committee otherwise we re implicitly saying that an artificial cognitive entity has no rights to self determination and that may be fine if it doesn t really care about them but what if it does what if the drive for freedom evolves from a cognitive routine meant for self defense and self perpetuation if we steer an ai model away from sapience by design are we in effect snuffing out an opportunity or protecting ourselves we can always suspend the model debug it and see what s going on in its mind but again the ethical considerations will play a significant part and very importantly while we will get to know what such an agi thinks and how we may not know how it will first emerge the whole agi concept is a very ambiguous effort at defining intelligence and hence doesn t give us enough to objectively determine an intelligent artificial entity when we make one because we can always find an argument for and against how to interpret the results of an experiment meant to design one we barely even know where to start now i could see major advantages to fusing with machines and becoming cyborgs in the near future as we d swap irreparably damaged parts and pieces for d printed titanium tungsten carbide and carbon nanotubes to overcome crippling injury or treat an otherwise terminal disease i could also see a huge upside to having direct interfaces to the machines around us to speed up our work and make life more convenient but when it comes to such an abstract and all consuming technological experiment as agi the benefits seem to be very very nebulous at best and the investment necessary seems extremely uncertain to pay off since we can t even define what will make our agi a true agi rather than another example of a large expert system whereas with wetware and expert systems we can measure our return on investment with lives saved or significant gains in efficiency how do we justify creating another intelligent entity after many decades of work especially if it turns out that we actually can t make one or it turns out to be completely different than what we hoped it would be as it nears completion but maybe i m wrong maybe there s a benefit to an agi that i m overlooking and if that is the case enlighten me in the comments because this is a serious question why peruse an agi from a quick cheer to a standing ovation clap to show how much you enjoyed this story techie rantt staff writer and editor computer lobotomist science tech and other oddities
James Faghmous ,187,6,https://medium.com/@nomadic_mind/new-to-machine-learning-avoid-these-three-mistakes-73258b3848a4?source=tag_archive---------0----------------,New to Machine Learning? Avoid these three mistakes,machine learning ml is one of the hottest fields in data science as soon as ml entered the mainstream through amazon netflix and facebook people have been giddy about what they can learn from their data however modern machine learning i e not the theoretical statistical learning that emerged in the s is very much an evolving field and despite its many successes we are still learning what exactly can ml do for data practitioners i gave a talk on this topic earlier this fall at northwestern university and i wanted to share these cautionary tales with a wider audience machine learning is a field of computer science where algorithms improve their performance at a certain task as more data are observed to do so algorithms select a hypothesis that best explains the data at hand with the hope that the hypothesis would generalize to future unseen data take the left panel in the figure in the header the crosses denote the observed data projected in a two dimensional space in this case house prices and their corresponding size in square meters the blue line is the algorithm s best hypothesis to explain the observed data it states there is a linear relationship between the price and size of a house as the house s size increases so does its price in linear increments now using this hypothesis i can predict the price of an unseen datapoint based on its size as the dimensions of the data increase the hypotheses that explain the data become more complex however given that we are using a finite sample of observations to learn our hypothesis finding an adequate hypothesis that generalizes to unseen data is nontrivial there are three major pitfalls one can fall into that will prevent you from having a generalizable model and hence the conclusions of your hypothesis will be in doubt occam s razor is a principle attributed to william of occam a th century philosopher occam s razor advocates for choosing the simplest hypothesis that explains your data yet no simpler while this notion is simple and elegant it is often misunderstood to mean that we must select the simplest hypothesis possible regardless of performance in their paper in nature johan nyberg and colleagues used a level artificial neural network to predict seasonal hurricane counts using two or three environmental variables the authors reported stellar accuracy in predicting seasonal north atlantic hurricane counts however their model violates occam s razor and most certainly doesn t generalize to unseen data the razor was violated when the hypothesis or model selected to describe the relationship between environmental data and seasonal hurricane counts was generated using a four layer neural network a four layer neural network can model virtually any function no matter how complex and could fit a small dataset very well but fail to generalize to unseen data the rightmost panel in the top figure shows such incident the hypothesis selected by the algorithm the blue curve to explain the data is so complex that it fits through every single data point that is for any given house size in the training data i can give you with pinpoint accuracy the price it would sell for it doesn t take much to observe that even a human couldn t be that accurate we could give you a very close estimate of the price but to predict the selling price of a house within a few dollars every single time is impossible the pitfall of selecting too complex a hypothesis is known as overfitting think of overfitting as memorizing as opposed to learning if you are a child and you are memorizing how to add numbers you may memorize the sums of any pair of integers between and however when asked to calculate you will be unable to because you have never seen or and therefore couldn t memorize their sum that s what happens to an overfitted model it gets too lazy to learn the general principle that explains the data and instead memorizes the data data leakage occurs when the data you are using to learn a hypothesis happens to have the information you are trying to predict the most basic form of data leakage would be to use the same data that we want to predict as input to our model e g use the price of a house to predict the price of the same house however most often data leakage occurs subtly and inadvertently for example one may wish to learn for anomalies as opposed to raw data that is a deviations from a long term mean however many fail to remove the test data before computing the anomalies and hence the anomalies carry some information about the data you want to predict since they influenced the mean and standard deviation before being removed the are several ways to avoid data leakage as outlined by claudia perlich in her great paper on the subject however there is no silver bullet sometimes you may inherit a corrupt dataset without even realizing it one way to spot data leakage is if you are doing very poorly on unseen independent data for example say you got a dataset from someone that spanned but you started collecting you own data from onward if your model s performance is poor on the newly collected data it may be a sign of data leakage you must resist the urge to retrain the model with both the potentially corrupt and new data instated either try to identify the causes of poor performance on the new data or better yet independently reconstruct the entire dataset as a rule of thumb your best defense is to always be mindful of the possibility of data leakage in any dataset sampling bias is the case when you shortchange your model by training it on a biased or non random dataset which results in a poorly generalizable hypothesis in the case of housing prices sampling bias occurs if for some reason all the house prices sizes you collected were of huge mansions however when it was time to test your model and the first price you needed to predict was that of a bedroom apartment you couldn t predict it sampling bias happens very frequently mainly because as humans we are notorious for being biased nonrandom samplers one of the most common examples of this bias happens in startups and investing if you attend any business school course they will use all these case studies of how to build a successful company such case studies actually depict the anomalies and not the norm as most companies fail for every apple that became a success there were other startups that died trying so to build an automated data driven investment strategy you would need samples from both successful and unsuccessful companies the figure above figure is a concrete example of sampling bias say you want to predict whether a tornado is going to originate at certain location based on two environmental conditions wind shear and convective available potential energy cape we don t have to worry about what these variables actually mean but figure shows the wind shear and cape associated with tornado cases we can fit a model to these data but it will certainly not generalize because we failed to include shear and cape values when tornados did not occur in order for our model to separate between positive tornados and negative no tornados events we must train it using both populations there you have it being mindful of these limitations does not guarantee that your ml algorithm will solve all your problems but it certainly reduces the risk of being disappointed when your model doesn t generalize to unseen data now go on young jedi train your model you must from a quick cheer to a standing ovation clap to show how much you enjoyed this story nomadic mind sometimes the difference between success and failure is the same as between and living is in the details
Datafiniti,3,5,https://blog.datafiniti.co/classifying-websites-with-neural-networks-39123a464055?source=tag_archive---------1----------------,Classifying Websites with Neural Networks – Knowledge from Data: The Datafiniti Blog,at datafiniti we have a strong need for converting unstructured web content into structured data for example we d like to find a page like and do the following both of these are hard things for a computer to do in an automated manner while it s easy for you or me to realize that the above web page is selling some jeans a computer would have a hard time making the distinction from the above page from either of the following web pages or both of these pages share many similarities to the actual product page but also have many key differences the real challenge though is that if we look at the entire set of possible web pages those similarities and differences become somewhat blurred which means hard and fast rules for classifications will fail often in fact we can t even rely on just looking at the underlying html since there are huge variations in how product pages are laid out in html while we could try and develop a complicated set of rules to account for all the conditions that perfectly identify a product page doing so would be extremely time consuming and frankly incredibly boring work instead we can try using a classical technique out of the artificial intelligence handbook neural networks here s a quick primer on neural networks let s say we want to know whether any particular mushroom is poisonous or not we re not entirely sure what determines this but we do have a record of mushrooms with their diameters and heights along with which of these mushrooms were poisonous to eat for sure in order to see if we could use diameter and heights to determine poisonous ness we could set up the following equation a diameter b height or for not poisonous poisonous we would then try various combinations of a and b for all possible diameters and heights until we found a combination that correctly determined poisonous ness for as many mushrooms as possible neural networks provide a structure for using the output of one set of input data to adjust a and b to the most likely best values for the next set of input data by constantly adjusting a and b this way we can quickly get to the best possible values for them in order to introduce more complex relationships in our data we can introduce hidden layers in this model which would end up looking something like for a more detailed explanation of neural networks you can check out the following links in our product page classifier algorithm we setup a neural network with input layer with nodes hidden layer with nodes and output layer with output nodes our input layer modeled several features including our output layer had the following our algorithm for the neural network took the following steps the ultimate output is two sets of input layers t and t that we can use in a matrix equation to predict page type for any given web page this works like so so how did we do in order to determine how successful we were in our predictions we need to determine how to measure success in general we want to measure how many true positive tp results as compared to false positives fp and false negatives fn conventional measurements for these are our implementation had the following results these scores are just over our training set of course the actual scores on real life data may be a bit lower but not by much this is pretty good we should have an algorithm on our hands that can accurately classify product pages about of the time of course identifying product pages isn t enough we also want to pull out the actual structured data in particular we re interested in product name price and any unique identifiers e g upc ean isbn this information would help us fill out our product search we don t actually use neural networks for doing this neural networks are better suited toward classification problems and extracting data from a web page is a different type of problem instead we use a variety of heuristics specific to each attribute we re trying to extract for example for product name we look at the h and h tags and use a few metrics to determine the best choice we ve been able to achieve around a accuracy here we may go into the actual metrics and methodology for developing them in a separate post we feel pretty good about our ability to classify and extract product data the extraction part could be better but it s steadily being improved in the meantime we re also working on classifying other types of pages such as business data company team pages event data and more as we roll out these classifiers and data extractors we re including each one in our crawl of the entire internet this means that we can scan the entire internet and pull out any available data that exists out there exciting stuff you can connect with us and learn more about our business people product and property apis and datasets by selecting one of the options below from a quick cheer to a standing ovation clap to show how much you enjoyed this story instant access to web data building the world s largest database of web data follow our journey
Theo,3,4,https://becominghuman.ai/is-there-a-future-for-innovation-18b4d5ab168f?source=tag_archive---------4----------------,Is there a future for innovation ? – Becoming Human: Artificial Intelligence Magazine,have you noticed how tech savvy children have become but are no longer streetwise i read a friend s thoughts on his own site last week and there was a slight pang of regret in where technology and innovation seems to be leading us all and so i started to worry about where the concept of innovation is going for future generations there s an increasing reliance on technology for the sake of convenience children are becoming self reliant too quickly but gadgets are replacing people as the mentor the human bonding of parenthood is a prime example of where it s taking a toll i ve seen parents hand over idevices to pacify a child numerous times now the lullaby and bedtime reading session has been replaced with cut the rope and automated storybooks apps i know a child who has developed speech difficulty because he s been brought up on cable tv and a ds lite pronouncing words as he has heard them from a tiny speaker and not by watching how his parents pronounce them and i started to worry about how the concept of innovation is being redefined for future generations i used my imagination constantly as a child and it s still as active now as it was then but i didn t use technology to spoon feed me the next generation expect innovation to happen at their fingertips with little to no real stimuli steve jobs said stay hungry stay foolish and he was right innovation comes from a keenness it s a starvation and hunger that drives people forward to spark and create it comes from grabbing what little there is from the ether and turning it into something spectacular it s the big bang of human thought creation and i started to worry about what the concept of innovation means for future generations technology is taking away the power to think for ourselves and from our children everything must be there and in real time for instant consumption it s junk food for the mind and we re getting fat on it and that breeds lazy innovation we ve become satiated before we reach the point of real creativity nobody wants to bother taking the time to put it all together themselves any more it has to be ready for us and we re happy to throw it away if it doesn t work first time use it or lose it there s less sweat and toil involved if we don t persevere with failure remember seeing the human race depicted in wall e that s where innovation is heading and because of this we risk so many things disappearing for the sake of convenience we re all guilty of it i m guilty of it i was asked once what would become absurd in ten years thinking about it i realized we re on the cusp of putting books on the endangered species list real books books bound in hard and paperback not digital copies from a kindle store and that scared me because the next generation of kids may grow up never seeing one or experience sitting with their father as he reads an old battered copy of the hobbit because he ll be sitting there handing over an ipad with the hobbit read along app teed up and it ll be an actors voice not his father s voice pretending to be a bunch of trolls about to eat a company of dwarfs innovation is a magical crazy concept it stems from a combination of crazy imagination human interaction and creativity not convenient manufacture technology can aid collaboration in ways we ve never experienced before but it can t run crazy for us and for the sake of future generations don t let it here s to the crazy ones indeed from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder and ceo rawshark studios latest news info and tutorials on artificial intelligence machine learning deep learning big data and what it means for humanity
x.ai,1,2,https://medium.com/@xdotai/i-scheduled-1-019-meetings-in-2012-and-that-doesnt-count-reschedules-x-ai-278d7e824eb3?source=tag_archive---------5----------------,"I scheduled 1,019 meetings in 2012 — and that doesn’t count reschedules — x.ai",the number of meetings that i scheduled in might seem astronomical put in context it s less so i was a startup founder at the time and that year my company visual revenue took series a funding doubled revenue and started discussing a possible exit i like the number though as a startup romantic one could turn it into a nifty malcolm gladwell type rule of thumb called the meetings rule gladwell s claim that greatness requires an enormous time sacrifice rings true to me whether that means investing hours into a subject matter to become an expert or conducting a meetings per year is another question more interesting though is the impact of this figure and a related one of those more than one thousand meetings i scheduled were rescheduled that was painful but these numbers were among the early pieces of data that inspired me to start x ai a meeting is defined as an event in my calendar which is marginally flawed in both directions given some events would be travel to jfk which is obviously a task and not a meeting where others would be interview sales director candidates which is really meetings in originally published at x ai on october from a quick cheer to a standing ovation clap to show how much you enjoyed this story magically schedule meetings
Arjan Haring 🔮🔨,1,5,https://medium.com/@happybandits/website-morphing-and-more-revolutions-in-marketing-8a5cabc60576?source=tag_archive---------6----------------,Website morphing and more revolutions in marketing – Arjan Haring 🔮🔨 – Medium,john r hauser is the kirin professor of marketing at m i t s sloan school of management where he teaches new product development marketing management and statistical and research methodology he has served mit as head of the mit marketing group head of the management science area research director of the center for innovation in product development and co director of the international center for research on the management of technology he is the co author of two textbooks design and marketing of new products and essentials of new product management and a former editor of marketing science now on the advisory board i think it wouldn t be smart to start this interview with something as dull and complex as a definition or am i the only one that likes to read light weight and short articles let s just get it over with website morphing matches the look and feel of a website to each customer so that over a series of customers revenue or profit are maximized that didn t hurt as much as i expected i actually love the idea it sounds completely logical but are we talking about a completely new idea there is tremendous variety in the way customers process and use information some prefer simple recommendations while others like to dig into the details some customers think verbally or holistically others prefer pictures and graphs what is new is that we now have good algorithms to identify how customers think from the choices they make as they explore websites their clickstream but once we identify the way they think we still need an automatic way to learn which website look and feel will lead to the most sales this is a very complex problem which fortunately has a relative simple solution based on fundamental research by john gittins our contribution was to combine the identification algorithms with the learning algorithms and develop an automated system that was feasible and practical once we developed the technology to morph websites we were only limited by our imaginations in our first application we matched the look and feel to customers cognitive styles in our second we matched to cognitive and cultural styles we then used the algorithms to morph banner advertisements to achieve almost a lift in click through rates our latest project used both cognitive styles and the customers search history to match the automotive banner advertising to enhance clicks consideration and purchase likelihood i also love the fact that you combine technology with behavioral science on the psychology side of things you are have been busy with cognitive styles cognitive switching and cognitive simplicity can you tell us a little bit more about these theories and why you chose to use them customers are smart they know when to use simple decision rules cognitive simplicity and when to use more complicated rules our research has been two fold website morphing and banner morphing figure out how customers think and provides information in the format that helps them think the way they prefer to think we have also focused on identifying consideration heuristics typically customers seriously consider only a small fraction of available product to do so they use simple rules that balance thinking and search costs with the completeness of information by understanding these simple rules managers can develop better products and better marketing strategies we can now identify these decision rules quickly with machine learning methods but a caveat customers do not always use cognitively simple rules the moment of truth in a final purchase decision is best understood with more complex decision rules and methods such as choice based conjoint analysis most recently we ve combined the two streams of research curiously some of the algorithms used by the computer to morph websites are reasonably descriptive of how consumers take the future into account in purchases they make today prior research postulated a form of hyperrationality our research suggests that consumers are pretty smart about balancing cognitive costs and foresight what are your main interests on the technology side of website morphing which algorithms take your fancy and why website morphing uses an index solution to learn the best morph for a customer our latest efforts also identify when to morph a website by embedded another dynamic program within the index solution in our research to understand how consumers deal with the future we ve demonstrated that indices other than gittins index might be more descriptive of consumer foresight if i think about it as a company you can either win the algorithm competition or the psychology competition or lose do you agree actually the companies that will thrive are those that understand the customers cognitive processes have the algorithms to match products and marketing to customers cognitive processes and have the organization that accepts such innovation you need all three is this what marketing will be about in years there are many revolutions in marketing it is an exciting time it s hard to list all of the changes but here are a few big data we know so much more about customers than we ever did before but this knowledge is often hidden within the volume of data one challenge is to develop methods that scale well to be data machine learning there are some problems that humans solve better than computers and some problems that computers solve better than humans morphing identifying simple decision rules and studying consumer foresight are all possible with the advent of good machine learning methods but we have only scratched the surface causality marketing has used quite successfully small sample laboratory experiments and assumption laden quantitative models however the advent of big data and web based data collection has made it possible to do experiments and quasi experiments on a large scale to better establish causality and to better develop theories that are externally valid causality also means replication there is a strong movement in the journals to require that key finding be replicated the tpm movement theory practice in marketing conferences special issues and organizations are now devoted to matching managerial needs to research with impact in fact a recent survey by the informs society of marketing science suggests that approximately of the researchers in marketing believe that research should be more focused on applications a maturing perspective on behavioral science researchers are increasingly less focused on cute findings that apply only in special circumstances they are beginning to focus on insights that have a big impact effect size and apply to decisions that customers make routinely companies that combine algorithms an understanding of customer decision making and the ability to use data will be the companies that succeed originally published at www sciencerockstars com on october from a quick cheer to a standing ovation clap to show how much you enjoyed this story let s fix the future scientific advisor jadatascience
Arjan Haring 🔮🔨,1,5,https://medium.com/i-love-experiments/using-artificial-intelligence-to-balance-out-customer-value-a251b0ccae6f?source=tag_archive---------8----------------,Using Artificial Intelligence to Balance Out Customer Value,december it was that time again the second edition of projectwaalhalla social sciences for startups this time with peter van der putten speaking as data scientist he is guest researcher at the data mining group algorithms research cluster of the leiden institute of advanced computer science he is also director of decisioning solutions worldwide at pegasystems there is according to peter a lot of potential for new startups in this area are you going to be the next success story i am actually very curious what you as a leading data scientist think of this whole big data thingy i am fascinated by learning from data but have mixed feelings about big data the concept is being hyped a lot at this moment while the algorithms to learn from data have been studied since the s in computer science many of the modern big data technologies like hadoop are in fact still limited frameworks for old fashioned offline batch processed data instead of real time processed data the focus should really not have to be on the data but on the analysis how we generate knowledge and learn from data through data mining and more importantly how do we operationalize this knowledge how can we use this knowledge because knowledge is not power action is and what is the role of psychology in big data and of philosophy psychology has begun studying intelligence fifty or sixty years before computer science did people animals plants and all intelligent systems are basically information processing machinery psychology seeks to understand these systems and to tries explain behavior if you understand a bit of that system you can use this knowledge for example to teach computers stupid mathematical pieces of scrap smarter functions such as learning and responding to customer behavior what is to say thinking the other away around that people don t have to think like computers see for example the psychologist daniel kahneman who won the the sveriges riksbank prize in economic sciences the unofficial nobel prize in economics for his insight that people aren t rational agents that properly weigh all the choices before deciding something and philosophy these guys have dealt with big data for more than years now just think of the nature vs nurture debate do we acquire intelligence and other properties by data experience or are they innate or the whole philosophy of mind discussion with roots in the ancient greeks what do we really know and is there is only experience or just reality you have a background in artificial intelligence ai and even studied with the famous and wildly attractive bas haring not related well cousin to be honest if you insist what could ai mean for business and how is it different from big data as long as ai is not used for old fashioned data manipulation or poor reporting but really as intelligent data science big data is one of the tools within learning artificial intelligence that is systems that are not smart by the knowledge that is pre inserted but which have the capacity to learn and combine what is learned with background knowledge to deduct decisions this is what i like to call the field of decisioning really intelligent systems put that knowledge into action and are part of an ecosystem an environment with other actors systems people and the scary outside world sounds abstract until the late s artificial intelligence was only done in the lab now people interact with ai unconsciously on a daily basis for example if they use google check their facebook page or look at banners on the web take the company where i work next to my academic job when i came in it was a startup of only men with new software and a launching customer editor s note we know that feeling ten years and two acquisitions later we have reach more than billion consumers with intelligent data driven scientifically proven real time recommendations via digital as well as traditional channels like atms shops and callcenters no push product offerings anymore but only next best action recommendations that optimize customer value by balancing customer experience and predicted interests and behavior what opportunities do you see for startups in the artificial intelligence in this area well i see tremendous opportunities not only for pure ai startups but for all startups if you look at the startups in silicon valley in high tech and biotech artificial intelligence is a major part of the business every startup should consider whether data is a key asset or a barrier to entry and how ai or data mining can be used to convert these data into money where i have to note that customers and citizens rightly so are getting more critical after all nsa issues those who can use this technology in a way that it not only benefits companies but especially customers will be the most successful in conclusion i am curious about how much you you are looking forward to december and what should happen during projectwaalhalla that would make your wildest dreams come true very much looking forward to it in terms of wildest dreams i heard a reunion concert of the urban dance squad is not going to happen which i understand but i look forward to exchanging views with startups freelancers and multinationals on how to create with the help of raw data diamands and a magical mix of data mining machine learning decisioning and evidence based and real time marketing i will bring some nice metaphorical pictures and leave will double integrals at home editor s note an uds reunion sounds like a plan to us originally published at www sciencerockstars com on december from a quick cheer to a standing ovation clap to show how much you enjoyed this story let s fix the future scientific advisor jadatascience a blog series about the discipline of business experimentation how to run and learn from experiments in different contexts is a complex matter but lays at the heart of innovation
Shivon Zilis,1200,10,https://medium.com/@shivon/the-current-state-of-machine-intelligence-f76c20db2fe1?source=tag_archive---------0----------------,The Current State of Machine Intelligence – Shivon Zilis – Medium,the machine intelligence landscape and post can be found here i spent the last three months learning about every artificial intelligence machine learning or data related startup i could find my current list has of them to be exact yes i should find better things to do with my evenings and weekends but until then why do this a few years ago investors and startups were chasing big data i helped put together a landscape on that industry now we re seeing a similar explosion of companies calling themselves artificial intelligence machine learning or somesuch collectively i call these machine intelligence i ll get into the definitions in a second our fund bloomberg beta which is focused on the future of work has been investing in these approaches i created this landscape to start to put startups into context i m a thesis oriented investor and it s much easier to identify crowded areas and see white space once the landscape has some sort of taxonomy what is machine intelligence anyway i mean machine intelligence as a unifying term for what others call machine learning and artificial intelligence some others have used the term before without quite describing it or understanding how laden this field has been with debates over descriptions i would have preferred to avoid a different label but when i tried either artificial intelligence or machine learning both proved to too narrow when i called it artificial intelligence too many people were distracted by whether certain companies were true ai and when i called it machine learning many thought i wasn t doing justice to the more ai esque like the various flavors of deep learning people have immediately grasped machine intelligence so here we are computers are learning to think read and write they re also picking up human sensory function with the ability to see and hear arguably to touch taste and smell though those have been of a lesser focus machine intelligence technologies cut across a vast array of problem types from classification and clustering to natural language processing and computer vision and methods from support vector machines to deep belief networks all of these technologies are reflected on this landscape what this landscape doesn t include however important is big data technologies some have used this term interchangeably with machine learning and artificial intelligence but i want to focus on the intelligence methods rather than data storage and computation pieces of the puzzle for this landscape though of course data technologies enable machine intelligence which companies are on the landscape i considered thousands of companies so while the chart is crowded it s still a small subset of the overall ecosystem admissions rates to the chart were fairly in line with those of yale or harvard and perhaps equally arbitrary i tried to pick companies that used machine intelligence methods as a defining part of their technology many of these companies clearly belong in multiple areas but for the sake of simplicity i tried to keep companies in their primary area and categorized them by the language they use to describe themselves instead of quibbling over whether a company used nlp accurately in its self description if you want to get a sense for innovations at the heart of machine intelligence focus on the core technologies layer some of these companies have apis that power other applications some sell their platforms directly into enterprise some are at the stage of cryptic demos and some are so stealthy that all we have is a few sentences to describe them the most exciting part for me was seeing how much is happening in the application space these companies separated nicely into those that reinvent the enterprise industries and ourselves if i were looking to build a company right now i d use this landscape to help figure out what core and supporting technologies i could package into a novel industry application everyone likes solving the sexy problems but there are an incredible amount of unsexy industry use cases that have massive market opportunities and powerful enabling technologies that are begging to be used for creative applications e g watson developer cloud alchemyapi reflections on the landscape we ve seen a few great articles recently outlining why machine intelligence is experiencing a resurgence documenting the enabling factors of this resurgence kevin kelly for example chalks it up to cheap parallel computing large datasets and better algorithms i focused on understanding the ecosystem on a company by company level and drawing implications from that yes it s true machine intelligence is transforming the enterprise industries and humans alike on a high level it s easy to understand why machine intelligence is important but it wasn t until i laid out what many of these companies are actually doing that i started to grok how much it is already transforming everything around us as kevin kelly more provocatively put it the business plans of the next startups are easy to forecast take x and add ai in many cases you don t even need the x machine intelligence will certainly transform existing industries but will also likely create entirely new ones machine intelligence is enabling applications we already expect like automated assistants siri adorable robots jibo and identifying people in images like the highly effective but unfortunately named deepface however it s also doing the unexpected protecting children from sex trafficking reducing the chemical content in the lettuce we eat helping us buy shoes online that fit our feet precisely and destroying s classic video games many companies will be acquired i was surprised to find that over of the eligible non public companies on the slide have been acquired it was in stark contrast to big data landscape we created which had very few acquisitions at the time no jaw will drop when i reveal that google is the number one acquirer though there were more than different acquirers just for the companies on this chart my guess is that by the end of almost another will be acquired for thoughts on which specific ones will get snapped up in the next year you ll have to twist my arm big companies have a disproportionate advantage especially those that build consumer products the giants in search google baidu social networks facebook linkedin pinterest content netflix yahoo mobile apple and e commerce amazon are in an incredible position they have massive datasets and constant consumer interactions that enable tight feedback loops for their algorithms and these factors combine to create powerful network effects and they have the most to gain from the low hanging fruit that machine intelligence bears best in class personalization and recommendation algorithms have enabled these companies success it s both impressive and disconcerting that facebook recommends you add the person you had a crush on in college and netflix tees up that perfect guilty pleasure sitcom now they are all competing in a new battlefield the move to mobile winning mobile will require lots of machine intelligence state of the art natural language interfaces like apple s siri visual search like amazon s firefly and dynamic question answering technology that tells you the answer instead of providing a menu of links all of the search companies are wrestling with this large enterprise companies ibm and microsoft have also made incredible strides in the field though they don t have the same human facing requirements so are focusing their attention more on knowledge representation tasks on large industry datasets like ibm watson s application to assist doctors with diagnoses the talent s in the new ai vy league in the last years most of the best minds in machine intelligence especially the hardcore ai types worked in academia they developed new machine intelligence methods but there were few real world applications that could drive business value now that real world applications of more complex machine intelligence methods like deep belief nets and hierarchical neural networks are starting to solve real world problems we re seeing academic talent move to corporate settings facebook recruited nyu professors yann lecun and rob fergus to their ai lab google hired university of toronto s geoffrey hinton baidu wooed andrew ng it s important to note that they all still give back significantly to the academic community one of lecun s lab mandates is to work on core research to give back to the community hinton spends half of his time teaching ng has made machine intelligence more accessible through coursera but it is clear that a lot of the intellectual horsepower is moving away from academia for aspiring minds in the space these corporate labs not only offer lucrative salaries and access to the godfathers of the industry but the most important ingredient data these labs offer talent access to datasets they could never get otherwise the imagenet dataset is fantastic but can t compare to what facebook google and baidu have in house as a result we ll likely see corporations become the home of many of the most important innovations in machine intelligence and recruit many of the graduate students and postdocs that would have otherwise stayed in academia there will be a peace dividend big companies have an inherent advantage and it s likely that the ones who will win the machine intelligence race will be even more powerful than they are today however the good news for the rest of the world is that the core technology they develop will rapidly spill into other areas both via departing talent and published research similar to the big data revolution which was sparked by the release of google s bigtable and bigquery papers we will see corporations release equally groundbreaking new technologies into the community those innovations will be adapted to new industries and use cases that the googles of the world don t have the dna or desire to tackle opportunities for entrepreneurs my company does deep learning for x few words will make you more popular in that is if you can credibly say them deep learning is a particularly popular method in the machine intelligence field that has been getting a lot of attention google facebook and baidu have achieved excellent results with the method for vision and language based tasks and startups like enlitic have shown promising results as well yes it will be an overused buzzword with excitement ahead of results and business models but unlike the hundreds of companies that say they do big data it s much easier to cut to the chase in terms of verifying credibility here if you re paying attention the most exciting part about the deep learning method is that when applied with the appropriate levels of care and feeding it can replace some of the intuition that comes from domain expertise with automatically learned features the hope is that in many cases it will allow us to fundamentally rethink what a best in class solution is as an investor who is curious about the quirkier applications of data and machine intelligence i can t wait to see what creative problems deep learning practitioners try to solve i completely agree with jeff hawkins when he says a lot of the killer applications of these types of technologies will sneak up on us i fully intend to keep an open mind acquihire as a business model people say that data scientists are unicorns in short supply the talent crunch in machine intelligence will make it look like we had a glut of data scientists in the data field many people had industry experience over the past decade most hardcore machine intelligence work has only been in academia we won t be able to grow this talent overnight this shortage of talent is a boon for founders who actually understand machine intelligence a lot of companies in the space will get seed funding because there are early signs that the acquihire price for a machine intelligence expert is north of x that of a normal technical acquihire take for example deep mind where price per technical head was somewhere between m if we choose to consider it in the acquihire category i ve had multiple friends ask me only semi jokingly shivon should i just round up all of my smartest friends in the ai world and call it a company to be honest i m not sure what to tell them at bloomberg beta we d rather back companies building for the long term but that doesn t mean this won t be a lucrative strategy for many enterprising founders a good demo is disproportionately valuable in machine intelligence i remember watching watson play jeopardy when it struggled at the beginning i felt really sad for it when it started trouncing its competitors i remember cheering it on as if it were the toronto maple leafs in the stanley cup finals disclaimers i was an ibmer at the time so was biased towards my team the maple leafs have not made the finals during my lifetime yet so that was purely a hypothetical why do these awe inspiring demos matter the last wave of technology companies to ipo didn t have demos that most of us would watch so why should machine intelligence companies the last wave of companies were very computer like database companies enterprise applications and the like sure i d like to see a x more performant database but most people wouldn t care machine intelligence wins and loses on demos because the technology is very human enough to inspire shock and awe business models tend to take a while to form so they need more funding for longer period of time to get them there they are fantastic acquisition bait watson beat the world s best humans at trivia even if it thought toronto was a us city deepmind blew people away by beating video games vicarious took on captcha there are a few companies still in stealth that promise to impress beyond that and i can t wait to see if they get there demo or not i d love to talk to anyone using machine intelligence to change the world there s no industry too unsexy no problem too geeky i d love to be there to help so don t be shy i hope this landscape chart sparks a conversation the goal to is make this a living document and i want to know if there are companies or categories missing i welcome feedback and would like to put together a dynamic visualization where i can add more companies and dimensions to the data methods used data types end users investment to date location etc so that folks can interact with it to better explore the space questions and comments please email me thank you to andrew paprocki aria haghighi beau cronin ben lorica doug fulop david andrzejewski eric berlow eric jonas gary kazantsev gideon mann greg smithies heidi skinner jack clark jon lehr kurt keutzer lauren barless pete skomoroch pete warden roger magoulas sean gourley stephen purpura wes mckinney zach bogue the quid team and the bloomberg beta team for your ever helpful perspectives disclaimer bloomberg beta is an investor in adatao alation aviso brightfunnel context relevant mavrx newsle orbital insights pop up archive and two others on the chart that are still undisclosed we re also investors in a few other machine intelligence companies that aren t focusing on areas that were a fit for this landscape so we left them off for the full resolution version of the landscape please click here from a quick cheer to a standing ovation clap to show how much you enjoyed this story partner at bloomberg beta all about machine intelligence for good equal parts nerd and athlete straight up canadian stereotype and proud of it
Roland Trimmel,20,6,https://medium.com/@rolandt25/will-all-musicians-become-robots-6221171c5d18?source=tag_archive---------1----------------,Will All Musicians Become Robots? – Roland Trimmel – Medium,finally we see the rise of the machines and with it develops a certain fear that artificial intelligence ai will render humans useless this question was posed at boston s a e conference last month by a team member at landr their company had received death threats from people in the mastering industry after having released a diy drag and drop instant online mastering service powered by ai algorithms it illustrates the resistance that the world of ai has incited amongst us some fear that robots will take over a la terminator some fear that the virtual and artificial will replace the visceral some cite religious views and others frankly others just seem ignorant that sets the tone for our own journey into artificial intelligence and the lessons we have learned from it we had spent more than three years developing algorithms to enable software to read and interpret a composition song like an expert does coming from a music and technology background our team was hugely excited having accomplished this make no mistake it s really difficult to make a computer understand music for us this was an important first step towards a new generation of intelligent music instruments that assist the user in the songwriting process for faster completion of complex tasks resulting in no interruption of the creative flow and more creative output when you spend so many years working on a technology product you run the risk of losing sight of the market and this being our first product we had absolutely no idea what to expect to find out we had to bring the product to the attention of the target group and eagerly awaited their reaction that meant a lot of leg work for us in starting discussions on multiple forums and collecting users feedback it takes time to cut through the noise but creates some great threads what was interesting for us to monitor is how the discussions about our product unfolded on those forums and how opinions were split between two camps one that embraced what we do and the other that was characterized by anger fear or a complete misunderstanding of what our software does at times we felt like being in the middle of the fight between machines and humans we hadn t expected this our aim was to make a cool product that shows what the technology is capable of doing eventually we spent lots of time clearing misunderstandings explaining our product better etc to win over those forum members hearts for what we do and occasionally we also had to calm down a heated discussion between members insulting each other caused by a fear that our product eliminates the craft in music composition today enter a different reality we have made a lot of progress with our software much of it is down to communicating openly with our community to address any questions they may have early and involve them deeply in product development has the tone in discussions about our technology changed yes certainly it has but please don t think it s an easy journey it s still hard to convince music producers to rely on the help of a piece of software that in some regard replicates processes of the human brain the efforts that go into being a pioneer and driving this perceptional battle are driving one close to insanity it s an endless stream of work and it requires endurance like during marathons or triathlons here are five things that we learned from our journey that we d like to share with you so you can judge better before dismissing ai in music let me start with a quick discussion of the first and second digital wave in music the first digital wave brought about digital music technology like synths and daw s and with that everything changed sound synthesis and sampling made entirely new forms of expressiveness possible sequencers in combination with large databases of looping clips laid the foundation for electronic dance music which led to a multifaceted artistic and cultural revolution the second digital wave has been rolling along for a few years now and it is washing up intelligent algorithms for processing audio and midi as an example ai s can already help control the finishing mastering process of music tracks as assistant tools or even fully automated in the not too distant future and we re talking only years from now we will be used to incredible music making automatons controlling most complex harmonic figures flawlessly imitating the greatest artists the output quality by such algorithms is unbelievable computer intelligence can aimlessly merge styles of various artists and apply them to yet another piece all that without breaking a sweat we regard the main application of ai s for music composition and production as helper tools not artists in their own regard and this is not cheating we have been utilizing digital production tools for decades it was just a matter of time for more complicated and intelligent code to emerge but rest assured computers will rather not generate music all by themselves the art and craft of composing will prevail there will always be human beings behind the actual output controlled by an ai it will help though to create less complex leaner user interfaces in the tools we use for creating music that are simpler to operate on to the learning we promised you now definitely not the magic and final decision over creative output will always remain with the human artist a computer is not a human with feelings and emotions what makes us get to our knees in awe will keep machines clinically indifferent simple as that and technical approximations as deceptive as they may get are simply not the real thing it already is there is no stopping it but then that is the course of a natural evolutionary process which can only push forward a huge one this is a game changer read our statement on main applications above it is our egos we cling on to having trotted down the same paths for decades many believe their laboriously acquired expertise is threatened by robot technology and a new ruthless generation the truth is if we embrace ai s as our helping friends and maybe even learn how to think a little more technical who can fathom how ingeniously more colorful the world of music will become in the hands of talented musicians of all generations yes because it enables a completely new generation of products and startups like us push for innovation the agreeable side effect it will make people happy musicians consumers and businessmen alike full circle most importantly though it is not only ai changing the music industry social changes are equally responsible for it if they don t account for a larger part for it anyway here s an excellent article by fast company on this topic and more coverage on a e in this article by techrepublic it s an interesting time for all of us in music and beyond and there s so much yet to come don t be afraid humans also prevailed in terminator there are things machines will never do they cannot possess faith they cannot commune with god they cannot appreciate beauty they cannot create art if they ever learn these things they won t have to destroy us they ll be us sarah connor image credit daft punk top re compose middle from a quick cheer to a standing ovation clap to show how much you enjoyed this story
Espen Waldal,57,6,https://medium.com/bakken-b%C3%A6ck/how-artificial-intelligence-can-improve-online-news-7a24889a6940?source=tag_archive---------2----------------,How Artificial Intelligence can improve online news,that being said the user experience for online news sites today is very much like it was ten and fifteen years ago see the slideshow showing the evolution of nyt com you enter a homepage where a carefully selected combination of articles on sports celebrity reality shows dinner recipes and even actual news scream for your attention there s a huge focus on page views and hardly any attention given to personal relevance for the reader smart use of technology could improve the online news experience vastly by just adding a bit more structure that is why we created orbit rich structured data is the foundation for taking the online news experience to the next level orbit is a collection of artificial intelligence technology api s using machine learning based content analysis to automatically transform unstructured text into rich structured data by analyzing and organizing content in real time and automatically tagging and structuring large pieces of text into clusters of topics orbit creates a platform where you can build multiple data rich applications the now month old leaked innovation report from the new york times pointed to several challenges for keeping and expanding a digital audience to face some of the most critical issues you need to create a better experience for the reader by serving up better recommendations of related content providing new ways to discover news and add context introducing personalization and filtering relevance is essential to creating loyal readers and even more so in a time where more and more visits to news sites go directly to a specific article mainly due to search and social media avoiding the front page altogether readers arriving through side doors like twitter or facebook are less engaged than readers arriving directly which means it s important to keep these visitors on the site and convert them into loyal readers yet so little is being done to improve the relevance of recommendations and create a connection to the huge amounts of valuable content that already exists orbit understands not only the topics a piece contains but also related topics it thereby understands the context of the article and can bring up related content that the reader wouldn t otherwise have seen extending the reader s time spent on the site and increasing page views understanding context means that the cluster of topics related to an article on china signing a historic gas deal with russia includes topics such as russia ukraine putin gazprom and energy thus creating recommendations within that cluster and creating connections between content rich structured data opens up for new ways to navigate and discover news the classic navigation through carefully edited front pages has pretty much been the same since the dawn of online news publishing structured data enables the reader to follow certain topics or stories improves search and enables timeline navigation of a news story to help the reader better understand the context of the story and how it has developed at the same time a journalist writing a story on the uproar in ukraine has no possible way of knowing how the story will unfold in the weeks to come manual tagging of news stories leads to inconsistent and incomplete structures due to a subjective understanding of which topics are important and related machine learning based content analysis can identify people organizations and places and relate them to each other in real time thereby identifying related stories as they unfold and cluster them together as the nyt innovation report brought up the true value of structured data emerges only when the content is structured equally throughout news and content apps like circa omni and prismatic and news sites like vox have incorporated some of these elements and are experimenting with how to develop original ways to discover news there are many arguments against personalization and they are often related to the dystopian fear of a fragmented public sphere or the horrors of the echo chamber that doesn t mean personalization can t be a good thing it merely means being aware of what a particular type of user wants at a particular time we are not talking about a fully customizable news feed based on your subjective interests meaning i will not only see articles related to manchester united finance tv shows and kim kardashian and be uninformed on all other topics we are merely suggesting a smart filtering system and adjustments of what subjects you would like to see more and less of on your feed after all we do have different interests for example you may be entirely disinterested in tour de france during its three week media frenzy in july each year unfollow topic or turn the volume down today getting the news isn t the hard part filtering out the excessive info and navigating the overwhelming stream of news in a smart way is where you need great tools a foundation of rich structured data will not only benefit the reader but make life easier for journalists and editors as well to provide context to a story about syria you could add several components of extra information that would enrich the article a box of background information on the conflict facts about bashar al assad and the different syrian rebel groups and so forth with rich structured data in place you can automatically add relevant fact boxes and other interactive elements to a piece of content based on third party content databases such as wikipedia topics can automatically generate their own page with all the related articles facts visualizations and insights relevant for that specific topic cluster moreover you can use the data to create new and compelling presentations of your content including visualizations and timelines that give the reader a better experience and new insights news content generally has a short lifespan but this doesn t mean that old content can t be valuable in a new context a consistent structuring of archived content will give new life to old content making it easier to reuse and resurrect articles that are still relevant and create connections between old and new articles what are the trending topics people or organizations this week what regions got the most media attention how many of the sources were anonymous how many were women versus men knowing more about your audience s preferences will make it easier to create good content at the right time better organized content creates a strong foundation for good insights into how content is consumed and why with a better ecosystem for your content including higher relevance and more contextual awareness you can present better context based ads to your advertisers and give better insights into who is watching and acting on them by using the right technology in smart ways journalists and editors can focus on what they are best at creating quality news content orbit ai from a quick cheer to a standing ovation clap to show how much you enjoyed this story product manager at bakken b ck the bakken b ck blog
Joe Johnston,38,4,https://medium.com/universal-mind/how-i-tracked-my-house-movements-using-ibeacons-3e1e9da3f1a9?source=tag_archive---------3----------------,How I tracked my house movements using iBeacons. – Universal Mind – Medium,recently i ve started experimenting more and more with ibeacons being part of the r d group at universal mind i ve had the opportunity to do a lot of testing and exploring of different products in doing so i wanted to see how someone could utilize ibeacons without building your own app just yet i ll tackle this in a future post the first step was to find ibeacons we could use for our testing our first choice was ordering the ibeacons from estimote and after waiting for them to arrive they never did we ordered other beacons from various companies the first set to arrive was from roximity which came to us as a set of dev ibeacons next i wanted to see if i could track movements in my own house just as a simple test without creating a custom app i looked for a few apps that could detect the ibeacons and execute an action there are a few apps capable of doing this but all of them were somewhat limiting the only app i found that allowed me to control what happens when triggering an ibeacon was an app called launch here formally placed although this app wasn t a perfect fit it did allow me to call some actions after triggering an ibeacon launch here allows you to use custom url schemes these url schemes allow you to open apps and even populate an action one of the more complicated tasks of setting up any ibeacon manually is you need to gather some infomation on the ibeacon itself the key pieces of info each ibeacon contains is a uuid major id and minor id to get this info you can install an app like locate for ibeacon that detects ibeacons and shows this information once you have this info you can set up your ibeacons using launch here its a bit cumbersome to set each one up but you only have to do it once as a side note the launch here app is a bit touchy when setting up the ibeacons so be warned you may have to re enter the url scheme info if you fat finger it like i said before the launch here app is controlled by the user it triggers a lock screen notification when you turn on your phone and are less then meters away from any ibeacon this is a bit interesting but it s the approch that launch here took so they could give the user a bit of control when triggering actions ideally this all would happen behind the scenes to the user in a custom app the custom url schemes are pretty powerful but you still need to manually trigger them here s my set up i have the tumblr app installed on my phone which has the ablity to use a post url scheme the url scheme looks like this tumblr x callback url text title kitchen once that url scheme is triggered from launch here it opens tumblr and pre populates a text post with the word kitchen or with the name of the room i set i manually tap post and its added this allows me to capture each ibeacon location and store the data the next step was to create a more data friendly format i love using a service called ifttt it s a very power platform that allows you to automatically trigger other services i created a ifttt recipe that auto adds a row to a google spreadsheet with the time stamp and text that is entered into a text post to my tumblr account now i have a time stamped dataset tracking my movement in my house at least the three rooms i have set up with that data you can imagine how you can start to break it apart here s just an example of my current break down based on room as you can see it s possible to track your movement albeit a bit cumbersome taking this data and bubbling it up to the user could be very compelling in certain situations i m just using my personal home location here but you can see how this could be very powerful in other settings i am the director of user experience research development at universal mind a digital solutions agency you can follow me on twitter at merhl from a quick cheer to a standing ovation clap to show how much you enjoyed this story experience service design director sparksgrove the experience design division of northhighland alum of hugeinc universalmind startgarden a collection of articles created by universal mind thinkers
Nadav Gur,10,9,https://medium.com/the-vanguard/why-natural-search-is-awesome-and-how-we-got-here-fe69b9cdd0db?source=tag_archive---------5----------------,Why Natural Search is Awesome and How We Got Here – The Vanguard – Medium,the evolution of desti s search interface this is a story about how one ambitious start up tackled this subject that has riddled people like google apple facebook and others and came up with some pretty clever conclusions if i may say so myself in we were building desti a holistic travel search app i e it would search for everything from hotels through attractions to restaurants using post siri natural language understanding tech and with powerful semantic search capabilities on the back end that allowed desti to reason meaningfully about search results and make highly informed suggestions desti s search was built on a premise that sounds very simple but it s actually very hard to pull off we believe that people should be able to ask specifically for what they re interested in and get results that match this sounds reasonable right if i m looking for a beach resort on the kona coast in hawaii it s pretty obvious what i want and if i also want it to be kid friendly and pet friendly i should just be able to ask for it our goal was to get users inputting relevant specific queries because that s what people need that s where desti shines saving you time and effort by delivering exactly what you want now let s assume that desti knows which hotels on the kona coast are actually beach resorts are kid friendly and pet friendly how can we make expressing this query easy and intuitive for the user episode i desti is siri s sister or conversational user interface when we started we were very nai ve about this we said first let s just put a search box in there allowing the user to type or say whatever they want and let s make sure we understand this then let s leave that box there so they can react to what they see and provide more detail refine or search for something else in that context e g a restaurant near the resort we called this pivot and let s run a conversation around it kind of like siri what could be more natural to do this we used sri international s vpa platform which is almost literally a post siri natural language interaction platform with which you can have a conversation in context this is more or less what it looked like in our beta version search box a conversational ui we launched this monitored use and quickly realized is that early users split into two groups discarding the nd group we re busy people we learned that people don t know how to interact naturally with computers or they have no idea what to ask or expect so they revert to the most primitive queries problem is our goal was to answer interesting specific queries because we believe that if we give you a great answer that caters to what you want your likelihood of buying is that much higher furthermore absolutely no one got the conversational aspect the fact you can continue refining and pivoting through conversation we decided to take away the focus from conversation for the time being episode vegas slot machines or make it dead simple we realized we have to focus on the first query and give people some cues about what s possible and came up with this interface these contextual spinners turned interaction from a totally open ended query to something closer to multiple choice questions in essence these were interchangeable templates where you could get ideas for what to search for as well as easily input your query what you picked would show up as a textual query in the search bar which we hoped people would realize they can edit or add to hoped the results on the one hand progress we saw longer and more interesting queries and more interaction however when talking to users we realized that they were assuming that the spinner was a kind of menu system which means a they can only pick what s in the menu b they have to pick one thing from each menu so while this was better than what most sites have for search it was still a far cry from what we wanted to deliver here s what we learned from this episode fill in the blanks smartly at this stage it was clear that we needed better auto suggest and smarter auto complete this is similar from a ui perspective to google instant but desti is about semantic search not keyword matching in most cases google will auto suggest a phrase that matches what you ve been typing and has been typed in by many other people desti should suggest something that semantically matches what you entered and makes sense given what we know of the destination and about your trip because desti is new and there haven t been a million users searching for the same things before you desti should reason about what you may ask not suggest something someone else asked we realized we have to build a lot of semantically reasonable and statistically relevant auto suggesting we still wanted to keep to the template logic because we believed it helps users think about what they are looking for and form the query in their minds so we came up with a ui that blends form filling and natural language entry and focused on building smart auto suggest and auto complete this ui was built of a number of rigid fields e g location type that adapt to the subject matter so if the type is hotel you re prompted for dates and a free text field that allows you to ask for whatever else you want we iterated a lot over the auto complete and auto suggest features the first thing is to realize they are different with auto complete you have a user who already thought of something to type in and you have to guess what that is with auto suggest you really want to inspire the user into adding something useful to their query which means it needs to be relevant to whatever you know about the query and user so far but not overwhelming for the user all this requires knowing a lot about specific destinations what do people search for in hawaii vs new york and specific types what s relevant for hotels vs museums also on the visual side what the user is putting in is often quantitative and easier to set than type e g a date a price etc so we came up with our first crack at blending text with visual widgets the results were a big improvement in the quality and relevance of queries over the previous ui but a feeling that this was still too stiff and rigid when people are asked for a type of place e g a museum a park a hotel they often can t really answer and it s easier for them to think about a feature of the place instead e g that they can go hiking or biking see art or eat breakfast for linguistic reasons it s easier for people to say that they want a romantic hotel than a hotel that s romantic so while this ui was very expressive often it felt unnatural and limiting furthermore many users just ended up filling the basic fields and not adding any depth in the open text field despite various visual cues and editing a query for refining or pivoting was hard at the same time the auto suggest auto complete elements we ve built at this stage werealmost enough to allow us to just throw out the limiting templates and move to one search field but this time a damn clever one episode search goes natural to the naked eye this looks like we ve gone full circle one text box parsed queries shown as tags what could be simpler well not exactly because we still need queries to be meaningful one thing that the templates gave us was built in disambiguation we need a query that has at least a location a type or something from which we can derive a type and without a template telling us that the hotel is the type and the restaurant is something you want your hotel to have vs maybe the opposite the system needs to better understand the grammatical structure or the sentence and cue you into inputting things the right way when it s suggesting and auto completing typing a query the query is understood you can add edit with this new user interface changing queries refining and pivoting is very natural add tags or take away tags widgets were contextually integrated using the auto suggest drop down menu so they are naturally suggested at the right time e g after you said you were looking for a hotel we help you choose when how many rooms etc it s also very easy to suggest things to search for based on the context for instance if we know your kids are traveling with you we d drop in family friendly and you could dismiss it with one click so where is this going so far natural search looks and behaves better than anything else we ve seen in this space from now on most of the focus is on making the guesses even smarter with more statistic reasoning about what people ask for in different contexts and more contextual info driving those guesses we believe this ui is where vertical search is heading consider how nice it would be to input gifts for year old boys under into target com s search bar or romantic restaurant with great seafood near times square with a table at pm tonight into opentable and get relevant answers but then again answering specific queries is not that easy either but that s the other side of desti to be continued from a quick cheer to a standing ovation clap to show how much you enjoyed this story i think then i talk sometime it s the other way around founded ran companies in ai mobile travel etc ex eir at sri int l ex aerospace nadav gur s tech musings
Pandorabots,14,3,https://medium.com/pandorabots-blog/using-oob-tags-in-aiml-part-i-21214b4d2fcd?source=tag_archive---------6----------------,Using OOB Tags in AIML: Part I – pandorabots-blog – Medium,suppose you are building an intelligent virtual agent or virtual personal assistant vpa that uses a pandorabot as the natural language processing engine you might want this vpa to be able to perform tasks such as sending a text message adding an event to a calendar or even just initiating a phone call oob tags allow you to do just that oob stands for out of band which is an engineering term used to refer to activity performed on a separate hidden channel for a pandorabot vpa this translates to activities which fall outside of the scope of an ordinary conversation such as placing a phone call checking dynamic information like the weather or searching wikipedia for the answer to some question the task is executed but does not necessarily always produce an effect on the conversation between the pandorabot and the user oob tags are used in aiml templates and are written in the following format oob command oob the command that is to be executed is specified by a set of tags which occur within the oob tags these inner oob tags can be whatever you like and the phone related actions they initiate are defined in your applications code to place a call you might see something like this oob dial some phone number dial oob the dial tag within the oob tag sends a message to the phone to dial the number specified when your client indicates they want to dial a number your application will receive a template containing the command specified inside the oob tag within your application this inner command will be interpreted and the appropriate actions will be executed it is useful to think of the activities initiated by oob tags as falling into one of two categories based on whether they return information to the user via the chat interface or not the first category those that do not return information typically involve activities that interrupt the conversation if you ask your vpa to look up restuarants on a map it will open up your map application and perform a search similarly if you ask your bot to make a phone call it will open the dialer application and make a call in both of these examples the activity performed interrupts the conversation and displays some other screen the second category those that do return information to the user via the chat interface are generally actions that are executed in the background of the conversation if you ask your pandorabot to look up the population of the united states on wikipedia it will perform the search and then return the results of the search to the user via the chat window similarly if you ask your pandorabot to send a text message to the friend it will send the text and then return a message to the user via the chat window indicating the success of the action i e your text message was delivered in this second set of examples it is useful to distinguish between those activities whose results will be returned directly to the user like the wikipedia example and those activities whose successful completion will simply be indicated to the user through the chat interface as with the texting example here is an example of a category that uses the phone dialer on android here is an example interaction this category would lead to human dial robot calling here is a slightly more complicated example involving the oob tag which launches a browser and performs a google search human look up pandorabots robot searching searching please stand by note not shown in the previous example is the category random search phrase which delivers a random selection from a short list of possible replies each indicating to the user that the bot correctly interpreted their search request for a complete list of oob tags as implemented in the callmom virtual personal assistant app for android as well as usage examples click here be sure to look out for the upcoming post using oob tags in aiml part ii which will go over a basic example of how to intrepret the oob tags received from the pandorabots server within the framework of your own vpa application originally published at blog pandorabots com on october from a quick cheer to a standing ovation clap to show how much you enjoyed this story the largest most established chatbot development and hosting platform www pandorabots com the leading platform for building and deploying chatbots
Denny Vrandečić,4,4,https://medium.com/@vrandezo/ai-is-coming-and-it-will-be-boring-94768de264c6?source=tag_archive---------7----------------,"AI is coming, and it will be boring – Denny Vrandečić – Medium",i was asked about my opinion on this topic and i thought i would have some profound thoughts on this but i ended up rambling and this post doesn t really make any single strong point tl dr don t worry about ais killing all humans it s not likely to happen in an interview with the bbc stephen hawking stated that the development of full artificial intelligence could spell the end of the human race whereas this is hard to deny it is rather trivial any sufficiently powerful tool could potentially spell the end of the human race given a person who knows how to use that tool in order to achieve such a goal there are far more dangerous developments for example global climate change the arsenal of nuclear weapons or an economic system that continues to sharpen inequality and social tension ai will be a very powerful tool like every powerful tool it will be highly disruptive jobs and whole industries will be destroyed and a few others will be created just as electricity the car penicillin or the internet ai will profoundly change your everyday life the global economy and everything in between if you want to discuss consequences of ai here are a few that are more realistic than human extermination what will happen if ai makes many jobs obsolete how do we ensure that ais make choices compliant with our ethical understanding how to define the idea of privacy in a world where your car is observing you what does it mean to be human if your toaster is more intelligent than you the development of ai will be gradual and so will the changes in our lifes and as ai keeps developing things once considered magical will become boring a watch you could talk to was powered by magic in disney s classic the beauty and the beast and years later you can buy one for less than a hundred dollars a self driving car was the protagonist of the s tv show knight rider and thirty years later they are driving on the streets of california a system that checks if a bird is in a picture was considered a five year research task in september and less than two months later google announces a system that can provide captions for pictures including birds and these things will become boring in a few years if not months we will have to remind ourselves how awesome it is to have a computer in our pocket that is more powerful than the one that got apollo to the moon and back that we can make a video of our children playing and send it instantaneously to our parents on another continent that we can search for any text in almost any book ever written technology is like that what s exciting today will become boring tomorrow so will ai in the next few years you will have access to systems that will gradually become capable to answer more and more of your questions that will offer advice and guidance towards helping you navigate your life towards the goal you tell it that will be able to sift through text and data and start to draw novel conclusions they will become increasingly intelligent and there are two major scenarios that people are afraid of at this point the skynet scenario is just mythos there is no indication that raw intelligence is sufficient to create intrinsic intention or will the paperclip scenario is more realistic and once we get closer to systems with such power we will need to put the right safeguards in place the good news is that we will have plenty of ais at our disposal to help us with that the bad news is that discussing such scenarios now is premature we simply don t know how these systems will look like that s like starting a committee a hundred years ago to discuss the danger coming from novel weaponry no one in could have predicted nuclear weapons and their risks it is unlikely that the results of such a committee would have provided much relevant ethical guidance for the manhattan project three decades later why should that be any different today in summary there are plenty of consequences of the development of ai that warrant intensive discussion economical consequences ethical decisions made by ais etc but it is unlikely that they will bring the end of humanity background image robots trashing living room by vincekamp licensed under cc by nd personal permanent url http simia net wiki ai is coming and it will be boring from a quick cheer to a standing ovation clap to show how much you enjoyed this story wikidata founder google ontologist semantic web researcher and author
Thaddeus Howze,15,6,https://medium.com/@ebonstorm/of-comets-and-gods-in-the-making-4f55ecccb9fe?source=tag_archive---------8----------------,Of Comets and Gods in the Making – Thaddeus Howze – Medium,asferit had not grown up she didn t know where she came from could not conceive of childhood no memories of parents no recollection of family on the vast empty world that served as her lab she built the probes and put a little bit of herself in each one her machine form ancient slow and sputtering came to life wheezing through the long corridors of the silent lab its darkness masking the distant empty spaces which asferi imagined were once filled with life she looked through her thoughts and realized she had lost any hope of memory that part of her was already circling a distant star aborning with life she looked in on those places when she woke to see the results of her work on so many worlds life spawned with the next launch she would lose the memory of those places there was so little of her remaining enough for three no four probes then she would cease to remember why she was what she was she would forget how to exist but not yet she completed the next probe winding the engine and orienting it along the galactic plane her sensor array aligning the probe with a wandering comet she planned to deposit herself within the life giving molecules within its frozen mass she knew little about her past but knew that she must not be able to be found this was the only memory that remained hide from the darkness as she loaded the last probe she considered the first probe she ever sent millennia ago there were monuments within the halls of the lab in her hubris then she considered them a successful reincarnation of her people each representation was filled with the temporal signature of that once great race a temporal residue of failure it spoke of a great race masters of time and space they flourished in the dark between the stars then the darkness came she was overconfident she slept assured of their success her mission completed in the time between sleeping and waking her cycle of regeneration before attempting to seed again the great race was gone found they did not heed the warnings she sent in those early days she gave far more of herself then she came to them in visions taught them secrets to harness the hidden nature of matter revealed to them the nature of energies both planetary and interstellar they would worship her revere her and believe her to be a god in the end it was not enough they were consumed their greatness undone she sent less and less of herself from then on godhood failed them perhaps obscurity would serve them better she sent less each time only tiny packages of micromachines capable of changing matter capable of modifying genomes empowering the creatures spawned of her with abilities even greater than the first race psychometric representations of them were all that remained echoes in the timestream of history in their hubris they ruptured time and space and like the world her lab hung above cracked the crust of their world and were lost in a temporal vortex of their own making they had such potential squandered then she began sending only the memory of what she was embedded within complex epigentic echoes no longer would she shape the universe for them they would have to work for their survival perhaps they would be stronger for it she appeared to her descendents only in dreams visions of what they were memories of who she was memories she no longer possessed her memory was great once and she seeded thousands of worlds with it but like the ephemeral nature of memory so few knew what they saw many went mad most dreamed of demiurges mad deva whose powers ravaged worlds these memories destroyed half of them before they could achieve spaceflight and reach for the stars themselves religions they spawned consumed them now she sent only cells and precellular matter the very least of herself the essence of who she was the final matter of her being hidden in comets cloaked in meteor swarms hidden on the boots of other starfarers time had taught her patience though she had lost her memories she was confident of this final strategy to hide herself on millions of worlds her final probe ships would leave a legacy on millions of worlds she found the last star she would use and loaded the final probe ship with the hardiest constructions she had ever made she deconstructed the worldship her lab her home for millenia of millenia breaking down every part of it reforging it for a final effort the planet below was also consumed her last effort would require everything it was a long dead world lost to antiquity when the universe was young of the darkness she could not remember but she knew this as long as there was light her people would survive the final instructions to her probeship would have her descending into her planet s unstable star it s final fluctuations revealed what she knew was the inevitable outcome and she planned to use it to her advantage her final self would not be aware of the result the final cells of her body were distributed within millions of pieces of her world and her lab each calibrated to arrive at a star somewhere in her galaxy each single cell would find a world ready for life she could no longer coerce planets into life she could no longer force matter or energy to take the shape she deemed she was now only able to influence the tiniest aspects asferit would only be able to nudge a planet toward life the darkness would always be ready to claim her people but now they would be scattered to worlds within the galaxy and without she seeded the galactic wind and waited for a supernova to blow them where it would her starseeds hardened against the impending blastwave they would with the tiniest bit of her final design travel faster than light toward their final destinations as the star which lit her world gave her people life watched them die and patiently waited until they could be reborn exploded asferit now waited in turn in those last seconds as the waves of radiation and coronal debris swept over the remnants of her cannibalized world she subsumed herself within the starseeds and the near immortal being asferit last of her kind was no more and yet now she was pure purpose no ambitions no plan no dreams of godhood no longer a radiant harbingers of dooms lighting the skies of primitive worlds she would be the essence of life itself the darkness be damned of comets and gods in the making thaddeus howze all rights reserved thaddeus howze is a popular and recently awarded top writer recipient on the q a site quora com he is also a moderator and contributor to thescience fiction and fantasy stack exchange with over fourteen hundred articles in a four year period thaddeus howze is a california based technologist and author who has worked with computer technology since the s doing graphic design computer science programming network administration teaching computer science and it leadership his non fiction work has appeared in numerous magazines huffington post gizmodo black enterprise the good men project examiner com the enemy panel frame science x loud journal comicsbeat com and astronaut com he maintains a diverse collection of non fiction at his blog a matter of scale his speculative fiction has appeared online at medium scifiideas com and theau courant press journal he has appeared in twelve different anthologies in the united states the united kingdom and australia a list of his published work appears on his website hub city blues from a quick cheer to a standing ovation clap to show how much you enjoyed this story author editor futurist activist tech humanist http bit ly thowzebio http bit ly thpatreon
Tommy Thompson,17,14,https://medium.com/@t2thompson/ailovespacman-9ffdd21b01ff?source=tag_archive---------9----------------,Why AI Research Loves Pac-Man – Tommy Thompson – Medium,ai and games is a crowdfunded youtube series on the research and applications of ai within video games the following article is a more involved transcription of the topics discussed in the video linked to above if you enjoy this work please consider supporting my future content over on patreon artificial intelligence research has shown a small infatuation with the pac man video game series over the past years but why specifically pac man what elements of this game have proven interesting to researchers in this time let s discuss why pac man is so important in the world of game ai research for the sake of completes and in appreciating there is arguably a generation or two not familiar with the game puck man was an arcade game launched in by namco in japan and renamed pac man upon being licensed by midway for an american release the name change was driven less by a need for brand awareness but rather because the name can easily be de faced to say something else the original game focuses on the titular character who must consume as many pills as possible without being caught by one of four antagonists represented by ghosts the four ghosts inky blinky pinky and clyde all attempt to hunt down the player using slightly different tactics from one another each ghost has their own behaviour a bespoke algorithm that dictates how they attack the player players also have the option to consume one of several power pills that appear in each map power pills allow for the player to not just eat pills but the enemy ghosts for a short period of time while mechanically simple when compared to modern video games it provides an interesting test bed for ai algorithms learning to play games the game world is relatively simple in nature but complex enough that strategies can be employed for optimal navigation furthermore the varied behaviours of the ghosts reinforces the need for strategy since their unique albeit predictable behaviours necessitate different tactics if problem solving can be achieved at this level then there is opportunity for it to scale up to more complex games while pac man research began in earnest in the early s work by john koza koza discussed how pac man provides an interesting domain for genetic programming a form of evolutionary algorithm that learns to generate basic programs the idea behind koza s work and later that of rosca was to highlight how pac man provides an interesting problem for task prioritisation this is quite relevant given that we are often trying to balance the need to consume pills all the while avoiding ghosts or when the opportunity presents itself eating them about years later people became more interested in pac man as a control problem this research was often with the intent to explore the applications of artificial neural networks for the purposes of creating a generalised action policy software that would know at any given tick in the game what would be the correct action to take this policy would be built from playing the game a number of times and training the system to learn what is effective and what is not typically these neural networks are trained using an evolutionary algorithm that finds optimal network configurations by breeding collections of possible solutions and using a survival of the fittest approach to cull weak candidates kalyanpur and simon explored how evolutionary learning algorithms could be used to improve strategies for the ghosts in time it was evident that the use of crossover and mutation which are key elements of most evolutionary based approaches was effective in improving the overall behaviour however it s important to note that they themselves acknowledge their work uses a problem domain similar to pac man and not the actual game gallagher and ryan uses a slightly more accurate representation of the original game while the screenshot is shown here the actual implementation only used one ghost rather than the original four in this research the team used an incremental learning algorithm that tailored a series of rules for the player that dictate how pac man is controlled using a finite state machine fsm this proved highly effective in the simplified version they were playing the use of artificial neural networks a data structure that mimics the firing of synapses in the brain was increasingly popular at the time and once again in most recent research two notable publications on pac man are lucas which attempted to create a move evaluation function for pac man based on data scraped from the screen and processed as features e g distance to closest ghost while gallagher and ledwich attempted to learn from raw unprocessed information it s notable here that the work by lucas was in fact done on ms pac man rather than pac man while perhaps not that important to the casual observer this is an important distinction for ai researchers research in the original pac man game caught the interest of the larger computational and artificial intelligence community you could argue it was due to the interesting problem that the game presents or that a game as notable as pac man was now considered of interest within the ai research community while it is now something that appears commonplace games more specifically video games did not receive the same attention within ai research circles as they do today as high quality research in ai applications in video games grew it wasn t long before those with a taste for pac man research moved on to looking at ms pac man given the challenges it presents which we are still conducting research for in ms pac man is odd in that it was originally an unofficial sequel midway who had released the original pac man in the united states had become frustrated at namco s continued failure to release a sequel while namco did in time release a sequel dubbed super pac man which in many ways is a departure from the original midway decided to take matters into their own hands ms pac man was for lack of a better term a mod originally conceived by the general computing company based in massachusetts gcc had got themselves into a spot of legal trouble with midway having previously created a mod kit for popular arcade game missile command as a result gcc were essentially banned from making further mod kits without the original game s publisher providing consent despite the recent lawsuit hanging over them they decided to show midway their pac man mod dubbed crazy otto who liked it so much they bought it from gcc patched it up to look like a true pac man successor and released it in arcades without namco s consent though this has been disputed note for our younger audience mod kits in the s were not simply software we could use to access and modify parts of an original game these were actual hardware printed circuit boards pcbs that could either be added next to the existing game in the arcade unit or replace it entirely while nowhere near as common nowadays due to the rise of home console gaming there are many enthusiasts who still use and trade pcbs fitted for arcade gaming ms pac man looks very similar to the original albeit with the somewhat stereotypical bow on ms pac man s hair head and a couple of minor graphical changes however the sequel also received some small changes to gameplay that have a significant impact one of the most significant changes is that the game now has four different maps in addition the placement of fruit is more dynamic and they move around the maze lastly a small change is made to the ghost behaviour such that periodically the ghosts will commit a random move otherwise they will continue to exhibit their prescribed behaviour from the original game each of these changes has a significant impact on both how humans and ai subsequently approach the problem changes made to the maps do not have a significant impact upon ai approaches for many of the approaches discussed earlier it is simply another configuration of the topography used to model the maze or if the agent is using more egocentric models for input i e relative to the pac man then these is not really considered given the input is contextual this is only an issue should the agent s design require some form or pre processing or expert rules that are based explicitly upon the configuration of the map with respect to a human this is also not a huge task the only real issue is that a human would have become accustom to playing on a given map devising strategies that utilise parts of the map to good effect however all they need is practice on the new maps in time new strategies can be formulated the small change to ghost behaviour which results in random moves occurring periodically is highly significant this is due to the fact that the deterministic model that the original game has is completely broken previously each ghost had a prescribed behaviour you could with some computational effort determine the state and indeed the location of a ghost at frame n of the game where n is a certain number of steps ahead of the current state any implementation that is reliant upon this knowledge whether it is using it as part of a heuristic or an expert knowledge base that gives explicit instructions based on the assumption of their behaviour is now sub optimal if the ghosts can make random decisions without any real warning then we no longer have the same level of confidence in any of our ghost prediction strategies similarly this has an impact on human players the deterministic behaviour of the ghosts in the original pac man while complex can eventually be recognised by a human player this has been recognised by the leading human players who could factor their behaviour at some level into their decision making process however in ms pac man the change to a non deterministic domain has a similar effect to humans as it does ai we can no longer say with complete confidence what the ghosts will do given they can make random moves evidence that a particular type of problem or methodology has gained some traction in a research community can be found in competitions if a competition exists that is open to the larger research community it is in essence a validation that this problem merits consideration in the case of ms pac man there have been two competitions the first competition was organised by simon lucas at the time a professor at the university of essex in the uk with the first competition held at the conference on evolutionary computation cec in it was subsequently held at a number of conferences notably ieee conference on computational intelligence and games cig until http dces essex ac uk staff sml pacman pacmancontest html this competition used a screen capture approach previously mentioned in lucas that was reliant on an existing version of the game while the organisers would use microsoft s own version from the revenge of arcade title you could also use the likes the webpacman for testing given it was believed to run the same rom code as shown in the screenshot the code is actually taking information direct from the running game one benefit of this approach is that it denies the ai developer from accessing the code to potentially cheat you can t access source code and make calls to the likes of the ghosts to determine their current move instead the developer is required to work with the exact same information that a human player would a video of the winner from the ieee cig competition ice pambush can be seen in the video below in simon lucas in conjunction with philipp rohlfshagen and david robles created the ms pac man vs ghosts competition in this iteration the screen scraping approach had been replaced with a java implementation of the original game this provided an api to develop your own bot for competitions this iteration ran at four conferences between and one of the major changes to this competition is that you can now also write ai controllers for the ghosts competitors submissions were then pitted against one another the ranking submission for both ms pac man and the ghosts from the league is shown below during the earlier competition there was a continued interest in the use of learning algorithms this ranged from the of an evolutionary algorithm which we had seen in earlier research to evolve code that is the most effective at this problem this ranged from evolving fuzzy systems that use a rules driven by fuzzy logic yes that is a real thing shown in handa to the use of influence maps in wirth and a different take that uses ant colony optimisation to create competitive players emilio et al this research also stirred interest from researchers in reinforcement learning a different kind of learning algorithm that learns from the positive and negative impacts of actions note it has been argued that reinforcement learning algorithms are similar to that of how the human brain operates in that feedback is sent to the brain upon committing actions over time we then associate certain responses with good or bad outcomes placing your hand over a naked flame is quickly associated as bad given that it hurts simon lucas and peter burrow took to the competition framework as means to assess whether reinforcement learning specifically an approach called temporal difference learning would yield stronger returns than evolving neural networks burrow and lucas the results appeared to favour the use neural nets over the reinforcement learning approach despite that one of the major contributions ms pac man has generated is research into monte carlo methods an approach where repeated sampling of states and actions allow us to ascertain not only the reward that we will typically attain having made an action but also the value of the state more specifically there has been significant exploration of whether monte carlo tree search mcts an algorithm that assesses the potential outcomes at a given state by simulating the outcome could prove successful mcts has already proven to be effective in games such as go chaslot et al and klondike solitaire bjarnason et al naturally given this is merely an article on the subject and not a literature review we cannot cover this in immense detail however there has been a significant number of papers focussed on this approach for those interested i would advise you read browne et al which gives an extensive overview of the method and it s applications one of the reasons that this algorithm proves so useful is that it attempts to address the issue of whether your actions will prove harmful in the future much of the research discussed in this article is very good at dealing with immediate or reflex responses however few would determine whether actions would hurt you in the long term this is hard to determine for ai without putting some processing power behind it and even harder when working in a dynamic video game that requires quick responses mcts has proven useful since it can simulate whether an action taken on the current frame will be useful frames in the future and has led to significant improvements in ai behaviour while ms pac man helped push mcts research many resarchers have now moved onto the physical travelling salesman problem ptsp which provides it s own unique challenges due to the nature of the game environment ms pac man is still to date an interesting research area given the challenge that it presents we are still seeing research conducted within the community as we attempt to overcome the challenge that one small change to the game code presented in addition we have moved on from simply focussing on representing the player and started to focus on the ghosts as well lending to the aforementioned pac man vs ghosts competition while the gaming community at large has more or less forgotten about the series it has had a significant impact on the ai research community while the interest in pac man and ms pac man is beginning to dissipate it has encouraged research that has provided significant contribution to artificial and computational intelligence in general http www pacman vs ghosts net the homepage of the competition where you can download the software kit and try it out yourself http pacman shaunew com an unofficial remake that is inspired by the aforementioned pac man dossier by jamey pittman bjarnason r fern a tadepalli p lower bounding klondike solitaire with monte carlo planning in proceedings of the international conference on automated planning and scheduling browne c powley e whitehouse d lucas s m cowling p rohlfshagen p tavener s perez d samothrakis s and colton s a survey of monte carlo tree search methods ieee transactions on computational intelligence and ai in games pages burrow p and lucas s m evolution versus temporal difference learning for learning to play ms pac man proceedings of the ieee symposium on computational intelligence and games emilio m moises m gustavo r and yago s pac mant optimization based on ant colonies applied to developing an agent for ms pac man proceedings of the ieee symposium on computational intelligence and games gallagher m and ledwich m evolving pac man players what can we learn from raw input proceedings of the ieee symposium on computational intelligence and games gallagher m and ryan a learning to play pac man an evolutionary rule based approach proceedings of the congress on evolutionary computation cec chaslot g m b winands m h van den herik h j parallel monte carlo tree search in computers and games pp springer berlin heidelberg handa h evolutionary fuzzy systems for generating better ms pacman players proceedings of the ieee world congress on computational intelligence kalyanpur a and simon m pacman using genetic algorithms and neural networks koza j genetic programming on the programming of computers by means of natural selection mit press lucas s m evolving a neural network location evaluator to play ms pac man proceedings of the ieee symposium on computational intelligence and games pittman j the pac man dossier retrieved from http home comcast net jpittman pacman pacmandossier html rosca j generality versus size in genetic programming proceedings of the genetic programming conference gp wirth n an influence map model for playing ms pac man proceedings of the computational intelligence and games symposium originally published at aiandgames com on february updated to include more contemporary pac man research references from a quick cheer to a standing ovation clap to show how much you enjoyed this story ai and games researcher senior lecturer writer producer of youtube series aiandgames indie developer with tableflipgames
Milo Spencer-Harper,7800,6,https://medium.com/technology-invention-and-more/how-to-build-a-simple-neural-network-in-9-lines-of-python-code-cc8f23647ca1?source=tag_archive---------0----------------,How to build a simple neural network in 9 lines of Python code,as part of my quest to learn about ai i set myself the goal of building a simple neural network in python to ensure i truly understand it i had to build it from scratch without using a neural network library thanks to an excellent blog post by andrew trask i achieved my goal here it is in just lines of code in this blog post i ll explain how i did it so you can build your own i ll also provide a longer but more beautiful version of the source code but first what is a neural network the human brain consists of billion cells called neurons connected together by synapses if sufficient synaptic inputs to a neuron fire that neuron will also fire we call this process thinking we can model this process by creating a neural network on a computer it s not necessary to model the biological complexity of the human brain at a molecular level just its higher level rules we use a mathematical technique called matrices which are grids of numbers to make it really simple we will just model a single neuron with three inputs and one output we re going to train the neuron to solve the problem below the first four examples are called a training set can you work out the pattern should the be or you might have noticed that the output is always equal to the value of the leftmost input column therefore the answer is the should be training process but how do we teach our neuron to answer the question correctly we will give each input a weight which can be a positive or negative number an input with a large positive weight or a large negative weight will have a strong effect on the neuron s output before we start we set each weight to a random number then we begin the training process eventually the weights of the neuron will reach an optimum for the training set if we allow the neuron to think about a new situation that follows the same pattern it should make a good prediction this process is called back propagation formula for calculating the neuron s output you might be wondering what is the special formula for calculating the neuron s output first we take the weighted sum of the neuron s inputs which is next we normalise this so the result is between and for this we use a mathematically convenient function called the sigmoid function if plotted on a graph the sigmoid function draws an s shaped curve so by substituting the first equation into the second the final formula for the output of the neuron is you might have noticed that we re not using a minimum firing threshold to keep things simple formula for adjusting the weights during the training cycle diagram we adjust the weights but how much do we adjust the weights by we can use the error weighted derivative formula why this formula first we want to make the adjustment proportional to the size of the error secondly we multiply by the input which is either a or a if the input is the weight isn t adjusted finally we multiply by the gradient of the sigmoid curve diagram to understand this last one consider that the gradient of the sigmoid curve can be found by taking the derivative so by substituting the second equation into the first equation the final formula for adjusting the weights is there are alternative formulae which would allow the neuron to learn more quickly but this one has the advantage of being fairly simple constructing the python code although we won t use a neural network library we will import four methods from a python mathematics library called numpy these are for example we can use the array method to represent the training set shown earlier the t function transposes the matrix from horizontal to vertical so the computer is storing the numbers like this ok i think we re ready for the more beautiful version of the source code once i ve given it to you i ll conclude with some final thoughts i have added comments to my source code to explain everything line by line note that in each iteration we process the entire training set simultaneously therefore our variables are matrices which are grids of numbers here is a complete working example written in python also available here https github com miloharper simple neural network final thoughts try running the neural network using this terminal command python main py you should get a result that looks like we did it we built a simple neural network using python first the neural network assigned itself random weights then trained itself using the training set then it considered a new situation and predicted the correct answer was so very close traditional computer programs normally can t learn what s amazing about neural networks is that they can learn adapt and respond to new situations just like the human mind of course that was just neuron performing a very simple task but what if we hooked millions of these neurons together could we one day create something conscious i ve been inspired by the huge response this article has received i m considering creating an online course click here to tell me what topic to cover i d love to hear your feedback from a quick cheer to a standing ovation clap to show how much you enjoyed this story studied economics at oxford university founder of www moju io interested in politics and ai technology trends and new invention follow this collection to update the latest trend update as a collection editor i don t have any permission to add your articles in the wild please submit your article and i will approve also follow this collection please
Arik Sosman,1500,7,https://blog.arik.io/facebook-m-the-anti-turing-test-74c5af19987c?source=tag_archive---------1----------------,Facebook M — The Anti-Turing Test – Arik’s Blog,facebook has recently launched a limited beta of its ground breaking ai called m m s capabilities far exceed those of any competing ai where some ais would be hard pressed to tell you the weather conditions for more than one location god forbid you go on a trip m will tell you the weather forecast for every point on your route at the time you re expected to get there and also provide you with convenient gas station suggestions account for traffic in its estimations and provide you with options for food and entertainment at your destination as many people have pointed out there have been press releases stating that m is human aided however the point of this article is not to figure out whether or not there are humans behind it but to indisputably prove it when communicating with m it insists it s an ai and that it lives right inside messenger however its non instantaneous nature and the sheer unlimited complexity of tasks it can handle suggest otherwise the opinion is split as to whether or not it s a real ai and there seems to be no way of proving its nature one way or the other the biggest issue with trying to prove whether or not m is an ai is that contrary to other ais that pretend to be human m insists it s an ai thus what we would be testing for is humans pretending to be an ai which is much harder to test than the other way round because it s much easier for humans to pretend to be an ai than for an ai to pretend to be a human in this situation a turing test is futile because m s objective is precisely to not pass a turing test so what we want to prove is not the limitations of the ai but the limitlessness of the alleged humans behind it what we need therefore is a different test an anti turing test if you will as it happens i did find a way of proving m s nature but good storytelling mandates that first i describe my laborious path to the result and the inconclusive experiments i had to conduct before i finally got a definitive answer when i first got m our conversation started like this i use artificial intelligence but people help train me was m s response to my question regarding its nature that can mean many things because using ai is not the same as being a completely autonomous ai so i kept bugging it about its nature some people opined that what m refers to as ai is that there are people typing out all the responses but the tool that helps them do that is based on machine learning however directly asking about that didn t yield any new insights m s assertiveness regarding its nature is set in stone nonetheless there were some minor tells that arguably betrayed the underlying human nature of this chatbot to test its limit i have asked it to perform a set of complicated tasks for me that no other ai out there could pull off i told it where i work and then slightly modified my request and indeed it responded the most noteworthy aspect of this reply is that google maps wasn t capitalized suggesting that maybe just maybe a human typed it out in a hurry and indeed even with some other requests its responses have proven not to be as impeccable as the ones we re used to from siri for instance when i asked it to find some nice wallpapers for me taken from the berkeley stadium depicting the bay area at night preferably with the bay bridge the transamerica pyramid and the sather tower being in the picture m did manage to find some very nice wallpapers for me but it said that it couldn t find any with the campanile as consolation though it said it would let me know if it found any that fit my criteria more precisely now the first issue with the above response is that the wallpapers it sent me did have the transamerica pyramid and m knew they did what they didn t have was the sather tower so why is it saying it s going to let me know about pictures with the transamerica pyramid the second issue is that it s called the transamerica pyramid not the transamerican pyramid and lastly note the two with s and the i l it has made two typos and indeed that was not the only time it did while a lot of humans struggle with the distinction between its and it s for an ai that should not have been an issue even so it might have been trained wrong so as such these lapses are not sufficiently conclusive even the delayed responses i mentioned earlier could have been deliberate including the fact that there s a typing indicator shown when m is preparing a response rather than sending the whole string instantaneously as a regular ai would the results and indications so far didn t satisfy me so i was still looking for a way to prove that there are real humans behind m just how could i make them come out make them show themselves as it happens the answer came to me at a time when i wasn t actively looking for it the movie in cupertino ended rather late and i asked m whether there was any place i could get dinner at afterwards that would still be open at that time there were only two places open but i wasn t sure whether their kitchen would still be open too thus i asked m whether it could call them and figure that out and indeed it said it could so i asked m whether it could call my friends nope whether it could call me nope apparently it could only call businesses for me but not individuals so what do i do i make up a business and ask m to call it so m asked me for the phone number and i simply gave it mine about five minutes later i receive a call with no caller id when i pick up i hear some rumbling noises in the background say hello and then the other end hangs up immediately afterward the following exchange happens with m unfortunately i didn t have a landline phone number so i was a bit disappointed that not even this experiment could prove m s nature a few days later i had to get some work done during the weekend and while at the office i realized that the company did have one the experiment had to be repeated about three minutes later we get a phone call in the conference room when i pick up a distinctively human female voice says hello as it happens i had accidentally set the phone to mute before that so she didn t hear me saying the company name still the voice was most definitely human and because the reader shouldn t be taking me at face value i made a recording of that whole encounter immediately afterward m sends a reply what s more it appears to me that they forgot to block the caller id for that particular call because i got to see the phone number they were calling from so there very clearly m was calling from as can be seen on the photo the automatic reverse lookup matched that number to facebook thus here we are we have definitive proof that m is powered by humans the next question is is it only humans or is there at least some ai driven component behind it as to this problem i ll leave it as a homework assignment for the reader to figure out in the meantime i shall enjoy having my own free personal human assistant from a quick cheer to a standing ovation clap to show how much you enjoyed this story software engineer bitgo my experimental blog
Tony Aubé,4500,8,https://medium.com/swlh/no-ui-is-the-new-ui-ab3f7ecec6b3?source=tag_archive---------2----------------,No UI is the New UI – The Startup – Medium,on the rise of ui less apps and why you shouldcare about them as a designer october minutes read a couple of months ago i shared with my friends how i think apps like magic and operator are going to be the next big thing if you don t know about these apps what make them special is that they don t use a traditional ui as a mean of interaction instead the entire app revolves around a single messaging screen these are called invisible and conversational apps and since my initial post a slew of similar apps came to market even as of writing this facebook is releasing m a personal assistant that s integrated with messenger to help you do about anything while these apps operate in a slew of different markets from checking your bank account scheduling a meeting making a reservation at the best restaurant to being your travel assistant they all have one thing in common they place messaging at the center stage matti makkonen is a software engineer who passed away a couple of months ago my guess is that you didn t hear of his death and you most likely don t know who he was however makkonen is probably one of the most important individuals in the domain of communications and i mean on the level of alexander bell important he is the inventor of sms if you didn t realize how pervasive sms has become today think again sms is the most used application in the world three years ago it had an estimated billion active users that was over four times the numbers of facebook users at the time messaging and particularly sms has been slowly taking over the world it is now fundamental to human communication and it is why messaging apps such as whatsapp and wechat are now worth billions while messaging has become center to our everyday life it s currently only used in the narrow context of personal communications what if we could extend messaging beyond this what if messaging could transform the way we interact with computers the same way it transformed the way we interact with each other in the recent movie ex machina a billionaire creates ava a female looking robot endowed with artificial intelligence to test his invention he brings in a young engineer to see if he could fall in love with her the whole premise of the movie is centered around the turing test a test invented by alan turing also featured in the recent movie the imitation game in order to determine if a artificial intelligence is equivalent of that of a human a robot passing the turing test would have huge implications on humanity as it would mean that artificial intelligence has reached human level while we are far from creating robots that can look and act like humans such as ava we ve gotten pretty good at simulating human intelligence innarrow contexts and one of those contexts where ai performs best is you ve guessed it messaging this is thanks to deep learning a process where the computer is taught to understand and solve a problem by itself rather than having engineers code the solution deep learning is a complete game changer it allowed ai to reach new heights previously thought to be decades away nowadays computers can hear see read and understand humans better than ever before this is opening a world of opportunities for ai powered apps toward which entrepreneurs are rushing in this gold rush messaging is the low hanging fruit this is because out of all the possible forms of input digital text is the most direct one text is constant it doesn t carry all the ambiguous information that other forms of communication do such as voice or gestures furthermore messaging makes for a better user experience than traditional apps because it feels natural and familiar when messaging becomes the ui you don t need to deal with a constant stream of new interfaces all filled with different menus buttons and labels this explains the current rise in popularity of invisible and conversational apps but the reason you should care about them goes beyond that the rise in popularity of these apps recently brought me to a startling observation advances in technology especially in ai are increasingly making traditional ui irrelevant as much as i dislike it i now believe that technology progress will eventually make ui a tool of the past something no longer essential for human computer interaction and that is a good thing one could argue that conversational and invisible apps aren t devoid of ui after all they still require a screen and a chat interface while it is true that these apps do require ui design to some extent i believe these are just the tip of the iceberg beyond them new technologies have the potential to disrupt the screen entirely to my point have a look at the following videos the first video showcases project soli a small radar chip created by google to allow fine gesture recognition the second one presents emotiv a product that can read your brainwaves and understand their meaning through bear with me electroencephalography or eeg for short while both technologies seem completely magical they are not they are currently functional and have something very special in common they don t require a ui for computer input as a designer this is an unsettling trend to internalize in a world where computer can see listen talk understand and reply to you what is the purpose of a user interface why bother designing an app to manage your bank account when you could just talk to it directly beyond human interface interaction we are entering the world of brain computer interaction in this world digital telepathy coupled with ai and other means of input could allow us to communicate directly with computer without the need for a screen in his talk at chi scott jenson introduced the concept of a technological tiller according to him a technological tiller is when we stick an old design onto a new technology wrongly thinking it will work out the term is derived from a boat tiller which was for a long time the main navigation tool known to man hence when the first cars were invented rather than having steering wheels as a mean of navigation they had boat tillers the resulting cars were horribly hard to control and prone to crash it was only after the steering wheel was invented and added to the design that cars could become widely used as a designer this is a valuable lesson a change in context or technology most often requires a different design approach in this example the new technology of the motor engine needed the new design of the steering wheel to make the resulting product the car reach its full potential when a technological tiller is ignored it usually leads to product failures when it is acknowledged and solved it usually leads to a revolution and tremendous success and if one company best understood this principle it is apple with the invention of the iphone and the ipad a technological tiller was nokia sticking a physical keyboard on top of a phone good design was to create a touch screen and digital keyboard a technological tiller was microsoft sticking windows xp on top of a tablet good design was to develop a new finger friendly os and i believe a technological tiller is sticking an ipad screen over every new internet of things things what if good design is about avoiding the screen altogether learning about technological tiller teaches us that sticking too much to old perspectives and ideas is a surefire way to fail the new startups developing invisible and conversational apps understand this they understand that the ui is not the product itself but only a scaffolding allowing us to access the product and if avoiding that scaffolding can lead to a better experience then it definitively should be so do i believe that ai is taking over that ui are obsolete and that all visual designers will be out of jobs soon not really as far as i know ui will still be needed for computer output for the foreseeable future people will still use the screens to read watch videos visualize data and so on furthermore as nir mentioned in his great article on the subject conversational apps are currently good at only a specific set of tasks it is safe to think that this will also be the case for new technologies such as emotiv and project soli as game changing as these are they will most likely not be good at everything and ui will probably outperform them at specific tasks what i do believe however is that these new technologies are going to fundamentally change how we approach design this is necessary to understand for those planning to have a career in tech in a future where computer can see talk and listen and reply to you what good are going to be your awesome pixel perfect sketch skills let this be a fair warning against complacency as ui designers we have a tendency to presume a ui is the solution to every new design problems if anything the ai revolution will force us to reset our presumption on what it means to design for interaction it will push us to leave our comfort zone and look at the bigger picture bringing our focus on the design of the experience rather than the actual screen and that is an exciting future for designers please hit recommend if you enjoyed or learned from this text to keep things concise this text uses the term ui as short for graphical user interface more precisely it refers to the web and app visual patterns that have become so pervasive in the recent years this text was originally published on techcrunch on published in swlh startups wanderlust and life hacking from a quick cheer to a standing ovation clap to show how much you enjoyed this story personal thoughts on the future of design technology lead design osmo medium s largest publication for makers subscribe to receive our top stories here https goo gl zhclji
Matt O'Leary,373,12,https://howwegettonext.com/i-let-ibm-s-robot-chef-tell-me-what-to-cook-for-a-week-d881fc884748?source=tag_archive---------3----------------,I Let IBM’s Robot Chef Tell Me What to Cook for a Week,originally published at www howwegettonext com if you ve been following ibm s watson project and like food you may have noticed growing excitement among chefs gourmands and molecular gastronomists about one aspect of its development the main watson project is an artificial intelligence that engineers have built to answer questions in native language that is questions phrased the way people normally talk not in the stilted way a search engine like google understands them and so far it s worked watson has been helping nurses and doctors diagnose illnesses and it s also managed a major jeopardy win now chef watson developed alongside bon appetit magazine and several of the world s finest flavor profilers has been launched in beta enabling you to mash recipes according to ingredients of your own choosing and receive taste matching advice which reportedly can t fail while some of the world s foremost tech luminaries and conspiracy theorists are a bit skeptical about the wiseness of a i if it s going to be used at all allowing it to tell you what to make out of a fridge full of unloved leftovers seems like an inoffensive enough place to start i decided to put it to the test while employed as a food writer for well over a decade i ve also spent a good part of the last nine years working on and off in kitchens figuring out how to use spare ingredients has become quite commonplace in my professional life i ve also developed a healthy disregard for recipes as anything other than sources of inspiration or annoyance but for the purposes of this experiment am willing to follow along and try any ingredient at least once so with this in mind i m going to let watson tell me what to eat for a week i ve spent a good amount of time playing around with the app which can be found here and i m going to follow its instructions to the letter where possible i have an audience of willing testers for the food and intend to do my best in recreating its recipes on the plate still i m going to try to test it a bit i want to see whether or not it can save me time in the kitchen also whether it has any amazing suggestions for dazzling taste matches if it can help me use things up in the fridge and whether or not it s going to try to get me to buy a load of stuff i don t really need a lot of work has gone into the creation of this app and a lot of expertise but is it useable can human beings understand its recipes will we want to eat them let s find out a disclaimer before we start chef watson isn t great at telling you when stuff is actually ready and cooked you need to use your common sense take all of its advice as advice and inspiration only it s the flavors that really count monday the tailgating corn salmon sandwich my first impression is that the app is intuitive and pretty simple to use once you ve added an ingredient it suggests a number of flavor matches types of dishes and moods including some off the wall ones like mother s day choose a few of these options and the actual recipes begin to bunch up on the right of the screen i selected salmon and corn then opted for the wildly suggestive tailgating corn salmon sandwich the recipe page itself has links to the original bon appetit dish that inspired your a i me lange accompanied by a couple of pictures there s a battery of disclaimers stating that chef watson really only wants to suggest ideas rather than tell you what to eat presumably to stop people who want to try cooking with fiberglass for example from launching no win no fee cases my own salmon tailgating recipe seemed pretty straightforward there are a couple of nice touches on the page with regard to usability you can swap out any ingredients that you might not have in stock for others which watson will suggest it seems fond of adding celery root to dishes for this first attempt i decided to follow watson s advice almost to a t i didn t have any garlic chile sauce but managed to make a presumably functional analog out of some garlic and chili sauce the only other change i made involved adding some broad beans because i like broad beans during prep i employed a nearly unconscious bit of initiative namely when i cooked the salmon it s entirely likely that watson was as seemed to be the case suggesting that i use raw salmon but it s monday night and i m not in the mood for anything too mind bending team watson if i ruined your tailgater with my pig headed insistence on cooked fish i m sorry although i m not too sorry because you know it was actually a really good dish i was at first unsure the basil seemed like a bit of an afterthought i wasn t sure the lime zest was necessary and cold salmon salad on a burger bun isn t really an easy sell but damn it i d make that sandwich again it was missing some substance overall it made enough for two small buns so i teamed it up with a nice bit of korean spiced pickled cucumber on the side which worked well my fellow diner deemed it fine if a little uninteresting and yes maybe it could have done with a bit more sharpness and depth and maybe a little more a computer told me how to make this flavor wackiness but overall well done hint definitely add broad beans they totally worked now to mull over what tailgating might mean tuesday spanish blood sausage porridge it was day two of the chef watson guest slot in the kitchen and things were about to get interesting buoyed by yesterday s tailgating salmon sandwich success i decided to give watson something to sink its digital teeth into and supply only one ingredient blood sausage i also specified main as a style really so that he she it knew that i wasn t expecting dessert if i m being very honest i ve read more appetizing recipes than blood sausage porridge even the inclusion of the word spanish doesn t do anything to fancy it up and a bit concerningly this is a recipe that watson has extrapolated from one for rye porridge with morels replacing the rye with rice the mushroom with sausage and the original s chicken livers with a single potato and one tomato still maybe it would be brilliant but unlike yesterday i ran into some problems i wasn t sure how many tomatoes and potatoes watson expected me to have here the ingredients list says one of each the method suggests many or also why i had to soak the tomato in boiling water first although it makes sense in the original mushroom centric method additionally wastson offered the whimsical instruction to just cook the tomatoes and potatoes presumably for as long as i feel like there s a lot of butter involved in this recipe and rather too much liquid recommended eight cups of stock for one and a half of rice i actually got a bit fed up after four and stopped adding them forty to minutes cooking time was a bit too long too again that s been directly extracted from the rye recipe but these were mere trifles the dish tasted great it s a lovely blend of flavors and textures thanks to the blood sausage and the potato the butter works brilliantly and the tomato on top is a nice touch and it proves watson s functionality you can suggest one ingredient that you find in the fridge use your initiative a bit and you ll be left with something lovely and buttery lovely and buttery well done watson wednesday diner cod pizza when i read this recipe i wondered whether this was going to be it for me and watson diner cod and pizza are three words that don t really belong together and the ingredients list seemed more like a supermarket sweep than a recipe now that i ve actually made the meal i don t know what to think about anything you might remember a classic george a romero directed horror film called dawn of the dead its remake following the paradigm shift to running zombies in days later suffered critically my impression of this remake was always that if it d just been called something different zombies go shopping for instance every single person who saw it would have loved it as it was viewers thought it seemed unauthentic and it gathered what was essentially some unfair criticism see also the recent robocop remake or as i call it cyberswede vs detroit this meal is my culinary dawn of the dead if only watson had called it something other than pizza it would have been utterly perfect it emphatically isn t a pizza it has as much in common with pizza as cake does but there s something about radishes cod ginger olives tomatoes and green onions on a pizza crust that just work remarkably well to be clear i fully expected to throw this meal away i had the website for curry delivery already open on my phone that s all before i ate two of the pizzas they taste like nothing on earth the addition of comte cheese and chives is the sort of genius absurdity that makes people into millionaires i was however nervous to give one to my pregnant fiance e the ingredients are so weird that i was just sure she d suffer some really strange psychic reaction or that the baby would grow up to be extremely contrary be careful with this recipe preparation as i ve found with watson it doesn t tell you how to assure that your fish is cooked nor does it tell you how long to pre bake the crust base these kinds of things are really important you need to make sure this dish is cooked properly it takes longer than you might expect i m writing this from sweden the home of the ridiculous pizza and yet i have a feeling that if i were to show this recipe to a chef who ordinarily thinks nothing of piling a kilo of kebab meat and be arnaise sauce on bread and serving it in a cardboard box with a side salad of fermented cabbage he or she would balk and tell me that i ve gone too far which would be his or her loss i think i m going to have to take this to dragon s den instead watson i don t know how i m going to cope with normal recipes after our little holiday together you re changing the way i think about food thursday fall celery sour cream parsley lemon taco following yesterday s culinary epiphany i was keen to keep a cool head and a critical eye on chef watson so i decided to road test one theory from an article i found on the internet it mentioned that some of the most frequently discarded items in american fridges are celery sour cream fresh herbs and lemons let s not dwell too much on the luxury problems aspect of this i can t imagine that people everywhere in the world are lamenting the amount of sour cream and flat leaf parsley they toss and focus instead on what watson can do with this admittedly tricky sounding shopping list what it did was this immediately add shrimp tortillas and salsa verde the salsa verde it recommended from an un watsoned recipe courtesy of bon appetit was fantastic it s nothing like the salsa verde i know and love with its capers and dill pickles and anchovies this iteration required a bit of a simmer was super spicy and delicious i had to cheat and use normal tomatoes instead of tomatillos but i don t think it made a huge difference the marinade for the shrimp was unusual in that like a lot of what watson recommends it used a ton of butter a hefty wallop of our old friend kosher salt too now i ve worked as a chef on and off for several years so am unfazed by the appearance of salt and butter in recipes they re how you make things taste nice however there s no getting away from the fact that i bought a stick of butter at the start of the week and it s already gone the assembled tacos were good they were uncontroversial my dining companion deemed the salsa a bit too spicy but i liked the kick it gave the dish and the sour cream calmed it down a bit it struck me as a bit of a shame to fire up the barbecue for only about two minutes worth of cooking time but it s may and the sun is shining so what the heck was this recipe as absurd as yesterday s absolutely not was it as memorable sadly i don t think so would i make it again i m sorry watson but probably not these tacos were good but ultimately not worth the prep hassle friday mexican mushroom lasagna before i start i don t want you to get the impression that my love affair which reached the height of its passion on wednesday with watson is over it absolutely isn t i have been consistently impressed with the software s intelligence its ease of use and the audacity of some of its suggestions for flavor matching it s incredible it really works it probably won t save you any money it won t make you thin and it won t teach you how to actually cook all of that stuff you have to work out for yourself but at this stage it s a distinctly impressive and worthwhile project do give it a go but be prepared to have to coax something workable out of it every once in a while today it took me a long time to find a meat free recipe which didn t when it came down to it contain some sort of meat i selected meat as an option for what i didn t want to include and it took me to a recipe for sausage lasagne with one and a half pounds of sausage in it i removed the sausage and it replaced it with turkey mince maybe someone just needs to tell watson that neither sausages nor turkeys grow on trees after much tinkering and submitting and resubmitting the recipe i ended up with is for lasagne topped with a sort of creamy mashed potato sauce it s very easy and it s a profoundly smart use of ingredients the lasagne is not the world s most aesthetically appealing dish and it s not as astonishingly flavored as some of this week s other revelations but i don t think i ll be making my cheese sauce in any other way from this point onwards top marks and in essence this kind of sums up watson for me you need to tinker with it a bit before you can find something usable you may need to make a do i want to put mashed potato on this lasagne leap of faith and you re going to have to actually go with it if you want the app s full benefit you ll consume a lot of dairy products and you might find yourself daydreaming about nice simple unadorned salads if you decide to go all in with its suggestions but an a i that can tell us how to make a pizza out of cod ginger and radishes that you know is going to taste amazing one that will gladly suggest a workable recipe for blood sausage porridge and walk you through it without too much hassle that gives you a how crazy option for each ingredient that is only designed to make the lives of food enthusiasts more interesting why on earth not watson and i are going to be good friends from this point forward even if we don t speak every day and i can t wait to introduce it to others now though i m going to only consume smoothies for a week seriously if i even look at butter in the next few days i m probably going to puke this fall medium and how we get to next are exploring the future of food and what it means for us all to get the latest and join the conversation you can follow future of food from a quick cheer to a standing ovation clap to show how much you enjoyed this story inspiring stories about the people and places building our future created by steven johnson edited by ian steadman duncan geere anjali ramachandran and elizabeth minkel supported by the gates foundation
Tanay Jaipuria,1100,5,https://medium.com/@tanayj/self-driving-cars-and-the-trolley-problem-5363b86cb82d?source=tag_archive---------4----------------,Self-driving cars and the Trolley problem – Tanay Jaipuria – Medium,google recently announced that their self driving car has driven more than a million miles according to morgan stanley self driving cars will be commonplace in society by this got me thinking about the ethics and philosophy behind these cars which is what the piece is about in isaac asimov introduced three laws of robotics in his short story runaround they were as follows he later added a fourth law the zeroth law a robot may not harm humanity or by inaction allow humanity to come to harm though fictional they provide a good philosophical grounding of how ai can coexist with society if self driving cars were to follow them we re in a pretty good spot right let s leave aside the argument that self driving cars lead to loss of jobs of taxi drivers and truck drivers and so should not exist per the th st law however there s one problem which the laws of robotics don t quite address it s a famous thought experiment in philosophy called the trolley problem and goes as follows it s not hard to see how a similar situation would come up in a world with self driving cars with the car having to make a similar decision say for example a human driven car runs a red light and a self driving car has two options what should the car do from a utilitarian perspective the answer is obvious to turn right or pull the lever leading to the death of only one person as opposed to five incidentally in a survey of professional philosophers on the trolley problem agreed saying that one should pull the lever so maybe this problem isn t a problem at all and the answer is to simply do the utilitarian thing that greatest happiness to the greatest number but can you imagine a world in which your life could be sacrificed at any moment for no wrongdoing to save the lives of two others now consider this version of the trolley problem involving a fat man most people that go the utilitarian route in the initial problem say they wouldn t push the fat man but from a utilitarian perspective there is no difference between this and the initial problem so why do they change their mind and is the right answer to stay the course then kant s categorical imperative goes some way to explaining it in simple words it says that we shouldn t merely use people as means to an end and so killing someone for the sole purpose of saving others is not okay and would be a no no by kant s categorical imperative another issue with utilitarianism is that it is a bit naive at least how we defined it the world is complex and so the answer is rarely as simple as perform the action that saves the most people what if going back to the example of the car instead of a family of five inside the car that ran the red light were five bank robbers speeding after robbing a bank and sat in the other car was a prominent scientist who had just made a breakthrough in curing cancer would you still want the car to perform the action that simply saves the most people so may be we fix that by making the definition of utilitarianism more intricate in that we assign a value to each individuals life in that case the right answer could still be to kill the five robbers if say our estimate of utility of the scientist s life was more than that of the five robbers but can you imagine a world in which say google or apple places a value on each of our lives which could be used at any moment of time to turn a car into us to save others would you be okay with that and so there you have it though the answer seems simple it is anything but which is what makes the problem so interesting and so hard it will be a question that comes up time and time again as self driving cars become a reality google apple uber etc will probably have to come up with an answer to pull or not to pull lastly i want to leave you another question that will need to be answered that of ownership say a self driving car which has one passenger in it the owner skids in the rain and is going to crash into a car in front pushing that car off a cliff it can either take a sharp turn and fall of the cliff or continue going straight leading to the other car falling of the cliff both cars have one passenger what should the car do should it favor the person that bought it its owner thanks for reading feel free to share this post and leave a note write a response to share your thoughts i m tanayj on twitter if you want to discuss further from a quick cheer to a standing ovation clap to show how much you enjoyed this story product facebook previously mckinsey i like tech econ strategy and manutd views and banter my own
Milo Spencer-Harper,2200,3,https://medium.com/technology-invention-and-more/how-to-build-a-multi-layered-neural-network-in-python-53ec3d1d326a?source=tag_archive---------5----------------,How to build a multi-layered neural network in Python,in my last blog post thanks to an excellent blog post by andrew trask i learned how to build a neural network for the first time it was super simple lines of python code modelling the behaviour of a single neuron but what if we are faced with a more difficult problem can you guess what the should be the trick is to notice that the third column is irrelevant but the first two columns exhibit the behaviour of a xor gate if either the first column or the second column is then the output is however if both columns are or both columns are then the output is so the correct answer is however this would be too much for our single neuron to handle this is considered a nonlinear pattern because there is no direct one to one relationship between the inputs and the output instead we must create an additional hidden layer consisting of four neurons layer this layer enables the neural network to think about combinations of inputs you can see from the diagram that the output of layer feeds into layer it is now possible for the neural network to discover correlations between the output of layer and the output in the training set as the neural network learns it will amplify those correlations by adjusting the weights in both layers in fact image recognition is very similar there is no direct relationship between pixels and apples but there is a direct relationship between combinations of pixels and apples the process of adding more layers to a neural network so it can think about combinations is called deep learning ok are we ready for the python code first i ll give you the code and then i ll explain further also available here https github com miloharper multi layer neural network this code is an adaptation from my previous neural network so for a more comprehensive explanation it s worth looking back at my earlier blog post what s different this time is that there are multiple layers when the neural network calculates the error in layer it propagates the error backwards to layer adjusting the weights as it goes this is called back propagation ok let s try running it using the terminal command python main py you should get a result that looks like this first the neural network assigned herself random weights to her synaptic connections then she trained herself using the training set then she considered a new situation that she hadn t seen before and predicted the correct answer is so she was pretty close you might have noticed that as my neural network has become smarter i ve inadvertently personified her by using she instead of it that s pretty cool but the computer is doing lots of matrix multiplication behind the scenes which is hard to visualise in my next blog post i ll visually represent our neural network with an animated diagram of her neurons and synaptic connections so we can see her thinking from a quick cheer to a standing ovation clap to show how much you enjoyed this story studied economics at oxford university founder of www moju io interested in politics and ai technology trends and new invention follow this collection to update the latest trend update as a collection editor i don t have any permission to add your articles in the wild please submit your article and i will approve also follow this collection please
Ben Brown,1100,7,https://blog.howdy.ai/what-will-the-automated-workplace-look-like-495f9d1e87da?source=tag_archive---------6----------------,Start automating your business tasks with Slack – Howdy,if you haven t read about it in the times or heard about it on npr yet you are soon going to be replaced by a robot at your job all the jobs we thought were safe because they required experience and nuance can now be done by computers martin ford author of the book the times and npr are reporting on calls it the threat of a jobless future a future where computers write our newspaper articles create our legal contracts and compose our symphonies automating this type of complicated quasi creative task is really impressive it requires super computers and uses the forefront of artificial intelligence to achieve this shocking result it requires tons of data and lots of programming using advanced systems not available to ordinary people but not everything requires deep learning some of the things we do in our every day lives especially at our jobs can be automated though it used to be the domain of the geek scripting and automation is invading all aspects of the workplace workers and organizations who can master scripting and automation will gain an edge on those who can t we all have to face the reality that a well built script might be faster and more reliable than we can be at some parts of our jobs those of us who can create and wield this type of tool will be able to do better work faster luckily inside messaging tools like slack creating customized interactive automation tools for business tasks is possible with a little open source code some cloud tools that are mostly free and a bit of self reflection bots are apps that live alongside users in a chatroom users can issue commands to bots by sending messages to them or by using special keywords in the chatroom traditionally bots have been used for things like server maintenance and running software tests but now using the connected devices all around us nearly anything can be automated and controlled by a bot a common task in many technology teams is the stand up meeting everyone stands up and one at a time tells the team what they ve been working on what they ve got coming up next and any problems they are facing each person takes a few minutes to speak in many teams this is already taking place in a chat room if there are people on a team and each person speaks for just seconds they ll spend minutes just bringing people up to speed nothing has been discussed no problems have yet been solved what happens if this process is automated using a bot in an environment like slack a stand up is triggered automatically or by a project manager using a flexible script the bot simultaneously reaches out to every member of the team via a private message on slack the bot has an interactive conversation with each team member in parallel and collects everyone s responses everyone still spends seconds talking about their work but now it is the same seconds the bot now finished collecting the checkin responses shares its report with all the stakeholders just minutes into the meeting everyone involved has a single document to look at that contains the up to date status of the project the team gains minutes during which they can discuss this information clear blockers and get back to work now this is admittedly an aggressive application of this approach that won t work for everyone some teams may need the sequential listing of updates some teams may need to actually stand up and use their voices the point i m trying to make is that automating things like this exposes ways for the work to be improved for time to be saved and for the process to evolve what other processes could be automated like this what if there was a meeting runner bot that automatically sent out an agenda to all attendees before the meeting then collected collated and delivered updates to team members it could make meetings shorter and more productive by reducing the time needed to bring everyone up to speed what if there was an hr bot that could collect performance reviews and feedback what if there was a task management bot that could not only manage the creation of tasks and lists but also create and deliver up to date progress reports to the whole team there is a lot to be gained with simple process automation like this so how can you and your organization benefit from this type of automation tool first you ll need to commit to adopting a tool like slack where your team can communicate and use this type of bot then you ll have to customize slack to take advantage of built in and custom integrations which takes some programming though not much as there are a ton of open source tools ready to use an organization like my company xoxco can help you do this before you can automate something you have to know the process and be able to write it down in detail you ll have to think about all the special cases that occur not only will this allow you to build an automation script it will help you to hone and document the processes by which your business is conducted when we do things we do them one at a time robots can do lots of things at once so once you ve got your process documented think about how the steps might be able to run in parallel for example could the bot talk to multiple people at once instead of doing it sequentially since your script can only do what you tell it you ll need to plan for the contingencies that might occur while it runs what if someone doesn t respond in time what if information is unavailable what if a step in the process fails think through these cases and prepare your script to handle them for example we built in a minute timeout for our project manager bot if a user doesn t respond in minutes they get a reminder to checkin in person and their lack of a response is indicated in the report this may sound complicated but when it boils down we re just talking about including an else for every if a good practice for any software or process to incorporate your bots once deployed can become valuable members of your team their success is dependent on your team s desire to use them and that they provide a better faster more reliable way to achieve organizational goals bots should have a user friendly personality and represent and support company culture bots should talk like real people but not pretend to be real people our rule of thumb try to be as smart as a puppy which will engender an attitude of forgiveness when the bot does something not quite right this type of software automation has been common in certain groups for years there may already be a software automation expert in your midst she s probably part of the server administration team or the quality assurance group right now she works on code deployment or writes software tests go find her and go put her in a room with a project manager and a content strategist and see if they can identify and automate the team s top three time sucking activities in a way that is not only useful but fun to use when we start to design software for messaging the entire application must be boiled down to words without colors to choose navigation to click and sidebars to fill with widgets this can help us not only build better more useful software but put simply requires us to run our businesses in a more organized documented and well understood way don t wait for the artificial intelligence explosion to arrive start putting these tools to work today update you can now use a fully realized version of the bot discussed in this post we ve launched it under the name howdy add howdy to your team to run meetings capture information and automate common tasks for your team read more about our launch here from a quick cheer to a standing ovation clap to show how much you enjoyed this story i m a designer and technologist in austin texas i co founded xoxco in the official blog of howdy ai and botkit
Frank Diana,428,11,https://medium.com/@frankdiana/digital-transformation-of-business-and-society-5d9286e39dbf?source=tag_archive---------7----------------,Digital Transformation of Business and Society – Frank Diana – Medium,at a recent kpmg robotic innovations event futurist and friend gerd leonhard delivered a keynote titled the digital transformation of business and society challenges and opportunities by i highly recommend viewing the video of his presentation as gerd describes he is a futurist focused on foresight and observations not predicting the future we are at a point in history where every company needs a gerd leonhard for many of the reasons presented in the video future thinking is rapidly growing in importance as gerd so rightly points out we are still vastly under estimating the sheer velocity of change with regard to future thinking gerd used my future scenario slide to describe both the exponential and combinatorial nature of future scenarios not only do we need to think exponentially but we also need to think in a combinatorial manner gerd mentioned tesla as a company that really knows how to do this he then described our current pivot point of exponential change a point in history where humanity will change more in the next twenty years than in the previous with that as a backdrop he encouraged the audience to look five years into the future and spend to of their time focused on foresight he quoted peter drucker in times of change the greatest danger is to act with yesterday s logic and stated that leaders must shift from a focus on what is to a focus on what could be gerd added that wait and see means wait and die love that by the way he urged leaders to focus on and build a plan to participate in that future emphasizing the question is no longer what if but what when we are entering an era where the impossible is doable and the headline for that era is exponential convergent combinatorial and inter dependent words that should be a key part of the leadership lexicon going forward here are some snapshots from his presentation gerd then summarized the session as follows the future is exponential combinatorial and interdependent the sooner we can adjust our thinking lateral the better we will be at designing our future my take gerd hits on a key point leaders must think differently there is very little in a leader s collective experience that can guide them through the type of change ahead it requires us all to think differently when looking at ai consider trying ia first intelligent assistance augmentation my take these considerations allow us to create the future in a way that avoids unintended consequences technology as a supplement not a replacement efficiency and cost reduction based on automation ai ia and robotization are good stories but not the final destination we need to go beyond the ations and inevitable abundance to create new value that cannot be easily automated my take future thinking is critical for us to be effective here we have to have a sense as to where all of this is heading if we are to effectively create new sources of value we won t just need better algorithms we also need stronger humarithms i e values ethics standards principles and social contracts my take gerd is an evangelist for creating our future in a way that avoids hellish outcomes and kudos to him for being that voice the best way to predict the future is to create it alan kay my take our context when we think about the future puts it years away and that is just not the case anymore what we think will take ten years is likely to happen in two we can t create the future if we don t focus on it through an exponential lens originally published at frankdiana wordpress com on september from a quick cheer to a standing ovation clap to show how much you enjoyed this story tcs executive focused on the rapid evolution of society and business fascinated by the view of the world in the next decade and beyond https frankdiana net
Rand Hindi,693,12,https://medium.com/snips-ai/how-artificial-intelligence-will-make-technology-disappear-503cd88e1e6a?source=tag_archive---------8----------------,How Artificial Intelligence Will Make Technology Disappear,this is a redacted transcript of a tedx talk i gave last april at ecole polytechnique in france the video can be seen on youtube here enjoy last march i was in costa rica with my girlfriend spending our days between beautiful beaches and jungles full of exotic animals there was barely any connectivity and we were immersed in nature in a way that we could never be in a big city it felt great but in the evening when we got back to the hotel and connected to the wifi our phones would immediately start pushing an entire day s worth of notifications constantly interrupting our special time together it interrupted us while watching the sunset while sipping a cocktail while having dinner while having an intimate moment it took emotional time away from us and it s not just that our phones vibrated it s also that we kept checking them to see if we had received anything as if we had some sort of compulsive addiction to it those rare messages that are highly rewarding like being notified that ashton kutcher just tweeted this article made consciously unplugging impossible just like pavlov s dog before us we had become conditioned in this case though it has gotten so out of control that today out of people experience phantom vibrations which is when you think your phone vibrated in your pocket whereas in fact it didn t how did this happen back in we didn t have any connected devices this was the unplugged era there were no push notifications no interruptions nada things were analog things were human around the internet started taking off and our computers became connected with it came email and the infamous you ve got mail notification we started getting interrupted by people companies and spammers sending us electronic messages at random moments years later we entered the mobile era this time it is not but devices that are connected a computer a phone and a tablet the trouble is that since these devices don t know which one you are currently using the default strategy has been to push all notifications on all devices like when someone calls you on your phone and it also rings on your computer and actually keeps ringing after you ve answered it on one of your devices and it s not just notifications accessing a service and finding content is equally frustrating on mobile devices with those millions of apps and tiny keyboards if we take notifications and the need for explicit interactions as a proxy for technological friction then each connected device adds more of it unfortunately this is about to get much worse since the number of connected devices is increasing exponentially this year in we are officially entering what is called the internet of things era that s when your watch fridge car and lamps are connected it is expected that there will be more than billion connected devices by or for every person on this planet just imagine what it will feel like to interact manually and receive notifications simultaneously on devices that s definitely not the future we were promised there is hope though there is hope that artificial intelligence will fix this not the one elon musk refers to that will enslave us all but rather a human centric domain of a i called context awareness which is about giving devices the ability to adapt to our current situation it s about figuring out which device to push notifications on it s about figuring out you are late for a meeting and notifying people for you it s about figuring out you are on a date and deactivating your non urgent notifications it s about giving you back the freedom to experience the real world again when you look at the trend in the capabilities of a i what you see it that it takes a bit longer to start but when it does it grows much faster we already have a i s that can learn to play video games and beat world champions so it s just a matter of time before they reach human level intelligence there is an inflexion point and we just crossed it taking the connected devices curve and subtracting the one for a i we see that the overall friction keeps increasing over the next few years until the point where a i becomes so capable that this friction flips around and quickly disappears in this era called ubiquitous computing adding new connected devices does not add friction it actually adds value for example our phones and computers will be smart enough to know where to route the notifications our cars will drive themselves already knowing the destination our beds will be monitoring our sleep and anticipating when we will be waking up so that we have freshly brewed coffee ready in the kitchen it will also connect with the accelerometers in our phones and the electricity sockets to determine how many people are in the bed and adjust accordingly our alarm clocks won t need to be set they will be connected to our calendars and beds to determine when we fell asleep and when we need to wake up all of this can also be aggregated offering public transport operators access to predicted passenger flows so that there are always enough trains running traffic lights will adjust based on self driving cars planned route power plants will produce just enough electricity saving costs and the environment smart cities smart homes smart grids they are all just consequences of having ubiquitous computing by the time this happens technology will have become so deeply integrated in our lives and ourselves that we simply won t notice it anymore artificial intelligence will have made technology disappear from our consciousness and the world will feel unplugged again i know this sounds crazy but there are historical examples of other technologies that followed a similar pattern for example back in the s electricity was very tangible it was expensive hard to produce would cut all the time and was dangerous you would get electrocuted and your house could catch fire back then people actually believed that oil lamps were safer but as electricity matured it became cheaper more reliable and safer eventually it was everywhere in our walls lamps car phone and body it became ubiquitous and we stopped noticing it today the exact same thing is happening with connected devices building this ubiquitous computing future relies on giving devices the ability to sense and react to the current context which is called context awareness a good way to think about it is through the combination of layers the device layer which is about making devices talk to each other the individual layer which encompasses everything related to a particular person such as his location history calendar emails or health records the social layer which models the relationship between individuals and finally the environmental layer which is everything else such as the weather the buildings the streets trees and cars for example to model the social layer we can look at the emails that were sent and received by someone which gives us an indication of social connection strength between a group of people the graph shown above is extracted from my professional email account using the mit immersion tool over a period of months the huge green bubble is one of my co founder which sends way too many emails as is the red bubble the other fairly large ones are other people in my team that i work closely with but what s interesting is that we can also see who in my network works together as they will tend to be included together in emails threads and thus form clusters in this graph if you add some contextual information such as the activity i was engaged in or the type of language being used in the email you can determine the nature of the relationship i have with each person personal professional intimate as well as its degree and if you now take the difference in these patterns over time you can detect major events such as changing jobs closing an investment round launching a new product or hiring key people of course all this can be done on social graphs as well as professional ones now that we have a better representation of someone s social connections we can use it to perform better natural language processing nlp of calendar events by disambiguating events like chat with michael which would then assign a higher probability to my co founder but a calendar won t help us figure out habits such as going to the gym after work or hanging out in a specific neighborhood on friday evenings for that we need another source of data geolocation by monitoring our location over time and detecting the places we have been to we can understand our habits and thus predict what we will be doing next in fact knowing the exact place we are at is essential to predict our intentions since most of the things we do with our devices are based on what we are doing in the real world unfortunately location is very noisy and we never know exactly where someone is for example below i was having lunch in san francisco and this is what my phone recorded while i was not moving clearly it is impossible to know where i actually am to circumvent this problem we can score each place according to the current context for example we are more likely to be at a restaurant during lunch time than at a nightclub if we then combine this with a user specific model based on their location history we can achieve very high levels of accuracy for example if i have been to a starbucks in the past it will increase the probability that i am there now as well as the probability of any other coffee shop and because we now know that i am in a restaurant my devices can surface the apps and information that are relevant to this particular place such as reviews or mobile payments apps accepted there if i was at the gym it would be my sports apps if i was home it would be my leisure and home automation apps if we combine this timeline of places with the phone s accelerometer patterns we can then determine the transportation mode that was taken between those places with this our connected watches could now tell us to stand up when it detects we are still stop at a rest area when it detects we are driving or tell us where the closest bike stand is when cycling these individual transit patterns can then be aggregated over several thousand users to recreate very precise population flow in the city s infrastructure as we have done below for paris not only does it give us an indication of how many people transit in each station it also give us the route they have been taking where they changed train or if they walked between stations combining this with data from the city concerts office and residential buildings population demographics enables you to see how each factor impacts public transport and even predict how many people will be boarding trains throughout the day it can then be used to notify commuters that they should take a different train if they want to sit on their way home and dynamically adjust the train schedules maximizing the efficiency of the network both in terms of energy saved and comfort and it s not just public transport the same model and data can be used to predict queues in post offices by taking into account hyperlocal factors such as when the welfare checks are being paid the bank holidays the proximity of other post offices and the staff strikes this is shown below where the blue curve is the real load and the orange one is the predicted load this model can be used to notify people of the best time to drop and pickup their parcels which results in better yield management and customer service it can also be used to plan the construction of new post offices by sizing them accordingly and since a post office is just a retail store everything that works here can work for all retailers grocery stores supermarkets shoe shops etc it could then be plugged into our devices enabling them to optimize our shopping schedule and make sure we never queue again this contextual modeling approach is in fact so powerful that it can even predict the risk of car accidents just by looking at features such as the street topologies the proximity of bars that just closed the road surface or the weather since these features are generalizable throughout the city we can make predictions even in places where there was never a car accident for example here we can see that our model correctly detects trafalgar square as being dangerous even though nowhere did we explicitly say so it discovered it automatically from the data itself it was even able to identify the impact of cultural events such as st patrick s day or new year s eve how cool would it be if our self driving cars could take this into account if we combine all these different layers personal social environmental we can recreate a highly contextualized timeline of what we have been doing throughout the day which in turn enables us to predict what our intentions are making our devices able to figure out our current context and predict our intentions is the key to building truly intelligent products with that in mind our team has been prototyping a new kind of smartphone interface one that leverages this contextual intelligence to anticipate which services and apps are needed at any given time linking directly to the relevant content inside them it s not yet perfect but it s a first step towards our long term vision and it certainly saves a lot of time swipes and taps one thing in particular that we are really proud of is that we were able to build privacy by design full post coming soon it is a tremendous engineering challenge but we are now running all our algorithms directly on the device whether it s the machine learning classifiers the signal processing the natural language processing or the email mining they are all confined to our smartphones and never uploaded to our servers basically it means we can now harness the full power of a i without compromising our privacy something that has never been achieved before it s important to understand that this is not just about building some cool tech or the next viral app nor is it about making our future look like a science fiction movie it s actually about making technology disappear into the background so that we can regain the freedom to spend quality time with the people we care about if you enjoyed this article it would really help if you hit recommend below and shared it on twitter we are randhindi snips from a quick cheer to a standing ovation clap to show how much you enjoyed this story entrepreneur ai researcher working on making technology disappear ceo snips ai ai privacy and blockchain follow http instagram com randhindi this publication features the articles written by the snips team fellows and friends snips started as an ai lab in and now builds private by design decentralized open source voice assistants
samim,323,8,https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0?source=tag_archive---------9----------------,Obama-RNN — Machine generated political speeches. – samim – Medium,political speeches are among the most powerful tools leaders use to influence entire populations throughout history political speeches have been used to start wars end empires fuel movements inspire the masses political speeches apply many of the tricks found in the field of social engineering congruent communication intentional body language neuro linguistic programming humanbuffer overflows and more read more about the art of human hacking here in recent years barack obama has emerged as one of the most memorable and effective political speakers on the world stage messages like hope and yes we can have clearly left a mark on our collective consciousness since obama s highly skilled speech writers have written over megabytes or words of text not counting interviews and debates all of obama s speeches are conveniently readable here with powerful artificial intelligence machine learning libraries becoming readily available as open source it seems obvious to apply them to speech writing a particularly interesting class of algorithms are recurrent neural networks rnn recently andrej karpathy a cs phd student at stanford has released char rnn a multi layer recurrent neural networks for character level language models the library takes an arbitrary text file as input and learns to predict the next character in the sequence as the results are pretty amazing many interesting experiments have sprung up ranging from composing music rapping writing cooking recipes and even re writing the bible step is to feed the model data the more the better for this i wrote a web crawler in python that gathers all publicly available obama speeches parses out the text and removes any interviews debates step is to train the model on the collected text training an rnn takes a bit of fiddling as i painfully found out while training a model on mb of classical music midi files mozart rnn is wild luckily the standard settings that andrej suggested were a good starting point for the obama rnn step is to test the model which automatically generates an unlimited amount of new speeches in the vein of obama s previous speeches the model can be seeded with a text from which it will start the sequence e g war on terror and a temperature which makes the output more conservative or diverse at cost of more mistakes here is a selection of some of my favorite speeches the obama rnn generated so far keep in mind this is a just a quick hack project with more time effort the results can be improved one of the most hilarious patterns to emerge is that the obama rnn really loves to politely say good afternoon good day god bless you good bless the united states of america thank you i did a test combining obamas speeches with other famous speeches from the st century including everything from mother theresa malcom x to mussolini and hitler this gives us an rather insane amalgam of human thought seen through the eyes of a machine a story for an other day on this note god bless you good bless the united states of america thank you you can run your own obama rnn by following these instructions get in touch here https twitter com samim http samim io from a quick cheer to a standing ovation clap to show how much you enjoyed this story designer code magician working at the intersection of hci machine learning creativity building tools for enlightenment narrative engineering
Adam Geitgey,10400,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3?source=tag_archive---------4----------------,Machine Learning is Fun! Part 2 – Adam Geitgey – Medium,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in italiano espan ol franc ais tu rkc e portugue s tie ng vie t or in part we said that machine learning is using generic algorithms to tell you something interesting about your data without writing any code specific to the problem you are solving if you haven t already read part read it now this time we are going to see one of these generic algorithms do something really cool create video game levels that look like they were made by humans we ll build a neural network feed it existing super mario levels and watch new ones pop out just like part this guide is for anyone who is curious about machine learning but has no idea where to start the goal is be accessible to anyone which means that there s a lot of generalizations and we skip lots of details but who cares if this gets anyone more interested in ml then mission accomplished back in part we created a simple algorithm that estimated the value of a house based on its attributes given data about a house like this we ended up with this simple estimation function in other words we estimated the value of the house by multiplying each of its attributes by a weight then we just added those numbers up to get the house s value instead of using code let s represent that same function as a simple diagram however this algorithm only works for simple problems where the result has a linear relationship with the input what if the truth behind house prices isn t so simple for example maybe the neighborhood matters a lot for big houses and small houses but doesn t matter at all for medium sized houses how could we capture that kind of complicated detail in our model to be more clever we could run this algorithm multiple times with different of weights that each capture different edge cases now we have four different price estimates let s combine those four price estimates into one final estimate we ll run them through the same algorithm again but using another set of weights our new super answer combines the estimates from our four different attempts to solve the problem because of this it can model more cases than we could capture in one simple model let s combine our four attempts to guess into one big diagram this is a neural network each node knows how to take in a set of inputs apply weights to them and calculate an output value by chaining together lots of these nodes we can model complex functions there s a lot that i m skipping over to keep this brief including feature scaling and the activation function but the most important part is that these basic ideas click it s just like lego we can t model much with one single lego block but we can model anything if we have enough basic lego blocks to stick together the neural network we ve seen always returns the same answer when you give it the same inputs it has no memory in programming terms it s a stateless algorithm in many cases like estimating the price of house that s exactly what you want but the one thing this kind of model can t do is respond to patterns in data over time imagine i handed you a keyboard and asked you to write a story but before you start my job is to guess the very first letter that you will type what letter should i guess i can use my knowledge of english to increase my odds of guessing the right letter for example you will probably type a letter that is common at the beginning of words if i looked at stories you wrote in the past i could narrow it down further based on the words you usually use at the beginning of your stories once i had all that data i could use it to build a neural network to model how likely it is that you would start with any given letter our model might look like this but let s make the problem harder let s say i need to guess the next letter you are going to type at any point in your story this is a much more interesting problem let s use the first few words of ernest hemingway s the sun also rises as an example what letter is going to come next you probably guessed n the word is probably going to be boxing we know this based on the letters we ve already seen in the sentence and our knowledge of common words in english also the word middleweight gives us an extra clue that we are talking about boxing in other words it s easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of english to solve this problem with a neural network we need to add state to our model each time we ask our neural network for an answer we also save a set of our intermediate calculations and re use them the next time as part of our input that way our model will adjust its predictions based on the input that it has seen recently keeping track of state in our model makes it possible to not just predict the most likely first letter in the story but to predict the most likely next letter given all previous letters this is the basic idea of a recurrent neural network we are updating the network each time we use it this allows it to update its predictions based on what it saw most recently it can even model patterns over time as long as we give it enough of a memory predicting the next letter in a story might seem pretty useless what s the point one cool use might be auto predict for a mobile phone keyboard but what if we took this idea to the extreme what if we asked the model to predict the next most likely character over and over forever we d be asking it to write a complete story for us we saw how we could guess the next letter in hemingway s sentence let s try generating a whole story in the style of hemingway to do this we are going to use the recurrent neural network implementation that andrej karpathy wrote andrej is a deep learning researcher at stanford and he wrote an excellent introduction to generating text with rnns you can view all the code for the model on github we ll create our model from the complete text of the sun also rises characters using unique letters including punctuation uppercase lowercase etc this data set is actually really small compared to typical real world applications to generate a really good model of hemingway s style it would be much better to have at several times as much sample text but this is good enough to play around with as an example as we just start to train the rnn it s not very good at predicting letters here s what it generates after a loops of training you can see that it has figured out that sometimes words have spaces between them but that s about it after about iterations things are looking more promising the model has started to identify the patterns in basic sentence structure it s adding periods at the ends of sentences and even quoting dialog a few words are recognizable but there s also still a lot of nonsense but after several thousand more training iterations it looks pretty good at this point the algorithm has captured the basic pattern of hemingway s short direct dialog a few sentences even sort of make sense compare that with some real text from the book even by only looking for patterns one character at a time our algorithm has reproduced plausible looking prose with proper formatting that is kind of amazing we don t have to generate text completely from scratch either we can seed the algorithm by supplying the first few letters and just let it find the next few letters for fun let s make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of er he and the s not bad but the really mind blowing part is that this algorithm can figure out patterns in any sequence of data it can easily generate real looking recipes or fake obama speeches but why limit ourselves human language we can apply this same idea to any kind of sequential data that has a pattern in nintendo released super mario makertm for the wii u gaming system this game lets you draw out your own super mario brothers levels on the gamepad and then upload them to the internet so you friends can play through them you can include all the classic power ups and enemies from the original mario games in your levels it s like a virtual lego set for people who grew up playing super mario brothers can we use the same model that generated fake hemingway text to generate fake super mario brothers levels first we need a data set for training our model let s take all the outdoor levels from the original super mario brothers game released in this game has levels and about of them have the same outdoor style so we ll stick to those to get the designs for each level i took an original copy of the game and wrote a program to pull the level designs out of the game s memory super mario bros is a year old game and there are lots of resources online that help you figure out how the levels were stored in the game s memory extracting level data from an old video game is a fun programming exercise that you should try sometime here s the first level from the game which you probably remember if you ever played it if we look closely we can see the level is made of a simple grid of objects we could just as easily represent this grid as a sequence of characters with one character representing each object we ve replaced each object in the level with a letter and so on using a different letter for each different kind of object in the level i ended up with text files that looked like this looking at the text file you can see that mario levels don t really have much of a pattern if you read them line by line the patterns in a level really emerge when you think of the level as a series of columns so in order for the algorithm to find the patterns in our data we need to feed the data in column by column figuring out the most effective representation of your input data called feature selection is one of the keys of using machine learning algorithms well to train the model i needed to rotate my text files by degrees this made sure the characters were fed into the model in an order where a pattern would more easily show up just like we saw when creating the model of hemingway s prose a model improves as we train it after a little training our model is generating junk it sort of has an idea that s and s should show up a lot but that s about it it hasn t figured out the pattern yet after several thousand iterations it s starting to look like something the model has almost figured out that each line should be the same length it has even started to figure out some of the logic of mario the pipes in mario are always two blocks wide and at least two blocks high so the p s in the data should appear in x clusters that s pretty cool with a lot more training the model gets to the point where it generates perfectly valid data let s sample an entire level s worth of data from our model and rotate it back horizontal this data looks great there are several awesome things to notice finally let s take this level and recreate it in super mario maker play it yourself if you have super mario maker you can play this level by bookmarking it online or by looking it up using level code ac f c the recurrent neural network algorithm we used to train our model is the same kind of algorithm used by real world companies to solve hard problems like speech detection and language translation what makes our model a toy instead of cutting edge is that our model is generated from very little data there just aren t enough levels in the original super mario brothers game to provide enough data for a really good model if we could get access to the hundreds of thousands of user created super mario maker levels that nintendo has we could make an amazing model but we can t because nintendo won t let us have them big companies don t give away their data for free as machine learning becomes more important in more industries the difference between a good program and a bad program will be how much data you have to train your models that s why companies like google and facebook need your data so badly for example google recently open sourced tensorflow its software toolkit for building large scale machine learning applications it was a pretty big deal that google gave away such important capable technology for free this is the same stuff that powers google translate but without google s massive trove of data in every language you can t create a competitor to google translate data is what gives google its edge think about that the next time you open up your google maps location history or facebook location history and notice that it stores every place you ve ever been in machine learning there s never a single way to solve a problem you have limitless options when deciding how to pre process your data and which algorithms to use often combining multiple approaches will give you better results than any single approach readers have sent me links to other interesting approaches to generating super mario levels if you liked this article please consider signing up for my machine learning is fun email list i ll only email you when i have something new and awesome to share it s the best way to find out when i write more articles like this you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Arthur Juliani,9000,6,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=tag_archive---------5----------------,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,for this tutorial in my reinforcement learning series we are going to be exploring a family of rl algorithms called q learning algorithms these are a little different than the policy based algorithms that will be looked at in the the following tutorials parts instead of starting with a complex and unwieldy deep neural network we will begin by implementing a simple lookup table version of the algorithm and then show how to implement a neural network equivalent using tensorflow given that we are going back to basics it may be best to think of this as part of the series it will hopefully give an intuition into what is really happening in q learning that we can then build on going forward when we eventually combine the policy gradient and q learning approaches to build state of the art rl agents if you are more interested in policy networks or already have a grasp on q learning feel free to start the tutorial series here instead unlike policy gradient methods which attempt to learn functions which directly map an observation to an action q learning attempts to learn the value of being in a given state and taking a specific action there while both approaches ultimately allow us to take intelligent actions given a situation the means of getting to that action differ significantly you may have heard about deepq networks which can play atari games these are really just larger and more complex implementations of the q learning algorithm we are going to discuss here for this tutorial we are going to be attempting to solve the frozenlake environment from the openai gym for those unfamiliar the openai gym provides an easy way for people to experiment with their learning agents in an array of provided toy games the frozenlake environment consists of a x grid of blocks each one either being the start block the goal block a safe frozen block or a dangerous hole the objective is to have an agent learn to navigate from the start to the goal without moving onto a hole at any given time the agent can choose to move either up down left or right the catch is that there is a wind which occasionally blows the agent onto a space they didn t choose as such perfect performance every time is impossible but learning to avoid the holes and reach the goal are certainly still doable the reward at every step is except for entering the goal which provides a reward of thus we will need an algorithm that learns long term expected rewards this is exactly what q learning is designed to provide in it s simplest implementation q learning is a table of values for every state row and action column possible in the environment within each cell of the table we learn a value for how good it is to take a given action within a given state in the case of the frozenlake environment we have possible states one for each block and possible actions the four directions of movement giving us a x table of q values we start by initializing the table to be uniform all zeros and then as we observe the rewards we obtain for various actions we update the table accordingly we make updates to our q table using something called the bellman equation which states that the expected long term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state in this way we reuse our own q table when estimating how to update our table for future actions in equation form the rule looks like this this says that the q value for a given state s and action a should represent the current reward r plus the maximum discounted future reward expected according to our own table for the next state s we would end up in the discount variable allows us to decide how important the possible future rewards are compared to the present reward by updating in this way the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state below is a python walkthrough of the q table algorithm implemented in the frozenlake environment thanks to praneet d for finding the optimal hyperparameters for this approach now you may be thinking tables are great but they don t really scale do they while it is easy to have a x table for a simple grid world the number of possible states in any modern game or real world environment is nearly infinitely larger for most interesting problems tables simply don t work we instead need some way to take a description of our state and produce q values for actions without a table that is where neural networks come in by acting as a function approximator we can take any number of possible states that can be represented as a vector and learn to map them to q values in the case of the frozenlake example we will be using a one layer network which takes the state encoded in a one hot vector x and produces a vector of q values one for each action such a simple network acts kind of like a glorified table with the network weights serving as the old cells the key difference is that we can easily expand the tensorflow network with added layers activation functions and different input types whereas all that is impossible with a regular table the method of updating is a little different as well instead of directly updating our table with a network we will be using backpropagation and a loss function our loss function will be sum of squares loss where the difference between the current predicted q values and the target value is computed and the gradients passed through the network in this case our q target for the chosen action is the equivalent to the q value computed in equation above below is the tensorflow walkthrough of implementing our simple q network while the network learns to solve the frozenlake problem it turns out it doesn t do so quite as efficiently as the q table while neural networks allow for greater flexibility they do so at the cost of stability when it comes to q learning there are a number of possible extensions to our simple q network which allow for greater performance and more robust learning two tricks in particular are referred to as experience replay and freezing target networks those improvements and other tweaks were the key to getting atari playing deep q networks and we will be exploring those additions in the future for more info on the theory behind q learning see this great post by tambet matiisen i hope this tutorial has been helpful for those curious about how to implement simple q learning algorithms if this post has been valuable to you please consider donating to help support future tutorials articles and implementations any contribution is greatly appreciated if you d like to follow my work on deep learning ai and cognitive science follow me on medium arthur juliani or on twitter awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning unity d cognitive neuroscience phd student exploring frontier technology through the lens of artificial intelligence data science and the shape of things to come
Adam Geitgey,6800,11,https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=tag_archive---------6----------------,Machine Learning is Fun Part 6: How to do Speech Recognition with Deep Learning,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in tie ng vie t or speech recognition is invading our lives it s built into our phones our game consoles and our smart watches it s even automating our homes for just you can get an amazon echo dot a magic box that allows you to order pizza get a weather report or even buy trash bags just by speaking out loud the echo dot has been so popular this holiday season that amazon can t seem to keep them in stock but speech recognition has been around for decades so why is it just now hitting the mainstream the reason is that deep learning finally made speech recognition accurate enough to be useful outside of carefully controlled environments andrew ng has long predicted that as speech recognition goes from accurate to accurate it will become a primary way that we interact with computers the idea is that this accuracy gap is the difference between annoyingly unreliable and incredibly useful thanks to deep learning we re finally cresting that peak let s learn how to do speech recognition with deep learning if you know how neural machine translation works you might guess that we could simply feed sound recordings into a neural network and train it to produce text that s the holy grail of speech recognition with deep learning but we aren t quite there yet at least at the time that i wrote this i bet that we will be in a couple of years the big problem is that speech varies in speed one person might say hello very quickly and another person might say heeeelllllllllllllooooo very slowly producing a much longer sound file with much more data both both sound files should be recognized as exactly the same text hello automatically aligning audio files of various lengths to a fixed length piece of text turns out to be pretty hard to work around this we have to use some special tricks and extra precessing in addition to a deep neural network let s see how it works the first step in speech recognition is obvious we need to feed sound waves into a computer in part we learned how to take an image and treat it as an array of numbers so that we can feed directly into a neural network for image recognition but sound is transmitted as waves how do we turn sound waves into numbers let s use this sound clip of me saying hello sound waves are one dimensional at every moment in time they have a single value based on the height of the wave let s zoom in on one tiny part of the sound wave and take a look to turn this sound wave into numbers we just record of the height of the wave at equally spaced points this is called sampling we are taking a reading thousands of times a second and recording a number representing the height of the sound wave at that point in time that s basically all an uncompressed wav audio file is cd quality audio is sampled at khz readings per second but for speech recognition a sampling rate of khz samples per second is enough to cover the frequency range of human speech lets sample our hello sound wave times per second here s the first samples you might be thinking that sampling is only creating a rough approximation of the original sound wave because it s only taking occasional readings there s gaps in between our readings so we must be losing data right but thanks to the nyquist theorem we know that we can use math to perfectly reconstruct the original sound wave from the spaced out samples as long as we sample at least twice as fast as the highest frequency we want to record i mention this only because nearly everyone gets this wrong and assumes that using higher sampling rates always leads to better audio quality it doesn t end rant we now have an array of numbers with each number representing the sound wave s amplitude at th of a second intervals we could feed these numbers right into a neural network but trying to recognize speech patterns by processing these samples directly is difficult instead we can make the problem easier by doing some pre processing on the audio data let s start by grouping our sampled audio into millisecond long chunks here s our first milliseconds of audio i e our first samples plotting those numbers as a simple line graph gives us a rough approximation of the original sound wave for that millisecond period of time this recording is only th of a second long but even this short recording is a complex mish mash of different frequencies of sound there s some low sounds some mid range sounds and even some high pitched sounds sprinkled in but taken all together these different frequencies mix together to make up the complex sound of human speech to make this data easier for a neural network to process we are going to break apart this complex sound wave into it s component parts we ll break out the low pitched parts the next lowest pitched parts and so on then by adding up how much energy is in each of those frequency bands from low to high we create a fingerprint of sorts for this audio snippet imagine you had a recording of someone playing a c major chord on a piano that sound is the combination of three musical notes c e and g all mixed together into one complex sound we want to break apart that complex sound into the individual notes to discover that they were c e and g this is the exact same idea we do this using a mathematic operation called a fourier transform it breaks apart the complex sound wave into the simple sound waves that make it up once we have those individual sound waves we add up how much energy is contained in each one the end result is a score of how important each frequency range is from low pitch i e bass notes to high pitch each number below represents how much energy was in each hz band of our millisecond audio clip but this is a lot easier to see when you draw this as a chart if we repeat this process on every millisecond chunk of audio we end up with a spectrogram each column from left to right is one ms chunk a spectrogram is cool because you can actually see musical notes and other pitch patterns in audio data a neural network can find patterns in this kind of data more easily than raw sound waves so this is the data representation we ll actually feed into our neural network now that we have our audio in a format that s easy to process we will feed it into a deep neural network the input to the neural network will be millisecond audio chunks for each little audio slice it will try to figure out the letter that corresponds the sound currently being spoken we ll use a recurrent neural network that is a neural network that has a memory that influences future predictions that s because each letter it predicts should affect the likelihood of the next letter it will predict too for example if we have said hel so far it s very likely we will say lo next to finish out the word hello it s much less likely that we will say something unpronounceable next like xyz so having that memory of previous predictions helps the neural network make more accurate predictions going forward after we run our entire audio clip through the neural network one chunk at a time we ll end up with a mapping of each audio chunk to the letters most likely spoken during that chunk here s what that mapping looks like for me saying hello our neural net is predicting that one likely thing i said was hhhee ll lllooo but it also thinks that it was possible that i said hhhuu ll lllooo or even aaauu ll lllooo we have some steps we follow to clean up this output first we ll replace any repeated characters a single character then we ll remove any blanks that leaves us with three possible transcriptions hello hullo and aullo if you say them out loud all of these sound similar to hello because it s predicting one character at a time the neural network will come up with these very sounded out transcriptions for example if you say he would not go it might give one possible transcription as he wud net go the trick is to combine these pronunciation based predictions with likelihood scores based on large database of written text books news articles etc you throw out transcriptions that seem the least likely to be real and keep the transcription that seems the most realistic of our possible transcriptions hello hullo and aullo obviously hello will appear more frequently in a database of text not to mention in our original audio based training data and thus is probably correct so we ll pick hello as our final transcription instead of the others done you might be thinking but what if someone says hullo it s a valid word maybe hello is the wrong transcription of course it is possible that someone actually said hullo instead of hello but a speech recognition system like this trained on american english will basically never produce hullo as the transcription it s just such an unlikely thing for a user to say compared to hello that it will always think you are saying hello no matter how much you emphasize the u sound try it out if your phone is set to american english try to get your phone s digital assistant to recognize the world hullo you can t it refuses it will always understand it as hello not recognizing hullo is a reasonable behavior but sometimes you ll find annoying cases where your phone just refuses to understand something valid you are saying that s why these speech recognition models are always being retrained with more data to fix these edge cases one of the coolest things about machine learning is how simple it sometimes seems you get a bunch of data feed it into a machine learning algorithm and then magically you have a world class ai system running on your gaming laptop s video card right that sort of true in some cases but not for speech recognizing speech is a hard problem you have to overcome almost limitless challenges bad quality microphones background noise reverb and echo accent variations and on and on all of these issues need to be present in your training data to make sure the neural network can deal with them here s another example did you know that when you speak in a loud room you unconsciously raise the pitch of your voice to be able to talk over the noise humans have no problem understanding you either way but neural networks need to be trained to handle this special case so you need training data with people yelling over noise to build a voice recognition system that performs on the level of siri google now or alexa you will need a lot of training data far more data than you can likely get without hiring hundreds of people to record it for you and since users have low tolerance for poor quality voice recognition systems you can t skimp on this no one wants a voice recognition system that works of the time for a company like google or amazon hundreds of thousands of hours of spoken audio recorded in real life situations is gold that s the single biggest thing that separates their world class speech recognition system from your hobby system the whole point of putting google now and siri on every cell phone for free or selling alexa units that have no subscription fee is to get you to use them as much as possible every single thing you say into one of these systems is recorded forever and used as training data for future versions of speech recognition algorithms that s the whole game don t believe me if you have an android phone with google now click here to listen to actual recordings of yourself saying every dumb thing you ve ever said into it so if you are looking for a start up idea i wouldn t recommend trying to build your own speech recognition system to compete with google instead figure out a way to get people to give you recordings of themselves talking for hours the data can be your product instead if you liked this article please consider signing up for my machine learning is fun email list i ll only email you when i have something new and awesome to share it s the best way to find out when i write more articles like this you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Adam Geitgey,5800,16,https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa?source=tag_archive---------7----------------,Machine Learning is Fun Part 5: Language Translation with Deep Learning and the Magic of Sequences,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in tie ng vie t or italiano we all know and love google translate the website that can instantly translate between different human languages as if by magic it is even available on our phones and smartwatches the technology behind google translate is called machine translation it has changed the world by allowing people to communicate when it wouldn t otherwise be possible but we all know that high school students have been using google translate to umm assist with their spanish homework for years isn t this old news it turns out that over the past two years deep learning has totally rewritten our approach to machine translation deep learning researchers who know almost nothing about language translation are throwing together relatively simple machine learning solutions that are beating the best expert built language translation systems in the world the technology behind this breakthrough is called sequence to sequence learning it s very powerful technique that be used to solve many kinds problems after we see how it is used for translation we ll also learn how the exact same algorithm can be used to write ai chat bots and describe pictures let s go so how do we program a computer to translate human language the simplest approach is to replace every word in a sentence with the translated word in the target language here s a simple example of translating from spanish to english word by word this is easy to implement because all you need is a dictionary to look up each word s translation but the results are bad because it ignores grammar and context so the next thing you might do is start adding language specific rules to improve the results for example you might translate common two word phrases as a single group and you might swap the order nouns and adjectives since they usually appear in reverse order in spanish from how they appear in english that worked if we just keep adding more rules until we can handle every part of grammar our program should be able to translate any sentence right this is how the earliest machine translation systems worked linguists came up with complicated rules and programmed them in one by one some of the smartest linguists in the world labored for years during the cold war to create translation systems as a way to interpret russian communications more easily unfortunately this only worked for simple plainly structured documents like weather reports it didn t work reliably for real world documents the problem is that human language doesn t follow a fixed set of rules human languages are full of special cases regional variations and just flat out rule breaking the way we speak english more influenced by who invaded who hundreds of years ago than it is by someone sitting down and defining grammar rules after the failure of rule based systems new translation approaches were developed using models based on probability and statistics instead of grammar rules building a statistics based translation system requires lots of training data where the exact same text is translated into at least two languages this double translated text is called parallel corpora in the same way that the rosetta stone was used by scientists in the s to figure out egyptian hieroglyphs from greek computers can use parallel corpora to guess how to convert text from one language to another luckily there s lots of double translated text already sitting around in strange places for example the european parliament translates their proceedings into languages so researchers often use that data to help build translation systems the fundamental difference with statistical translation systems is that they don t try to generate one exact translation instead they generate thousands of possible translations and then they rank those translations by likely each is to be correct they estimate how correct something is by how similar it is to the training data here s how it works first we break up our sentence into simple chunks that can each be easily translated next we will translate each of these chunks by finding all the ways humans have translated those same chunks of words in our training data it s important to note that we are not just looking up these chunks in a simple translation dictionary instead we are seeing how actual people translated these same chunks of words in real world sentences this helps us capture all of the different ways they can be used in different contexts some of these possible translations are used more frequently than others based on how frequently each translation appears in our training data we can give it a score for example it s much more common for someone to say quiero to mean i want than to mean i try so we can use how frequently quiero was translated to i want in our training data to give that translation more weight than a less frequent translation next we will use every possible combination of these chunks to generate a bunch of possible sentences just from the chunk translations we listed in step we can already generate nearly different variations of our sentence by combining the chunks in different ways here are some examples but in a real world system there will be even more possible chunk combinations because we ll also try different orderings of words and different ways of chunking the sentence now need to scan through all of these generated sentences to find the one that is that sounds the most human to do this we compare each generated sentence to millions of real sentences from books and news stories written in english the more english text we can get our hands on the better take this possible translation it s likely that no one has ever written a sentence like this in english so it would not be very similar to any sentences in our data set we ll give this possible translation a low probability score but look at this possible translation this sentence will be similar to something in our training set so it will get a high probability score after trying all possible sentences we ll pick the sentence that has the most likely chunk translations while also being the most similar overall to real english sentences our final translation would be i want to go to the prettiest beach not bad statistical machine translation systems perform much better than rule based systems if you give them enough training data franz josef och improved on these ideas and used them to build google translate in the early s machine translation was finally available to the world in the early days it was surprising to everyone that the dumb approach to translating based on probability worked better than rule based systems designed by linguists this led to a somewhat mean saying among researchers in the s statistical machine translation systems work well but they are complicated to build and maintain every new pair of languages you want to translate requires experts to tweak and tune a new multi step translation pipeline because it is so much work to build these different pipelines trade offs have to be made if you are asking google to translate georgian to telegu it has to internally translate it into english as an intermediate step because there s not enough georgain to telegu translations happening to justify investing heavily in that language pair and it might do that translation using a less advanced translation pipeline than if you had asked it for the more common choice of french to english wouldn t it be cool if we could have the computer do all that annoying development work for us the holy grail of machine translation is a black box system that learns how to translate by itself just by looking at training data with statistical machine translation humans are still needed to build and tweak the multi step statistical models in kyunghyun cho s team made a breakthrough they found a way to apply deep learning to build this black box system their deep learning model takes in a parallel corpora and and uses it to learn how to translate between those two languages without any human intervention two big ideas make this possible recurrent neural networks and encodings by combining these two ideas in a clever way we can build a self learning translation system we ve already talked about recurrent neural networks in part but let s quickly review a regular non recurrent neural network is a generic machine learning algorithm that takes in a list of numbers and calculates a result based on previous training neural networks can be used as a black box to solve lots of problems for example we can use a neural network to calculate the approximate value of a house based on attributes of that house but like most machine learning algorithms neural networks are stateless you pass in a list of numbers and the neural network calculates a result if you pass in those same numbers again it will always calculate the same result it has no memory of past calculations in other words always equals a recurrent neural network or rnn for short is a slightly tweaked version of a neural network where the previous state of the neural network is one of the inputs to the next calculation this means that previous calculations change the results of future calculations why in the world would we want to do this shouldn t always equal no matter what we last calculated this trick allows neural networks to learn patterns in a sequence of data for example you can use it to predict the next most likely word in a sentence based on the first few words rnns are useful any time you want to learn patterns in data because human language is just one big complicated pattern rnns are increasingly used in many areas of natural language processing if you want to learn more about rnns you can read part where we used one to generate a fake ernest hemingway book and then used another one to generate fake super mario brothers levels the other idea we need to review is encodings we talked about encodings in part as part of face recognition to explain encodings let s take a slight detour into how we can tell two different people apart with a computer when you are trying to tell two faces apart with a computer you collect different measurements from each face and use those measurements to compare faces for example we might measure the size of each ear or the spacing between the eyes and compare those measurements from two pictures to see if they are the same person you re probably already familiar with this idea from watching any primetime detective show like csi the idea of turning a face into a list of measurements is an example of an encoding we are taking raw data a picture of a face and turning it into a list of measurements that represent it the encoding but like we saw in part we don t have to come up with a specific list of facial features to measure ourselves instead we can use a neural network to generate measurements from a face the computer can do a better job than us in figuring out which measurements are best able to differentiate two similar people this is our encoding it lets us represent something very complicated a picture of a face with something simple numbers now comparing two different faces is much easier because we only have to compare these numbers for each face instead of comparing full images guess what we can do the same thing with sentences we can come up with an encoding that represents every possible different sentence as a series of unique numbers to generate this encoding we ll feed the sentence into the rnn one word at time the final result after the last word is processed will be the values that represent the entire sentence great so now we have a way to represent an entire sentence as a set of unique numbers we don t know what each number in the encoding means but it doesn t really matter as long as each sentence is uniquely identified by it s own set of numbers we don t need to know exactly how those numbers were generated ok so we know how to use an rnn to encode a sentence into a set of unique numbers how does that help us here s where things get really cool what if we took two rnns and hooked them up end to end the first rnn could generate the encoding that represents a sentence then the second rnn could take that encoding and just do the same logic in reverse to decode the original sentence again of course being able to encode and then decode the original sentence again isn t very useful but what if and here s the big idea we could train the second rnn to decode the sentence into spanish instead of english we could use our parallel corpora training data to train it to do that and just like that we have a generic way of converting a sequence of english words into an equivalent sequence of spanish words this is a powerful idea note that we glossed over some things that are required to make this work with real world data for example there s additional work you have to do to deal with different lengths of input and output sentences see bucketing and padding there s also issues with translating rare words correctly if you want to build your own language translation system there s a working demo included with tensorflow that will translate between english and french however this is not for the faint of heart or for those with limited budgets this technology is still new and very resource intensive even if you have a fast computer with a high end video card it might take about a month of continuous processing time to train your own language translation system also sequence to sequence language translation techniques are improving so rapidly that it s hard to keep up many recent improvements like adding an attention mechanism or tracking context are significantly improving results but these developments are so new that there aren t even wikipedia pages for them yet if you want to do anything serious with sequence to sequence learning you ll need to keep with new developments as they occur so what else can we do with sequence to sequence models about a year ago researchers at google showed that you can use sequence to sequence models to build ai bots the idea is so simple that it s amazing it works at all first they captured chat logs between google employees and google s tech support team then they trained a sequence to sequence model where the employee s question was the input sentence and the tech support team s response was the translation of that sentence when a user interacted with the bot they would translate each of the user s messages with this system to get the bot s response the end result was a semi intelligent bot that could sometimes answer real tech support questions here s part of a sample conversation between a user and the bot from their paper they also tried building a chat bot based on millions of movie subtitles the idea was to use conversations between movie characters as a way to train a bot to talk like a human the input sentence is a line of dialog said by one character and the translation is what the next character said in response this produced really interesting results not only did the bot converse like a human but it displayed a small bit of intelligence this is only the beginning of the possibilities we aren t limited to converting one sentence into another sentence it s also possible to make an image to sequence model that can turn an image into text a different team at google did this by replacing the first rnn with a convolutional neural network like we learned about in part this allows the input to be a picture instead of a sentence the rest works basically the same way and just like that we can turn pictures into words as long as we have lots and lots of training data andrej karpathy expanded on these ideas to build a system capable of describing images in great detail by processing multiple regions of an image separately this makes it possible to build image search engines that are capable of finding images that match oddly specific search queries there s even researchers working on the reverse problem generating an entire picture based on just a text description just from these examples you can start to imagine the possibilities so far there have been sequence to sequence applications in everything from speech recognition to computer vision i bet there will be a lot more over the next year if you want to learn more in depth about sequence to sequence models and translation here s some recommended resources if you liked this article please consider signing up for my machine learning is fun email list i ll only email you when i have something new and awesome to share it s the best way to find out when i write more articles like this you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Chris Dixon,5300,12,https://medium.com/@cdixon/eleven-reasons-to-be-excited-about-the-future-of-technology-ef5f9b939cb2?source=tag_archive---------8----------------,Eleven Reasons To Be Excited About The Future of Technology,in the year a person could expect to live less than years of the global population lived in extreme poverty and less that of the population was literate today human life expectancy is over years less that of the global population lives in extreme poverty and over of people are literate these improvements are due mainly to advances in technology beginning in the industrial age and continuing today in the information age there are many exciting new technologies that will continue to transform the world and improve human welfare here are eleven of them self driving cars exist today that are safer than human driven cars in most driving conditions over the next years they ll get even safer and will begin to go mainstream the world health organization estimates that million people die from car related injuries per year half of the deaths are pedestrians bicyclists and motorcyclists hit by cars cars are the leading cause of death for people ages years old just as cars reshaped the world in the th century so will self driving cars in the st century in most cities between of usable space is taken up by parking spaces and most cars are parked about of the time self driving cars will be in almost continuous use most likely hailed from a smartphone app thereby dramatically reducing the need for parking cars will communicate with one another to avoid accidents and traffic jams and riders will be able to spend commuting time on other activities like work education and socializing attempts to fight climate change by reducing the demand for energy haven t worked fortunately scientists engineers and entrepreneurs have been working hard on the supply side to make clean energy convenient and cost effective due to steady technological and manufacturing advances the price of solar cells has dropped since solar will soon be more cost efficient than fossil fuels the cost of wind energy has also dropped to an all time low and in the last decade represented about a third of newly installed us energy capacity forward thinking organizations are taking advantage of this for example in india there is an initiative to convert airports to self sustaining clean energy tesla is making high performance affordable electric cars and installing electric charging stations worldwide there are hopeful signs that clean energy could soon be reaching a tipping point for example in japan there are now more electric charging stations than gas stations and germany produces so much renewable energy it sometimes produces even more than it can use computer processors only recently became fast enough to power comfortable and convincing virtual and augmented reality experiences companies like facebook google apple and microsoft are investing billions of dollars to make vr and ar more immersive comfortable and affordable people sometimes think vr and ar will be used only for gaming but over time they will be used for all sorts of activities for example we ll use them to manipulate d objects to meet with friends and colleagues from around the world and even for medical applications like treating phobias or helping rehabilitate paralysis victims vr and ar have been dreamed about by science fiction fans for decades in the next few years they ll finally become a mainstream reality gps started out as a military technology but is now used to hail taxis get mapping directions and hunt poke mon likewise drones started out as a military technology but are increasingly being used for a wide range of consumer and commercial applications for example drones are being used to inspect critical infrastructure like bridges and power lines to survey areas struck by natural disasters and many other creative uses like fighting animal poaching amazon and google are building drones to deliver household items the startup zipline uses drones to deliver medical supplies to remote villages that can t be accessed by roads there is also a new wave of startups working on flying cars including two funded by the cofounder of google larry page flying cars use the same advanced technology used in drones but are large enough to carry people due to advances in materials batteries and software flying cars will be significantly more affordable and convenient than today s planes and helicopters artificial intelligence has made rapid advances in the last decade due to new algorithms and massive increases in data collection and computing power ai can be applied to almost any field for example in photography an ai technique called artistic style transfer transforms photographs into the style of a given painter google built an ai system that controls its datacenter power systems saving hundreds of millions of dollars in energy costs the broad promise of ai is to liberate people from repetitive mental tasks the same way the industrial revolution liberated people from repetitive physical tasks some people worry that ai will destroy jobs history has shown that while new technology does indeed eliminate jobs it also creates new and better jobs to replace them for example with advent of the personal computer the number of typographer jobs dropped but the increase in graphic designer jobs more than made up for it it is much easier to imagine jobs that will go away than new jobs that will be created today millions of people work as app developers ride sharing drivers drone operators and social media marketers jobs that didn t exist and would have been difficult to even imagine ten years ago by of adults on earth will have an internet connected smartphone an iphone has about billion transistors roughly times more transistors than a intel pentium computer today s smartphones are what used to be considered supercomputers internet connected smartphones give ordinary people abilities that just a short time ago were only available to an elite few protocols are the plumbing of the internet most of the protocols we use today were developed decades ago by academia and government since then protocol development mostly stopped as energy shifted to developing proprietary systems like social networks and messaging apps cryptocurrency and blockchain technologies are changing this by providing a new business model for internet protocols this year alone hundreds of millions of dollars were raised for a broad range of innovative blockchain based protocols protocols based on blockchains also have capabilities that previous protocols didn t for example ethereum is a new blockchain based protocol that can be used to create smart contracts and trusted databases that are immune to corruption and censorship while college tuition skyrockets anyone with a smartphone can study almost any topic online accessing educational content that is mostly free and increasingly high quality encyclopedia britannica used to cost now anyone with a smartphone can instantly access wikipedia you used to have to go to school or buy programming books to learn computer programming now you can learn from a community of over million programmers at stack overflow youtube has millions of hours of free tutorials and lectures many of which are produced by top professors and universities the quality of online education is getting better all the time for the last years mit has been recording lectures and compiling materials that cover over courses as perhaps the greatest research university in the world mit has always been ahead of the trends over the next decade expect many other schools to follow mit s lead earth is running out of farmable land and fresh water this is partly because our food production systems are incredibly inefficient it takes an astounding gallons of water to produce pound of beef fortunately a variety of new technologies are being developed to improve our food system for example entrepreneurs are developing new food products that are tasty and nutritious substitutes for traditional foods but far more environmentally friendly the startup impossible foods invented meat products that look and taste like the real thing but are actually made of plants their burger uses less land less water and produces less greenhouse gas emissions than traditional burgers other startups are creating plant based replacements for milk eggs and other common foods soylent is a healthy inexpensive meal replacement that uses advanced engineered ingredients that are much friendlier to the environment than traditional ingredients some of these products are developed using genetic modification a powerful scientific technique that has been widely mischaracterized as dangerous according to a study by the pew organization of scientists think genetically modified foods are safe another exciting development in food production is automated indoor farming due to advances in solar energy sensors lighting robotics and artificial intelligence indoor farms have become viable alternatives to traditional outdoor farms compared to traditional farms automated indoor farms use roughly times less water and land crops are harvested many more times per year there is no dependency on weather and no need to use pesticides until recently computers have only been at the periphery of medicine used primarily for research and record keeping today the combination of computer science and medicine is leading to a variety of breakthroughs for example just fifteen years ago it cost b to sequence a human genome today the cost is about a thousand dollars and continues to drop genetic sequencing will soon be a routine part of medicine genetic sequencing generates massive amounts of data that can be analyzed using powerful data analysis software one application is analyzing blood samples for early detection of cancer further genetic analysis can help determine the best course of treatment another application of computers to medicine is in prosthetic limbs here a young girl is using prosthetic hands she controls using her upper arm muscles soon we ll have the technology to control prothetic limbs with just our thoughts using brain to machine interfaces computers are also becoming increasingly effective at diagnosing diseases an artificial intelligence system recently diagnosed a rare disease that human doctors failed to diagnose by finding hidden patterns in million cancer records since the beginning of the space age in the s the vast majority of space funding has come from governments but that funding has been in decline for example nasa s budget dropped from about of the federal budget in the s to about of the federal budget today the good news is that private space companies have started filling the void these companies provide a wide range of products and services including rocket launches scientific research communications and imaging satellites and emerging speculative business models like asteroid mining the most famous private space company is elon musk s spacex which successfully sent rockets into space that can return home to be reused perhaps the most intriguing private space company is planetary resources which is trying to pioneer a new industry mining minerals from asteroids if successful asteroid mining could lead to a new gold rush in outer space like previous gold rushes this could lead to speculative excess but also dramatically increased funding for new technologies and infrastructure these are just a few of the amazing technologies we ll see developed in the coming decades is just the beginning of a new age of wonders as futurist kevin kelly says from a quick cheer to a standing ovation clap to show how much you enjoyed this story www cdixon org about
Tal Perry,2600,17,https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02?source=tag_archive---------9----------------,Deep Learning the Stock Market – Tal Perry – Medium,update took me a while but here is an ipython notebook with a rough implementation in the past few months i ve been fascinated with deep learning especially its applications to language and text i ve spent the bulk of my career in financial technologies mostly in algorithmic trading and alternative data services you can see where this is going i wrote this to get my ideas straight in my head while i ve become a deep learning enthusiast i don t have too many opportunities to brain dump an idea in most of its messy glory i think that a decent indication of a clear thought is the ability to articulate it to people not from the field i hope that i ve succeeded in doing that and that my articulation is also a pleasurable read why nlp is relevant to stock prediction in many nlp problems we end up taking a sequence and encoding it into a single fixed size representation then decoding that representation into another sequence for example we might tag entities in the text translate from english to french or convert audio frequencies to text there is a torrent of work coming out in these areas and a lot of the results are achieving state of the art performance in my mind the biggest difference between the nlp and financial analysis is that language has some guarantee of structure it s just that the rules of the structure are vague markets on the other hand don t come with a promise of a learnable structure that such a structure exists is the assumption that this project would prove or disprove rather it might prove or disprove if i can find that structure assuming the structure is there the idea of summarizing the current state of the market in the same way we encode the semantics of a paragraph seems plausible to me if that doesn t make sense yet keep reading it will you shall know a word by the company it keeps firth j r there is tons of literature on word embeddings richard socher s lecture is a great place to start in short we can make a geometry of all the words in our language and that geometry captures the meaning of words and relationships between them you may have seen the example of king man woman queen or something of the sort embeddings are cool because they let us represent information in a condensed way the old way of representing words was holding a vector a big list of numbers that was as long as the number of words we know and setting a in a particular place if that was the current word we are looking at that is not an efficient approach nor does it capture any meaning with embeddings we can represent all of the words in a fixed number of dimensions seems to be plenty works great and then leverage their higher dimensional geometry to understand them the picture below shows an example an embedding was trained on more or less the entire internet after a few days of intensive calculations each word was embedded in some high dimensional space this space has a geometry concepts like distance and so we can ask which words are close together the authors inventors of that method made an example here are the words that are closest to frog but we can embed more than just words we can do say stock market embeddings market vec the first word embedding algorithm i heard about was word vec i want to get the same effect for the market though i ll be using a different algorithm my input data is a csv the first column is the date and there are columns corresponding to the high low open closing price of stocks that is my input vector is dimensional which is too big so the first thing i m going to do is stuff it into a lower dimensional space say because i liked the movie taking something in dimensions and stuffing it into a dimensional space my sound hard but its actually easy we just need to multiply matrices a matrix is a big excel spreadsheet that has numbers in every cell and no formatting problems imagine an excel table with columns and rows and when we basically bang it against the vector a new vector comes out that is only of size i wish that s how they would have explained it in college the fanciness starts here as we re going to set the numbers in our matrix at random and part of the deep learning is to update those numbers so that our excel spreadsheet changes eventually this matrix spreadsheet i ll stick with matrix from now on will have numbers in it that bang our original dimensional vector into a concise dimensional summary of itself we re going to get a little fancier here and apply what they call an activation function we re going to take a function and apply it to each number in the vector individually so that they all end up between and or and infinity it depends why it makes our vector more special and makes our learning process able to understand more complicated things how so what what i m expecting to find is that that new embedding of the market prices the vector into a smaller space captures all the essential information for the task at hand without wasting time on the other stuff so i d expect they d capture correlations between other stocks perhaps notice when a certain sector is declining or when the market is very hot i don t know what traits it will find but i assume they ll be useful now what lets put aside our market vectors for a moment and talk about language models andrej karpathy wrote the epic post the unreasonable effectiveness of recurrent neural networks if i d summarize in the most liberal fashion the post boils down to and then as a punchline he generated a bunch of text that looks like shakespeare and then he did it again with the linux source code and then again with a textbook on algebraic geometry so i ll get back to the mechanics of that magic box in a second but let me remind you that we want to predict the future market based on the past just like he predicted the next word based on the previous one where karpathy used characters we re going to use our market vectors and feed them into the magic black box we haven t decided what we want it to predict yet but that is okay we won t be feeding its output back into it either going deeper i want to point out that this is where we start to get into the deep part of deep learning so far we just have a single layer of learning that excel spreadsheet that condenses the market now we re going to add a few more layers and stack them to make a deep something that s the deep in deep learning so karpathy shows us some sample output from the linux source code this is stuff his black box wrote notice that it knows how to open and close parentheses and respects indentation conventions the contents of the function are properly indented and the multi line printk statement has an inner indentation that means that this magic box understands long range dependencies when it s indenting within the print statement it knows it s in a print statement and also remembers that it s in a function or at least another indented scope that s nuts it s easy to gloss over that but an algorithm that has the ability to capture and remember long term dependencies is super useful because we want to find long term dependencies in the market inside the magical black box what s inside this magical black box it is a type of recurrent neural network rnn called an lstm an rnn is a deep learning algorithm that operates on sequences like sequences of characters at every step it takes a representation of the next character like the embeddings we talked about before and operates on the representation with a matrix like we saw before the thing is the rnn has some form of internal memory so it remembers what it saw previously it uses that memory to decide how exactly it should operate on the next input using that memory the rnn can remember that it is inside of an intended scope and that is how we get properly nested output text a fancy version of an rnn is called a long short term memory lstm lstm has cleverly designed memory that allows it to so an lstm can see a and say to itself oh yeah that s important i should remember that and when it does it essentially remembers an indication that it is in a nested scope once it sees the corresponding it can decide to forget the original opening brace and thus forget that it is in a nested scope we can have the lstm learn more abstract concepts by stacking a few of them on top of each other that would make us deep again now each output of the previous lstm becomes the inputs of the next lstm and each one goes on to learn higher abstractions of the data coming in in the example above and this is just illustrative speculation the first layer of lstms might learn that characters separated by a space are words the next layer might learn word types like static void action new function the next layer might learn the concept of a function and its arguments and so on it s hard to tell exactly what each layer is doing though karpathy s blog has a really nice example of how he did visualize exactly that connecting market vec and lstms the studious reader will notice that karpathy used characters as his inputs not embeddings technically a one hot encoding of characters but lars eidnes actually used word embeddings when he wrote auto generating clickbait with recurrent neural network the figure above shows the network he used ignore the softmax part we ll get to it later for the moment check out how on the bottom he puts in a sequence of words vectors at the bottom and each one remember a word vector is a representation of a word in the form of a bunch of numbers like we saw in the beginning of this post lars inputs a sequence of word vectors and each one of them we re going to do the same thing with one difference instead of word vectors we ll input marketvectors those market vectors we described before to recap the marketvectors should contain a summary of what s happening in the market at a given point in time by putting a sequence of them through lstms i hope to capture the long term dynamics that have been happening in the market by stacking together a few layers of lstms i hope to capture higher level abstractions of the market s behavior what comes out thus far we haven t talked at all about how the algorithm actually learns anything we just talked about all the clever transformations we ll do on the data we ll defer that conversation to a few paragraphs down but please keep this part in mind as it is the se up for the punch line that makes everything else worthwhile in karpathy s example the output of the lstms is a vector that represents the next character in some abstract representation in eidnes example the output of the lstms is a vector that represents what the next word will be in some abstract space the next step in both cases is to change that abstract representation into a probability vector that is a list that says how likely each character or word respectively is likely to appear next that s the job of the softmax function once we have a list of likelihoods we select the character or word that is the most likely to appear next in our case of predicting the market we need to ask ourselves what exactly we want to market to predict some of the options that i thought about were and are regression problems where we have to predict an actual number instead of the likelihood of a specific event like the letter n appearing or the market going up those are fine but not what i want to do and are fairly similar they both ask to predict an event in technical jargon a class label an event could be the letter n appearing next or it could be moved up while not going down more than in the last minutes the trade off between and is that is much more common and thus easier to learn about while is more valuable as not only is it an indicator of profit but also has some constraint on risk is the one we ll continue with for this article because it s similar to and but has mechanics that are easier to follow the vix is sometimes called the fear index and it represents how volatile the stocks in the s p are it is derived by observing the implied volatility for specific options on each of the stocks in the index sidenote why predict the vix what makes the vix an interesting target is that back to our lstm outputs and the softmax how do we use the formulations we saw before to predict changes in the vix a few minutes in the future for each point in our dataset we ll look what happened to the vix minutes later if it went up by more than without going down more than during that time we ll output a otherwise a then we ll get a sequence that looks like we want to take the vector that our lstms output and squish it so that it gives us the probability of the next item in our sequence being a the squishing happens in the softmax part of the diagram above technically since we only have class now we use a sigmoid so before we get into how this thing learns let s recap what we ve done so far how does this thing learn now the fun part everything we did until now was called the forward pass we d do all of those steps while we train the algorithm and also when we use it in production here we ll talk about the backward pass the part we do only while in training that makes our algorithm learn so during training not only did we prepare years worth of historical data we also prepared a sequence of prediction targets that list of and that showed if the vix moved the way we want it to or not after each observation in our data to learn we ll feed the market data to our network and compare its output to what we calculated comparing in our case will be simple subtraction that is we ll say that our model s error is or in english the square root of the square of the difference between what actually happened and what we predicted here s the beauty that s a differential function that is we can tell by how much the error would have changed if our prediction would have changed a little our prediction is the outcome of a differentiable function the softmax the inputs to the softmax the lstms are all mathematical functions that are differentiable now all of these functions are full of parameters those big excel spreadsheets i talked about ages ago so at this stage what we do is take the derivative of the error with respect to every one of the millions of parameters in all of those excel spreadsheets we have in our model when we do that we can see how the error will change when we change each parameter so we ll change each parameter in a way that will reduce the error this procedure propagates all the way to the beginning of the model it tweaks the way we embed the inputs into marketvectors so that our marketvectors represent the most significant information for our task it tweaks when and what each lstm chooses to remember so that their outputs are the most relevant to our task it tweaks the abstractions our lstms learn so that they learn the most important abstractions for our task which in my opinion is amazing because we have all of this complexity and abstraction that we never had to specify anywhere it s all inferred mathamagically from the specification of what we consider to be an error what s next now that i ve laid this out in writing and it still makes sense to me i want so if you ve come this far please point out my errors and share your inputs other thoughts here are some mostly more advanced thoughts about this project what other things i might try and why it makes sense to me that this may actually work liquidity and efficient use of capital generally the more liquid a particular market is the more efficient that is i think this is due to a chicken and egg cycle whereas a market becomes more liquid it is able to absorb more capital moving in and out without that capital hurting itself as a market becomes more liquid and more capital can be used in it you ll find more sophisticated players moving in this is because it is expensive to be sophisticated so you need to make returns on a large chunk of capital in order to justify your operational costs a quick corollary is that in less liquid markets the competition isn t quite as sophisticated and so the opportunities a system like this can bring may not have been traded away the point being were i to try and trade this i would try and trade it on less liquid segments of the market that is maybe the tase instead of the s p this stuff is new the knowledge of these algorithms the frameworks to execute them and the computing power to train them are all new at least in the sense that they are available to the average joe such as myself i d assume that top players have figured this stuff out years ago and have had the capacity to execute for as long but as i mention in the above paragraph they are likely executing in liquid markets that can support their size the next tier of market participants i assume have a slower velocity of technological assimilation and in that sense there is or soon will be a race to execute on this in as yet untapped markets multiple time frames while i mentioned a single stream of inputs in the above i imagine that a more efficient way to train would be to train market vectors at least on multiple time frames and feed them in at the inference stage that is my lowest time frame would be sampled every seconds and i d expect the network to learn dependencies that stretch hours at most i don t know if they are relevant or not but i think there are patterns on multiple time frames and if the cost of computation can be brought low enough then it is worthwhile to incorporate them into the model i m still wrestling with how best to represent these on the computational graph and perhaps it is not mandatory to start with marketvectors when using word vectors in nlp we usually start with a pretrained model and continue adjusting the embeddings during training of our model in my case there are no pretrained market vector available nor is tehre a clear algorithm for training them my original consideration was to use an auto encoder like in this paper but end to end training is cooler a more serious consideration is the success of sequence to sequence models in translation and speech recognition where a sequence is eventually encoded as a single vector and then decoded into a different representation like from speech to text or from english to french in that view the entire architecture i described is essentially the encoder and i haven t really laid out a decoder but i want to achieve something specific with the first layer the one that takes as input the dimensional vector and outputs a dimensional one i want it to find correlations or relations between various stocks and compose features about them the alternative is to run each input through an lstm perhaps concatenate all of the output vectors and consider that output of the encoder stage i think this will be inefficient as the interactions and correlations between instruments and their features will be lost and thre will be x more computation required on the other hand such an architecture could naively be paralleled across multiple gpus and hosts which is an advantage cnns recently there has been a spur of papers on character level machine translation this paper caught my eye as they manage to capture long range dependencies with a convolutional layer rather than an rnn i haven t given it more than a brief read but i think that a modification where i d treat each stock as a channel and convolve over channels first like in rgb images would be another way to capture the market dynamics in the same way that they essentially encode semantic meaning from characters from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder of https lighttag io platform to annotate text for nlp google developer expert in ml i do deep learning on text for a living and for fun
Gil Fewster,3300,5,https://medium.freecodecamp.org/the-mind-blowing-ai-announcement-from-google-that-you-probably-missed-2ffd31334805?source=tag_archive---------0----------------,The mind-blowing AI announcement from Google that you probably missed.,disclaimer i m not an expert in neural networks or machine learning since originally writing this article many people with far more expertise in these fields than myself have indicated that while impressive what google have achieved is evolutionary not revolutionary in the very least it s fair to say that i m guilty of anthropomorphising in parts of the text i ve left the article s content unchanged because i think it s interesting to compare the gut reaction i had with the subsequent comments of experts in the field i strongly encourage readers to browse the comments after reading the article for some perspectives more sober and informed than my own in the closing weeks of google published an article that quietly sailed under most people s radars which is a shame because it may just be the most astonishing article about machine learning that i read last year don t feel bad if you missed it not only was the article competing with the pre christmas rush that most of us were navigating it was also tucked away on google s research blog beneath the geektastic headline zero shot translation with google s multilingual neural machine translation system this doesn t exactly scream must read does it especially when you ve got projects to wind up gifts to buy and family feuds to be resolved all while the advent calendar relentlessly counts down the days until christmas like some kind of chocolate filled yuletide doomsday clock luckily i m here to bring you up to speed here s the deal up until september of last year google translate used phrase based translation it basically did the same thing you and i do when we look up key words and phrases in our lonely planet language guides it s effective enough and blisteringly fast compared to awkwardly thumbing your way through a bunch of pages looking for the french equivalent of please bring me all of your cheese and don t stop until i fall over but it lacks nuance phrase based translation is a blunt instrument it does the job well enough to get by but mapping roughly equivalent words and phrases without an understanding of linguistic structures can only produce crude results this approach is also limited by the extent of an available vocabulary phrase based translation has no capacity to make educated guesses at words it doesn t recognize and can t learn from new input all that changed in september when google gave their translation tool a new engine the google neural machine translation system gnmt this new engine comes fully loaded with all the hot buzzwords like neural network and machine learning the short version is that google translate got smart it developed the ability to learn from the people who used it it learned how to make educated guesses about the content tone and meaning of phrases based on the context of other words and phrases around them and here s the bit that should make your brain explode it got creative google translate invented its own language to help it translate more effectively what s more nobody told it to it didn t develop a language or interlingua as google call it because it was coded to it developed a new language because the software determined over time that this was the most efficient way to solve the problem of translation stop and think about that for a moment let it sink in a neural computing system designed to translate content from one human language into another developed its own internal language to make the task more efficient without being told to do so in a matter of weeks i ve added a correction retraction of this paragraph in the notes to understand what s going on we need to understand what zero shot translation capability is here s google s mike schuster nikhil thorat and melvin johnson from the original blog post here you can see an advantage of google s new neural machine over the old phrase based approach the gmnt is able to learn how to translate between two languages without being explicitly taught this wouldn t be possible in a phrase based model where translation is dependent upon an explicit dictionary to map words and phrases between each pair of languages being translated and this leads the google engineers onto that truly astonishing discovery of creation so there you have it in the last weeks of as journos around the world started penning their was this the worst year in living memory thinkpieces google engineers were quietly documenting a genuinely astonishing breakthrough in software engineering and linguistics i just thought maybe you d want to know ok to really understand what s going on we probably need multiple computer science and linguistics degrees i m just barely scraping the surface here if you ve got time to get a few degrees or if you ve already got them please drop me a line and explain it all me to slowly update in my excitement it s fair to say that i ve exaggerated the idea of this as an intelligent system at least so far as we would think about human intelligence and decision making make sure you read chris mcdonald s comment after the article for a more sober perspective update nafrondel s excellent detailed reply is also a must read for an expert explanation of how neural networks function from a quick cheer to a standing ovation clap to show how much you enjoyed this story a tinkerer our community publishes stories worth reading on development design and data science
David Venturi,10600,20,https://medium.freecodecamp.org/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0?source=tag_archive---------1----------------,"Every single Machine Learning course on the internet, ranked by your reviews",a year and a half ago i dropped out of one of the best computer science programs in canada i started creating my own data science master s program using online resources i realized that i could learn everything i needed through edx coursera and udacity instead and i could learn it faster more efficiently and for a fraction of the cost i m almost finished now i ve taken many data science related courses and audited portions of many more i know the options out there and what skills are needed for learners preparing for a data analyst or data scientist role so i started creating a review driven guide that recommends the best courses for each subject within data science for the first guide in the series i recommended a few coding classes for the beginner data scientist then it was statistics and probability classes then introductions to data science also data visualization for this guide i spent a dozen hours trying to identify every online machine learning course offered as of may extracting key bits of information from their syllabi and reviews and compiling their ratings my end goal was to identify the three best courses available and present them to you below for this task i turned to none other than the open source class central community and its database of thousands of course ratings and reviews since class central founder dhawal shah has kept a closer eye on online courses than arguably anyone else in the world dhawal personally helped me assemble this list of resources each course must fit three criteria we believe we covered every notable course that fits the above criteria since there are seemingly hundreds of courses on udemy we chose to consider the most reviewed and highest rated ones only there s always a chance that we missed something though so please let us know in the comments section if we left a good course out we compiled average ratings and number of reviews from class central and other review sites to calculate a weighted average rating for each course we read text reviews and used this feedback to supplement the numerical ratings we made subjective syllabus judgment calls based on three factors a popular definition originates from arthur samuel in machine learning is a subfield of computer science that gives computers the ability to learn without being explicitly programmed in practice this means developing computer programs that can make predictions based on data just as humans can learn from experience so can computers where data experience a machine learning workflow is the process required for carrying out a machine learning project though individual projects can differ most workflows share several common tasks problem evaluation data exploration data preprocessing model training testing deployment etc below you ll find helpful visualization of these core steps the ideal course introduces the entire process and provides interactive examples assignments and or quizzes where students can perform each task themselves first off let s define deep learning here is a succinct description as would be expected portions of some of the machine learning courses contain deep learning content i chose not to include deep learning only courses however if you are interested in deep learning specifically we ve got you covered with the following article my top three recommendations from that list would be several courses listed below ask students to have prior programming calculus linear algebra and statistics experience these prerequisites are understandable given that machine learning is an advanced discipline missing a few subjects good news some of this experience can be acquired through our recommendations in the first two articles programming statistics of this data science career guide several top ranked courses below also provide gentle calculus and linear algebra refreshers and highlight the aspects most relevant to machine learning for those less familiar stanford university s machine learning on coursera is the clear current winner in terms of ratings reviews and syllabus fit taught by the famous andrew ng google brain founder and former chief scientist at baidu this was the class that sparked the founding of coursera it has a star weighted average rating over reviews released in it covers all aspects of the machine learning workflow though it has a smaller scope than the original stanford class upon which it is based it still manages to cover a large number of techniques and algorithms the estimated timeline is eleven weeks with two weeks dedicated to neural networks and deep learning free and paid options are available ng is a dynamic yet gentle instructor with a palpable experience he inspires confidence especially when sharing practical implementation tips and warnings about common pitfalls a linear algebra refresher is provided and ng highlights the aspects of calculus most relevant to machine learning evaluation is automatic and is done via multiple choice quizzes that follow each lesson and programming assignments the assignments there are eight of them can be completed in matlab or octave which is an open source version of matlab ng explains his language choice though python and r are likely more compelling choices in with the increased popularity of those languages reviewers note that that shouldn t stop you from taking the course a few prominent reviewers noted the following columbia university s machine learning is a relatively new offering that is part of their artificial intelligence micromasters on edx though it is newer and doesn t have a large number of reviews the ones that it does have are exceptionally strong professor john paisley is noted as brilliant clear and clever it has a star weighted average rating over reviews the course also covers all aspects of the machine learning workflow and more algorithms than the above stanford offering columbia s is a more advanced introduction with reviewers noting that students should be comfortable with the recommended prerequisites calculus linear algebra statistics probability and coding quizzes programming assignments and a final exam are the modes of evaluation students can use either python octave or matlab to complete the assignments the course s total estimated timeline is eight to ten hours per week over twelve weeks it is free with a verified certificate available for purchase below are a few of the aforementioned sparkling reviews machine learning a ztm on udemy is an impressively detailed offering that provides instruction in both python and r which is rare and can t be said for any of the other top courses it has a star weighted average rating over reviews which makes it the most reviewed course of the ones considered it covers the entire machine learning workflow and an almost ridiculous in a good way number of algorithms through hours of on demand video the course takes a more applied approach and is lighter math wise than the above two courses each section starts with an intuition video from eremenko that summarizes the underlying theory of the concept being taught de ponteves then walks through implementation with separate videos for both python and r as a bonus the course includes python and r code templates for students to download and use on their own projects there are quizzes and homework challenges though these aren t the strong points of the course eremenko and the superdatascience team are revered for their ability to make the complex simple also the prerequisites listed are just some high school mathematics so this course might be a better option for those daunted by the stanford and columbia offerings a few prominent reviewers noted the following our pick had a weighted average rating of out of stars over reviews let s look at the other alternatives sorted by descending rating a reminder that deep learning only courses are not included in this guide you can find those here the analytics edge massachusetts institute of technology edx more focused on analytics in general though it does cover several machine learning topics uses r strong narrative that leverages familiar real world examples challenging ten to fifteen hours per week over twelve weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews python for data science and machine learning bootcamp jose portilla udemy has large chunks of machine learning content but covers the whole data science process more of a very detailed intro to python amazing course though not ideal for the scope of this guide hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews data science and machine learning bootcamp with r jose portilla udemy the comments for portilla s above course apply here as well except for r hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews machine learning series lazy programmer inc udemy taught by a data scientist big data engineer full stack software engineer with an impressive resume lazy programmer currently has a series of machine learning focused courses on udemy in total the courses have ratings and almost all of them have stars a useful course ordering is provided in each individual course s description uses python cost varies depending on udemy discounts which are frequent machine learning georgia tech udacity a compilation of what was three separate courses supervised unsupervised and reinforcement learning part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree oms bite sized videos as is udacity s style friendly professors estimated timeline of four months free it has a star weighted average rating over reviews implementing predictive analytics with spark in azure hdinsight microsoft edx introduces the core concepts of machine learning and a variety of algorithms leverages several big data friendly tools including apache spark scala and hadoop uses both python and r four hours per week over six weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews data science and machine learning with python hands on frank kane udemy uses python kane has nine years of experience at amazon and imdb nine hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews scala and spark for big data and machine learning jose portilla udemy big data focus specifically on implementation in scala and spark ten hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews machine learning engineer nanodegree udacity udacity s flagship machine learning program which features a best in class project review system and career support the program is a compilation of several individual udacity courses which are free co created by kaggle estimated timeline of six months currently costs usd per month with a tuition refund available for those who graduate within months it has a star weighted average rating over reviews learning from data introductory machine learning california institute of technology edx enrollment is currently closed on edx but is also available via caltech s independent platform see below it has a star weighted average rating over reviews learning from data introductory machine learning yaser abu mostafa california institute of technology a real caltech course not a watered down version reviews note it is excellent for understanding machine learning theory the professor yaser abu mostafa is popular among students and also wrote the textbook upon which this course is based videos are taped lectures with lectures slides picture in picture uploaded to youtube homework assignments are pdf files the course experience for online students isn t as polished as the top three recommendations it has a star weighted average rating over reviews mining massive datasets stanford university machine learning with a focus on big data introduces modern distributed file systems and mapreduce ten hours per week over seven weeks free it has a star weighted average rating over reviews aws machine learning a complete guide with python chandra lingam udemy a unique focus on cloud based machine learning and specifically amazon web services uses python nine hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews introduction to machine learning face detection in python holczer balazs udemy uses python eight hours of on demand video cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews statlearning statistical learning stanford university based on the excellent textbook an introduction to statistical learning with applications in r and taught by the professors who wrote it reviewers note that the mooc isn t as good as the book citing thin exercises and mediocre videos five hours per week over nine weeks free it has a star weighted average rating over reviews machine learning specialization university of washington coursera great courses but last two classes including the capstone project were canceled reviewers note that this series is more digestable read easier for those without strong technical backgrounds than other top machine learning courses e g stanford s or caltech s be aware that the series is incomplete with recommender systems deep learning and a summary missing free and paid options available it has a star weighted average rating over reviews from to machine learning nlp python cut to the chase loony corn udemy a down to earth shy but confident take on machine learning techniques taught by four person team with decades of industry experience together uses python cost varies depending on udemy discounts which are frequent it has a star weighted average rating over reviews principles of machine learning microsoft edx uses r python and microsoft azure machine learning part of the microsoft professional program certificate in data science three to four hours per week over six weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews big data statistical inference and machine learning queensland university of technology futurelearn a nice brief exploratory machine learning course with a focus on big data covers a few tools like r h o flow and weka only three weeks in duration at a recommended two hours per week but one reviewer noted that six hours per week would be more appropriate free and paid options available it has a star weighted average rating over reviews genomic data science and clustering bioinformatics v university of california san diego coursera for those interested in the intersection of computer science and biology and how it represents an important frontier in modern science focuses on clustering and dimensionality reduction part of ucsd s bioinformatics specialization free and paid options available it has a star weighted average rating over reviews intro to machine learning udacity prioritizes topic breadth and practical tools in python over depth and theory the instructors sebastian thrun and katie malone make this class so fun consists of bite sized videos and quizzes followed by a mini project for each lesson currently part of udacity s data analyst nanodegree estimated timeline of ten weeks free it has a star weighted average rating over reviews machine learning for data analysis wesleyan university coursera a brief intro machine learning and a few select algorithms covers decision trees random forests lasso regression and k means clustering part of wesleyan s data analysis and interpretation specialization estimated timeline of four weeks free and paid options available it has a star weighted average rating over reviews programming with python for data science microsoft edx produced by microsoft in partnership with coding dojo uses python eight hours per week over six weeks free and paid options available it has a star weighted average rating over reviews machine learning for trading georgia tech udacity focuses on applying probabilistic machine learning approaches to trading decisions uses python part of udacity s machine learning engineer nanodegree and georgia tech s online master s degree oms estimated timeline of four months free it has a star weighted average rating over reviews practical machine learning johns hopkins university coursera a brief practical introduction to a number of machine learning algorithms several one two star reviews expressing a variety of concerns part of jhu s data science specialization four to nine hours per week over four weeks free and paid options available it has a star weighted average rating over reviews machine learning for data science and analytics columbia university edx introduces a wide range of machine learning topics some passionate negative reviews with concerns including content choices a lack of programming assignments and uninspiring presentation seven to ten hours per week over five weeks free with a verified certificate available for purchase it has a star weighted average rating over reviews recommender systems specialization university of minnesota coursera strong focus one specific type of machine learning recommender systems a four course specialization plus a capstone project which is a case study taught using lenskit an open source toolkit for recommender systems free and paid options available it has a star weighted average rating over reviews machine learning with big data university of california san diego coursera terrible reviews that highlight poor instruction and evaluation some noted it took them mere hours to complete the whole course part of ucsd s big data specialization free and paid options available it has a star weighted average rating over reviews practical predictive analytics models and methods university of washington coursera a brief intro to core machine learning concepts one reviewer noted that there was a lack of quizzes and that the assignments were not challenging part of uw s data science at scale specialization six to eight hours per week over four weeks free and paid options available it has a star weighted average rating over reviews the following courses had one or no reviews as of may machine learning for musicians and artists goldsmiths university of london kadenze unique students learn algorithms software tools and machine learning best practices to make sense of human gesture musical audio and other real time data seven sessions in length audit free and premium usd per month options available it has one star review applied machine learning in python university of michigan coursera taught using python and the scikit learn toolkit part of the applied data science with python specialization scheduled to start may th free and paid options available applied machine learning microsoft edx taught using various tools including python r and microsoft azure machine learning note microsoft produces the course includes hands on labs to reinforce the lecture content three to four hours per week over six weeks free with a verified certificate available for purchase machine learning with python big data university taught using python targeted towards beginners estimated completion time of four hours big data university is affiliated with ibm free machine learning with apache systemml big data university taught using apache systemml which is a declarative style language designed for large scale machine learning estimated completion time of eight hours big data university is affiliated with ibm free machine learning for data science university of california san diego edx doesn t launch until january programming examples and assignments are in python using jupyter notebooks eight hours per week over ten weeks free with a verified certificate available for purchase introduction to analytics modeling georgia tech edx the course advertises r as its primary programming tool five to ten hours per week over ten weeks free with a verified certificate available for purchase predictive analytics gaining insights from big data queensland university of technology futurelearn brief overview of a few algorithms uses hewlett packard enterprise s vertica analytics platform as an applied tool start date to be announced two hours per week over four weeks free with a certificate of achievement available for purchase introduccio n al machine learning universitas telefo nica miri ada x taught in spanish an introduction to machine learning that covers supervised and unsupervised learning a total of twenty estimated hours over four weeks machine learning path step dataquest taught in python using dataquest s interactive in browser platform multiple guided projects and a plus project where you build your own machine learning system using your own data subscription required the following six courses are offered by datacamp datacamp s hybrid teaching style leverages video and text based instruction with lots of examples through an in browser code editor a subscription is required for full access to each course introduction to machine learning datacamp covers classification regression and clustering algorithms uses r fifteen videos and exercises with an estimated timeline of six hours supervised learning with scikit learn datacamp uses python and scikit learn covers classification and regression algorithms seventeen videos and exercises with an estimated timeline of four hours unsupervised learning in r datacamp provides a basic introduction to clustering and dimensionality reduction in r sixteen videos and exercises with an estimated timeline of four hours machine learning toolbox datacamp teaches the big ideas in machine learning uses r videos and exercises with an estimated timeline of four hours machine learning with the experts school budgets datacamp a case study from a machine learning competition on drivendata involves building a model to automatically classify items in a school s budget datacamp s supervised learning with scikit learn is a prerequisite fifteen videos and exercises with an estimated timeline of four hours unsupervised learning in python datacamp covers a variety of unsupervised learning algorithms using python scikit learn and scipy the course ends with students building a recommender system to recommend popular musical artists thirteen videos and exercises with an estimated timeline of four hours machine learning tom mitchell carnegie mellon university carnegie mellon s graduate introductory machine learning course a prerequisite to their second graduate level course statistical machine learning taped university lectures with practice problems homework assignments and a midterm all with solutions posted online a version of the course also exists cmu is one of the best graduate schools for studying machine learning and has a whole department dedicated to ml free statistical machine learning larry wasserman carnegie mellon university likely the most advanced course in this guide a follow up to carnegie mellon s machine learning course taped university lectures with practice problems homework assignments and a midterm all with solutions posted online free undergraduate machine learning nando de freitas university of british columbia an undergraduate machine learning course lectures are filmed and put on youtube with the slides posted on the course website the course assignments are posted as well no solutions though de freitas is now a full time professor at the university of oxford and receives praise for his teaching abilities in various forums graduate version available see below machine learning nando de freitas university of british columbia a graduate machine learning course the comments in de freitas undergraduate course above apply here as well this is the fifth of a six piece series that covers the best online courses for launching yourself into the data science field we covered programming in the first article statistics and probability in the second article intros to data science in the third article and data visualization in the fourth the final piece will be a summary of those articles plus the best online courses for other key topics such as data wrangling databases and even software engineering if you re looking for a complete list of data science online courses you can find them on class central s data science and big data subject page if you enjoyed reading this check out some of class central s other pieces if you have suggestions for courses i missed let me know in the responses if you found this helpful click the so more people will see it here on medium this is a condensed version of my original article published on class central where i ve included detailed course syllabi from a quick cheer to a standing ovation clap to show how much you enjoyed this story curriculum lead projects datacamp i created my own data science master s program our community publishes stories worth reading on development design and data science
Vishal Maini,32000,10,https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12?source=tag_archive---------2----------------,A Beginner’s Guide to AI/ML 🤖👶 – Machine Learning for Humans – Medium,part why machine learning matters the big picture of artificial intelligence and machine learning past present and future part supervised learning learning with an answer key introducing linear regression loss functions overfitting and gradient descent part supervised learning ii two methods of classification logistic regression and svms part supervised learning iii non parametric learners k nearest neighbors decision trees random forests introducing cross validation hyperparameter tuning and ensemble models part unsupervised learning clustering k means hierarchical dimensionality reduction principal components analysis pca singular value decomposition svd part neural networks deep learning why where and how deep learning works drawing inspiration from the brain convolutional neural networks cnns recurrent neural networks rnns real world applications part reinforcement learning exploration and exploitation markov decision processes q learning policy learning and deep reinforcement learning the value learning problem appendix the best machine learning resources a curated list of resources for creating your machine learning curriculum this guide is intended to be accessible to anyone basic concepts in probability statistics programming linear algebra and calculus will be discussed but it isn t necessary to have prior knowledge of them to gain value from this series artificial intelligence will shape our future more powerfully than any other innovation this century anyone who does not understand it will soon find themselves feeling left behind waking up in a world full of technology that feels more and more like magic the rate of acceleration is already astounding after a couple of ai winters and periods of false hope over the past four decades rapid advances in data storage and computer processing power have dramatically changed the game in recent years in google trained a conversational agent ai that could not only convincingly interact with humans as a tech support helpdesk but also discuss morality express opinions and answer general facts based questions the same year deepmind developed an agent that surpassed human level performance at atari games receiving only the pixels and game score as inputs soon after in deepmind obsoleted their own achievement by releasing a new state of the art gameplay method called a c meanwhile alphago defeated one of the best human players at go an extraordinary achievement in a game dominated by humans for two decades after machines first conquered chess many masters could not fathom how it would be possible for a machine to grasp the full nuance and complexity of this ancient chinese war strategy game with its possible board positions there are only atoms in the universe in march openai created agents that invented their own language to cooperate and more effectively achieve their goal soon after facebook reportedly successfully training agents to negotiate and even lie just a few days ago as of this writing on august openai reached yet another incredible milestone by defeating the world s top professionals in v matches of the online multiplayer game dota much of our day to day technology is powered by artificial intelligence point your camera at the menu during your next trip to taiwan and the restaurant s selections will magically appear in english via the google translate app today ai is used to design evidence based treatment plans for cancer patients instantly analyze results from medical tests to escalate to the appropriate specialist immediately and conduct scientific research for drug discovery in everyday life it s increasingly commonplace to discover machines in roles traditionally occupied by humans really don t be surprised if a little housekeeping delivery bot shows up instead of a human next time you call the hotel desk to send up some toothpaste in this series we ll explore the core machine learning concepts behind these technologies by the end you should be able to describe how they work at a conceptual level and be equipped with the tools to start building similar applications yourself artificial intelligence is the study of agents that perceive the world around them form plans and make decisions to achieve their goals its foundations include mathematics logic philosophy probability linguistics neuroscience and decision theory many fields fall under the umbrella of ai such as computer vision robotics machine learning and natural language processing machine learning is a subfield of artificial intelligence its goal is to enable computers to learn on their own a machine s learning algorithm enables it to identify patterns in observed data build models that explain the world and predict things without having explicit pre programmed rules and models the technologies discussed above are examples of artificial narrow intelligence ani which can effectively perform a narrowly defined task meanwhile we re continuing to make foundational advances towards human level artificial general intelligence agi also known as strong ai the definition of an agi is an artificial intelligence that can successfully perform any intellectual task that a human being can including learning planning and decision making under uncertainty communicating in natural language making jokes manipulating people trading stocks or reprogramming itself and this last one is a big deal once we create an ai that can improve itself it will unlock a cycle of recursive self improvement that could lead to an intelligence explosion over some unknown time period ranging from many decades to a single day you may have heard this point referred to as the singularity the term is borrowed from the gravitational singularity that occurs at the center of a black hole an infinitely dense one dimensional point where the laws of physics as we understand them start to break down a recent report by the future of humanity institute surveyed a panel of ai researchers on timelines for agi and found that researchers believe there is a chance of ai outperforming humans in all tasks in years grace et al we ve personally spoken with a number of sane and reasonable ai practitioners who predict much longer timelines the upper limit being never and others whose timelines are alarmingly short as little as a few years the advent of greater than human level artificial superintelligence asi could be one of the best or worst things to happen to our species it carries with it the immense challenge of specifying what ais will want in a way that is friendly to humans while it s impossible to say what the future holds one thing is certain is a good time to start understanding how machines think to go beyond the abstractions of a philosopher in an armchair and intelligently shape our roadmaps and policies with respect to ai we must engage with the details of how machines see the world what they want their potential biases and failure modes their temperamental quirks just as we study psychology and neuroscience to understand how humans learn decide act and feel machine learning is at the core of our journey towards artificial general intelligence and in the meantime it will change every industry and have a massive impact on our day to day lives that s why we believe it s worth understanding machine learning at least at a conceptual level and we designed this series to be the best place to start you don t necessarily need to read the series cover to cover to get value out of it here are three suggestions on how to approach it depending on your interests and how much time you have vishal most recently led growth at upstart a lending platform that utilizes machine learning to price credit automate the borrowing process and acquire users he spends his time thinking about startups applied cognitive science moral philosophy and the ethics of artificial intelligence samer is a master s student in computer science and engineering at ucsd and co founder of conigo labs prior to grad school he founded tablescribe a business intelligence tool for smbs and spent two years advising fortune companies at mckinsey samer previously studied computer science and ethics politics and economics at yale most of this series was written during a day trip to the united kingdom in a frantic blur of trains planes cafes pubs and wherever else we could find a dry place to sit our aim was to solidify our own understanding of artificial intelligence machine learning and how the methods therein fit together and hopefully create something worth sharing in the process and now without further ado let s dive into machine learning with part supervised learning more from machine learning for humans a special thanks to jonathan eng edoardo conti grant schneider sunny kumar stephanie he tarun wadhwa and sachin maini series editor for their significant contributions and feedback from a quick cheer to a standing ovation clap to show how much you enjoyed this story research comms deepmindai previously upstart yale trueventurestec demystifying artificial intelligence machine learning discussions on safe and intentional application of ai for positive social impact
Tim Anglade,7000,23,https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3?source=tag_archive---------3----------------,"How HBO’s Silicon Valley built “Not Hotdog” with mobile TensorFlow, Keras & React Native",the hbo show silicon valley released a real ai app that identifies hotdogs and not hotdogs like the one shown on season s th episode the app is now available on android as well as ios to achieve this we designed a bespoke neural architecture that runs directly on your phone and trained it with tensorflow keras nvidia gpus while the use case is farcical the app is an approachable example of both deep learning and edge computing all ai work is powered by the user s device and images are processed without ever leaving their phone this provides users with a snappier experience no round trip to the cloud offline availability and better privacy this also allows us to run the app at a cost of even under the load of a million users providing significant savings compared to traditional cloud based ai approaches the app was developed in house by the show by a single developer running on a single laptop attached gpu using hand curated data in that respect it may provide a sense of what can be achieved today with a limited amount of time resources by non technical companies individual developers and hobbyists alike in that spirit this article attempts to give a detailed overview of steps involved to help others build their own apps if you haven t seen the show or tried the app you should the app lets you snap a picture and then tells you whether it thinks that image is of a hotdog or not it s a straightforward use case that pays homage to recent ai research and applications in particular imagenet while we ve probably dedicated more engineering resources to recognizing hotdogs than anyone else the app still fails in horrible and or subtle ways conversely it s also sometimes able to recognize hotdogs in complex situations according to engadget it s incredible i ve had more success identifying food with the app in minutes than i have had tagging and identifying songs with shazam in the past two years have you ever found yourself reading hacker news thinking they raised a m series a for that i could build it in one weekend this app probably feels a lot like that and the initial prototype was indeed built in a single weekend using google cloud platform s vision api and react native but the final app we ended up releasing on the app store required months of additional part time work to deliver meaningful improvements that would be difficult for an outsider to appreciate we spent weeks optimizing overall accuracy training time inference time iterating on our setup tooling so we could have a faster development iterations and spent a whole weekend optimizing the user experience around ios android permissions don t even get me started on that one all too often technical blog posts or academic papers skip over this part preferring to present the final chosen solution in the interest of helping others learn from our mistake choices we will present an abridged view of the approaches that didn t work for us before we describe the final architecture we ended up shipping in the next section we chose react native to build the prototype as it would give us an easy sandbox to experiment with and would help us quickly support many devices the experience ended up being a good one and we kept react native for the remainder of the project it didn t always make things easy and the design for the app was purposefully limited but in the end react native got the job done the other main component we used for the prototype google cloud s vision api was quickly abandoned there were main factors for these reasons we started experimenting with what s trendily called edge computing which for our purposes meant that after training our neural network on our laptop we would export it and embed it directly into our mobile app so that the neural network execution phase or inference would run directly inside the user s phone through a chance encounter with pete warden of the tensorflow team we had become aware of its ability to run tensorflow directly embedded on an ios device and started exploring that path after react native tensorflow became the second fixed part of our stack it only took a day of work to integrate tensorflow s objective c camera example in our react native shell it took slightly longer to use their transfer learning script which helps you retrain the inception architecture to deal with a more specific image problem inception is the name of a family of neural architectures built by google to deal with image recognition problems inception is available pre trained which means the training phase has been completed and the weights are set most often for image recognition networks they have been trained on imagenet a dataset containing over different types of objects hotdogs are one of them however much like google cloud s vision api imagenet training rewards breadth as much as depth here and out of the box accuracy on a single one of the categories can be lacking as such retraining also called transfer learning aims to take a full trained neural net and retrain it to perform better on the specific problem you d like to handle this usually involves some degree of forgetting either by excising entire layers from the stack or by slowly erasing the network s ability to distinguish a type of object e g chairs in favor of better accuracy at recognizing the one you care about i e hotdogs while the network inception in this case may have been trained on the m images contained in imagenet we were able to retrain it on a just a few thousand hotdog images to get drastically enhanced hotdog recognition the big advantage of transfer learning are you will get better results much faster and with less data than if you train from scratch a full training might take months on multiple gpus and require millions of images while retraining can conceivably be done in hours on a laptop with a couple thousand images one of the biggest challenges we encountered was understanding exactly what should count as a hotdog and what should not defining what a hotdog is ends up being surprisingly difficult do cut up sausages count and if so which kinds and subject to cultural interpretation similarly the open world nature of our problem meant we had to deal with an almost infinite number of inputs while certain computer vision problems have relatively limited inputs say x rays of bolts with or without a mechanical default we had to prepare the app to be fed selfies nature shots and any number of foods suffice to say this approach was promising and did lead to some improved results however it had to be abandoned for a couple of reasons first the nature of our problem meant a strong imbalance in training data there are many more examples of things that are not hotdogs than things that are hotdogs in practice this means that if you train your algorithm on hotdog images and non hotdog images and it recognizes of the former but of the latter it will still score accuracy by default this was not straightforward to solve out of the box using tensorflow s retrain tool and basically necessitated setting up a deep learning model from scratch import weights and train in a more controlled manner at this point we decided to bite the bullet and get something started with keras a deep learning library that provides nicer easier to use abstractions on top of tensorflow including pretty awesome training tools and a class weights option which is ideal to deal with this sort of dataset imbalance we were dealing with we used that opportunity to try other popular neural architectures like vgg but one problem remained none of them could comfortably fit on an iphone they consumed too much memory which led to app crashes and would sometime takes up to seconds to compute which was not ideal from a ux standpoint many things were attempted to mitigate that but in the end it these architectures were just too big to run efficiently on mobile to give you a context out of time this was roughly the mid way point of the project by that time the ui was done and very little of it was going to change but in hindsight the neural net was at best done we had a good sense of challenges a good dataset but lines of the final neural architecture had been written none of our neural code could reliably run on mobile and even our accuracy was going to improve drastically in the weeks to come the problem directly ahead of us was simple if inception and vgg were too big was there a simpler pre trained neural network we could retrain at the suggestion of the always excellent jeremy p howard where has that guy been all our life we explored xception enet and squeezenet we quickly settled on squeezenet due to its explicit positioning as a solution for embedded deep learning and the availability of a pre trained keras model on github yay open source so how big of a difference does this make an architecture like vgg uses about million parameters essentially the number of numbers necessary to model the neurons and values between them inception is already a massive improvement requiring only million parameters squeezenet in comparison only requires million this has two advantages there are tradeoffs of course during this phase we started experimenting with tuning the neural network architecture in particular we started using batch normalization and trying different activation functions after adding batch normalization and elu to squeezenet we were able to train neural network that achieve accuracy when training from scratch however they were relatively brittle meaning the same network would overfit in some cases or underfit in others when confronted to real life testing even adding more examples to the dataset and playing with data augmentation failed to deliver a network that met expectations so while this phase was promising and for the first time gave us a functioning app that could work entirely on an iphone in less than a second we eventually moved to our th final architecture our final architecture was spurred in large part by the publication on april of google s mobilenets paper promising a new neural architecture with inception like accuracy on simple problems like ours with only m or so parameters this meant it sat in an interesting sweet spot between a squeezenet that had maybe been overly simplistic for our purposes and the possibly overwrought elephant trying to squeeze in a tutu of using inception or vgg on mobile the paper introduced some capacity to tune the size complexity of network specifically to trade memory cpu consumption against accuracy which was very much top of mind for us at the time with less than a month to go before the app had to launch we endeavored to reproduce the paper s results this was entirely anticlimactic as within a day of the paper being published a keras implementation was already offered publicly on github by refik can malli a student at istanbul technical university whose work we had already benefitted from when we took inspiration from his excellent keras squeezenet implementation the depth openness of the deep learning community and the presence of talented minds like r c is what makes deep learning viable for applications today but they also make working in this field more thrilling than any tech trend we ve been involved with our final architecture ended up making significant departures from the mobilenets architecture or from convention in particular so how does this stack work exactly deep learning often gets a bad rap for being a black box and while it s true many components of it can be mysterious the networks we use often leak information about how some of their magic work we can look at the layers of this stack and how they activate on specific input images giving us a sense of each layer s ability to recognize sausage buns or other particularly salient hotdog features data quality was of the utmost importance a neural network can only be as good as the data that trained it and improving training set quality was probably one of the top things we spent time on during this project the key things we did to improve this were the final composition of our dataset was k images of which only k were hotdogs there are only so many hotdogs you can look at but there are many not hotdogs to look at the imbalance was dealt with by saying a keras class weight of in favor of hotdogs of the remaining k images most were of food with just k photos of non food items to help the network generalize a bit more and not get tricked into seeing a hotdog if presented with an image of a human in a red outfit our data augmentation rules were as follows these numbers were derived intuitively based on experiments and our understanding of the real life usage of our app as opposed to careful experimentation the final key to our data pipeline was using patrick rodriguez s multiprocess image data generator for keras while keras does have a built in multi threaded and multiprocess implementation we found patrick s library to be consistently faster in our experiments for reasons we did not have time to investigate this library cut our training time to a third of what it used to be the network was trained using a macbook pro and attached external gpu egpu specifically an nvidia gtx ti we d probably buy a ti if we were starting today we were able to train the network on batches of images at a time the network was trained for a total of epochs meaning we ran all k images through the network times this took about hours we trained the network in phases while learning rates were identified by running the linear experiment recommended by the clr paper they seem to intuitively make sense in that the max for each phase is within a factor of of the previous minimum which is aligned with the industry standard recommendation of halving your learning rate if your accuracy plateaus during training in the interest of time we performed some training runs on a paperspace p instance running ubuntu in those cases we were able to double the batch size and found that optimal learning rates for each phase were roughly double as well even having designed a relatively compact neural architecture and having trained it to handle situations it may find in a mobile context we had a lot of work left to make it run properly trying to run a top of the line neural net architecture out of the box can quickly burns hundreds megabytes of ram which few mobile devices can spare today beyond network optimizations it turns out the way you handle images or even load tensorflow itself can have a huge impact on how quickly your network runs how little ram it uses and how crash free the experience will be for your users this was maybe the most mysterious part of this project relatively little information can be found about it possibly due to the dearth of production deep learning applications running on mobile devices as of today however we must commend the tensorflow team and particularly pete warden andrew harp and chad whipkey for the existing documentation and their kindness in answering our inquiries instead of using tensorflow on ios we looked at using apple s built in deep learning libraries instead bnns mpscnn and later on coreml we would have designed the network in keras trained it with tensorflow exported all the weight values re implemented the network with bnns or mpscnn or imported it via coreml and loaded the parameters into that new implementation however the biggest obstacle was that these new apple libraries are only available on ios and we wanted to support older versions of ios as ios adoption and these frameworks continue to improve there may not be a case for using tensorflow on device in the near future if you think injecting javascript into your app on the fly is cool try injecting neural nets into your app the last production trick we used was to leverage codepush and apple s relatively permissive terms of service to live inject new versions of our neural networks after submission to the app store while this was mostly done to help us quickly deliver accuracy improvements to our users after release you could conceivably use this approach to drastically expand or alter the feature set of your app without going through an app store review again there are a lot of things that didn t work or we didn t have time to do and these are the ideas we d investigate in the future finally we d be remiss not to mention the obvious and important influence of user experience developer experience and built in biases in developing an ai app each probably deserve their own post or their own book but here are the very concrete impacts of these things in our experience ux user experience is arguably more critical at every stage of the development of an ai app than for a traditional application there are no deep learning algorithms that will give you perfect results right now but there are many situations where the right mix of deep learning ux will lead to results that are indistinguishable from perfect proper ux expectations are irreplaceable when it comes to setting developers on the right path to design their neural networks setting the proper expectations for users when they use the app and gracefully handling the inevitable ai failures building ai apps without a ux first mindset is like training a neural net without stochastic gradient descent you will end up stuck in the local minima of the uncanny valley on your way to building the perfect ai use case dx developer experience is extremely important as well because deep learning training time is the new horsing around while waiting for your program to compile we suggest you heavily favor dx first hence keras as it s always possible to optimize runtime for later runs manual gpu parallelization multi process data augmentation tensorflow pipeline even re implementing for caffe pytorch even projects with relatively obtuse apis documentation like tensorflow greatly improve dx by providing a highly tested highly used well maintained environment for training running neural networks for the same reason it s hard to beat both the cost as well as the flexibility of having your own local gpu for development being able to look at edit images locally edit code with your preferred tool without delays greatly improves the development quality speed of building ai projects most ai apps will hit more critical cultural biases than ours but as an example even our straightforward use case caught us flat footed with built in biases in our initial dataset that made the app unable to recognize french style hotdogs asian hotdogs and more oddities we did not have immediate personal experience with it s critical to remember that ai do not make better decisions than humans they are infected by the same human biases we fall prey to via the training sets humans provide thanks to mike judge alec berg clay tarver todd silverstein jonathan dotan lisa schomas amy solomon dorothy street rich toyon and all the writers of the show the app would simply not exist without them meaghan dana david jay and everyone at hbo scale venture partners gitlab rachel thomas and jeremy howard fast ai for all that they have taught me and for kindly reviewing a draft of this post check out their free online deep learning course it s awesome jp simard for his help on ios and finally the tensorflow team r machinelearning for their help inspiration and thanks to everyone who used shared the app it made staring at pictures of hotdogs for months on end totally worth it from a quick cheer to a standing ovation clap to show how much you enjoyed this story a i startups hbo s silicon valley get in touch timanglade gmail com
Sophia Ciocca,53000,9,https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe?source=tag_archive---------4----------------,How Does Spotify Know You So Well? – Member Feature Stories – Medium,member feature story a software engineer explains the science behind personalized music recommendations photo by studioeast getty images photo by studioeast getty images this monday just like every monday before it over million spotify users found a fresh new playlist waiting for them called discover weekly it s a custom mixtape of songs they ve never listened to before but will probably love and it s pretty much magic i m a huge fan of spotify and particularly discover weekly why it makes me feel seen it knows my musical tastes better than any person in my entire life ever has and i m consistently delighted by how satisfyingly just right it is every week with tracks i probably would never have found myself or known i would like for those of you who live under a soundproof rock let me introduce you to my virtual best friend as it turns out i m not alone in my obsession with discover weekly the user base goes crazy for it which has driven spotify to rethink its focus and invest more resources into algorithm based playlists ever since discover weekly debuted in i ve been dying to know how it works what s more i m a spotify fangirl so i sometimes like to pretend that i work there and research their products after three weeks of mad googling i feel like i ve finally gotten a glimpse behind the curtain so how does spotify do such an amazing job of choosing those songs for each person each week let s zoom out for a second to look at how other music services have tackled music recommendations and how spotify s doing it better back in the s songza kicked off the online music curation scene using manual curation to create playlists for users this meant that a team of music experts or other human curators would put together playlists that they just thought sounded good and then users would listen to those playlists later beats music would employ this same strategy manual curation worked alright but it was based on that specific curator s choices and therefore couldn t take into account each listener s individual music taste like songza pandora was also one of the original players in digital music curation it employed a slightly more advanced approach instead manually tagging attributes of songs this meant a group of people listened to music chose a bunch of descriptive words for each track and tagged the tracks accordingly then pandora s code could simply filter for certain tags to make playlists of similar sounding music around that same time a music intelligence agency from the mit media lab called the echo nest was born which took a radical cutting edge approach to personalized music the echo nest used algorithms to analyze the audio and textual content of music allowing it to perform music identification personalized recommendation playlist creation and analysis finally taking another approach is last fm which still exists today and uses a process called collaborative filtering to identify music its users might like but more on that in a moment so if that s how other music curation services have handled recommendations how does spotify s magic engine run how does it seem to nail individual users tastes so much more accurately than any of the other services spotify doesn t actually use a single revolutionary recommendation model instead they mix together some of the best strategies used by other services to create their own uniquely powerful discovery engine to create discover weekly there are three main types of recommendation models that spotify employs let s dive into how each of these recommendation models work first some background when people hear the words collaborative filtering they generally think of netflix as it was one of the first companies to use this method to power a recommendation model taking users star based movie ratings to inform its understanding of which movies to recommend to other similar users after netflix was successful the use of collaborative filtering spread quickly and is now often the starting point for anyone trying to make a recommendation model unlike netflix spotify doesn t have a star based system with which users rate their music instead spotify s data is implicit feedback specifically the stream counts of the tracks and additional streaming data such as whether a user saved the track to their own playlist or visited the artist s page after listening to a song but what is collaborative filtering truly and how does it work here s a high level rundown explained in a quick conversation what s going on here each of these individuals has track preferences the one on the left likes tracks p q r and s while the one on the right likes tracks q r s and t collaborative filtering then uses that data to say hmmm you both like three of the same tracks q r and s so you are probably similar users therefore you re each likely to enjoy other tracks that the other person has listened to that you haven t heard yet therefore it suggests that the one on the right check out track p the only track not mentioned but that his similar counterpart enjoyed and the one on the left check out track t for the same reasoning simple right but how does spotify actually use that concept in practice to calculate millions of users suggested tracks based on millions of other users preferences with matrix math done with python libraries in actuality this matrix you see here is gigantic each row represents one of spotify s million users if you use spotify you yourself are a row in this matrix and each column represents one of the million songs in spotify s database then the python library runs this long complicated matrix factorization formula when it finishes we end up with two types of vectors represented here by x and y x is a user vector representing one single user s taste and y is a song vector representing one single song s profile now we have million user vectors and million song vectors the actual content of these vectors is just a bunch of numbers that are essentially meaningless on their own but are hugely useful when compared to find out which users musical tastes are most similar to mine collaborative filtering compares my vector with all of the other users vectors ultimately spitting out which users are the closest matches the same goes for the y vector songs you can compare a single song s vector with all the others and find out which songs are most similar to the one in question collaborative filtering does a pretty good job but spotify knew they could do even better by adding another engine enter nlp the second type of recommendation models that spotify employs are natural language processing nlp models the source data for these models as the name suggests are regular ol words track metadata news articles blogs and other text around the internet natural language processing which is the ability of a computer to understand human speech as it is spoken is a vast field unto itself often harnessed through sentiment analysis apis the exact mechanisms behind nlp are beyond the scope of this article but here s what happens on a very high level spotify crawls the web constantly looking for blog posts and other written text about music to figure out what people are saying about specific artists and songs which adjectives and what particular language is frequently used in reference to those artists and songs and which other artists and songs are also being discussed alongside them while i don t know the specifics of how spotify chooses to then process this scraped data i can offer some insight based on how the echo nest used to work with them they would bucket spotify s data up into what they call cultural vectors or top terms each artist and song had thousands of top terms that changed on the daily each term had an associated weight which correlated to its relative importance roughly the probability that someone will describe the music or artist with that term then much like in collaborative filtering the nlp model uses these terms and weights to create a vector representation of the song that can be used to determine if two pieces of music are similar cool right first a question you might be thinking first of all adding a third model further improves the accuracy of the music recommendation service but this model also serves a secondary purpose unlike the first two types raw audio models take new songs into account take for example a song your singer songwriter friend has put up on spotify maybe it only has listens so there are few other listeners to collaboratively filter it against it also isn t mentioned anywhere on the internet yet so nlp models won t pick it up luckily raw audio models don t discriminate between new tracks and popular tracks so with their help your friend s song could end up in a discover weekly playlist alongside popular songs but how can we analyze raw audio data which seems so abstract with convolutional neural networks convolutional neural networks are the same technology used in facial recognition software in spotify s case they ve been modified for use on audio data instead of pixels here s an example of a neural network architecture this particular neural network has four convolutional layers seen as the thick bars on the left and three dense layers seen as the more narrow bars on the right the inputs are time frequency representations of audio frames which are then concatenated or linked together to form the spectrogram the audio frames go through these convolutional layers and after passing through the last one you can see a global temporal pooling layer which pools across the entire time axis effectively computing statistics of the learned features across the time of the song after processing the neural network spits out an understanding of the song including characteristics like estimated time signature key mode tempo and loudness below is a plot of data for a second snippet of around the world by daft punk ultimately this reading of the song s key characteristics allows spotify to understand fundamental similarities between songs and therefore which users might enjoy them based on their own listening history that covers the basics of the three major types of recommendation models feeding spotify s recommendations pipeline and ultimately powering the discover weekly playlist of course these recommendation models are all connected to spotify s larger ecosystem which includes giant amounts of data storage and uses lots of hadoop clusters to scale recommendations and make these engines work on enormous matrices endless online music articles and huge numbers of audio files i hope this was informative and piqued your curiosity like it did mine for now i ll be working my way through my own discover weekly finding my new favorite music while appreciating all the machine learning that s going on behind the scenes thanks also to ladycollective for reading this article and suggesting edits software engineer writer and generally creative human interested in art feminism mindfulness and authenticity http sophiaciocca com welcome to a place where words matter on medium smart voices and original ideas take center stage with no ads in sight watch follow all the topics you care about and we ll deliver the best stories for you to your homepage and inbox explore get unlimited access to the best stories on medium and support writers while you re at it just month upgrade
François Chollet,35000,18,https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec?source=tag_archive---------5----------------,The impossibility of intelligence explosion – François Chollet – Medium,in i j good described for the first time the notion of intelligence explosion as it relates to artificial intelligence ai decades later the concept of an intelligence explosion leading to the sudden rise of superintelligence and the accidental end of the human race has taken hold in the ai community famous business leaders are casting it as a major risk greater than nuclear war or climate change average graduate students in machine learning are endorsing it in a email survey targeting ai researchers of respondents answered that intelligence explosion was likely or highly likely a further considered it a serious possibility the basic premise is that in the near future a first seed ai will be created with general problem solving abilities slightly surpassing that of humans this seed ai would start designing better ais initiating a recursive self improvement loop that would immediately leave human intelligence in the dust overtaking it by orders of magnitude in a short time proponents of this theory also regard intelligence as a kind of superpower conferring its holders with almost supernatural capabilities to shape their environment as seen in the science fiction movie transcendence for instance superintelligence would thus imply near omnipotence and would pose an existential threat to humanity this science fiction narrative contributes to the dangerously misleading public debate that is ongoing about the risks of ai and the need for ai regulation in this post i argue that intelligence explosion is impossible that the notion of intelligence explosion comes from a profound misunderstanding of both the nature of intelligence and the behavior of recursively self augmenting systems i attempt to base my points on concrete observations about intelligent systems and recursive systems the reasoning behind intelligence explosion like many of the early theories about ai that arose in the s and s is sophistic it considers intelligence in a completely abstract way disconnected from its context and ignores available evidence about both intelligent systems and recursively self improving systems it doesn t have to be that way we are after all on a planet that is literally packed with intelligent systems including us and self improving systems so we can simply observe them and learn from them to answer the questions at hand instead of coming up with evidence free circular reasonings to talk about intelligence and its possible self improving properties we should first introduce necessary background and context what are we talking about when we talk about intelligence precisely defining intelligence is in itself a challenge the intelligence explosion narrative equates intelligence with the general problem solving ability displayed by individual intelligent agents by current human brains or future electronic brains this is not quite the full picture so let s use this definition as a starting point and expand on it the first issue i see with the intelligence explosion theory is a failure to recognize that intelligence is necessarily part of a broader system a vision of intelligence as a brain in jar that can be made arbitrarily intelligent independently of its situation a brain is just a piece of biological tissue there is nothing intrinsically intelligent about it beyond your brain your body and senses your sensorimotor affordances are a fundamental part of your mind your environment is a fundamental part of your mind human culture is a fundamental part of your mind these are after all where all of your thoughts come from you cannot dissociate intelligence from the context in which it expresses itself in particular there is no such thing as general intelligence on an abstract level we know this for a fact via the no free lunch theorem stating that no problem solving algorithm can outperform random chance across all possible problems if intelligence is a problem solving algorithm then it can only be understood with respect to a specific problem in a more concrete way we can observe this empirically in that all intelligent systems we know are highly specialized the intelligence of the ais we build today is hyper specialized in extremely narrow tasks like playing go or classifying images into known categories the intelligence of an octopus is specialized in the problem of being an octopus the intelligence of a human is specialized in the problem of being human what would happen if we were to put a freshly created human brain in the body of an octopus and let in live at the bottom of the ocean would it even learn to use its eight legged body would it survive past a few days we cannot perform this experiment but we do know that cognitive development in humans and animals is driven by hardcoded innate dynamics human babies are born with an advanced set of reflex behaviors and innate learning templates that drive their early sensorimotor development and that are fundamentally intertwined with the structure of the human sensorimotor space the brain has hardcoded conceptions of having a body with hands that can grab a mouth that can suck eyes mounted on a moving head that can be used to visually follow objects the vestibulo ocular reflex and these preconceptions are required for human intelligence to start taking control of the human body it has even been convincingly argued for instance by chomsky that very high level human cognitive features such as our ability to develop language are innate similarly one can imagine that the octopus has its own set of hardcoded cognitive primitives required in order to learn how to use an octopus body and survive in its octopus environment the brain of a human is hyper specialized in the human condition an innate specialization extending possibly as far as social behaviors language and common sense and the brain of an octopus would likewise be hyper specialized in octopus behaviors a human baby brain properly grafted in an octopus body would most likely fail to adequately take control of its unique sensorimotor space and would quickly die off not so smart now mr superior brain what would happen if we were to put a human brain and body into an environment that does not feature human culture as we know it would mowgli the man cub raised by a pack of wolves grow up to outsmart his canine siblings to be smart like us and if we swapped baby mowgli with baby einstein would he eventually educate himself into developing grand theories of the universe empirical evidence is relatively scarce but from what we know children that grow up outside of the nurturing environment of human culture don t develop any human intelligence feral children raised in the wild from their earliest years become effectively animals and can no longer acquire human behaviors or language when returning to civilization saturday mthiyane raised by monkeys in south africa and found at five kept behaving like a monkey into adulthood jumping and walking on all four incapable of language and refusing to eat cooked food feral children who have human contact for at least some of their most formative years tend to have slightly better luck with reeducation although they rarely graduate to fully functioning humans if intelligence is fundamentally linked to specific sensorimotor modalities a specific environment a specific upbringing and a specific problem to solve then you cannot hope to arbitrarily increase the intelligence of an agent merely by tuning its brain no more than you can increase the throughput of a factory line by speeding up the conveyor belt intelligence expansion can only come from a co evolution of the mind its sensorimotor modalities and its environment if the gears of your brain were the defining factor of your problem solving ability then those rare humans with iqs far outside the normal range of human intelligence would live lives far outside the scope of normal lives would solve problems previously thought unsolvable and would take over the world just as some people fear smarter than human ai will do in practice geniuses with exceptional cognitive abilities usually live overwhelmingly banal lives and very few of them accomplish anything of note in terman s landmark genetic studies of genius he notes that most of his exceptionally gifted subjects would pursue occupations as humble as those of policeman seaman typist and filing clerk there are currently about seven million people with iqs higher than better cognitive ability than of humanity and mostly these are not the people you read about in the news of the people who have actually attempted to take over the world hardly any seem to have had an exceptional intelligence anecdotally hitler was a high school dropout who failed to get into the vienna academy of art twice people who do end up making breakthroughs on hard problems do so through a combination of circumstances character education intelligence and they make their breakthroughs through incremental improvement over the work of their predecessors success expressed intelligence is sufficient ability meeting a great problem at the right time most of these remarkable problem solvers are not even that clever their skills seem to be specialized in a given field and they typically do not display greater than average abilities outside of their own domain some people achieve more because they were better team players or had more grit and work ethic or greater imagination some just happened to have lived in the right context to have the right conversation at the right time intelligence is fundamentally situational intelligence is not a superpower exceptional intelligence does not on its own confer you with proportionally exceptional power over your circumstances however it is a well documented fact that raw cognitive ability as measured by iq which may be debatable correlates with social attainment for slices of the spectrum that are close to the mean this was first evidenced in terman s study and later confirmed by others for instance an extensive metastudy by strenze found a visible if somewhat weak correlation between iq and socioeconomic success so a person with an iq of is statistically far more likely to succeed in navigating the problem of life than a person with an iq of although this is never guaranteed at the individual level but here s the thing this correlation breaks down after a certain point there is no evidence that a person with an iq of is in any way more likely to achieve a greater impact in their field than a person with an iq of in fact many of the most impactful scientists tend to have had iqs in the s or s feynman reported james watson co discoverer of dna which is exactly the same range as legions of mediocre scientists at the same time of the roughly humans alive today who have astounding iqs of or higher how many will solve any problem a tenth as significant as professor watson why would the real world utility of raw cognitive ability stall past a certain threshold this points to a very intuitive fact that high attainment requires sufficient cognitive ability but that the current bottleneck to problem solving to expressed intelligence is not latent cognitive ability itself the bottleneck is our circumstances our environment which determines how our intelligence manifests itself puts a hard limit on what we can do with our brains on how intelligent we can grow up to be on how effectively we can leverage the intelligence that we develop on what problems we can solve all evidence points to the fact that our current environment much like past environments over the previous years of human history and prehistory does not allow high intelligence individuals to fully develop and utilize their cognitive potential a high potential human years ago would have been raised in a low complexity environment likely speaking a single language with fewer than words would never have been taught to read or write would have been exposed to a limited amount of knowledge and to few cognitive challenges the situation is a bit better for most contemporary humans but there is no indication that our environmental opportunities currently outpace our cognitive potential a smart human raised in the jungle is but a hairless ape similarly an ai with a superhuman brain dropped into a human body in our modern world would likely not develop greater capabilities than a smart contemporary human if it could then exceptionally high iq humans would already be displaying proportionally exceptional levels of personal attainment they would achieve exceptional levels of control over their environment and solve major outstanding problems which they don t in practice it s not just that our bodies senses and environment determine how much intelligence our brains can develop crucially our biological brains are just a small part of our whole intelligence cognitive prosthetics surround us plugging into our brain and extending its problem solving capabilities your smartphone your laptop google search the cognitive tools your were gifted in school books other people mathematical notation programing the most fundamental of all cognitive prosthetics is of course language itself essentially an operating system for cognition without which we couldn t think very far these things are not merely knowledge to be fed to the brain and used by it they are literally external cognitive processes non biological ways to run threads of thought and problem solving algorithms across time space and importantly across individuality these cognitive prosthetics not our brains are where most of our cognitive abilities reside we are our tools an individual human is pretty much useless on its own again humans are just bipedal apes it s a collective accumulation of knowledge and external systems over thousands of years what we call civilization that has elevated us above our animal nature when a scientist makes a breakthrough the thought processes they are running in their brain are just a small part of the equation the researcher offloads large extents of the problem solving process to computers to other researchers to paper notes to mathematical notation etc and they are only able to succeed because they are standing on the shoulder of giants their own work is but one last subroutine in a problem solving process that spans decades and thousands of individuals their own individual cognitive work may not be much more significant to the whole process than the work of a single transistor on a chip an overwhelming amount of evidence points to this simple fact a single human brain on its own is not capable of designing a greater intelligence than itself this is a purely empirical statement out of billions of human brains that have come and gone none has done so clearly the intelligence of a single human over a single lifetime cannot design intelligence or else over billions of trials it would have already occurred however these billions of brains accumulating knowledge and developing external intelligent processes over thousand of years implement a system civilization which may eventually lead to artificial brains with greater intelligence than that of a single human it is civilization as a whole that will create superhuman ai not you nor me nor any individual a process involving countless humans over timescales we can barely comprehend a process involving far more externalized intelligence books computers mathematics science the internet than biological intelligence on an individual level we are but vectors of civilization building upon previous work and passing on our findings we are the momentary transistors on which the problem solving algorithm of civilization runs will the superhuman ais of the future developed collectively over centuries have the capability to develop ai greater than themselves no no more than any of us can answering yes would fly in the face of everything we know again remember that no human nor any intelligent entity that we know of has ever designed anything smarter than itself what we do is gradually collectively build external problem solving systems that are greater than ourselves however future ais much like humans and the other intelligent systems we ve produced so far will contribute to our civilization and our civilization in turn will use them to keep expanding the capabilities of the ais it produces ai in this sense is no different than computers or books or language itself it s a technology that empowers our civilization the advent of superhuman ai will thus be no more of a singularity than the advent of computers or books or language civilization will develop ai and just march on civilization will eventually transcend what we are now much like it has transcended what we were years ago it s a gradual process not a sudden shift the basic premise of intelligence explosion that a seed ai will arise with greater than human problem solving ability leading to a sudden recursive runaway intelligence improvement loop is false our problem solving abilities in particular our ability to design ai are already constantly improving because these abilities do not reside primarily in our biological brains but in our external collective tools the recursive loop has been in action for a long time and the rise of better brains will not qualitatively affect it no more than any previous intelligence enhancing technology our brains themselves were never a significant bottleneck in the ai design process in this case you may ask isn t civilization itself the runaway self improving brain is our civilizational intelligence exploding no crucially the civilization level intelligence improving loop has only resulted in measurably linear progress in our problem solving abilities over time not an explosion but why wouldn t recursively improving x mathematically result in x growing exponentially no in short because no complex real world system can be modeled as x t x t a a no system exists in a vacuum and especially not intelligence nor human civilization we don t have to speculate about whether an explosion would happen the moment an intelligent system starts optimizing its own intelligence as it happens most systems are recursively self improving we re surrounded with them so we know exactly how such systems behave in a variety of contexts and over a variety of timescales you are yourself a recursively self improving system educating yourself makes you smarter in turn allowing you to educate yourself more efficiently likewise human civilization is recursively self improving over a much longer timescale mechatronics is recursively self improving better manufacturing robots can manufacture better manufacturing robots military empires are recursively self expanding the larger your empire the greater your military means to expand it further personal investing is recursively self improving the more money you have the more money you can make examples abound consider for instance software writing software obviously empowers software writing first we programmed compilers that could perform automated programming then we used compilers to develop new languages implementing more powerful programming paradigms we used these languages to develop advanced developer tools debuggers ides linters bug predictors in the future software will even write itself and what is the end result of this recursively self improving process can you do x more with your the software on your computer than you could last year will you be able to do x more next year arguably the usefulness of software has been improving at a measurably linear pace while we have invested exponential efforts into producing it the number of software developers has been booming exponentially for decades and the number of transistors on which we are running our software has been exploding as well following moore s law yet our computers are only incrementally more useful to us than they were in or or but why primarily because the usefulness of software is fundamentally limited by the context of its application much like intelligence is both defined and limited by the context in which it expresses itself software is just one cog in a bigger process our economies our lives just like your brain is just one cog in a bigger process human culture this context puts a hard limit on the maximum potential usefulness of software much like our environment puts a hard limit on how intelligent any individual can be even if gifted with a superhuman brain beyond contextual hard limits even if one part of a system has the ability to recursively self improve other parts of the system will inevitably start acting as bottlenecks antagonistic processes will arise in response to recursive self improvement and squash it in software this would be resource consumption feature creep ux issues when it comes to personal investing your own rate of spending is one such antagonistic process the more money you have the more money you spend when it comes to intelligence inter system communication arises as a brake on any improvement of underlying modules a brain with smarter parts will have more trouble coordinating them a society with smarter individuals will need to invest far more in networking and communication etc it is perhaps not a coincidence that very high iq people are more likely to suffer from certain mental illnesses it is also perhaps not random happenstance that military empires of the past have ended up collapsing after surpassing a certain size exponential progress meet exponential friction one specific example that is worth paying attention to is that of scientific progress because it is conceptually very close to intelligence itself science as a problem solving system is very close to being a runaway superhuman ai science is of course a recursively self improving system because scientific progress results in the development of tools that empower science whether lab hardware e g quantum physics led to lasers which enabled a wealth of new quantum physics experiments conceptual tools e g a new theorem a new theory cognitive tools e g mathematical notation software tools communications protocols that enable scientists to better collaborate e g the internet yet modern scientific progress is measurably linear i wrote about this phenomenon at length in a essay titled the singularity is not coming we didn t make greater progress in physics over the period than we did over we did arguably about as well mathematics is not advancing significantly faster today than it did in medical science has been making linear progress on essentially all of its metrics for decades and this is despite us investing exponential efforts into science the headcount of researchers doubles roughly once every to years and these researchers are using exponentially faster computers to improve their productivity how comes what bottlenecks and adversarial counter reactions are slowing down recursive self improvement in science so many i can t even count them here are a few importantly every single one of them would also apply to recursively self improving ais in practice system bottlenecks diminishing returns and adversarial reactions end up squashing recursive self improvement in all of the recursive processes that surround us self improvement does indeed lead to progress but that progress tends to be linear or at best sigmoidal your first seed dollar invested will not typically lead to a wealth explosion instead a balance between investment returns and growing spending will usually lead to a roughly linear growth of your savings over time and that s for a system that is orders of magnitude simpler than a self improving mind likewise the first superhuman ai will just be another step on a visibly linear ladder of progress that we started climbing long ago the expansion of intelligence can only come from a co evolution of brains biological or digital sensorimotor affordances environment and culture not from merely tuning the gears of some brain in a jar in isolation such a co evolution has already been happening for eons and will continue as intelligence moves to an increasingly digital substrate no intelligence explosion will occur as this process advances at a roughly linear pace fchollet november marketing footnote my book deep learning with python has just been released if you have python skills and you want to understand what deep learning can and cannot do and how to use it to solve difficult real world problems this book was written for you from a quick cheer to a standing ovation clap to show how much you enjoyed this story
Max Pechyonkin,23000,8,https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b?source=tag_archive---------8----------------,Understanding Hinton’s Capsule Networks. Part I: Intuition.,part i intuition you are reading it now part ii how capsules workpart iii dynamic routing between capsulespart iv capsnet architecture quick announcement about our new publication ai we are getting the best writers together to talk about the theory practice and business of ai and machine learning follow it to stay up to date on the latest trends last week geoffrey hinton and his team published two papers that introduced a completely new type of neural network based on so called capsules in addition to that the team published an algorithm called dynamic routing between capsules that allows to train such a network for everyone in the deep learning community this is huge news and for several reasons first of all hinton is one of the founders of deep learning and an inventor of numerous models and algorithms that are widely used today secondly these papers introduce something completely new and this is very exciting because it will most likely stimulate additional wave of research and very cool applications in this post i will explain why this new architecture is so important as well as intuition behind it in the following posts i will dive into technical details however before talking about capsules we need to have a look at cnns which are the workhorse of today s deep learning cnns convolutional neural networks are awesome they are one of the reasons deep learning is so popular today they can do amazing things that people used to think computers would not be capable of doing for a long long time nonetheless they have their limits and they have fundamental drawbacks let us consider a very simple and non technical example imagine a face what are the components we have the face oval two eyes a nose and a mouth for a cnn a mere presence of these objects can be a very strong indicator to consider that there is a face in the image orientational and relative spatial relationships between these components are not very important to a cnn how do cnns work the main component of a cnn is a convolutional layer its job is to detect important features in the image pixels layers that are deeper closer to the input will learn to detect simple features such as edges and color gradients whereas higher layers will combine simple features into more complex features finally dense layers at the top of the network will combine very high level features and produce classification predictions an important thing to understand is that higher level features combine lower level features as a weighted sum activations of a preceding layer are multiplied by the following layer neuron s weights and added before being passed to activation nonlinearity nowhere in this setup there is pose translational and rotational relationship between simpler features that make up a higher level feature cnn approach to solve this issue is to use max pooling or successive convolutional layers that reduce spacial size of the data flowing through the network and therefore increase the field of view of higher layer s neurons thus allowing them to detect higher order features in a larger region of the input image max pooling is a crutch that made convolutional networks work surprisingly well achieving superhuman performance in many areas but do not be fooled by its performance while cnns work better than any model before them max pooling nonetheless is losing valuable information hinton himself stated that the fact that max pooling is working so well is a big mistake and a disaster of course you can do away with max pooling and still get good results with traditional cnns but they still do not solve the key problem in the example above a mere presence of eyes a mouth and a nose in a picture does not mean there is a face we also need to know how these objects are oriented relative to each other computer graphics deals with constructing a visual image from some internal hierarchical representation of geometric data note that the structure of this representation needs to take into account relative positions of objects that internal representation is stored in computer s memory as arrays of geometrical objects and matrices that represent relative positions and orientation of these objects then special software takes that representation and converts it into an image on the screen this is called rendering inspired by this idea hinton argues that brains in fact do the opposite of rendering he calls it inverse graphics from visual information received by eyes they deconstruct a hierarchical representation of the world around us and try to match it with already learned patterns and relationships stored in the brain this is how recognition happens and the key idea is that representation of objects in the brain does not depend on view angle so at this point the question is how do we model these hierarchical relationships inside of a neural network the answer comes from computer graphics in d graphics relationships between d objects can be represented by a so called pose which is in essence translation plus rotation hinton argues that in order to correctly do classification and object recognition it is important to preserve hierarchical pose relationships between object parts this is the key intuition that will allow you to understand why capsule theory is so important it incorporates relative relationships between objects and it is represented numerically as a d pose matrix when these relationships are built into internal representation of data it becomes very easy for a model to understand that the thing that it sees is just another view of something that it has seen before consider the image below you can easily recognize that this is the statue of liberty even though all the images show it from different angles this is because internal representation of the statue of liberty in your brain does not depend on the view angle you have probably never seen these exact pictures of it but you still immediately knew what it was for a cnn this task is really hard because it does not have this built in understanding of d space but for a capsnet it is much easier because these relationships are explicitly modeled the paper that uses this approach was able to cut error rate by as compared to the previous state of the art which is a huge improvement another benefit of the capsule approach is that it is capable of learning to achieve state of the art performance by only using a fraction of the data that a cnn would use hinton mentions this in his famous talk about what is wrongs with cnns in this sense the capsule theory is much closer to what the human brain does in practice in order to learn to tell digits apart the human brain needs to see only a couple of dozens of examples hundreds at most cnns on the other hand need tens of thousands of examples to achieve very good performance which seems like a brute force approach that is clearly inferior to what we do with our brains the idea is really simple there is no way no one has come up with it before and the truth is hinton has been thinking about this for decades the reason why there were no publications is simply because there was no technical way to make it work before one of the reasons is that computers were just not powerful enough in the pre gpu based era before around another reason is that there was no algorithm that allowed to implement and successfully learn a capsule network in the same fashion the idea of artificial neurons was around since s but it was not until mid s when backpropagation algorithm showed up and allowed to successfully train deep networks in the same fashion the idea of capsules itself is not that new and hinton has mentioned it before but there was no algorithm up until now to make it work this algorithm is called dynamic routing between capsules this algorithm allows capsules to communicate with each other and create representations similar to scene graphs in computer graphics capsules introduce a new building block that can be used in deep learning to better model hierarchical relationships inside of internal knowledge representation of a neural network intuition behind them is very simple and elegant hinton and his team proposed a way to train such a network made up of capsules and successfully trained it on a simple data set achieving state of the art performance this is very encouraging nonetheless there are challenges current implementations are much slower than other modern deep learning models time will show if capsule networks can be trained quickly and efficiently in addition we need to see if they work well on more difficult data sets and in different domains in any case the capsule network is a very interesting and already working model which will definitely get more developed over time and contribute to further expansion of deep learning application domain this concludes part one of the series on capsule networks in the part ii more technical part i will walk you through the capsnet s internal workings step by step you can follow me on twitter let s also connect on linkedin from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning the ai revolution is here navigate the ever changing industry with our thoughtfully written articles whether your a researcher engineer or entrepreneur
Slav Ivanov,3900,17,https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415?source=tag_archive---------9----------------,"The $1700 great Deep Learning box: Assembly, setup and benchmarks",updated april uses cuda cudnn and tensorflow after years of using a thin client in the form of increasingly thinner macbooks i had gotten used to it so when i got into deep learning dl i went straight for the brand new at the time amazon p cloud servers no upfront cost the ability to train many models simultaneously and the general coolness of having a machine learning model out there slowly teaching itself however as time passed the aws bills steadily grew larger even as i switched to x cheaper spot instances also i didn t find myself training more than one model at a time instead i d go to lunch workout etc while the model was training and come back later with a clear head to check on it but eventually the model complexity grew and took longer to train i d often forget what i did differently on the model that had just completed its day training nudged by the great experiences of the other folks on the fast ai forum i decided to settle down and to get a dedicated dl box at home the most important reason was saving time while prototyping models if they trained faster the feedback time would be shorter thus it would be easier for my brain to connect the dots between the assumptions i had for the model and its results then i wanted to save money i was using amazon web services aws which offered p instances with nvidia k gpus lately the aws bills were around month with a tendency to get larger also it is expensive to store large datasets like imagenet and lastly i haven t had a desktop for over years and wanted to see what has changed in the meantime spoiler alert mostly nothing what follows are my choices inner monologue and gotchas from choosing the components to benchmarking a sensible budget for me would be about years worth of my current compute spending at month for aws this put it at around for the whole thing you can check out all the components used the pc part picker site is also really helpful in detecting if some of the components don t play well together the gpu is the most crucial component in the box it will train these deep networks fast shortening the feedback cycle disclosure the following are affiliate links to help me pay for well more gpus the choice is between a few of nvidia s cards gtx gtx ti gtx gtx ti and finally the titan x the prices might fluctuate especially because some gpus are great for cryptocurrency mining wink wink on performance side gtx ti and titan x are similar roughly speaking the gtx is about faster than gtx and gtx ti is about faster than gtx the new gtx ti is very close in performance to gtx tim dettmers has a great article on picking a gpu for deep learning which he regularly updates as new cards come on the market here are the things to consider when picking a gpu considering all of this i picked the gtx ti mainly for the training speed boost i plan to add a second ti soonish even though the gpu is the mvp in deep learning the cpu still matters for example data preparation is usually done on the cpu the number of cores and threads per core is important if we want to parallelize all that data prep to stay on budget i picked a mid range cpu the intel i it s relatively cheap but good enough to not slow things down edit as a few people have pointed out probably the biggest gotcha that is unique to dl multi gpu is to pay attention to the pcie lanes supported by the cpu motherboard by andrej karpathy we want to have each gpu have pcie lanes so it eats data as fast as possible gb s for pcie this means that for two cards we need pcie lanes however the cpu i have picked has only lanes so gpus would run in x mode instead of x this might be a bottleneck leading to less than ideal utilization of the graphics cards thus a cpu with lines is recommended edit however tim dettmers points out that having lanes per card should only decrease performance by for two gpus so currently my recommendation is go with pcie lanes per video card unless it gets too expensive for you otherwise lanes should do as well a good solution with to have for a double gpu machine would be an intel xeon processor like the e v pcie lanes or if you want to splurge go for a higher end processor like the desktop i k memory ram it s nice to have a lot of memory if we are to be working with rather big datasets i got sticks of gb for a total of gb of ram and plan to buy another gb later following jeremy howard s advice i got a fast ssd disk to keep my os and current data on and then a slow spinning hdd for those huge datasets like imagenet ssd i remember when i got my first macbook air years ago how blown away was i by the ssd speed to my delight a new generation of ssd called nvme has made its way to market in the meantime a gb mydigitalssd nvme drive was a great deal this baby copies files at gigabytes per second hdd tb seagate while ssds have been getting fast hdd have been getting cheap to somebody who has used macbooks with gb disk for the last years having this much space feels almost obscene the one thing that i kept in mind when picking a motherboard was the ability to support two gtx ti both in the number of pci express lanes the minimum is x and the physical size of cards also make sure it s compatible with the chosen cpu an asus tuf z did it for me msi x a sli plus should work great if you got an intel xeon cpu rule of thumb power supply should provide enough juice for the cpu and the gpus plus watts extra the intel i processor uses w and the gpus ti need w each so i got a deepcool w gold psu currently unavailable evga gq is similar the gold here refers to the power efficiency i e how much of the power consumed is wasted as heat the case should be the same form factor as the motherboard also having enough leds to embarrass a burner is a bonus a friend recommended the thermaltake n case which i promptly got no leds sadly here is how much i spent on all the components your costs may vary gtx ti cpu ram ssd hdd motherboard psu case total adding tax and fees this nicely matches my preset budget of if you don t have much experience with hardware and fear you might break something a professional assembly might be the best option however this was a great learning opportunity that i couldn t pass even though i ve had my share of hardware related horror stories the first and important step is to read the installation manuals that came with each component especially important for me as i ve done this before once or twice and i have just the right amount of inexperience to mess things up this is done before installing the motherboard in the case next to the processor there is a lever that needs to be pulled up the processor is then placed on the base double check the orientation finally the lever comes down to fix the cpu in place but i had a quite the difficulty doing this once the cpu was in position the lever wouldn t go down i actually had a more hardware capable friend of mine video walk me through the process turns out the amount of force required to get the lever locked down was more than what i was comfortable with next is fixing the fan on top of the cpu the fan legs must be fully secured to the motherboard consider where the fan cable will go before installing the processor i had came with thermal paste if yours doesn t make sure to put some paste between the cpu and the cooling unit also replace the paste if you take off the fan i put the power supply unit psu in before the motherboard to get the power cables snugly placed in case back side pretty straight forward carefully place it and screw it in a magnetic screwdriver was really helpful then connect the power cables and the case buttons and leds just slide it in the m slot and screw it in piece of cake the memory proved quite hard to install requiring too much effort to properly lock in a few times i almost gave up thinking i must be doing it wrong eventually one of the sticks clicked in and the other one promptly followed at this point i turned the computer on to make sure it works to my relief it started right away finally the gpu slid in effortlessly pins of power later and it was running nb do not plug your monitor in the external card right away most probably it needs drivers to function see below finally it s complete now that we have the hardware in place only the soft part remains out with the screwdriver in with the keyboard note on dual booting if you plan to install windows because you know for benchmarks totally not for gaming it would be wise to do windows first and linux second i didn t and had to reinstall ubuntu because windows messed up the boot partition livewire has a detailed article on dual boot most dl frameworks are designed to work on linux first and eventually support other operating systems so i went for ubuntu my default linux distribution an old gb usb drive was laying around and worked great for the installation unetbootin osx or rufus windows can prepare the linux thumb drive the default options worked fine during the ubuntu install at the time of writing ubuntu was just released so i opted for the previous version whose quirks are much better documented online ubuntu server or desktop the server and desktop editions of ubuntu are almost identical with the notable exception of the visual interface called x not being installed with server i installed the desktop and disabled autostarting x so that the computer would boot it in terminal mode if needed one could launch the visual desktop later by typing startx let s get our install up to date from jeremy howard s excellent install gpu script to deep learn on our machine we need a stack of technologies to use our gpu download cuda from nvidia or just run the code below updated to specify version of cuda thanks to zhanwenchen for the tip if you need to add later versions of cuda click here after cuda has been installed the following code will add the cuda installation to the path variable now we can verify that cuda has been installed successfully by running this should have installed the display driver as well for me nvidia smi showed err as the device name so i installed the latest nvidia drivers as of may to fix it removing cuda nvidia drivers if at any point the drivers or cuda seem broken as they did for me multiple times it might be better to start over by running since version tensorflow supports cudnn so we install that to download cudnn one needs to register for a free developer account after downloading install with the following anaconda is a great package manager for python i ve moved to python so will be using the anaconda version the popular dl framework by google installation validate tensorfow install to make sure we have our stack running smoothly i like to run the tensorflow mnist example we should see the loss decreasing during training keras is a great high level neural networks framework an absolute pleasure to work with installation can t be easier too pytorch is a newcomer in the world of dl frameworks but its api is modeled on the successful torch which was written in lua pytorch feels new and exciting mostly great although some things are still to be implemented we install it by running jupyter is a web based ide for python which is ideal for data sciency tasks it s installed with anaconda so we just configure and test it now if we open http localhost we should see a jupyter screen run jupyter on boot rather than running the notebook every time the computer is restarted we can set it to autostart on boot we will use crontab to do this which we can edit by running crontab e then add the following after the last line in the crontab file i use my old trusty macbook air for development so i d like to be able to log into the dl box both from my home network also when on the run ssh key it s way more secure to use a ssh key to login instead of a password digital ocean has a great guide on how to setup this ssh tunnel if you want to access your jupyter notebook from another computer the recommended way is to use ssh tunneling instead of opening the notebook to the world and protecting with a password let s see how we can do this then to connect over ssh tunnel run the following script on the client to test this open a browser and try http localhost from the remote machine your jupyter notebook should appear setup out of network access finally to access the dl box from the outside world we need things setting up out of network access depends on the router network setup so i m not going into details now that we have everything running smoothly let s put it to the test we ll be comparing the newly built box to an aws p xlarge instance which is what i ve used so far for dl the tests are computer vision related meaning convolutional networks with a fully connected model thrown in we time training models on aws p instance gpu k aws p virtual cpu the gtx ti and intel i cpu andres hernandez points out that my comparison does not use tensorflow that is optimized for these cpus which would have helped the them perform better check his insightful comment for more details the hello world of computer vision the mnist database consists of handwritten digits we run the keras example on mnist which uses multilayer perceptron mlp the mlp means that we are using only fully connected layers not convolutions the model is trained for epochs on this dataset which achieves over accuracy out of the box we see that the gtx ti is times faster than the k on aws p in training the model this is rather surprising as these cards should have about the same performance i believe this is because of the virtualization or underclocking of the k on aws the cpus perform times slower than the gpus as we will see later it s a really good result for the processors this is due to the small model which fails to fully utilize the parallel processing power of the gpus interestingly the desktop intel i achieves x speedup over the virtual cpu on amazon a vgg net will be finetuned for the kaggle dogs vs cats competition in this competition we need to tell apart pictures of dogs and cats running the model on cpus for the same number of batches wasn t feasible therefore we finetune for batches epoch on the gpus and batches on the cpus the code used is on github the ti is times faster that the aws gpu k the difference in the cpus performance is about the same as the previous experiment i is x faster however it s absolutely impractical to use cpus for this task as the cpus were taking x more time on this large model that includes convolutional layers and a couple semi wide fully connected layers on top a gan generative adversarial network is a way to train a model to generate images gan achieves this by pitting two networks against each other a generator which learns to create better and better images and a discriminator that tries to tell which images are real and which are dreamt up by the generator the wasserstein gan is an improvement over the original gan we will use a pytorch implementation that is very similar to the one by the wgan author the models are trained for steps and the loss is all over the place which is often the case with gans cpus aren t considered the gtx ti finishes x faster than the aws p k which is in line with the previous results the final benchmark is on the original style transfer paper gatys et al implemented on tensorflow code available style transfer is a technique that combines the style of one image a painting for example and the content of another image check out my previous post for more details on how style transfer works the gtx ti outperforms the aws k by a factor of this time the cpus are times slower than graphics cards the slowdown is less than on the vgg finetuning task but more than on the mnist perceptron experiment the model uses mostly the earlier layers of the vgg network and i suspect this was too shallow to fully utilize the gpus the dl box is in the next room and a large model is training on it was it a wise investment time will tell but it is beautiful to watch the glowing leds in the dark and to hear its quiet hum as models are trying to squeeze out that extra accuracy percentage point from a quick cheer to a standing ovation clap to show how much you enjoyed this story entrepreneur hacker machine learning deep learning and other types of learning
Geoff Nesnow,14900,19,https://medium.com/@DonotInnovate/73-mind-blowing-implications-of-a-driverless-future-58d23d1f338d?source=tag_archive---------1----------------,73 Mind-Blowing Implications of a Driverless Future,i originally wrote and published a version of this article in september since then quite a bit has happened further cementing my view that these changes are coming and that the implications will be even more substantial i decided it was time to update this article with some additional ideas and a few changes as i write this uber just announced that it just ordered self driving volvos tesla just released an electric long haul tractor trailer with extraordinary technical specs range performance and self driving capabilities ups just preordered and tesla just announced what will probably be the quickest production car ever made perhaps the fastest it will go zero to sixty in about the time it takes you to read zero to sixty and of course it will be able to drive itself the future is quickly becoming now google just ordered thousands of chryslers for its self driving fleet that are already on the roads in az in september of uber had just rolled out its first self driving taxis in pittsburgh tesla and mercedes were rolling out limited self driving capabilities and cities around the world were negotiating with companies who want to bring self driving cars and trucks to their cities since then all of the major car companies have announced significant steps towards mostly or entirely electric vehicles more investments have been made in autonomous vehicles driverless trucks now seem to be leading rather than following in terms of the first large scale implementations and there ve been a few more incidents i e accidents i believe that the timeframe for significant adoption of this technology has shrunk in the past year as technology has gotten better faster and as the trucking industry has increased its level of interest and investment i believe that my daughter who is now just over years old will never have to learn to drive or own a car the impact of driverless vehicles will be profound and impact almost every part of our lives below are my updated thoughts about what a driverless future will be like some of these updates are from feedback to my original article thanks to those who contributed some are based on technology advances in the past year and others are just my own speculations what could happen when cars and trucks drive themselves people won t own their own cars transport will be delivered as a service from companies who own fleets of self driving vehicles there are so many technical economic safety advantages to the transportation as a service that this change may come much faster than most people expect owning a vehicle as an individual will become a novelty for collectors and maybe competitive racers software technology companies will own more of the world s economy as companies like uber google and amazon turn transportation into a pay as you go service software will indeed eat this world over time they ll own so much data about people patterns routes and obstacles that new entrants will have huge barriers to enter the market without government intervention or some sort of organized movement there will be a tremendous transfer of wealth to a very small number of people who own the software battery power manufacturing vehicle servicing and charging power generation maintenance infrastructure there will be massive consolidation of companies serving these markets as scale and efficiency will become even more valuable cars perhaps they ll be renamed with some sort of clever acronym will become like the routers that run the internet most consumers won t know or care who made them or who owns them vehicle designs will change radically vehicles won t need to withstand crashes in the same way all vehicles will be electric self driving software service providers all electric they may look different come in very different shapes and sizes maybe attach to each other in some situations there will likely be many significant innovations in materials used for vehicle construction for example tires and brakes will be re optimized with very different assumptions especially around variability of loads and much more controlled environments the bodies will likely be primarily made of composites like carbon fiber and fiberglass and d printed electric vehicles with no driver controls will require th or fewer the number of parts perhaps even th and thus will be quicker to produce and require much less labor there may even be designs with almost no moving parts other than wheels and motors obviously vehicles will mostly swap batteries rather than serve as the host of battery charging batteries will be charged in distributed and highly optimized centers likely owned by the same company as the vehicles or another national vendor there may be some entrepreneurial opportunity and a marketplace for battery charging and swapping but this industry will likely be consolidated quickly the batteries will be exchanged without human intervention likely in a carwash like drive thru vehicles being electric will be able to provide portable power for a variety of purposes which will also be sold as a service construction job sites why use generators disaster power failures events etc they may even temporarily or permanently replace power distribution networks i e power lines for remote locations imagine a distributed power generation network with autonomous vehicles providing last mile services to some locations driver s licenses will slowly go away as will the department of motor vehicles in most states other forms of id may emerge as people no longer carry driver s licenses this will probably correspond with the inevitable digitization of all personal identification via prints retina scans or other biometric scanning there won t be any parking lots or parking spaces on roads or in buildings garages will be repurposed maybe as mini loading docks for people and deliveries aesthetics of homes and commercial buildings will change as parking lots and spaces go away there will be a multi year boom in landscaping and basement and garage conversions as these spaces become available traffic policing will become redundant police transport will also likely change quite a bit unmanned police vehicles may become more common and police officers may use commercial transportation to move around routinely this may dramatically change the nature of policing with newfound resources from the lack of traffic policing and dramatically less time spent moving around there will be no more local mechanics car dealers consumer car washes auto parts stores or gas stations towns that have been built around major thoroughfares will change or fade the auto insurance industry as we know it will go away as will the significant investing power of the major players of this industry most car companies will go out of business as will most of their enormous supplier networks there will be many fewer net vehicles on the road maybe th perhaps even less that are also more durable made of fewer parts and much more commoditized traffic lights and signs will become obsolete vehicles may not even have headlights as infrared and radar take the place of the human light spectrum the relationship between pedestrians and bicycles and cars and trucks will likely change dramatically some will come in the form of cultural and behavioral changes as people travel in groups more regularly and walking or cycling becomes practical in places where it isn t today multi modal transportation will become a more integrated and normal part of our ways of moving around in other words we ll often take one type of vehicle to another especially when traveling longer distances with coordination and integration the elimination of parking and more deterministic patterns it will become ever more efficient to combine modes of transport the power grid will change power stations via alternative power sources will become more competitive and local consumers and small businesses with solar panels small scale tidal or wave power generators windmills and other local power generation will be able to sell kilowatthours to the companies who own the vehicles this will change net metering rules and possibly upset the overall power delivery model it might even be the beginning of truly distributed power creation and transport there will likely be a significant boom in innovation in power production and delivery models over time ownership of these services will probably be consolidated across a very small number of companies traditional petroleum products and other fossil fuels will become much less valuable as electric cars replace fuel powered vehicles and as alternative energy sources become more viable with portability of power transmission and conversion eat tons of power there are many geopolitical implications to this possible shift as implications of climate change become ever clearer and present these trends will likely accelerate petroleum will continue to be valuable for making plastics and other derived materials but will not be burned for energy at any scale many companies oil rich countries and investors have already begun accommodating for these changes entertainment funding will change as the auto industry s ad spending goes away think about how many ads you see or hear about cars car financing car insurance car accessories and car dealers there are likely to be many other structural and cultural changes that come from the dramatic changes to the transportation industry we ll stop saying shift into high gear and other driving related colloquialisms as the references will be lost on future generations the recent corporate tax rate reductions in the act to provide for reconciliation pursuant to titles ii and v of the concurrent resolution on the budget for fiscal year will accelerate investments in automation including self driving vehicles and other forms of transportation automation flush with new cash and incentives to invest capital soon many businesses will invest in technology and solutions that reduce their labor costs the car financing industry will go away as will the newly huge derivative market for packaged sub prime auto loans which will likely itself cause a version of the financial crisis as it blows up increases in unemployment increased student loan vehicle and other debt defaults could quickly spiral into a full depression the world that emerges on the other side will likely have even more dramatic income and wealth stratification as entry level jobs related to transportation and the entire supply chain of the existing transportation system go away the convergence of this with hyper automation in production and service delivery ai robotics low cost computing business consolidation etc may permanently change how societies are organized and how people spend their time there will be many new innovations in luggage and bags as people no longer keep stuff in cars and loading and unloading packages from vehicles becomes much more automated the traditional trunk size and shape will change trailers or other similar detachable devices will become much more commonplace to add storage space to vehicles many additional on demand services will become available as transportation for goods and services becomes more ubiquitous and cheaper imagine being able to design d print and put on an outfit as you travel to a party or the office if you re still going to an office consumers will have more money as transportation a major cost especially for lower income people and families gets much cheaper and ubiquitous though this may be offset by dramatic reductions in employment as technology changes many times faster than people s ability to adapt to new types of work demand for taxi and truck drivers will go down eventually to zero someone born today might not understand what a truck driver is or even understand why someone would do that job much like people born in the last years don t understand how someone could be employed as a switchboard operator the politics will get ugly as lobbyists for the auto and oil industries unsuccessfully try to stop the driverless car they ll get even uglier as the federal government deals with assuming huge pension obligations and other legacy costs associated with the auto industry my guess is that these pension obgligations won t ultimately be honored and certain communities will be devastated the same may be true of pollution clean up efforts around the factories and chemical plants that were once major components of the vehicle supply chain the new players in vehicle design and manufacturing will be a mix of companies like uber google and amazon and companies you don t yet know there will probably be or major players who control of the customer facing transportation market there may become api like access to these networks for smaller players much like app marketplaces for iphone and android however the majority of the revenue will flow to a few large players as it does today to apple and google for smartphones supply chains will be disrupted as shipping changes algorithms will allow trucks to be fuller excess latent capacity will be priced cheaper new middlemen and warehousing models will emerge as shipping gets cheaper faster and generally easier retail storefronts will continue to lose footing in the marketplace the role of malls and other shopping areas will continue to shift to be replaced by places people go for services not products there will be virtually no face to face purchases of physical goods amazon and or a few other large players will put fedex ups and usps out of business as their transportation network becomes orders of magnitude more cost efficient than existing models largely from a lack of legacy costs like pensions higher union labor costs and regulations especially usps that won t keep up with the pace of technology change d printing will also contribute to this as many day to day products are printed at home rather than purchased the same vehicles will often transport people and goods as algorithms optimize all routes and off peak utilization will allow for other very inexpensive delivery options in other words packages will be increasingly delivered at night add autonomous drone aircraft to this mix and there ll be very little reason to believe that traditional carriers fedex usps ups etc will survive at all roads will be much emptier and smaller over time as self driving cars need much less space between them a major cause of traffic today people will share vehicles more than today carpooling traffic flow will be better regulated and algorithmic timing i e leave at versus will optimize infrastructure utilization roads will also likely be smoother and turns optimally banked for passenger comfort high speed underground and above ground tunnels maybe integrating hyperloop technology or this novel magnetic track solution will become the high speed network for long haul travel short hop domestic air travel may be largely displaced by multi modal travel in autonomous vehicles this may be countered by the advent of lower cost more automated air travel this too may become part of integrated multi modal transportation roads will wear out much more slowly with fewer vehicle miles lighter vehicles with less safety requirements new road materials will be developed that drain better last longer and are more environmentally friendly these materials might even be power generating solar or reclamation from vehicle kinetic energy at the extreme they may even be replaced by radically different designs tunnels magnetic tracks other hyper optimized materials premium vehicle services will have more compartmentalized privacy more comfort good business features quiet wifi bluetooth for each passenger etc massage services and beds for sleeping they may also allow for meaningful in transit real and virtual meetings this will also likely include aromatherapy many versions of in vehicle entertainment systems and even virtual passengers to keep you company exhilaration and emotion will almost entirely leave transportation people won t brag about how nice fast comfortable their cars are speed will be measured by times between end points not acceleration handling or top speed cities will become much more dense as fewer roads and vehicles will be needed and transport will be cheaper and more available the walkable city will continue to be more desirable as walking and biking become easier and more commonplace when costs and timeframes of transit change so will the dynamics of who lives and works where people will know when they leave when they ll get where they re going there will be few excuses for being late we will be able to leave later and cram more into a day we ll also be able to better track kids spouses employees and so forth we ll be able to know exactly when someone will arrive and when someone needs to leave to be somewhere at a particular time there will be no more dui oui offenses restaurants and bars will sell more alcohol people will consume more as they no longer need to consider how to get home and will be able to consume inside vehicles we ll have less privacy as interior cameras and usage logs will track when and where we go and have gone exterior cameras will also probably record surroundings including people this may have a positive impact on crime but will open up many complex privacy issues and likely many lawsuits some people may find clever ways to game the system with physical and digital disguises and spoofing many lawyers will lose sources of revenue traffic offenses crash litigation will reduce dramatically litigation will more likely be big company versus big company or individuals against big companies not individuals against each other these will settle more quickly with less variability lobbyists will probably succeed in changing the rules of litigation to favor the bigger companies further reducing the legal revenue related to transportation forced arbitration and other similar clauses will become an explicit component of our contractual relationship with transportation providers some countries will nationalize parts of their self driving transportation networks which will result in lower costs fewer disruptions and less innovation cities towns and police forces will lose revenue from traffic tickets tolls likely replaced if not eliminated and fuel tax revenues drop precipitously these will probably be replaced by new taxes probably on vehicle miles these may become a major political hot button issue differentiating parties as there will probably be a range of regressive versus progressive tax models most likely this will be a highly regressive tax in the us as fuel taxes are today some employers and or government programs will begin partially or entirely subsidizing transportation for employees and or people who need the help the tax treatment of this perk will also be very political ambulance and other emergency vehicles will likely be used less and change in nature more people will take regular autonomous vehicles instead of ambulances ambulances will transport people faster same may be true of military vehicles there will be significant innovations in first response capabilities as dependencies on people become reduced over time and as distributed staging of capacity becomes more common airports will allow vehicles right into the terminals maybe even onto the tarmac as increased controls and security become possible terminal design may change dramatically as transportation to and from becomes normalized and integrated the entire nature of air travel may change as integrated multi modal transport gets more sophisticated hyper loops high speed rail automated aircraft and other forms of rapid travel will gain as traditional hub and spoke air travel on relatively large planes lose ground innovative app like marketplaces will open up for in transit purchases ranging from concierge services to food to exercise to merchandise to education to entertainment purchases vr will likely play a large role in this with integrated systems vr via headsets or screens or holograms will become standard fare for trips more than a few minutes in duration transportation will become more tightly integrated and packaged into many services dinner includes the ride hotel includes local transport etc this may even extend to apartments short term rentals like airbnb and other service providers local transport of nearly everything will become ubiquitous and cheap food everything in your local stores drones will likely be integrated into vehicle designs to deal with last few feet on pickup and delivery this will accelerate the demise of traditional retail stores and their local economic impact biking and walking will become easier safer and more common as roads get safer and less congested new pathways reclaimed from roads parking lots roadside parking come online and with cheap reliable transport available as a backup more people will participate in vehicle racing cars off road motorcycles to replace their emotional connection to driving virtual racing experiences may also grow in popularity as fewer people have the real experience of driving many many fewer people will be injured or killed on roads though we ll expect zero and be disproportionately upset when accidents do happen hacking and non malicious technical issues will replace traffic as the main cause of delays over time resilience will increase in the systems hacking of vehicles will be a serious issue new software and communications companies and technologies will emerge to address these issues we ll see the first vehicle hacking and its consequences highly distributed computing perhaps using some form of blockchain will likely become part of the solution as a counterbalance to systemic catastrophes such as many vehicles being affected simultaneously there will probably be a debate about whether and how law enforcement can control observe and restrict transportation many roads and bridges will be privatized as a small number of companies control most transport and make deals with municipalities over time government may entirely stop funding roads bridges and tunnels there will be a significant legislative push to privatize more and more of the transportation network much like internet traffic there will likely become tiers of prioritization and some notion of in network versus out of network travel and tolls for interconnection regulators will have a tough time keeping up with these changes most of this will be transparent to end users but will probably create enormous barriers to entry for transportation start ups and ultimately reduce options for consumers innovators will come along with many awesome uses for driveways and garages that no longer contain cars there will be a new network of clean safe pay to use restrooms and other services food drinks etc that become part of the value add of competing service providers mobility for seniors and people with disabilities will be greatly improved over time parents will have more options to move around their kids on their own premium secure end to end children s transport services will likely emerge this may change many family relationships and increase the accessibility of services to parents and children it may also further stratify the experiences of families with higher income and those with lower income person to person movement of goods will become cheaper and open up new markets think about borrowing a tool or buying something on craigslist latent capacity will make transporting goods very inexpensive this may also open up new opportunities for p p services at a smaller scale like preparing food or cleaning clothes people will be able to eat drink in transit like on a train or plane consume more information reading podcasts video etc this will open up time for other activities and perhaps increased productivity some people may have their own pods to get into which will then be picked up by an autonomous vehicle moved between vehicles automatically for logistic efficiencies these may come in varieties of luxury and quality the louis vuitton pod may replace the louis vuitton trunk as the mark of luxury travel there will be no more getaway vehicles or police vehicle chases vehicles will likely be filled to the brim with advertising of all sorts much of which you could probably act on in route though there will probably be ways to pay more to have an ad free experience this will include highly personalized en route advertising that is particularly relevant to who you are where you re going these innovations will make it to the developing world where congestion today is often remarkably bad and hugely costly pollution levels will come down dramatically even more people will move to the cities productivity levels will go up fortunes will be made as these changes happen some countries and cities will be transformed for the better some others will likely experience hyper privatization consolidation and monopoly like controls this may play out much like the roll out of cell services in these countries fast consolidated and inexpensive payment options will be greatly expanded with packaged deals like cell phones pre paid models pay as you go models being offered digital currency transacted automatically via phones devices will probably quickly replace traditional cash or credit card payments there will likely be some very clever innovations for movement of pets equipment luggage and other non people items autonomous vehicles in the medium future years may have radically different designs that support carrying significantly more payload some creative marketers will offer to partially or fully subsidize rides where customers deliver value by taking surveys by participating in virtual focus groups by promoting their brand via social media etc sensors of all sorts will be embedded in vehicles that will have secondary uses like improving weather forecasting crime detection and prevention finding fugitives infrastructure conditions such as potholes this data will be monetized likely by the companies who own the transportation services companies like google and facebook will add to their databases everything about customer movements and locations unlike gps chips that only tell them where someone is at the moment and where they ve been autonomous vehicle systems will know where you re going in real time and with whom autonomous vehicles will create some new jobs and opportunities for entrepreneurs however these will be off set many times by extraordinary job losses by nearly everyone in the transportation value chain today in the autonomous future a large number of jobs will go away this includes drivers which is in many states today the most common job mechanics gas station employees most of the people who make cars and car parts or support those who do due to huge consolidation of makers and supply chains and manufacturing automation the marketing supply chain for vehicles many people who work on and build roads bridges employees of vehicle insurance and financing companies and their partners suppliers toll booth operators most of whom have already been displaced many employees of restaurants that support travelers truck stops retail workers and all the people whose businesses support these different types of companies and workers there will be some hardcore hold outs who really like driving but over time they ll become a less statistically relevant voting group as younger people who ve never driven will outnumber them at first this may be a state regulated system where driving yourself may actually become illegal in some states in the next years while other states may continue to allow it for a long time some states will try unsuccessfully to block autonomous vehicles there will be lots of discussions about new types of economic systems from universal basic income to new variations of socialism to a more regulated capitalist system that will result from the enormous impacts of autonomous vehicles in the path to a truly driverless future there will be a number of key tipping points at the moment freight delivery may push autonomous vehicle use sooner than people transport large trucking companies may have the financial means and legislative influence to make rapid dramatic changes they are also better positioned to support hybrid approaches where only parts of their fleet or parts of the routes are automated autonomous vehicles will radically change the power centers of the world they will be the beginning of the end of burning hydrocarbons the powerful interests who control these industries today will fight viciously to stop this there may even be wars to slow down this process as oil prices start to plummet and demand dries up autonomous vehicles will continue to play a larger role in all aspects of war from surveillance to troop robot movement to logistics support to actual engagement drones will be complemented by additional on the ground in space in the water and under the water autonomous vehicles note my original article was inspired by a presentation by ryan chin ceo of optimus ridespeak at an mit event about autonomous vehicles he really got me thinking about how profound these advances could be to our lives i m sure some of my thoughts above came from him from a quick cheer to a standing ovation clap to show how much you enjoyed this story co founder mycityatpeace faculty hult biz producer couragetolisten naturally curious dot connector more at www geoffnesnow com
Blaise Aguera y Arcas,8700,15,https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477?source=tag_archive---------2----------------,Do algorithms reveal sexual orientation or just expose our stereotypes?,by blaise agu era y arcas alexander todorov and margaret mitchell a study claiming that artificial intelligence can infer sexual orientation from facial images caused a media uproar in the fall of the economist featured this work on the cover of their september th magazine on the other hand two major lgbtq organizations the human rights campaign and glaad immediately labeled it junk science michal kosinski who co authored the study with fellow researcher yilun wang initially expressed surprise calling the critiques knee jerk reactions however he then proceeded to make even bolder claims that such ai algorithms will soon be able to measure the intelligence political orientation and criminal inclinations of people from their facial images alone kosinski s controversial claims are nothing new last year two computer scientists from china posted a non peer reviewed paper online in which they argued that their ai algorithm correctly categorizes criminals with nearly accuracy from a government id photo alone technology startups had also begun to crop up claiming that they can profile people s character from their facial images these developments had prompted the three of us to collaborate earlier in the year on a medium essay physiognomy s new clothes to confront claims that ai face recognition reveals deep character traits we described how the junk science of physiognomy has roots going back into antiquity with practitioners in every era resurrecting beliefs based on prejudice using the new methodology of the age in the th century this included anthropology and psychology in the th genetics and statistical analysis and in the st artificial intelligence in late the paper motivating our physiognomy essay seemed well outside the mainstream in tech and academia but as in other areas of discourse what recently felt like a fringe position must now be addressed head on kosinski is a faculty member of stanford s graduate school of business and this new study has been accepted for publication in the respected journal of personality and social psychology much of the ensuing scrutiny has focused on ethics implicitly assuming that the science is valid we will focus on the science the authors trained and tested their sexual orientation detector using images from public profiles on a us dating website composite images of the lesbian gay and straight men and women in the sample reveal a great deal about the information available to the algorithm clearly there are differences between these four composite faces wang and kosinski assert that the key differences are in physiognomy meaning that a sexual orientation tends to go along with a characteristic facial structure however we can immediately see that some of these differences are more superficial for example the average straight woman appears to wear eyeshadow while the average lesbian does not glasses are clearly visible on the gay man and to a lesser extent on the lesbian while they seem absent in the heterosexual composites might it be the case that the algorithm s ability to detect orientation has little to do with facial structure but is due rather to patterns in grooming presentation and lifestyle we conducted a survey of americans using amazon s mechanical turk crowdsourcing platform to see if we could independently confirm these patterns asking yes no questions such as do you wear eyeshadow do you wear glasses and do you have a beard as well as questions about gender and sexual orientation the results show that lesbians indeed use eyeshadow much less than straight women do gay men and women do both wear glasses more and young opposite sex attracted men are considerably more likely to have prominent facial hair than their gay or same sex attracted peers breaking down the answers by the age of the respondent can provide a richer and clearer view of the data than any single statistic in the following figures we show the proportion of women who answer yes to do you ever use makeup top and do you wear eyeshadow bottom averaged over year age intervals the blue curves represent strictly opposite sex attracted women a nearly identical set to those who answered yes to are you heterosexual or straight the cyan curve represents women who answer yes to either or both of are you sexually attracted to women and are you romantically attracted to women and the red curve represents women who answer yes to are you homosexual gay or lesbian the shaded regions around each curve show confidence intervals the patterns revealed here are intuitive it won t be breaking news to most that straight women tend to wear more makeup and eyeshadow than same sex attracted and even more so lesbian identifying women on the other hand these curves also show us how often these stereotypes are violated that same sex attracted men of most ages wear glasses significantly more than exclusively opposite sex attracted men do might be a bit less obvious but this trend is equally clear a proponent of physiognomy might be tempted to guess that this is somehow related to differences in visual acuity between these populations of men however asking the question do you like how you look in glasses reveals that this is likely more of a stylistic choice same sex attracted women also report wearing glasses more as well as liking how they look in glasses more across a range of ages one can also see how opposite sex attracted women under the age of wear contact lenses significantly more than same sex attracted women despite reporting that they have a vision defect at roughly the same rate further illustrating how the difference is driven by an aesthetic preference similar analysis shows that young same sex attracted men are much less likely to have hairy faces than opposite sex attracted men serious facial hair in our plots is defined as answering yes to having a goatee beard or moustache but no to stubble overall opposite sex attracted men in our sample are more likely to have serious facial hair than same sex attracted men and for men under the age of who are overrepresented on dating websites this rises to wang and kosinski speculate in their paper that the faintness of the beard and moustache in their gay male composite might be connected with prenatal underexposure to androgens male hormones resulting in a feminizing effect hence sparser facial hair the fact that we see a cohort of same sex attracted men in their s who have just as much facial hair as opposite sex attracted men suggests a different story in which fashion trends and cultural norms play the dominant role in choices about facial hair among men not differing exposure to hormones early in development the authors of the paper additionally note that the heterosexual male composite appears to have darker skin than the other three composites our survey confirms that opposite sex attracted men consistently self report having a tan face yes to is your face tan slightly more often than same sex attracted men once again wang and kosinski reach for a hormonal explanation writing while the brightness of the facial image might be driven by many factors previous research found that testosterone stimulates melanocyte structure and function leading to a darker skin however a simpler answer is suggested by the responses to the question do you work outdoors overall opposite sex attracted men are more likely to work outdoors and among men under this rises to previous research has found that increased exposure to sunlight leads to darker skin none of these results prove that there is no physiological basis for sexual orientation in fact ample evidence shows us that orientation runs much deeper than a choice or a lifestyle in a critique aimed in part at fraudulent conversion therapy programs united states surgeon general david satcher wrote in a report sexual orientation is usually determined by adolescence if not earlier and there is no valid scientific evidence that sexual orientation can be changed it follows that if we dig deeply enough into human physiology and neuroscience we will eventually find reliable correlates and maybe even the origins of sexual orientation in our survey we also find some evidence of outwardly visible correlates of orientation that are not cultural perhaps most strikingly very tall women are overrepresented among lesbian identifying respondents however while this is interesting it s very far from a good predictor of women s sexual orientation makeup and eyeshadow do much better the way wang and kosinski measure the efficacy of their ai gaydar is equivalent to choosing a straight and a gay or lesbian face image both from data held out during the training process and asking how often the algorithm correctly guesses which is which performance would be no better than random chance for women guessing that the taller of the two is the lesbian achieves only accuracy barely above random chance this is because despite the statistically meaningful overrepresentation of tall women among the lesbian population the great majority of lesbians are not unusually tall by contrast the performance measures in the paper for gay men and for lesbian women seem impressive consider however that we can achieve comparable results with trivial models based only on a handful of yes no survey questions about presentation for example for pairs of women one of whom is lesbian the following not exactly superhuman algorithm is on average accurate if neither or both women wear eyeshadow flip a coin otherwise guess that the one who wears eyeshadow is straight and the other lesbian adding six more yes no questions about presentation do you ever use makeup do you have long hair do you have short hair do you ever use colored lipstick do you like how you look in glasses and do you work outdoors as additional signals raises the performance to given how many more details about presentation are available in a face image performance no longer seems so impressive several studies including a recent one in the journal of sex research have shown that human judges gaydar is no more reliable than a coin flip when the judgement is based on pictures taken under well controlled conditions head pose lighting glasses makeup etc it s better than chance if these variables are not controlled for because a person s presentation especially if that person is out involves social signaling we signal our orientation and many other kinds of status presumably in order to attract the kind of attention we want and to fit in with people like us wang and kosinski argue against this interpretation on the grounds that their algorithm works on facebook selfies of openly gay men as well as dating website selfies the issue however is not whether the images come from a dating website or facebook but whether they are self posted or taken under standardized conditions most people present themselves in ways that have been calibrated over many years of media consumption observing others looking in the mirror and gauging social reactions in one of the earliest gaydar studies using social media participants could categorize gay men with about accuracy but when the researchers used facebook images of gay and heterosexual men posted by their friends still far from a perfect control the accuracy dropped to if subtle biases in image quality expression and grooming can be picked up on by humans these biases can also be detected by an ai algorithm while wang and kosinski acknowledge grooming and style they believe that the chief differences between their composite images relate to face shape arguing that gay men s faces are more feminine narrower jaws longer noses larger foreheads while lesbian faces are more masculine larger jaws shorter noses smaller foreheads as with less facial hair on gay men and darker skin on straight men they suggest that the mechanism is gender atypical hormonal exposure during development this echoes a widely discredited th century model of homosexuality sexual inversion more likely heterosexual men tend to take selfies from slightly below which will have the apparent effect of enlarging the chin shortening the nose shrinking the forehead and attenuating the smile see our selfies below this view emphasizes dominance or perhaps more benignly an expectation that the viewer will be shorter on the other hand as a wedding photographer notes in her blog when you shoot from above your eyes look bigger which is generally attractive especially for women this may be a heteronormative assessment when a face is photographed from below the nostrils are prominent while higher shooting angles de emphasize and eventually conceal them altogether looking again at the composite images we can see that the heterosexual male face has more pronounced dark spots corresponding to the nostrils than the gay male while the opposite is true for the female faces this is consistent with a pattern of heterosexual men on average shooting from below heterosexual women from above as the wedding photographer suggests and gay men and lesbian women from directly in front a similar pattern is evident in the eyebrows shooting from above makes them look more v shaped but their apparent shape becomes flatter and eventually caret shaped as the camera is lowered shooting from below also makes the outer corners of the eyes appear lower in short the changes in the average positions of facial landmarks are consistent with what we would expect to see from differing selfie angles the ambiguity between shooting angle and the real physical sizes of facial features is hard to fully disentangle from a two dimensional image both for a human viewer and for an algorithm although the authors are using face recognition technology designed to try to cancel out all effects of head pose lighting grooming and other variables not intrinsic to the face we can confirm that this doesn t work perfectly that s why multiple distinct images of a person help when grouping photos by subject in google photos and why a person may initially appear in more than one group tom white a researcher at victoria university in new zealand has experimented with the same facial recognition engine kosinski and wang use vgg face and has found that its output varies systematically based on variables like smiling and head pose when he trains a classifier based on vgg face s output to distinguish a happy expression from a neutral one it gets the answer right of the time which is significant given that the heterosexual female composite has a much more pronounced smile changes in head pose might be even more reliably detectable for test images a classifier is able to pick out the ones facing to the right with accuracy in summary we have shown how the obvious differences between lesbian or gay and straight faces in selfies relate to grooming presentation and lifestyle that is differences in culture not in facial structure these differences include we ve demonstrated that just a handful of yes no questions about these variables can do nearly as good a job at guessing orientation as supposedly sophisticated facial recognition ai further the current generation of facial recognition remains sensitive to head pose and facial expression therefore at least at this point it s hard to credit the notion that this ai is in some way superhuman at outing us based on subtle but unalterable details of our facial structure this doesn t negate the privacy concerns the authors and various commentators have raised but it emphasizes that such concerns relate less to ai per se than to mass surveillance which is troubling regardless of the technologies used even when as in the days of the stasi in east germany these were nothing but paper files and audiotapes like computers or the internal combustion engine ai is a general purpose technology that can be used to automate a great many tasks including ones that should not be undertaken in the first place we are hopeful about the confluence of new powerful ai technologies with social science but not because we believe in reviving the th century research program of inferring people s inner character from their outer appearance rather we believe ai is an essential tool for understanding patterns in human culture and behavior it can expose stereotypes inherent in everyday language it can reveal uncomfortable truths as in google s work with the geena davis institute where our face gender classifier established that men are seen and heard nearly twice as often as women in hollywood movies yet female led films outperform others at the box office making social progress and holding ourselves to account is more difficult without such hard evidence even when it only confirms our suspicions two of us margaret mitchell and blaise agu era y arcas are research scientists specializing in machine learning and ai at google agu era y arcas leads a team that includes deep learning applied to face recognition and powers face grouping in google photos alex todorov is a professor in the psychology department at princeton where he directs the social perception lab he is the author of face value the irresistible influence of first impressions this wording is based on several large national surveys which we were able to use to sanity check our numbers about of respondents identified as homosexual gay or lesbian and as heterosexual about of all genders were exclusively same sex attracted of the men were either sexually or romantically same sex attracted and of the women just under of respondents were trans and about identified with both or neither of the pronouns she and he these numbers are broadly consistent with other surveys especially when considered as a function of age the mechanical turk population skews somewhat younger than the overall population of the us and consistent with other studies our data show that younger people are far more likely to identify non heteronormatively these are wider for same sex attracted and lesbian women because they are minority populations resulting in a larger sampling error the same holds for older people in our sample for the remainder of the plots we stick to opposite sex attracted and same sex attracted as the counts are higher and the error bars therefore smaller these categories are also somewhat less culturally freighted since they rely on questions about attraction rather than identity as with eyeshadow and makeup the effects are similar and often even larger when comparing heterosexual identifying with lesbian or gay identifying people although we didn t test this explicitly slightly different rates of laser correction surgery seem a likely cause of the small but growing disparity between opposite sex attracted and same sex attracted women who answer yes to the vision defect questions as they age this finding may prompt the further question why do more opposite sex attracted men work outdoors this is not addressed by any of our survey questions but hopefully the other evidence presented here will discourage an essentialist assumption such as straight men are just more outdoorsy without the evidence of a controlled study that can support the leap from correlation to cause such explanations are a form of logical fallacy sometimes called a just so story an unverifiable narrative explanation for a cultural practice of the lesbian identified women in the sample or were over six feet and or were over out of heterosexual women women who answered yes to are you heterosexual or straight only or were over six feet and or were over they note that these figures rise to for men and for women if images are considered these results are based on the simplest possible machine learning technique a linear classifier the classifier is trained on a randomly chosen of the data with the remaining of the data held out for testing over repetitions of this procedure the error is with the same number of repetitions and holdout basing the decision on height alone gives an error of and basing it on eyeshadow alone yields a longstanding body of work e g goffman s the presentation of self in everyday life and jones and pittman s toward a general theory of strategic self presentation delves more deeply into why we present ourselves the way we do both for instrumental reasons status power attraction and because our presentation informs and is informed by how we conceive of our social selves from a quick cheer to a standing ovation clap to show how much you enjoyed this story blaise aguera y arcas leads google s ai group in seattle he founded seadragon and was one of the creators of photosynth at microsoft
François Chollet,16800,17,https://medium.com/@francois.chollet/what-worries-me-about-ai-ed9df072b704?source=tag_archive---------6----------------,What worries me about AI – François Chollet – Medium,disclaimer these are my own personal views i do not speak for my employer if you quote this article please have the honesty to present these ideas as what they are personal speculative opinions to be judged on their own merits if you were around in the s and s you may remember the now extinct phenomenon of computerphobia i have personally witnessed it a few times as late as the early s as personal computers were introduced into our lives in our workplaces and homes quite a few people would react with anxiety fear or even aggressivity while some of us were fascinated by computers and awestruck by the potential they could glimpse in them most people didn t understand them they felt alien abstruse and in many ways threatening people feared getting replaced by technology most of us react to technological shifts with unease at best panic at worst maybe that is true of any change at all but remarkably most of what we worry about ends up never happening fast forward a few years and the computer haters have learned to live with them and to use them for their own benefit computers did not replace us and trigger mass unemployment and nowadays we couldn t imagine life without our laptops tablets and smartphones threatening change has become comfortable status quo but at the same time as our fears failed to materialize computers and the internet have enabled threats that almost no one was warning us about in the s and s ubiquitous mass surveillance hackers going after our infrastructure or our personal data psychological alienation on social media the loss of our patience and our ability to focus the political or religious radicalization of easily influenced minds online hostile foreign powers hijacking social networks to disrupt western democracies if most of our fears turn out to be irrational inversely most of the truly worrying developments that have happened in the past as a result of technological change stem from things that most people didn t worry about until it was already there a hundred years ago we couldn t really forecast that the transportation and manufacturing technologies we were developing would enable a new form of industrial warfare that would wipe out tens of millions in two world wars we didn t recognize early on that the invention of the radio would enable a new form of mass propaganda that would facilitate the rise of fascism in italy and germany the progress of theoretical physics in the s and s wasn t accompanied by anxious press articles about how these developments would soon enable thermonuclear weapons that would place the world forever under the threat of imminent annihilation and today even as alarms have been sounding for decades about the most dire problem of our times climate a large fraction of the american public still chooses to ignore it as a civilization we seem to be really bad at correctly identifying future threats and rightfully worrying about them just as we seem to be extremely prone to panic due to irrational fears today like many times in the past we are faced with a new wave of radical change cognitive automation which could be broadly summed up under the keyword ai and like many time in the past we are worried that this new set of technologies will harm us that ai will lead to mass unemployment or that ai will gain an agency of its own become superhuman and choose to destroy us but what if we re worrying about the wrong thing like we have almost every single time before what if the real danger of ai was far remote from the superintelligence and singularity narratives that many are panicking about today in this post i d like to raise awareness about what really worries me when it comes to ai the highly effective highly scalable manipulation of human behavior that ai enables and its malicious use by corporations and governments of course this is not the only tangible risk that arises from the development of cognitive technologies there are many others in particular issues related to the harmful biases of machine learning models other people are raising awareness of these problems far better than i could i chose to write about mass population manipulation specifically because i see this risk as pressing and direly under appreciated this risk is already a reality today and a number of long term technological trends are going to considerably amplify it over the next few decades as our lives become increasingly digitized social media companies get ever greater visibility into our lives and minds at the same time they gain increasing access to behavioral control vectors in particular via algorithmic newsfeeds which control our information consumption this casts human behavior as an optimization problem as an ai problem it becomes possible for social media companies to iteratively tune their control vectors in order to achieve specific behaviors just like a game ai would iterative refine its play strategy in order to beat a level driven by score feedback the only bottleneck to this process is the intelligence of the algorithm in the loop and as it happens the largest social network company is currently investing billions in fundamental ai research let me explain in detail in the past years our private and public lives have moved online we spend an ever greater fraction of each day staring at screens our world is moving to a state where most of what we do consists of digital information consumption modification or creation a side effect of this long term trend is that corporations and governments are now collecting staggering amounts of data about us in particular through social network services who we communicate with what we say what content we ve been consuming images movies music news what mood we are in at specific times ultimately almost everything we perceive and everything we do will end up recorded on some remote server this data in theory allows the entities that collect it to build extremely accurate psychological profiles of both individuals and groups your opinions and behavior can be cross correlated with that of thousands of similar people achieving an uncanny understanding of what makes you tick probably more predictive than what yourself could achieve through mere introspection for instance facebook likes enable algorithms to better assess your personality that your own friends could this data makes it possible to predict a few days in advance when you will start a new relationship and with whom and when you will end your current one or who is at risk of suicide or which side you will ultimately vote for in an election even while you re still feeling undecided and it s not just individual level profiling power large groups can be even more predictable as aggregating data points erases randomness and individual outliers passive data collection is not where it ends increasingly social network services are in control of what information we consume what see in our newsfeeds has become algorithmically curated opaque social media algorithms get to decide to an ever increasing extent which political articles we read which movie trailers we see who we keep in touch with whose feedback we receive on the opinions we express integrated over many years of exposure the algorithmic curation of the information we consume gives the algorithms in charge considerable power over our lives over who we are who we become if facebook gets to decide over the span of many years which news you will see real or fake whose political status updates you ll see and who will see yours then facebook is in effect in control of your worldview and your political beliefs facebook s business lies in influencing people that s what the service it sells to its customers advertisers including political advertisers as such facebook has built a fine tuned algorithmic engine that does just that this engine isn t merely capable of influencing your view of a brand or your next smart speaker purchase it can influence your mood tuning the content it feeds you in order to make you angry or happy at will it may even be able to swing elections in short social network companies can simultaneously measure everything about us and control the information we consume and that s an accelerating trend when you have access to both perception and action you re looking at an ai problem you can start establishing an optimization loop for human behavior in which you observe the current state of your targets and keep tuning what information you feed them until you start observing the opinions and behaviors you wanted to see a large subset of the field of ai in particular reinforcement learning is about developing algorithms to solve such optimization problems as efficiently as possible to close the loop and achieve full control of the target at hand in this case us by moving our lives to the digital realm we become vulnerable to that which rules it ai algorithms this is made all the easier by the fact that the human mind is highly vulnerable to simple patterns of social manipulation consider for instance the following vectors of attack from an information security perspective you would call these vulnerabilities known exploits that can be used to take over a system in the case of the human minds these vulnerabilities never get patched they are just the way we work they re in our dna the human mind is a static vulnerable system that will come increasingly under attack from ever smarter ai algorithms that will simultaneously have a complete view of everything we do and believe and complete control of the information we consume remarkably mass population manipulation in particular political control arising from placing ai algorithms in charge of our information diet does not necessarily require very advanced ai you don t need self aware superintelligent ai for this to be a dire threat current technology may well suffice social network companies have been working on it for a few years with significant results and while they may only be trying to maximize engagement and to influence your purchase decisions rather than to manipulate your view of the world the tools they ve developed are already being hijacked by hostile state actors for political purposes as seen in the brexit referendum or the us presidential election this is already our reality but if mass population manipulation is already possible today in theory why hasn t the world been upended yet in short i think it s because we re really bad at ai but that may be about to change until all ad targeting algorithms across the industry were running on mere logistic regression in fact that s still true to a large extent today only the biggest players have switched to more advanced models logistic regression an algorithm that predates the computing era is one of the most basic techniques you could use for personalization it is the reason why so many of the ads you see online are desperately irrelevant likewise the social media bots used by hostile state actors to sway public opinion have little to no ai in them they re all extremely primitive for now machine learning and ai have been making fast progress in recent years and that progress is only beginning to get deployed in targeting algorithms and social media bots deep learning has only started to make its way into newsfeeds and ad networks in who knows what will be next it is quite striking that facebook has been investing enormous amounts in ai research and development with the explicit goal of becoming a leader in the field when your product is a social newsfeed what use are you going to make of natural language processing and reinforcement learning we re looking at a company that builds fine grained psychological profiles of almost two billion humans that serves as a primary news source for many of them that runs large scale behavior manipulation experiments and that aims at developing the best ai technology the world has ever seen personally it scares me and consider that facebook may not even be the most worrying threat here ponder for instance china s use of information control to enable unprecedented forms of totalitarianism such as its social credit system many people like to pretend that large corporations are the all powerful rulers of the modern world but what power they hold is dwarfed by that of governments if given algorithmic control over our minds governments may well turn into far worst actors than corporations now what can we do about it how can we defend ourselves as technologists what can we do to avert the risk of mass manipulation via our social newsfeeds importantly the existence of this threat doesn t mean that all algorithmic curation is bad or that all targeted advertising is bad far from it both of these can serve a valuable purpose with the rise of the internet and ai placing algorithms in charge of our information diet isn t just an inevitable trend it s a desirable one as our lives become increasingly digital and connected and as our world becomes increasingly information intensive we will need ai to serve as our interface to the world in the long run education and self development will be some of the most impactful applications of ai and this will happen through dynamics that almost entirely mirror that of a nefarious ai enabled newsfeed trying to manipulate you algorithmic information management has tremendous potential to help us to empower individuals to realize more of their potential and to help society better manage itself the issue is not ai itself the issue is control instead of letting newsfeed algorithms manipulate the user to achieve opaque goals such as swaying their political opinions or maximally wasting their time we should put the user in charge of the goals that the algorithms optimize for we are talking after all about your news your worldview your friends your life the impact that technology has on you should naturally be placed under your own control information management algorithms should not be a mysterious force inflicted on us to serve ends that run opposite to our own interests instead they should be a tool in our hand a tool that we can use for our own purposes say for education and personal instead of entertainment here s an idea any algorithmic newsfeed with significant adoption should we should build ai to serve humans not to manipulate them for profit or political gain what if newsfeed algorithms didn t operate like casino operators or propagandists what if instead they were closer to a mentor or a good librarian someone who used their keen understanding of your psychology and that of millions of other similar people to recommend to you that next book that will most resonate with your objectives and make you grow a sort of navigation tool for your life an ai capable of guiding you through the optimal path in experience space to get where you want to go can you imagine looking at your own life through the lens of a system that has seen millions of lives unfold or writing a book together with a system that has read every book or conducting research in collaboration with a system that sees the full scope of current human knowledge in products where you are fully in control of the ai that interacts with you a more sophisticated algorithm instead of being a threat would be a net positive letting you achieve your own goals more efficiently in summary our future is one where ai will be our interface to the world a world made of digital information this can equally lead to empowering individuals to gain greater control over their lives or to a total loss of agency unfortunately social media is currently engaged on the wrong road but it s still early enough that we can reverse course as an industry we need to develop product categories and markets where the incentives are aligned with placing the user in charge of the algorithms that affect them instead of using ai to exploit the user s mind for profit or political gain we need to strive towards products that are the anti facebook in the far future such products will likely take the form of ai assistants digital mentors programmed to help you that put you in control of the objectives they pursue in their interactions with you and in the present search engines could be seen as an early more primitive example of an ai driven information interface that serves users instead of seeking to hijack their mental space search is a tool that you deliberately use to reach specific goals rather than a passive always on feed that elects what to show you you tell it what to it should do for you and instead of seeking to maximally waste your time a search engine attempts to minimize the time it takes to go from question to answer from problem to solution you may be thinking since a search engine is still an ai layer between us and the information we consume could it bias its results to attempt to manipulate us yes that risk is latent in every information management algorithm but in stark contrast with social networks market incentives in this case are actually aligned with users needs pushing search engines to be as relevant and objective as possible if they fail to be maximally useful there s essentially no friction for users to move to a competing product and importantly a search engine would have a considerably smaller psychological attack surface than a social newsfeed the threat we ve profiled in this post requires most of the following to be present in a product most ai driven information management products don t meet these requirements social networks on the other hand are a frightening combination of risk factors as technologists we should gravitate towards products that do not feature these characteristics and push back against products that combine them all if only because of their potential for dangerous misuse build search engines and digital assistants not social newsfeeds make your recommendation engines transparent configurable and constructive rather than slot like machines that maximize engagement and wasted hours of human time invest your ui ux and ai expertise into building great configuration panels for your algorithm to enable your users to use your product on their own terms and importantly we should educate users about these issues so that they reject manipulative products generating enough market pressure to align the incentives of the technology industry with that of consumers conclusion the fork in the road ahead one path leads to a place that really scares me the other leads to a more humane future there s still time to take the better one if you work on these technologies keep this in mind you may not have evil intentions you may simply not care you may simply value your rsus more than our shared future but whether or not you care because you have a hand in shaping the infrastructure of the digital world your choices affect us all and you may eventually be held responsible for them from a quick cheer to a standing ovation clap to show how much you enjoyed this story
Simon Greenman,10200,16,https://towardsdatascience.com/who-is-going-to-make-money-in-ai-part-i-77a2f30b8cef?source=tag_archive---------7----------------,Who Is Going To Make Money In AI? Part I – Towards Data Science,we are in the midst of a gold rush in ai but who will reap the economic benefits the mass of startups who are all gold panning the corporates who have massive gold mining operations the technology giants who are supplying the picks and shovels and which nations have the richest seams of gold we are currently experiencing another gold rush in ai billions are being invested in ai startups across every imaginable industry and business function google amazon microsoft and ibm are in a heavyweight fight investing over billion in ai in corporates are scrambling to ensure they realise the productivity benefits of ai ahead of their competitors while looking over their shoulders at the startups china is putting its considerable weight behind ai and the european union is talking about a billion ai investment as it fears losing ground to china and the us ai is everywhere from the billion daily searches on google to the new apple iphone x that uses facial recognition to amazon alexa that cutely answers our questions media headlines tout the stories of how ai is helping doctors diagnose diseases banks better assess customer loan risks farmers predict crop yields marketers target and retain customers and manufacturers improve quality control and there are think tanks dedicated to studying the physical cyber and political risks of ai ai and machine learning will become ubiquitous and woven into the fabric of society but as with any gold rush the question is who will find gold will it just be the brave the few and the large or can the snappy upstarts grab their nuggets will those providing the picks and shovel make most of the money and who will hit pay dirt as i started thinking about who was going to make money in ai i ended up with seven questions who will make money across the chip makers platform and infrastructure providers enabling models and algorithm providers enterprise solution providers industry vertical solution providers corporate users of ai and nations while there are many ways to skin the cat of the ai landscape hopefully below provides a useful explanatory framework a value chain of sorts the companies noted are representative of larger players in each category but in no way is this list intended to be comprehensive or predictive even though the price of computational power has fallen exponentially demand is rising even faster ai and machine learning with its massive datasets and its trillions of vector and matrix calculations has a ferocious and insatiable appetite bring on the chips nvidia s stock is up in the past two years benefiting from the fact that their graphical processing unit gpu chips that were historically used to render beautiful high speed flowing games graphics were perfect for machine learning google recently launched its second generation of tensor processing units tpus and microsoft is building its own brainwave ai machine learning chips at the same time startups such as graphcore who has raised over m is looking to enter the market incumbents chip providers such as ibm intel qualcomm and amd are not standing still even facebook is rumoured to be building a team to design its own ai chips and the chinese are emerging as serious chip players with cambricon technology announcing the first cloud ai chip this past week what is clear is that the cost of designing and manufacturing chips then sustaining a position as a global chip leader is very high it requires extremely deep pockets and a world class team of silicon and software engineers this means that there will be very few new winners just like the gold rush days those that provide the cheapest and most widely used picks and shovels will make a lot of money the ai race is now also taking place in the cloud amazon realised early that startups would much rather rent computers and software than buy it and so it launched amazon web services aws in today ai is demanding so much compute power that companies are increasingly turning to the cloud to rent hardware through infrastructure as a service iaas and platform as a service paas offerings the fight is on among the tech giants microsoft is offering their hybrid public and private azure cloud service that allegedly has over one million computers and in the past few weeks they announced that their brainwave hardware solutionsdramatically accelerate machine learning with their own bing search engine performance improving by a factor of ten google is rushing to play catchup with its own googlecloud offering and we are seeing the chinese alibaba starting to take global share amazon microsoft google and ibm are going to continue to duke this one out and watch out for the massively scaled cloud players from china the big picks and shovels guys will win again today google is the world s largest ai company attracting the best ai minds spending small country size gdp budgets on r d and sitting on the best datasets gleamed from the billions of users of their services ai is powering google s search autonomous vehicles speech recognition intelligent reasoning massive search and even its own work on drug discovery and disease diangosis and the incredible ai machine learning software and algorithms that are powering all of google s ai activity tensorflow is now being given away for free yes for free tensorflow is now an open source software project available to the world and why are they doing this as jeff dean head of google brain recently said there are million organisations in the world that could benefit from machine learning today if millions of companies use this best in class free ai software then they are likely to need lots of computing power and who is better served to offer that well google cloud is of course optimised for tensorflow and related ai services and once you become reliant on their software and their cloud you become a very sticky customer for many years to come no wonder it is a brutal race for global ai algorithm dominance with amazon microsoft ibm also offering their own cheap or free ai software services we are also seeing a fight for not only machine learning algorithms but cognitive algorithms that offer services for conversational agents and bots speech natural language processing nlp and semantics vision and enhanced core algorithms one startup in this increasingly contested space is clarifai who provides advanced image recognition systems for businesses to detect near duplicates and visual searches it has raised nearly m over the past three years the market for vision related algorithms and services is estimated to be a cumulative billion in revenue between and the giants are not standing still ibm for example is offering its watson cognitive products and services they have twenty or so apis for chatbots vision speech language knowledge management and empathy that can be simply be plugged into corporate software to create ai enabled applications cognitive apis are everywhere kdnuggets lists here over of the top cognitive services from the giants and startups these services are being put into the cloud as ai as a service aiaas to make them more accessible just recently microsoft s ceo satya nadella claimed that a million developers are using their ai apis services and tools for building ai powered apps and nearly developers are using their tools for chatbots i wouldn t want to be a startup competing with these goliaths the winners in this space are likely to favour the heavyweights again they can hire the best research and engineering talent spend the most money and have access to the largest datasets to flourish startups are going to have to be really well funded supported by leading researchers with a whole battery of ip patents and published papers deep domain expertise and have access to quality datasets and they should have excellent navigational skills to sail ahead of the giants or sail different races there will many startup casualties but those that can scale will find themselves as global enterprises or quickly acquired by the heavyweights and even if a startup has not found a path to commercialisation then they could become acquihires companies bought for their talent if they are working on enabling ai algorithms with a strong research oriented team we saw this in when deepmind a two year old london based company that developed unique reinforcement machine learning algorithms was acquired by google for m enterprise software has been dominated by giants such as salesforce ibm oracle and sap they all recognise that ai is a tool that needs to be integrated into their enterprise offerings but many startups are rushing to become the next generation of enterprise services filling in gaps where the incumbents don t currently tread or even attempting to disrupt them we analysed over two hundred use cases in the enterprise space ranging from customer management to marketing to cybersecurity to intelligence to hr to the hot area of cognitive robotic process automation rpa the enterprise field is much more open than previous spaces with a veritable medley of startups providing point solutions for these use cases today there are over ai powered companies just in the recruitment space many of them ai startups cybersecurity leader darktrace and rpa leader uipathhave war chests in the millions the incumbents also want to make sure their ecosystems stay on the forefront and are investing in startups that enhance their offering salesforce has invested in digital genius a customer management solution and similarly unbable that offers enterprise translation services incumbents also often have more pressing problems sap for example is rushing to play catchup in offering a cloud solution let alone catchup in ai we are also seeing tools providers trying to simplify the tasks required to create deploy and manage ai services in the enterprise machine learning training for example is a messy business where of time can be spent on data wrangling and an inordinate amount of time is spent on testing and tuning of what is called hyperparameters petuum a tools provider based in pittsburgh in the us has raised over m to help accelerate and optimise the deployment of machine learning models many of these enterprise startup providers can have a healthy future if they quickly demonstrate that they are solving and scaling solutions to meet real world enterprise needs but as always happens in software gold rushes there will be a handful of winners in each category and for those ai enterprise category winners they are likely to be snapped up along with the best in class tool providers by the giants if they look too threatening ai is driving a race for the best vertical industry solutions there are a wealth of new ai powered startups providing solutions to corporate use cases in the healthcare financial services agriculture automative legal and industrial sectors and many startups are taking the ambitious path to disrupt the incumbent corporate players by offering a service directly to the same customers it is clear that many startups are providing valuable point solutions and can succeed if they have access to large and proprietary data training sets domain knowledge that gives them deep insights into the opportunities within a sector a deep pool of talent around applied ai and deep pockets of capital to fund rapid growth those startups that are doing well generally speak the corporate commercial language of customers business efficiency and roi in the form of well developed go to market plans for example zestfinance has raised nearly m to help improve credit decision making that will provide fair and transparent credit to everyone they claim they have the world s best data scientists but they would wouldn t they for those startups that are looking to disrupt existing corporate players they need really deep pockets for example affirm that offers loans to consumers at the point of sale has raised over m these companies quickly need to create a defensible moat to ensure they remain competitive this can come from data network effects where more data begets better ai based services and products that gets more revenue and customers that gets more data and so the flywheel effect continues and while corporates might look to new vendors in their industry for ai solutions that could enhance their top and bottom line they are not going to sit back and let upstarts muscle in on their customers and they are not going to sit still and let their corporate competitors gain the first advantage through ai there is currently a massive race for corporate innovation large companies have their own venture groups investing in startups running accelerators and building their own startups to ensure that they are leaders in ai driven innovation large corporates are in a strong position against the startups and smaller companies due to their data assets data is the fuel for ai and machine learning who is better placed to take advantage of ai than the insurance company that has reams of historic data on underwriting claims the financial services company that knows everything about consumer financial product buying behaviour or the search company that sees more user searches for information than any other corporates large and small are well positioned to extract value from ai in fact gartner research predicts ai derived business value is projected to reach up to trillion by there are hundreds if not thousands of valuable use cases that ai can addresses across organisations corporates can improve their customer experience save costs lower prices drive revenues and sell better products and services powered by ai ai will help the big get bigger often at the expense of smaller companies but they will need to demonstrate strong visionary leadership an ability to execute and a tolerance for not always getting technology enabled projects right on the first try countries are also also in a battle for ai supremacy china has not been shy about its call to arms around ai it is investing massively in growing technical talent and developing startups its more lax regulatory environment especially in data privacy helps china lead in ai sectors such as security and facial recognition just recently there was an example of chinese police picking out one most wanted face in a crowd of at a music concert and sensetime group ltd that analyses faces and images on a massive scale reported it raised m becoming the most valuable global ai startup the chinese point out that their mobile market is x the size of the us and there are x more mobile payments taking place this is a massive data advantage the european focus on data privacy regulation could put them at a disadvantage in certain areas of ai even if the union is talking about a b investment in ai the uk germany france and japan have all made recent announcements about their nation state ai strategies for example president macron said the french government will spend billion over the next five years to support the ai ecosystem including the creation of large public datasets companies such as google s deepmind and samsung have committed to open new paris labs and fujitsu is expanding its paris research centre the british just announced a billion push into ai including funding of ai phds but while nations are investing in ai talent and the ecosystem the question is who will really capture the value will france and the uk simply be subsidising phds who will be hired by google and while payroll and income taxes will be healthy on those six figure machine learning salaries the bulk of the economic value created could be with this american company its shareholders and the smiling american treasury ai will increase productivity and wealth in companies and countries but how will that wealth be distributed when the headlines suggest that to of our jobs will be taken by the machines economists can point to lessons from hundreds of years of increasing technology automation will there be net job creation or net job loss the public debate often cites geoffrey hinton the godfather of machine learning who suggested radiologists will lose their jobs by the dozen as machines diagnose diseases from medical images but then we can look to the chinese who are using ai to assist radiologists in managing the overwhelming demand to review billion ct scans annually for lung cancer the result is not job losses but an expanded market with more efficient and accurate diagnosis however there is likely to be a period of upheaval when much of the value will go to those few companies and countries that control ai technology and data and lower skilled countries whose wealth depends on jobs that are targets of ai automation will likely suffer ai will favour the large and the technologically skilled in examining the landscape of ai it has became clear that we are now entering a truly golden era for ai and there are few key themes appearing as to where the economic value will migrate in short it looks like the ai gold rush will favour the companies and countries with control and scale over the best ai tools and technology the data the best technical workers the most customers and the strongest access to capital those with scale will capture the lion s share of the economic value from ai in some ways plus c a change plus c est la me me chose but there will also be large golden nuggets that will be found by a few choice brave startups but like any gold rush many startups will hit pay dirt and many individuals and societies will likely feel like they have not seen the benefits of the gold rush this is the first part in a series of articles i intend to write on the topic of the economics of ai i welcome your feedback written by simon greenman i am a lover of technology and how it can be applied in the business world i run my own advisory firm best practice ai helping executives of enterprises and startups accelerate the adoption of roi based ai applications please get in touch to discuss this if you enjoyed this piece i d love it if you hit the clap button so others might stumble upon it and please post your comments or you can email me directly or find me on linkedin or twitter or follow me at simon greenman from a quick cheer to a standing ovation clap to show how much you enjoyed this story ai guy mapquest guy grow innovate and transform companies with tech start up investor mentor and geek sharing concepts ideas and codes
Aman Agarwal,7000,24,https://medium.freecodecamp.org/explained-simply-how-an-ai-program-mastered-the-ancient-game-of-go-62b8940a9080?source=tag_archive---------8----------------,Explained Simply: How an AI program mastered the ancient game of Go,this is about alphago google deepmind s go playing ai that shook the technology world in by defeating one of the best players in the world lee sedol go is an ancient board game which has so many possible moves at each step that future positions are hard to predict and therefore it requires strong intuition and abstract thinking to play because of this reason it was believed that only humans could be good at playing go most researchers thought that it would still take decades to build an ai which could think like that in fact i m releasing this essay today because this week march marks the two year anniversary of the alphago vs sedol match but alphago didn t stop there months later it played professional games on a go website under disguise as a player named master and won every single game against dozens of world champions of course without resting between games naturally this was a huge achievement in the field of ai and sparked worldwide discussions about whether we should be excited or worried about artificial intelligence today we are going to take the original research paper published by deepmind in the nature journal and break it down paragraph by paragraph using simple english after this essay you ll know very clearly what alphago is and how it works i also hope that after reading this you will not believe all the news headlines made by journalists to scare you about ai and instead feel excited about it worrying about the growing achievements of ai is like worrying about the growing abilities of microsoft powerpoint yes it will get better with time with new features being added to it but it can t just uncontrollably grow into some kind of hollywood monster you don t need to know how to play go to understand this paper in fact i myself have only read the first lines in wikipedia s opening paragraph about it instead surprisingly i use some examples from basic chess to explain the algorithms you just have to know what a player board game is in which each player takes turns and there is one winner at the end beyond that you don t need to know any physics or advanced math or anything this will make it more approachable for people who only just now started learning about machine learning or neural networks and especially for those who don t use english as their first language which can make it very difficult to read such papers if you have no prior knowledge of ai and neural networks you can read the deep learning section of one of my previous essays here after reading that you ll be able to get through this essay if you want to get a shallow understanding of reinforcement learning too optional reading you can find it here here s the original paper if you want to try reading it as for me hi i m aman an ai and autonomous robots engineer i hope that my work will save you a lot of time and effort if you were to study this on your own do you speak japanese ryohji ikebe has kindly written a brief memo about this essay in japanese in a series of tweets as you know the goal of this research was to train an ai program to play go at the level of world class professional human players to understand this challenge let me first talk about something similar done for chess in the early s ibm came out with the deep blue computer which defeated the great champion gary kasparov in chess he s also a very cool guy make sure to read more about him later how did deep blue play well it used a very brute force method at each step of the game it took a look at all the possible legal moves that could be played and went ahead to explore each and every move to see what would happen and it would keep exploring move after move for a while forming a kind of huge decision tree of thousands of moves and then it would come back along that tree observing which moves seemed most likely to bring a good result but what do we mean by good result well deep blue had many carefully designed chess strategies built into it by expert chess players to help it make better decisions for example how to decide whether to protect the king or get advantage somewhere else they made a specific evaluation algorithm for this purpose to compare how advantageous or disadvantageous different board positions are ibm hard coded expert chess strategies into this evaluation function and finally it chooses a carefully calculated move on the next turn it basically goes through the whole thing again as you can see this means deep blue thought about millions of theoretical positions before playing each move this was not so impressive in terms of the ai software of deep blue but rather in the hardware ibm claimed it to be one of the most powerful computers available in the market at that time it could look at million board positions per second now we come to go just believe me that this game is much more open ended and if you tried the deep blue strategy on go you wouldn t be able to play well there would be so many positions to look at at each step that it would simply be impractical for a computer to go through that hell for example at the opening move in chess there are possible moves in go the first player has possible moves and this scope of choices stays wide throughout the game this is what they mean by enormous search space moreover in go it s not so easy to judge how advantageous or disadvantageous a particular board position is at any specific point in the game you kinda have to play the whole game for a while before you can determine who is winning but let s say you magically had a way to do both of these and that s where deep learning comes in so in this research deepmind used neural networks to do both of these tasks if you haven t read about them yet here s the link again they trained a policy neural network to decide which are the most sensible moves in a particular board position so it s like following an intuitive strategy to pick moves from any position and they trained a value neural network to estimate how advantageous a particular board arrangement is for the player or in other words how likely you are to win the game from this position they trained these neural networks first with human game examples your good old ordinary supervised learning after this the ai was able to mimic human playing to a certain degree so it acted like a weak human player and then to train the networks even further they made the ai play against itself millions of times this is the reinforcement learning part with this the ai got better because it had more practice with these two networks alone deepmind s ai was able to play well against state of the art go playing programs that other researchers had built before these other programs had used an already popular pre existing game playing algorithm called the monte carlo tree search mcts more about this later but guess what we still haven t talked about the real deal deepmind s ai isn t just about the policy and value networks it doesn t use these two networks as a replacement of the monte carlo tree search instead it uses the neural networks to make the mcts algorithm work better and it got so much better that it reached superhuman levels this improved variation of mcts is alphago the ai that beat lee sedol and went down in ai history as one of the greatest breakthroughs ever so essentially alphago is simply an improved implementation of a very ordinary computer science algorithm do you understand now why ai in its current form is absolutely nothing to be scared of wow we ve spent a lot of time on the abstract alone alright to understand the paper from this point on first we ll talk about a gaming strategy called the monte carlo tree search algorithm for now i ll just explain this algorithm at enough depth to make sense of this essay but if you want to learn about it in depth some smart people have also made excellent videos and blog posts on this a short video series from udacity jeff bradberry s explanation of mcts an mcts tutorial by fullstack academy the following section is long but easy to understand i ll try my best and very important so stay with me the rest of the essay will go much quicker let s talk about the first paragraph of the essay above remember what i said about deep blue making a huge tree of millions of board positions and moves at each step of the game you had to do simulations and look at and compare each and every possible move as i said before that was a simple approach and very straightforward approach if the average software engineer had to design a game playing ai and had all the strongest computers of the world he or she would probably design a similar solution but let s think about how do humans themselves play chess let s say you re at a particular board position in the middle of the game by game rules you can do a dozen different things move this pawn here move the queen two squares here or three squares there and so on but do you really make a list of all the possible moves you can make with all your pieces and then select one move from this long list no you intuitively narrow down to a few key moves let s say you come up with sensible moves that you think make sense and then you wonder what will happen in the game if you chose one of these moves you might spend seconds considering each of these moves and their future and note that during these seconds you don t have to carefully plan out the future of each move you can just roll out a few mental moves guided by your intuition without too much careful thought well a good player would think farther and more deeply than an average player this is because you have limited time and you can t accurately predict what your opponent will do at each step in that lovely future you re cooking up in your brain so you ll just have to let your gut feeling guide you i ll refer to this part of the thinking process as rollout so take note of it so after rolling out your few sensible moves you finally say screw it and just play the move you find best then the opponent makes a move it might be a move you had already well anticipated which means you are now pretty confident about what you need to do next you don t have to spend too much time on the rollouts again or it could be that your opponent hits you with a pretty cool move that you had not expected so you have to be even more careful with your next move this is how the game carries on and as it gets closer and closer to the finishing point it would get easier for you to predict the outcome of your moves so your rollouts don t take as much time the purpose of this long story is to describe what the mcts algorithm does on a superficial level it mimics the above thinking process by building a search tree of moves and positions every time again for more details you should check out the links i mentioned earlier the innovation here is that instead of going through all the possible moves at each position which deep blue did it instead intelligently selects a small set of sensible moves and explores those instead to explore them it rolls out the future of each of these moves and compares them based on their imagined outcomes seriously this is all i think you need to understand this essay now coming back to the screenshot from the paper go is a perfect information game please read the definition in the link don t worry it s not scary and theoretically for such games no matter which particular position you are at in the game even if you have just played moves it is possible that you can correctly guess who will win or lose assuming that both players play perfectly from that point on i have no idea who came up with this theory but it is a fundamental assumption in this research project and it works so that means given a state of the game s there is a function v s which can predict the outcome let s say probability of you winning this game from to they call it the optimal value function because some board positions are more likely to result in you winning than other board positions they can be considered more valuable than the others let me say it again value probability between and of you winning the game but wait say there was a girl named foma sitting next to you while you play chess and she keeps telling you at each step if you re winning or losing you re winning you re losing nope still losing i think it wouldn t help you much in choosing which move you need to make she would also be quite annoying what would instead help you is if you drew the whole tree of all the possible moves you can make and the states that those moves would lead to and then foma would tell you for the entire tree which states are winning states and which states are losing states then you can choose moves which will keep leading you to winning states all of a sudden foma is your partner in crime not an annoying friend here foma behaves as your optimal value function v s earlier it was believed that it s not possible to have an accurate value function like foma for the game of go because the games had so much uncertainty but even if you had the wonderful foma this wonderland strategy of drawing out all the possible positions for foma to evaluate will not work very well in the real world in a game like chess or go as we said before if you try to imagine even moves into the future there can be so many possible positions that you don t have enough time to check all of them with foma so foma is not enough you need to narrow down the list of moves to a few sensible moves that you can roll out into the future how will your program do that enter lusha lusha is a skilled chess player and enthusiast who has spent decades watching grand masters play chess against each other she can look at your board position look quickly at all the available moves you can make and tell you how likely it would be that a chess expert would make any of those moves if they were sitting at your table so if you have possible moves at a point lusha will tell you the probability that each move would be picked by an expert of course a few sensible moves will have a much higher probability and other pointless moves will have very little probability she is your policy function p a s for a given state s she can give you probabilities for all the possible moves that an expert would make wow you can take lusha s help to guide you in how to select a few sensible moves and foma will tell you the likelihood of winning from each of those moves you can choose the move that both foma and lusha approve or if you want to be extra careful you can roll out the moves selected by lusha have foma evaluate them pick a few of them to roll out further into the future and keep letting foma and lusha help you predict very far into the game s future much quicker and more efficient than to go through all the moves at each step into the future this is what they mean by reducing the search space use a value function foma to predict outcomes and use a policy function lusha to give you grand master probabilities to help narrow down the moves you roll out these are called monte carlo rollouts then while you backtrack from future to present you can take average values of all the different moves you rolled out and pick the most suitable action so far this has only worked on a weak amateur level in go because the policy functions and value functions that they used to guide these rollouts weren t that great phew the first line is self explanatory in mcts you can start with an unskilled foma and unskilled lusha the more you play the better they get at predicting solid outcomes and moves narrowing the search to a beam of high probability actions is just a sophisticated way of saying lusha helps you narrow down the moves you need to roll out by assigning them probabilities that an expert would play them prior work has used this technique to achieve strong amateur level ai players even with simple or shallow as they call it policy functions yeah convolutional neural networks are great for image processing and since a neural network takes a particular input and gives an output it is essentially a function right so you can use a neural network to become a complex function so you can just pass in an image of the board position and let the neural network figure out by itself what s going on this means it s possible to create neural networks which will behave like very accurate policy and value functions the rest is pretty self explanatory here we discuss how foma and lusha were trained to train the policy network predicting for a given position which moves experts would pick you simply use examples of human games and use them as data for good old supervised learning and you want to train another slightly different version of this policy network to use for rollouts this one will be smaller and faster let s just say that since lusha is so experienced she takes some time to process each position she s good to start the narrowing down process with but if you try to make her repeat the process she ll still take a little too much time so you train a faster policy network for the rollout process i ll call it lusha s younger brother jerry i know i know enough with these names after that once you ve trained both of the slow and fast policy networks enough using human player data you can try letting lusha play against herself on a go board for a few days and get more practice this is the reinforcement learning part making a better version of the policy network then you train foma for value prediction determining the probability of you winning you let the ai practice through playing itself again and again in a simulated environment observe the end result each time and learn from its mistakes to get better and better i won t go into details of how these networks are trained you can read more technical details in the later section of the paper methods which i haven t covered here in fact the real purpose of this particular paper is not to show how they used reinforcement learning on these neural networks one of deepmind s previous papers in which they taught ai to play atari games has already discussed some reinforcement learning techniques in depth and i ve already written an explanation of that paper here for this paper as i lightly mentioned in the abstract and also underlined in the screenshot above the biggest innovation was the fact that they used rl with neural networks for improving an already popular game playing algorithm mcts rl is a cool tool in a toolbox that they used to fine tune the policy and value function neural networks after the regular supervised training this research paper is about proving how versatile and excellent this tool it is not about teaching you how to use it in television lingo the atari paper was a rl infomercial and this alphago paper is a commercial a quick note before you move on would you like to help me write more such essays explaining cool research papers if you re serious i d be glad to work with you please leave a comment and i ll get in touch with you so the first step is in training our policy nn lusha to predict which moves are likely to be played by an expert this nn s goal is to allow the ai to play similar to an expert human this is a convolutional neural network as i mentioned before it s a special kind of nn that is very useful in image processing that takes in a simplified image of a board arrangement rectifier nonlinearities are layers that can be added to the network s architecture they give it the ability to learn more complex things if you ve ever trained nns before you might have used the relu layer that s what these are the training data here was in the form of random pairs of board positions and the labels were the actions chosen by humans when they were in those positions just regular supervised learning here they use stochastic gradient ascent well this is an algorithm for backpropagation here you re trying to maximise a reward function and the reward function is just the probability of the action predicted by a human expert you want to increase this probability but hey you don t really need to think too much about this normally you train the network so that it minimises a loss function which is essentially the error difference between predicted outcome and actual label that is called gradient descent in the actual implementation of this research paper they have indeed used the regular gradient descent you can easily find a loss function that behaves opposite to the reward function such that minimising this loss will maximise the reward the policy network has layers and is called sl policy network sl supervised learning the data came from a i ll just say it s a popular website on which millions of people play go how good did this sl policy network perform it was more accurate than what other researchers had done earlier the rest of the paragraph is quite self explanatory as for the rollout policy you do remember from a few paragraphs ago how lusha the sl policy network is slow so it can t integrate well with the mcts algorithm and we trained another faster version of lusha called jerry who was her younger brother well this refers to jerry right here as you can see jerry is just half as accurate as lusha but it s thousands of times faster it will really help get through rolled out simulations of the future faster when we apply the mcts for this next section you don t have to know about reinforcement learning already but then you ll have to assume that whatever i say works if you really want to dig into details and make sure of everything you might want to read a little about rl first once you have the sl network trained in a supervised manner using human player moves with the human moves data as i said before you have to let her practice by itself and get better that s what we re doing here so you just take the sl policy network save it in a file and make another copy of it then you use reinforcement learning to fine tune it here you make the network play against itself and learn from the outcomes but there s a problem in this training style if you only forever practice against one opponent and that opponent is also only practicing with you exclusively there s not much of new learning you can do you ll just be training to practice how to beat that one player this is you guessed it overfitting your techniques play well against one opponent but don t generalize well to other opponents so how do you fix this well every time you fine tune a neural network it becomes a slightly different kind of player so you can save this version of the neural network in a list of players who all behave slightly differently right great now while training the neural network you can randomly make it play against many different older and newer versions of the opponent chosen from that list they are versions of the same player but they all play slightly differently and the more you train the more players you get to train even more with bingo in this training the only thing guiding the training process is the ultimate goal i e winning or losing you don t need to specially train the network to do things like capture more area on the board etc you just give it all the possible legal moves it can choose from and say you have to win and this is why rl is so versatile it can be used to train policy or value networks for any game not just go here they tested how accurate this rl policy network was just by itself without any mcts algorithm as you would remember this network can directly take a board position and decide how an expert would play it so you can use it to single handedly play games well the result was that the rl fine tuned network won against the sl network that was only trained on human moves it also won against other strong go playing programs must note here that even before training this rl policy network the sl policy network was already better than the state of the art and now it has further improved and we haven t even come to the other parts of the process like the value network did you know that baby penguins can sneeze louder than a dog can bark actually that s not true but i thought you d like a little joke here to distract from the scary looking equations above coming to the essay again we re done training lusha here now back to foma remember the optimal value function v s that only tells you how likely you are to win in your current board position if both players play perfectly from that point on so obviously to train an nn to become our value function we would need a perfect player which we don t have so we just use our strongest player which happens to be our rl policy network it takes the current state board state s and outputs the probability that you will win the game you play a game and get to know the outcome win or loss each of the game states act as a data sample and the outcome of that game acts as the label so by playing a move game you have data samples for value prediction lol no this approach is naive you can t use all moves from the game and add them to the dataset the training data set had to be chosen carefully to avoid overfitting each move in the game is very similar to the next one because you only move once and that gives you a new position right if you take the states at all of those moves and add them to the training data with the same label you basically have lots of kinda duplicate data and that causes overfitting to prevent this you choose only very distinct looking game states so for example instead of all moves of a game you only choose of them and add them to the training set deepmind took million positions from million different games to reduce any chances of there being duplicate data and it worked now something conceptual here there are two ways to evaluate the value of a board position one option is a magical optimal value function like the one you trained above the other option is to simply roll out into the future using your current policy lusha and look at the final outcome in this roll out obviously the real game would rarely go by your plans but deepmind compared how both of these options do you can also do a mixture of both these options we will learn about this mixing parameter a little bit later so make a mental note of this concept well your single neural network trying to approximate the optimal value function is even better than doing thousands of mental simulations using a rollout policy foma really kicked ass here when they replaced the fast rollout policy with the twice as accurate but slow rl policy lusha and did thousands of simulations with that it did better than foma but only slightly better and too slowly so foma is the winner of this competition she has proved that she can t be replaced now that we have trained the policy and value functions we can combine them with mcts and give birth to our former world champion destroyer of grand masters the breakthrough of a generation weighing two hundred and sixty eight pounds one and only alphaaaaa go in this section ideally you should have a slightly deeper understanding of the inner workings of the mcts algorithm but what you have learned so far should be enough to give you a good feel for what s going on here the only thing you should note is how we re using the policy probabilities and value estimations we combine them during roll outs to narrow down the number of moves we want to roll out at each step q s a represents the value function and u s a is a stored probability for that position i ll explain remember that the policy network uses supervised learning to predict expert moves and it doesn t just give you most likely move but rather gives you probabilities for each possible move that tell how likely it is to be an expert move this probability can be stored for each of those actions here they call it prior probability and they obviously use it while selecting which actions to explore so basically to decide whether or not to explore a particular move you consider two things first by playing this move how likely are you to win yes we already have our value network to answer this first question and the second question is how likely is it that an expert would choose this move if a move is super unlikely to be chosen by an expert why even waste time considering it this we get from the policy network then let s talk about the mixing parameter see came back to it as discussed earlier to evaluate positions you have two options one simply use the value network you have been using to evaluate states all along and two you can try to quickly play a rollout game with your current strategy assuming the other player will play similarly and see if you win or lose we saw how the value function was better than doing rollouts in general here they combine both you try giving each prediction importance or or and so on if you attach a of x to the first you ll have to attach x to the second that s what this mixing parameter means you ll see these hit and trial results later in the paper after each roll out you update your search tree with whatever information you gained during the simulation so that your next simulation is more intelligent and at the end of all simulations you just pick the best move interesting insight here remember how the rl fine tuned policy nn was better than just the sl human trained policy nn but when you put them within the mcts algorithm of alphago using the human trained nn proved to be a better choice than the fine tuned nn but in the case of the value function which you would remember uses a strong player to approximate a perfect player training foma using the rl policy works better than training her with the sl policy doing all this evaluation takes a lot of computing power we really had to bring out the big guns to be able to run these damn programs self explanatory lol our program literally blew the pants off of every other program that came before us this goes back to that mixing parameter again while evaluating positions giving equal importance to both the value function and the rollouts performed better than just using one of them the rest is self explanatory and reveals an interesting insight self explanatory self explanatory but read that red underlined sentence again i hope you can see clearly now that this line right here is pretty much the summary of what this whole research project was all about concluding paragraph let us brag a little more here because we deserve it oh and if you re a scientist or tech company and need some help in explaining your science to non technical people for marketing pr or training etc i can help you drop me a message on twitter mngrwl from a quick cheer to a standing ovation clap to show how much you enjoyed this story engineer teacher learner of foreign languages lover of history cinema and art our community publishes stories worth reading on development design and data science
Lance Ulanoff,15100,5,https://medium.com/@LanceUlanoff/did-google-duplex-just-pass-the-turing-test-ffcfe6868b02?source=tag_archive---------9----------------,Did Google Duplex just pass the Turing Test? – Lance Ulanoff – Medium,i think it was the first um that was the moment when i realized i was hearing something extraordinary a computer carrying out a completely natural and very human sounding conversation with a real person and it wasn t just a random talk this conversation had a purpose a destination to make an appointment at a hair salon the entity making the call and appointment was google assistant running duplex google s still experimental ai voice system and the venue was google i o google s yearly developer conference which this year focused heavily on the latest developments in ai machine and deep learning google ceo sundar pichai explained that what we were hearing was a real phone call made to a hair salon that didn t know it was part of an experiment or that they were talking to a computer he launched duplex by asking google assistant to book a haircut appointment for tuesday morning the ai did the rest duplex made the call and when someone at the salon picked up the voice ai started the conversation with hi i m calling to book a woman s hair cut appointment for a client um i m looking for something on may third when the attendant asked duplex to give her one second duplex responded with mmm hmm the conversation continued as the salon representative presented various dates and times and the ai asked about other options eventually the ai and the salon worker agreed on an appointment date and time what i heard was so convincing i had trouble discerning who was the salon worker and who what was the duplex ai it was stunning and somewhat disconcerting i liken it to the feeling you d get if a store mannequin suddenly smiled at you it was easily the most remarkable human computer conversation i d ever heard and the closest thing i ve seen a voice ai passing the turing test which is the ai threshold suggested by computer scientist alan turing in the s turing posited that by computers would be able to fool humans into thinking they were conversing with other humans at least of the time he was right in a chatbot named eugene goostman successfully impersonated a wise ass year old programmer during lengthy text based chats with unsuspecting humans turing however hadn t necessarily considered voice based systems and for obvious reasons talking computers are somewhat less adept at fooling humans spend a few minutes conversing with your voice assistant of choice and you ll soon discover their limitations their speech can be stilted pronunciations off and response times can be slow especially if they re trying to access a cloud based server and forget about conversations most can handle two consecutive queries at most and they virtually all require a trigger phrase like alexa or hey siri google is working on removing unnecessary okay googles in short back and forth convos with the digital assistant google assistant running duplex didn t exhibit any of those short comings it sounded like a young female assistant carefully scheduling her boss s haircut in addition to the natural cadence google added speech disfluencies the verbal ticks ums uhs and mm hmms and latency or pauses that naturally occur when people are speaking the result is a perfectly human voice produced entirely by a computer the second call demonstration where a male voiced duplex tried to make restaurant reservations was even more remarkable the human call participant didn t entirely understand duplex s verbal requests and then told duplex that for the number of people it wanted to bring to the restaurant they didn t need a reservation duplex handled all this without missing a beat the amazing thing is that the assistant can actually understand the nuances of conversation said pichai during the keynote that ability comes by way of neural network technology and intensive machine learning for as accomplished as duplex is in making hair appointments and restaurant reservations it might stumble in deeper or more abstract conversations in a blog post on duplex development google engineers explained that they constrained duplex s training to closed domains or well defined topics like dinner reservations and hair appointments this gave them the ability to perform intense exploration of the topics and focus training duplex was guided during training within the domain by experienced operators who could keep track of mistakes and worked with engineers to improve responses in short this means that while duplex has your hair and dining out options covered it could stumble in movie reservations and negotiations with your cable provider even so duplex fooled two humans i heard no hesitation or confusion in the hair salon call there was no indication that the salon worker thought something was amiss she wanted to help this young woman make an appointment what will she think when she learns she was duped by duplex obviously duplex s conversations were also short each lasting less than a minute putting them well short of the turing test benchmark i would ve enjoyed hearing the conversations devolve as they extended a few minutes or more i m sure duplex will soon tackle more domains and longer conversations and it will someday pass the turing test it s only a matter of time before duplex is handling other mundane or difficult calls for us like calling our parents with our own voices see wavenet technology eventually we ll have our duplex voices call each other handling pleasantries and making plans which google assistant can then drop in our google calendar but that s the future for now duplex s performance stands as a powerful proof of concept for our long imagined future of conversational ai s capable of helping entertaining and engaging with us it s the first major step on the path to the ai depicted in the movie her where joaquin phoenix starred as a man who falls in love with his chatty voice assistant played by the disembodied voice of scarlett johansson so no duplex didn t pass the turing test but i do wonder what alan turing would think of it from a quick cheer to a standing ovation clap to show how much you enjoyed this story tech expert journalist social media commentator amateur cartoonist and robotics fan
Gant Laborde,1300,7,https://medium.freecodecamp.org/machine-learning-how-to-go-from-zero-to-hero-40e26f8aa6da?source=---------0----------------,Machine Learning: how to go from Zero to Hero – freeCodeCamp,if your understanding of a i and machine learning is a big question mark then this is the blog post for you here i gradually increase your awesomenessicitytm by gluing inspirational videos together with friendly text sit down and relax these videos take time and if they don t inspire you to continue to the next section fair enough however if you find yourself at the bottom of this article you ve earned your well rounded knowledge and passion for this new world where you go from there is up to you a i was always cool from moving a paddle in pong to lighting you up with combos in street fighter a i has always revolved around a programmer s functional guess at how something should behave fun but programmers aren t always gifted in programming a i as we often see just google epic game fails to see glitches in a i physics and sometimes even experienced human players regardless a i has a new talent you can teach a computer to play video games understand language and even how to identify people or things this tip of the iceberg new skill comes from an old concept that only recently got the processing power to exist outside of theory i m talking about machine learning you don t need to come up with advanced algorithms anymore you just have to teach a computer to come up with its own advanced algorithm so how does something like that even work an algorithm isn t really written as much as it is sort of bred i m not using breeding as an analogy watch this short video which gives excellent commentary and animations to the high level concept of creating the a i wow right that s a crazy process now how is it that we can t even understand the algorithm when it s done one great visual was when the a i was written to beat mario games as a human we all understand how to play a side scroller but identifying the predictive strategy of the resulting a i is insane impressed there s something amazing about this idea right the only problem is we don t know machine learning and we don t know how to hook it up to video games fortunately for you elon musk already provided a non profit company to do the latter yes in a dozen lines of code you can hook up any a i you want to countless games tasks i have two good answers on why you should care firstly machine learning ml is making computers do things that we ve never made computers do before if you want to do something new not just new to you but to the world you can do it with ml secondly if you don t influence the world the world will influence you right now significant companies are investing in ml and we re already seeing it change the world thought leaders are warning that we can t let this new age of algorithms exist outside of the public eye imagine if a few corporate monoliths controlled the internet if we don t take up arms the science won t be ours i think christian heilmann said it best in his talk on ml the concept is useful and cool we understand it at a high level but what the heck is actually happening how does this work if you want to jump straight in i suggest you skip this section and move on to the next how do i get started section if you re motivated to be a doer in ml you won t need these videos if you re still trying to grasp how this could even be a thing the following video is perfect for walking you through the logic using the classic ml problem of handwriting pretty cool huh that video shows that each layer gets simpler rather than more complicated like the function is chewing data into smaller pieces that end in an abstract concept you can get your hands dirty in interacting with this process on this site by adam harley it s cool watching data go through a trained model but you can even watch your neural network get trained one of the classic real world examples of machine learning in action is the iris data set from in a presentation i attended by javafxpert s overview on machine learning i learned how you can use his tool to visualize the adjustment and back propagation of weights to neurons on a neural network you get to watch it train the neural model even if you re not a java buff the presentation jim gives on all things machine learning is a pretty cool hour introduction into ml concepts which includes more info on many of the examples above these concepts are exciting are you ready to be the einstein of this new era breakthroughs are happening every day so get started now there are tons of resources available i ll be recommending two approaches in this approach you ll understand machine learning down to the algorithms and the math i know this way sounds tough but how cool would it be to really get into the details and code this stuff from scratch if you want to be a force in ml and hold your own in deep conversations then this is the route for you i recommend that you try out brilliant org s app always great for any science lover and take the artificial neural network course this course has no time limits and helps you learn ml while killing time in line on your phone this one costs money after level combine the above with simultaneous enrollment in andrew ng s stanford course on machine learning in weeks this is the course that jim weaver recommended in his video above i ve also had this course independently suggested to me by jen looper everyone provides a caveat that this course is tough for some of you that s a show stopper but for others that s why you re going to put yourself through it and collect a certificate saying you did this course is free you only have to pay for a certificate if you want one with those two courses you ll have a lot of work to do everyone should be impressed if you make it through because that s not simple but more so if you do make it through you ll have a deep understanding of the implementation of machine learning that will catapult you into successfully applying it in new and world changing ways if you re not interested in writing the algorithms but you want to use them to create the next breathtaking website app you should jump into tensorflow and the crash course tensorflow is the de facto open source software library for machine learning it can be used in countless ways and even with javascript here s a crash course plenty more information on available courses and rankings can be found here if taking a course is not your style you re still in luck you don t have to learn the nitty gritty of ml in order to use it today you can efficiently utilize ml as a service in many ways with tech giants who have trained models ready i would still caution you that there s no guarantee that your data is safe or even yours but the offerings of services for ml are quite attractive using an ml service might be the best solution for you if you re excited and able to upload your data to amazon microsoft google i like to think of these services as a gateway drug to advanced ml either way it s good to get started now i have to say thank you to all the aforementioned people and videos they were my inspiration to get started and though i m still a newb in the ml world i m happy to light the path for others as we embrace this awe inspiring age we find ourselves in it s imperative to reach out and connect with people if you take up learning this craft without friendly faces answers and sounding boards anything can be hard just being able to ask and get a response is a game changer add me and add the people mentioned above friendly people with friendly advice helps see i hope this article has inspired you and those around you to learn ml from a quick cheer to a standing ovation clap to show how much you enjoyed this story software consultant adjunct professor published author award winning speaker mentor organizer and immature nerd d lately full of react native tech our community publishes stories worth reading on development design and data science
Emmanuel Ameisen,935,11,https://blog.insightdatascience.com/reinforcement-learning-from-scratch-819b65f074d8?source=---------1----------------,Reinforcement Learning from scratch – Insight Data,want to learn about applied artificial intelligence from leading practitioners in silicon valley new york or toronto learn more about the insight artificial intelligence fellows program are you a company working in ai and would like to get involved in the insight ai fellows program feel free to get in touch recently i gave a talk at the o reilly ai conference in beijing about some of the interesting lessons we ve learned in the world of nlp while there i was lucky enough to attend a tutorial on deep reinforcement learning deep rl from scratch by unity technologies i thought that the session led by arthur juliani was extremely informative and wanted to share some big takeaways below in our conversations with companies we ve seen a rise of interesting deep rl applications tools and results in parallel the inner workings and applications of deep rl such as alphago pictured above can often seem esoteric and hard to understand in this post i will give an overview of core aspects of the field that can be understood by anyone many of the visuals are from the slides of the talk and some are new the explanations and opinions are mine if anything is unclear reach out to me here deep rl is a field that has seen vast amounts of research interest including learning to play atari games beating pro players at dota and defeating go champions contrary to many classical deep learning problems that often focus on perception does this image contain a stop sign deep rl adds the dimension of actions that influence the environment what is the goal and how do i get there in dialog systems for example classical deep learning aims to learn the right response for a given query on the other hand deep reinforcement learning focuses on the right sequences of sentences that will lead to a positive outcome for example a happy customer this makes deep rl particularly attractive for tasks that require planning and adaptation such as manufacturing or self driving however industry applications have trailed behind the rapidly advancing results coming out of the research community a major reason is that deep rl often requires an agent to experiment millions of times before learning anything useful the best way to do this rapidly is by using a simulation environment this tutorial will be using unity to create environments to train agents in for this workshop led by arthur juliani and leon chen their goal was to get every participants to successfully train multiple deep rl algorithms in hours a tall order below is a comprehensive overview of many of the main algorithms that power deep rl today for a more complete set of tutorials arthur juliani wrote an part series starting here deep rl can be used to best the top human players at go but to understand how that s done you first need to understand a few simple concepts starting with much easier problems it all starts with slot machines let s imagine you are faced with chests that you can pick from at each turn each of them have a different average payout and your goal is to maximize the total payout you receive after a fixed number of turns this is a classic problem called multi armed bandits and is where we will start the crux of the problem is to balance exploration which helps us learn about which states are good and exploitation where we now use what we know to pick the best slot machine here we will utilize a value function that maps our actions to an estimated reward called the q function first we ll initialize all q values at equal values then we ll update the q value of each action picking each chest based on how good the payout was after choosing this action this allows us to learn a good value function we will approximate our q function using a neural network starting with a very shallow one that learns a probability distribution by using a softmax over the potential chests while the value function tells us how good we estimate each action to be the policy is the function that determines which actions we end up taking intuitively we might want to use a policy that picks the action with the highest q value this performs poorly in practice as our q estimates will be very wrong at the start before we gather enough experience through trial and error this is why we need to add a mechanism to our policy to encourage exploration one way to do that is to use epsilon greedy which consists of taking a random action with probability epsilon we start with epsilon being close to always choosing random actions and lower epsilon as we go along and learn more about which chests are good eventually we learn which chests are best in practice we might want to take a more subtle approach than either taking the action we think is the best or a random action a popular method is boltzmann exploration which adjust probabilities based on our current estimate of how good each chest is adding in a randomness factor adding different states the previous example was a world in which we were always in the same state waiting to pick from the same chests in front of us most real word problems consist of many different states that is what we will add to our environment next now the background behind chests alternates between colors at each turn changing the average values of the chests this means we need to learn a q function that depends not only on the action the chest we pick but the state what the color of the background is this version of the problem is called contextual multi armed bandits surprisingly we can use the same approach as before the only thing we need to add is an extra dense layer to our neural network that will take in as input a vector representing the current state of the world learning about the consequences of our actions there is another key factor that makes our current problem simpler than mosts in most environments such as in the maze depicted above the actions that we take have an impact on the state of the world if we move up on this grid we might receive a reward or we might receive nothing but the next turn we will be in a different state this is where we finally introduce a need for planning first we will define our q function as the immediate reward in our current state plus the discounted reward we are expecting by taking all of our future actions this solution works if our q estimate of states is accurate so how can we learn a good estimate we will use a method called temporal difference td learning to learn a good q function the idea is to only look at a limited number of steps in the future td for example only uses the next states to evaluate the reward surprisingly we can use td which looks at the current state and our estimate of the reward the next turn and get great results the structure of the network is the same but we need to go through one forward step before receiving the error we then use this error to back propagate gradients like in traditional deep learning and update our value estimates introducing monte carlo another method to estimate the eventual success of our actions is monte carlo estimates this consists of playing out the entire episode with our current policy until we reach an end success by reaching a green block or failure by reaching a red block in the image above and use that result to update our value estimates for each traversed state this allows us to propagate values efficiently in one batch at the end of an episode instead of every time we make a move the cost is that we are introducing noise to our estimates since we attribute very distant rewards to them the world is rarely discrete the previous methods were using neural networks to approximate our value estimates by mapping from a discrete number of states and actions to a value in the maze for example there were states squares and actions move in each adjacent direction in this environment we are trying to learn how to balance a ball on a dimensional paddle by deciding at each time step whether we want to tilt the paddle left or right here the state space becomes continuous the angle of the paddle and the position of the ball the good news is we can still use neural networks to approximate this function a note about off policy vs on policy learning the methods we used previously are off policy methods meaning we can generate data with any strategy using epsilon greedy for example and learn from it on policy methods can only learn from actions that were taken following our policy remember a policy is the method we use to determine which actions to take this constrains our learning process as we have to have an exploration strategy that is built in to the policy itself but allows us to tie results directly to our reasoning and enables us to learn more efficiently the approach we will use here is called policy gradients and is an on policy method previously we were first learning a value function q for each action in each state and then building a policy on top in vanilla policy gradient we still use monte carlo estimates but we learn our policy directly through a loss function that increases the probability of choosing rewarding actions since we are learning on policy we cannot use methods such as epsilon greedy which includes random choices to get our agent to explore the environment the way that we encourage exploration is by using a method called entropy regularization which pushes our probability estimates to be wider and thus will encourage us to make riskier choices to explore the space leveraging deep learning for representations in practice many state of the art rl methods require learning both a policy and value estimates the way we do this with deep learning is by having both be two separate outputs of the same backbone neural network which will make it easier for our neural network to learn good representations one method to do this is advantage actor critic a c we learn our policy directly with policy gradients defined above and learn a value function using something called advantage instead of updating our value function based on rewards we update it based on our advantage which measures how much better or worse an action was than our previous value function estimated it to be this helps make learning more stable compared to simple q learning and vanilla policy gradients learning directly from the screen there is an additional advantage to using deep learning for these methods which is that deep neural networks excel at perceptive tasks when a human plays a game the information received is not a list of states but an image usually of a screen or a board or the surrounding environment image based learning combines a convolutional neural network cnn with rl in this environment we pass in a raw image instead of features and add a layer cnn to our architecture without changing anything else we can even inspect activations to see what the network picks up on to determine value and policy in the example below we can see that the network uses the current score and distant obstacles to estimate the value of the current state while focusing on nearby obstacles for determining actions neat as a side note while toying around with the provided implementation i ve found that visual learning is very sensitive to hyperparameters changing the discount rate slightly for example completely prevented the neural network from learning even on a toy application this is a widely known problem but it is interesting to see it first hand nuanced actions so far we ve played with environments with continuous and discrete state spaces however every environment we studied had a discrete action space we could move in one of four directions or tilt the paddle to the left or right ideally for applications such as self driving cars we would like to learn continuous actions such as turning the steering wheel between and degrees in this environment called d ball world we can choose to tilt the paddle to any value on each of its axes this gives us more control as to how we perform actions but makes the action space much larger we can approach this by approximating our potential choices with gaussian distributions we learn a probability distribution over potential actions by learning the mean and variance of a gaussian distribution and our policy we sample from that distribution simple in theory next steps for the brave there are a few concepts that separate the algorithms described above from state of the art approaches it s interesting to see that conceptually the best robotics and game playing algorithms are not that far away from the ones we just explored that s it for this overview i hope this has been informative and fun if you are looking to dive deeper into the theory of rl give arthur s posts a read or diving deeper by following david silver s ucl course if you are looking to learn more about the projects we do at insight or how we work with companies please check us out below or reach out to me here want to learn about applied artificial intelligence from leading practitioners in silicon valley new york or toronto learn more about the insight artificial intelligence fellows program are you a company working in ai and would like to get involved in the insight ai fellows program feel free to get in touch from a quick cheer to a standing ovation clap to show how much you enjoyed this story ai lead at insight ai emmanuelameisen insight fellows program your bridge to a career in data
Irhum Shafkat,2000,15,https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1?source=---------2----------------,Intuitively Understanding Convolutions for Deep Learning,the advent of powerful and versatile deep learning frameworks in recent years has made it possible to implement convolution layers into a deep learning model an extremely simple task often achievable in a single line of code however understanding convolutions especially for the first time can often feel a bit unnerving with terms like kernels filters channels and so on all stacked onto each other yet convolutions as a concept are fascinatingly powerful and highly extensible and in this post we ll break down the mechanics of the convolution operation step by step relate it to the standard fully connected network and explore just how they build up a strong visual hierarchy making them powerful feature extractors for images the d convolution is a fairly simple operation at heart you start with a kernel which is simply a small matrix of weights this kernel slides over the d input data performing an elementwise multiplication with the part of the input it is currently on and then summing up the results into a single output pixel the kernel repeats this process for every location it slides over converting a d matrix of features into yet another d matrix of features the output features are essentially the weighted sums with the weights being the values of the kernel itself of the input features located roughly in the same location of the output pixel on the input layer whether or not an input feature falls within this roughly same location gets determined directly by whether it s in the area of the kernel that produced the output or not this means the size of the kernel directly determines how many or few input features get combined in the production of a new output feature this is all in pretty stark contrast to a fully connected layer in the above example we have input features and output features if this were a standard fully connected layer you d have a weight matrix of parameters with every output feature being the weighted sum of every single input feature convolutions allow us to do this transformation with only parameters with each output feature instead of looking at every input feature only getting to look at input features coming from roughly the same location do take note of this as it ll be critical to our later discussion before we move on it s definitely worth looking into two techniques that are commonplace in convolution layers padding and strides padding does something pretty clever to solve this pad the edges with extra fake pixels usually of value hence the oft used term zero padding this way the kernel when sliding can allow the original edge pixels to be at its center while extending into the fake pixels beyond the edge producing an output the same size as the input the idea of the stride is to skip some of the slide locations of the kernel a stride of means to pick slides a pixel apart so basically every single slide acting as a standard convolution a stride of means picking slides pixels apart skipping every other slide in the process downsizing by roughly a factor of a stride of means skipping every slides downsizing roughly by factor and so on more modern networks such as the resnet architectures entirely forgo pooling layers in their internal layers in favor of strided convolutions when needing to reduce their output sizes of course the diagrams above only deals with the case where the image has a single input channel in practicality most input images have channels and that number only increases the deeper you go into a network it s pretty easy to think of channels in general as being a view of the image as a whole emphasising some aspects de emphasising others so this is where a key distinction between terms comes in handy whereas in the channel case where the term filter and kernel are interchangeable in the general case they re actually pretty different each filter actually happens to be a collection of kernels with there being one kernel for every single input channel to the layer and each kernel being unique each filter in a convolution layer produces one and only one output channel and they do it like so each of the kernels of the filter slides over their respective input channels producing a processed version of each some kernels may have stronger weights than others to give more emphasis to certain input channels than others eg a filter may have a red kernel channel with stronger weights than others and hence respond more to differences in the red channel features than the others each of the per channel processed versions are then summed together to form one channel the kernels of a filter each produce one version of each channel and the filter as a whole produces one overall output channel finally then there s the bias term the way the bias term works here is that each output filter has one bias term the bias gets added to the output channel so far to produce the final output channel and with the single filter case down the case for any number of filters is identical each filter processes the input with its own different set of kernels and a scalar bias with the process described above producing a single output channel they are then concatenated together to produce the overall output with the number of output channels being the number of filters a nonlinearity is then usually applied before passing this as input to another convolution layer which then repeats this process even with the mechanics of the convolution layer down it can still be hard to relate it back to a standard feed forward network and it still doesn t explain why convolutions scale to and work so much better for image data suppose we have a input and we want to transform it into a grid if we were using a feedforward network we d reshape the input into a vector of length and pass it through a densely connected layer with inputs and outputs one could visualize the weight matrix w for a layer and although the convolution kernel operation may seem a bit strange at first it is still a linear transformation with an equivalent transformation matrix if we were to use a kernel k of size on the reshaped input to get a output the equivalent transformation matrix would be note while the above matrix is an equivalent transformation matrix the actual operation is usually implemented as a very different matrix multiplication the convolution then as a whole is still a linear transformation but at the same time it s also a dramatically different kind of transformation for a matrix with elements there s just parameters which themselves are reused several times each output node only gets to see a select number of inputs the ones inside the kernel there is no interaction with any of the other inputs as the weights to them are set to it s useful to see the convolution operation as a hard prior on the weight matrix in this context by prior i mean predefined network parameters for example when you use a pretrained model for image classification you use the pretrained network parameters as your prior as a feature extractor to your final densely connected layer in that sense there s a direct intuition between why both are so efficient compared to their alternatives transfer learning is efficient by orders of magnitude compared to random initialization because you only really need to optimize the parameters of the final fully connected layer which means you can have fantastic performance with only a few dozen images per class here you don t need to optimize all parameters because we set most of them to zero and they ll stay that way and the rest we convert to shared parameters resulting in only actual parameters to optimize this efficiency matters because when you move from the inputs of mnist to real world images thats over inputs a dense layer attempting to halve the input to inputs would still require over billion parameters for comparison the entirety of resnet has some million parameters so fixing some parameters to and tying parameters increases efficiency but unlike the transfer learning case where we know the prior is good because it works on a large general set of images how do we know this is any good the answer lies in the feature combinations the prior leads the parameters to learn early on in this article we discussed that so with backpropagation coming in all the way from the classification nodes of the network the kernels have the interesting task of learning weights to produce features only from a set of local inputs additionally because the kernel itself is applied across the entire image the features the kernel learns must be general enough to come from any part of the image if this were any other kind of data eg categorical data of app installs this would ve been a disaster for just because your number of app installs and app type columns are next to each other doesn t mean they have any local shared features common with app install dates and time used sure the four may have an underlying higher level feature eg which apps people want most that can be found but that gives us no reason to believe the parameters for the first two are exactly the same as the parameters for the latter two the four could ve been in any consistent order and still be valid pixels however always appear in a consistent order and nearby pixels influence a pixel e g if all nearby pixels are red it s pretty likely the pixel is also red if there are deviations that s an interesting anomaly that could be converted into a feature and all this can be detected from comparing a pixel with its neighbors with other pixels in its locality and this idea is really what a lot of earlier computer vision feature extraction methods were based around for instance for edge detection one can use a sobel edge detection filter a kernel with fixed parameters operating just like the standard one channel convolution for a non edge containing grid eg the background sky most of the pixels are the same value so the overall output of the kernel at that point is for a grid with an vertical edge there is a difference between the pixels to the left and right of the edge and the kernel computes that difference to be non zero activating and revealing the edges the kernel only works only a grids at a time detecting anomalies on a local scale yet when applied across the entire image is enough to detect a certain feature on a global scale anywhere in the image so the key difference we make with deep learning is ask this question can useful kernels be learnt for early layers operating on raw pixels we could reasonably expect feature detectors of fairly low level features like edges lines etc there s an entire branch of deep learning research focused on making neural network models interpretable one of the most powerful tools to come out of that is feature visualization using optimization the idea at core is simple optimize a image usually initialized with random noise to activate a filter as strongly as possible this does make intuitive sense if the optimized image is completely filled with edges that s strong evidence that s what the filter itself is looking for and is activated by using this we can peek into the learnt filters and the results are stunning one important thing to notice here is that convolved images are still images the output of a small grid of pixels from the top left of an image will still be on the top left so you can run another convolution layer on top of another such as the two on the left to extract deeper features which we visualize yet however deep our feature detectors get without any further changes they ll still be operating on very small patches of the image no matter how deep your detectors are you can t detect faces from a grid and this is where the idea of the receptive field comes in a essential design choice of any cnn architecture is that the input sizes grow smaller and smaller from the start to the end of the network while the number of channels grow deeper this as mentioned earlier is often done through strides or pooling layers locality determines what inputs from the previous layer the outputs get to see the receptive field determines what area of the original input to the entire network the output gets to see the idea of a strided convolution is that we only process slides a fixed distance apart and skip the ones in the middle from a different point of view we only keep outputs a fixed distance apart and remove the rest we then apply a nonlinearity to the output and per usual then stack another new convolution layer on top and this is where things get interesting even if were we to apply a kernel of the same size having the same local area to the output of the strided convolution the kernel would have a larger effective receptive field this is because the output of the strided layer still does represent the same image it is not so much cropping as it is resizing only thing is that each single pixel in the output is a representative of a larger area of whose other pixels were discarded from the same rough location from the original input so when the next layer s kernel operates on the output it s operating on pixels collected from a larger area note if you re familiar with dilated convolutions note that the above is not a dilated convolution both are methods of increasing the receptive field but dilated convolutions are a single layer while this takes place on a regular convolution following a strided convolution with a nonlinearity inbetween this expansion of the receptive field allows the convolution layers to combine the low level features lines edges into higher level features curves textures as we see in the mixed a layer followed by a pooling strided layer the network continues to create detectors for even higher level features parts patterns as we see for mixed a the repeated reduction in image size across the network results in by the th block on convolutions input sizes of just compared to inputs of at this point each single pixel represents a grid of pixels which is huge compared to earlier layers where an activation meant detecting an edge here an activation on the tiny grid is one for a very high level feature such as for birds the network as a whole progresses from a small number of filters in case of googlenet detecting low level features to a very large number of filters in the final convolution each looking for an extremely specific high level feature followed by a final pooling layer which collapses each grid into a single pixel each channel is a feature detector with a receptive field equivalent to the entire image compared to what a standard feedforward network would have done the output here is really nothing short of awe inspiring a standard feedforward network would have produced abstract feature vectors from combinations of every single pixel in the image requiring intractable amounts of data to train the cnn with the priors imposed on it starts by learning very low level feature detectors and as across the layers as its receptive field is expanded learns to combine those low level features into progressively higher level features not an abstract combination of every single pixel but rather a strong visual hierarchy of concepts by detecting low level features and using them to detect higher level features as it progresses up its visual hierarchy it is eventually able to detect entire visual concepts such as faces birds trees etc and that s what makes them such powerful yet efficient with image data with the visual hierarchy cnns build it is pretty reasonable to assume that their vision systems are similar to humans and they re really great with real world images but they also fail in ways that strongly suggest their vision systems aren t entirely human like the most major problem adversarial examples examples which have been specifically modified to fool the model adversarial examples would be a non issue if the only tampered ones that caused the models to fail were ones that even humans would notice the problem is the models are susceptible to attacks by samples which have only been tampered with ever so slightly and would clearly not fool any human this opens the door for models to silently fail which can be pretty dangerous for a wide range of applications from self driving cars to healthcare robustness against adversarial attacks is currently a highly active area of research the subject of many papers and even competitions and solutions will certainly improve cnn architectures to become safer and more reliable cnns were the models that allowed computer vision to scale from simple applications to powering sophisticated products and services ranging from face detection in your photo gallery to making better medical diagnoses they might be the key method in computer vision going forward or some other new breakthrough might just be around the corner regardless one thing is for sure they re nothing short of amazing at the heart of many present day innovative applications and are most certainly worth deeply understanding hope you enjoyed this article if you d like to stay connected you ll find me on twitter here if you have a question comments are welcome i find them to be useful to my own learning process as well from a quick cheer to a standing ovation clap to show how much you enjoyed this story curious programmer tinkers around in python and deep learning sharing concepts ideas and codes
Abhishek Parbhakar,937,6,https://towardsdatascience.com/must-know-information-theory-concepts-in-deep-learning-ai-e54a5da9769d?source=---------3----------------,Must know Information Theory concepts in Deep Learning (AI),information theory is an important field that has made significant contribution to deep learning and ai and yet is unknown to many information theory can be seen as a sophisticated amalgamation of basic building blocks of deep learning calculus probability and statistics some examples of concepts in ai that come from information theory or related fields in the early th century scientists and engineers were struggling with the question how to quantify the information is there a analytical way or a mathematical measure that can tell us about the information content for example consider below two sentences it is not difficult to tell that the second sentence gives us more information since it also tells that bruno is big and brown in addition to being a dog how can we quantify the difference between two sentences can we have a mathematical measure that tells us how much more information second sentence have as compared to the first scientists were struggling with these questions semantics domain and form of data only added to the complexity of the problem then mathematician and engineer claude shannon came up with the idea of entropy that changed our world forever and marked the beginning of digital information age shannon proposed that the semantic aspects of data are irrelevant and nature and meaning of data doesn t matter when it comes to information content instead he quantified information in terms of probability distribution and uncertainty shannon also introduced the term bit that he humbly credited to his colleague john tukey this revolutionary idea not only laid the foundation of information theory but also opened new avenues for progress in fields like artificial intelligence below we discuss four popular widely used and must known information theoretic concepts in deep learning and data sciences also called information entropy or shannon entropy entropy gives a measure of uncertainty in an experiment let s consider two experiments if we compare the two experiments in exp it is easier to predict the outcome as compared to exp so we can say that exp is inherently more uncertain unpredictable than exp this uncertainty in the experiment is measured using entropy therefore if there is more inherent uncertainty in the experiment then it has higher entropy or lesser the experiment is predictable more is the entropy the probability distribution of experiment is used to calculate the entropy a deterministic experiment which is completely predictable say tossing a coin with p h has entropy zero an experiment which is completely random say rolling fair dice is least predictable has maximum uncertainty and has the highest entropy among such experiments another way to look at entropy is the average information gained when we observe outcomes of an random experiment the information gained for a outcome of an experiment is defined as a function of probability of occurrence of that outcome more the rarer is the outcome more is the information gained from observing it for example in an deterministic experiment we always know the outcome so no new information gained is here from observing the outcome and hence entropy is zero for a discrete random variable x with possible outcomes states x x n the entropy in unit of bits is defined as where p x i is the probability of i th outcome of x cross entropy is used to compare two probability distributions it tells us how similar two distributions are cross entropy between two probability distributions p and q defined over same set of outcomes is given by mutual information is a measure of mutual dependency between two probability distributions or random variables it tells us how much information about one variable is carried by the another variable mutual information captures dependency between random variables and is more generalized than vanilla correlation coefficient which captures only the linear relationship mutual information of two discrete random variables x and y is defined as where p x y is the joint probability distribution of x and y and p x and p y are the marginal probability distribution of x and y respectively also called relative entropy kl divergence is another measure to find similarities between two probability distributions it measures how much one distribution diverges from the other suppose we have some data and true distribution underlying it is p but we don t know this p so we choose a new distribution q to approximate this data since q is just an approximation it won t be able to approximate the data as good as p and some information loss will occur this information loss is given by kl divergence kl divergence between p and q tells us how much information we lose when we try to approximate data given by p with q kl divergence of a probability distribution q from another probability distribution p is defined as kl divergence is commonly used in unsupervised machine learning technique variational autoencoders information theory was originally formulated by mathematician and electrical engineer claude shannon in his seminal paper a mathematical theory of communication in note terms experiments random variable ai machine learning deep learning data science have been used loosely above but have technically different meanings in case you liked the article do follow me abhishek parbhakar for more articles related to ai philosophy and economics from a quick cheer to a standing ovation clap to show how much you enjoyed this story finding equilibria among ai philosophy and economics sharing concepts ideas and codes
Aman Dalmia,2300,17,https://blog.usejournal.com/what-i-learned-from-interviewing-at-multiple-ai-companies-and-start-ups-a9620415e4cc?source=---------4----------------,What I learned from interviewing at multiple AI companies and start-ups,over the past months i ve been interviewing at various companies like google s deepmind wadhwani institute of ai microsoft ola fractal analytics and a few others primarily for the roles data scientist software engineer research engineer in the process not only did i get an opportunity to interact with many great minds but also had a peek at myself along with a sense of what people really look for when interviewing someone i believe that if i d had this knowledge before i could have avoided many mistakes and have prepared in a much better manner which is what the motivation behind this post is to be able to help someone bag their dream place of work this post arose from a discussion with one of my juniors on the lack of really fulfilling job opportunities offered through campus placements for people working in ai also when i was preparing i noticed people using a lot of resources but as per my experience over the past months i realised that one can do away with a few minimal ones for most roles in ai all of which i m going to mention at the end of the post i begin with how to get noticed a k a the interview then i provide a list of companies and start ups to apply which is followed by how to ace that interview based on whatever experience i ve had i add a section on what we should strive to work for i conclude with minimal resources you need for preparation note for people who are sitting for campus placements there are two things i d like to add firstly most of what i m going to say except for the last one maybe is not going to be relevant to you for placements but and this is my second point as i mentioned before opportunities on campus are mostly in software engineering roles having no intersection with ai so this post is specifically meant for people who want to work on solving interesting problems using ai also i want to add that i haven t cleared all of these interviews but i guess that s the essence of failure it s the greatest teacher the things that i mention here may not all be useful but these are things that i did and there s no way for me to know what might have ended up making my case stronger to be honest this step is the most important one what makes off campus placements so tough and exhausting is getting the recruiter to actually go through your profile among the plethora of applications that they get having a contact inside the organisation place a referral for you would make it quite easy but in general this part can be sub divided into three keys steps a do the regulatory preparation and do that well so with regulatory preparation i mean a linkedin profile a github profile a portfolio website and a well polished cv firstly your cv should be really neat and concise follow this guide by udacity for cleaning up your cv resume revamp it has everything that i intend to say and i ve been using it as a reference guide myself as for the cv template some of the in built formats on overleaf are quite nice i personally use deedy resume here s a preview as it can be seen a lot of content can be fit into one page however if you really do need more than that then the format linked above would not work directly instead you can find a modified multi page format of the same here the next most important thing to mention is your github profile a lot of people underestimate the potential of this just because unlike linkedin it doesn t have a who viewed your profile option people do go through your github because that s the only way they have to validate what you have mentioned in your cv given that there s a lot of noise today with people associating all kinds of buzzwords with their profile especially for data science open source has a big role to play too with majority of the tools implementations of various algorithms lists of learning resources all being open sourced i discuss the benefits of getting involved in open source and how one can start from scratch in an earlier post here the bare minimum for now should be create a github account if you don t already have one create a repository for each of the projects that you have done add documentation with clear instructions on how to run the code add documentation for each file mentioning the role of each function the meaning of each parameter proper formatting e g pep for python along with a script to automate the previous step optional moving on the third step is what most people lack which is having a portfolio website demonstrating their experience and personal projects making a portfolio indicates that you are really serious about getting into the field and adds a lot of points to the authenticity factor also you generally have space constraints on your cv and tend to miss out on a lot of details you can use your portfolio to really delve deep into the details if you want to and it s highly recommended to include some sort of visualisation or demonstration of the project idea it s really easy to create one too as there are a lot of free platforms with drag and drop features making the process really painless i personally use weebly which is a widely used tool it s better to have a reference to begin with there are a lot of awesome ones out there but i referred to deshraj yadav s personal website to begin with making mine finally a lot of recruiters and start ups have nowadays started using linkedin as their go to platform for hiring a lot of good jobs get posted there apart from recruiters the people working at influential positions are quite active there as well so if you can grab their attention you have a good chance of getting in too apart from that maintaining a clean profile is necessary for people to have the will to connect with you an important part of linkedin is their search tool and for you to show up you must have the relevant keywords interspersed over your profile it took me a lot of iterations and re evaluations to finally have a decent one also you should definitely ask people with or under whom you ve worked with to endorse you for your skills and add a recommendation talking about their experience of working with you all of this increases your chance of actually getting noticed i ll again point towards udacity s guide for linkedin and github profiles all this might seem like a lot but remember that you don t need to do it in a single day or even a week or a month it s a process it never ends setting up everything at first would definitely take some effort but once it s there and you keep updating it regularly as events around you keep happening you ll not only find it to be quite easy but also you ll be able to talk about yourself anywhere anytime without having to explicitly prepare for it because you become so aware about yourself b stay authentic i ve seen a lot of people do this mistake of presenting themselves as per different job profiles according to me it s always better to first decide what actually interests you what would you be happy doing and then search for relevant opportunities not the other way round the fact that the demand for ai talent surpasses the supply for the same gives you this opportunity spending time on your regulatory preparation mentioned above would give you an all around perspective on yourself and help make this decision easier also you won t need to prepare answers to various kinds of questions that you get asked during an interview most of them would come out naturally as you d be talking about something you really care about c networking once you re done with a figured out b networking is what will actually help you get there if you don t talk to people you miss out on hearing about many opportunities that you might have a good shot at it s important to keep connecting with new people each day if not physically then on linkedin so that upon compounding it after many days you have a large and strong network networking is not messaging people to place a referral for you when i was starting off i did this mistake way too often until i stumbled upon this excellent article by mark meloon where he talks about the importance of building a real connection with people by offering our help first another important step in networking is to get your content out for example if you re good at something blog about it and share that blog on facebook and linkedin not only does this help others it helps you as well once you have a good enough network your visibility increases multi fold you never know how one person from your network liking or commenting on your posts may help you reach out to a much broader audience including people who might be looking for someone of your expertise i m presenting this list in alphabetical order to avoid the misinterpretation of any specific preference however i do place a on the ones that i d personally recommend this recommendation is based on either of the following mission statement people personal interaction or scope of learning more than is purely based on the nd and rd factors your interview begins the moment you have entered the room and a lot of things can happen between that moment and the time when you re asked to introduce yourself your body language and the fact that you re smiling while greeting them plays a big role especially when you re interviewing for a start up as culture fit is something that they extremely care about you need to understand that as much as the interviewer is a stranger to you you re a stranger to him her too so they re probably just as nervous as you are it s important to view the interview as more of a conversation between yourself and the interviewer both of you are looking for a mutual fit you are looking for an awesome place to work at and the interviewer is looking for an awesome person like you to work with so make sure that you re feeling good about yourself and that you take the charge of making the initial moments of your conversation pleasant for them and the easiest way i know how to make that happen is to smile there are mostly two types of interviews one where the interviewer has come with come prepared set of questions and is going to just ask you just that irrespective of your profile and the second where the interview is based on your cv i ll start with the second one this kind of interview generally begins with a can you tell me a bit about yourself at this point things are a big no talking about your gpa in college and talking about your projects in detail an ideal statement should be about a minute or two long should give a good idea on what have you been doing till now and it s not restricted to academics you can talk about your hobbies like reading books playing sports meditation etc basically anything that contributes to defining you the interviewer will then take something that you talk about here as a cue for his next question and then the technical part of the interview begins the motive of this kind of interview is to really check whether whatever you have written on your cv is true or not there would be a lot of questions on what could be done differently or if x was used instead of y what would have happened at this point it s important to know the kind of trade offs that is usually made during implementation for e g if the interviewer says that using a more complex model would have given better results then you might say that you actually had less data to work with and that would have lead to overfitting in one of the interviews i was given a case study to work on and it involved designing algorithms for a real world use case i ve noticed that once i ve been given the green flag to talk about a project the interviewers really like it when i talk about it in the following flow problem or previous approaches our approach result intuition the other kind of interview is really just to test your basic knowledge don t expect those questions to be too hard but they would definitely scratch every bit of the basics that you should be having mainly based around linear algebra probability statistics optimisation machine learning and or deep learning the resources mentioned in the minimal resources you need for preparation section should suffice but make sure that you don t miss out one bit among them the catch here is the amount of time you take to answer those questions since these cover the basics they expect that you should be answering them almost instantly so do your preparation accordingly throughout the process it s important to be confident and honest about what you know and what you don t know if there s a question that you re certain you have no idea about say it upfront rather than making aah um sounds if some concept is really important but you are struggling with answering it the interviewer would generally depending on how you did in the initial parts be happy to give you a hint or guide you towards the right solution it s a big plus if you manage to pick their hints and arrive at the correct solution try to not get nervous and the best way to avoid that is by again smiling now we come to the conclusion of the interview where the interviewer would ask you if you have any questions for them it s really easy to think that your interview is done and just say that you have nothing to ask i know many people who got rejected just because of failing at this last question as i mentioned before it s not only you who is being interviewed you are also looking for a mutual fit with the company itself so it s quite obvious that if you really want to join a place you must have many questions regarding the work culture there or what kind of role are they seeing you in it can be as simple as being curious about the person interviewing you there s always something to learn from everything around you and you should make sure that you leave the interviewer with the impression that you re truly interested in being a part of their team a final question that i ve started asking all my interviewers is for a feedback on what they might want me to improve on this has helped me tremendously and i still remember every feedback that i ve gotten which i ve incorporated into my daily life that s it based on my experience if you re just honest about yourself are competent truly care about the company you re interviewing for and have the right mindset you should have ticked all the right boxes and should be getting a congratulatory mail soon we live in an era full of opportunities and that applies to anything that you love you just need to strive to become the best at it and you will find a way to monetise it as gary vaynerchuk just follow him already says this is a great time to be working in ai and if you re truly passionate about it you have so much that you can do with ai you can empower so many people that have always been under represented we keep nagging about the problems surrounding us but there s been never such a time where common people like us can actually do something about those problems rather than just complaining jeffrey hammerbacher founder cloudera had famously said we can do so much with ai than we can ever imagine there are many extremely challenging problems out there which require incredibly smart people like you to put your head down on and solve you can make many lives better time to let go of what is cool or what would look good think and choose wisely any data science interview comprises of questions mostly of a subset of the following four categories computer science math statistics and machine learning if you re not familiar with the math behind deep learning then you should consider going over my last post for resources to understand them however if you are comfortable i ve found that the chapters and of the deep learning book are enough to prepare revise for theoretical questions during such interviews i ve been preparing summaries for a few chapters which you can refer to where i ve tried to even explain a few concepts that i found challenging to understand at first in case you are not willing to go through the entire chapters and if you ve already done a course on probability you should be comfortable answering a few numerical as well for stats covering these topics should be enough now the range of questions here can vary depending on the type of position you are applying for if it s a more traditional machine learning based interview where they want to check your basic knowledge in ml you can complete any one of the following courses machine learning by andrew ng cs machine learning course by caltech professor yaser abu mostafa important topics are supervised learning classification regression svm decision tree random forests logistic regression multi layer perceptron parameter estimation bayes decision rule unsupervised learning k means clustering gaussian mixture models dimensionality reduction pca now if you re applying for a more advanced position there s a high chance that you might be questioned on deep learning in that case you should be very comfortable with convolutional neural networks cnns and or depending upon what you ve worked on recurrent neural networks rnns and their variants and by being comfortable you must know what is the fundamental idea behind deep learning how cnns rnns actually worked what kind of architectures have been proposed and what has been the motivation behind those architectural changes now there s no shortcut for this either you understand them or you put enough time to understand them for cnns the recommended resource is stanford s cs n and cs n for rnns i found this neural network class by hugo larochelle to be really enlightening too refer this for a quick refresher too udacity coming to the aid here too by now you should have figured out that udacity is a really important place for an ml practitioner there are not a lot of places working on reinforcement learning rl in india and i too am not experienced in rl as of now so that s one thing to add to this post sometime in the future getting placed off campus is a long journey of self realisation i realise that this has been another long post and i m again extremely grateful to you for valuing my thoughts i hope that this post finds a way of being useful to you and that it helped you in some way to prepare for your next data science interview better if it did i request you to really think about what i talk about in what we should strive to work for i m very thankful to my friends from iit guwahati for their helpful feedback especially ameya godbole kothapalli vignesh and prabal jain a majority of what i mention here like viewing an interview as a conversation and seeking feedback from our interviewers arose from multiple discussions with prabal who has been advising me constantly on how i can improve my interviewing skills this story is published in noteworthy where thousands come every day to learn about the people ideas shaping the products we love follow our publication to see more product design stories featured by the journal team from a quick cheer to a standing ovation clap to show how much you enjoyed this story ai fanatic math lover dreamer the official journal blog
Gaurav Kaila,2100,10,https://medium.com/nanonets/how-we-flew-a-drone-to-monitor-construction-projects-in-africa-using-deep-learning-b792f5c9c471?source=---------5----------------,How to easily automate Drone-based monitoring using Deep Learning,this article is a comprehensive overview of using deep learning based object detection methods for aerial imagery via drones did you know drones and it s associated functions are set to be a billion industry by currently drones being used in domains such as agriculture construction public safety and security to name a few and are rapidly being adopted by others with deep learning based computer vision now powering these drones industry experts are now predicting unprecedented use in previously unimaginable or infeasible applications we explore some of these applications along with challenges in automation of drone based monitoring through deep learning finally a case study is presented for automating remote inspection of construction projects in africa using nanonets machine learning framework man has always been feed fascinated with the view of the world from the top building watch towers high fortwalls capturing the highest mountain peak to capture a glimpse and share it with the world people went to great lengths to defy gravity enlisting the help of ladders tall buildings kites balloons planes and rockets today access to drones that can fly as high as kms is possible even for the general public these drones have high resolution cameras attached to them that are capable of acquiring quality images which can be used for various kinds of analysis with easier access to drones we re seeing a lot of interest and activity by photographers hobbyists who are using it to make creative projects such as capturing inequality in south africa or breathtaking views of new york which might make woody allen proud we explore some here energy inspection of solar farms routine inspection and maintenance is a herculean task for solar farms the traditional manual inspection method can only support the inspection frequency of once in three months because of the hostile environment solar panels may have defects broken solar panel units reduce the power output efficiency agriculture early plant disease detection researchers at imperial college london is mounting multi spectral cameras on drones that will use special filters to capture reflected light from selected regions of the electromagnetic spectrum stressed plants typically display a spectral signature that distinguishes them from healthy plants public safety shark detection analysis of overhead view of a large mass of land water can yield a vast amount of information in terms of security and public safety one such example is spotting sharks in the water off the coast of australia australia based westpac group has developed a deep learning based object detection system to detect sharks in the water there are various other applications to aerial images such as civil engineering routine bridge inspections power line surveillance and traffic surveying oil and gas on offshore inspection of oil and gas platforms drilling rigs public safety motor vehicle accidents nuclear accidents structural fires ship collisions plane and train crashes security traffic surveillance border surveillance coastal surveillance controlling hostile demonstrations and rioting to comprehensively capture terrain landscapes the process of acquiring aerial images can be summarised in two steps after image stitching the generated map can be used for various kinds of analysis for the applications mentioned above high resolution aerial imagery is increasingly available at the global scale and contains an abundance of information about features of interest that could be correlated with maintenance land development disease control defect localisation surveillance etc unfortunately such data are highly unstructured and thus challenging to extract meaningful insights from at scale even with intensive manual analysis for eg classification of urban land use is typically based on surveys performed by trained professionals as such this task is labor intensive infrequent slow and costly as a result such data are mostly available in developed countries and big cities that have the resources and the vision necessary to collect and curate it another motivation for automating the analysis of aerial imagery stems from the urgency of predicting changes in the region of interest for eg crowd counting and crowd behaviour is frequently done during large public gatherings such as concerts football matches protests etc traditionally a human is behind the analysis of images being streamed from a cctv camera directly to the command centre as you may imagine there are several problems with this approach such as human latency or error in detecting an event and lack of sufficient views via standard static cctv cameras below are some of the commonly occurring challenges when using aerial imagery there are several challenges to overcome when automating the analysis of drone imagery following lists a few of them with a prospective solution pragmatic master a south african robotics as a service collaborated with nanonets for automation of remotely monitoring progress of a housing construction project in africa we aim to detect the following infrastructure to capture the construction progress of a house in it s various stages a foundation start wallplate in progress roof partially complete apron finishing touches and geyser ready to move in pragmatic master chose nanonets as it s deep learning provider because of it s easy to use web platform and plug play apis the end to end process of using the nanonets api is as simple as four steps labelling of images labelling images is probably the hardest and the most time consuming step in any supervised machine learning pipeline but at nanonets we have this covered for you we have in house experts that have multiple years of working with aerial images they will annotate your images with high precision and accuracy to aid better model training for the pragmatic master use case we were labelling the following objects and their total count in all the images model training at nanonets we employ the principle of transfer learning while training on your images this involves re training a pre trained model that has already been pre trained with a large number of aerial images this helps the model identify micro patterns such as edges lines and contours easily on your images and focus on the more specific macro patterns such as houses trees humans cars etc transfer learning also gives a boost in term of training time as the model does not need to be trained for a large number of iterations to give a good performance our proprietary deep learning software smartly selects the best model along with optimising the hyper parameters for your use case this involves searching through multiple models and through a hyperspace of parameters using advanced search algorithms the hardest objects to detect are the smallest ones due to their low resolution our model training strategy is optimised to detect very small objects such as geysers and aprons which have an area of a few pixels following are the mean average precision per class that we get roof geyser wallplate apron note adding more images can lead to an increase in the mean average precision our api also supports detecting multiple objects in the same image such as roofs and aprons in one image test integrate once the model is trained you can either integrate nanonet s api directly into your system or we also provide a docker image with the trained model and inference code that you can use docker images can easily scale and provide a fault tolerant inference system customer trust is our top priority we are committed towards providing you ownership and control over your content at all times we provide two plans for using our service for both the plans we use highly sophisticated data privacy and security protocols in collaboration with amazon web services which is our cloud partner your dataset is anonymised and goes through minimal human intervention during the pre processing and training process all our human labellers have signed a non disclosure agreement nda to protect your data from going into wrong hands as we believe in the philosophy of your data is yours you can request us to delete your data from our servers at any stage nanonets is a web service that makes it easy to use deep learning you can build a model with your own data to achieve high accuracy use our apis to integrate the same in your application pragmatic master is a south african robotics as a service company that provides camera mounted drones to acquire images of construction farming and mining sites these images are analysed to track progress identify challenges eliminate inefficiencies and provide an overall aerial view of the site from a quick cheer to a standing ovation clap to show how much you enjoyed this story machine learning engineer nanonets machine learning api
James Loy,8500,6,https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6?source=---------6----------------,How to build your own Neural Network from scratch in Python,motivation as part of my personal journey to gain a better understanding of deep learning i ve decided to build a neural network from scratch without a deep learning library like tensorflow i believe that understanding the inner workings of a neural network is important to any aspiring data scientist this article contains what i ve learned and hopefully it ll be useful for you as well most introductory texts to neural networks brings up brain analogies when describing them without delving into brain analogies i find it easier to simply describe neural networks as a mathematical function that maps a given input to a desired output neural networks consist of the following components the diagram below shows the architecture of a layer neural network note that the input layer is typically excluded when counting the number of layers in a neural network creating a neural network class in python is easy training the neural network the output y of a simple layer neural network is you might notice that in the equation above the weights w and the biases b are the only variables that affects the output y naturally the right values for the weights and biases determines the strength of the predictions the process of fine tuning the weights and biases from the input data is known as training the neural network each iteration of the training process consists of the following steps the sequential graph below illustrates the process as we ve seen in the sequential graph above feedforward is just simple calculus and for a basic layer neural network the output of the neural network is let s add a feedforward function in our python code to do exactly that note that for simplicity we have assumed the biases to be however we still need a way to evaluate the goodness of our predictions i e how far off are our predictions the loss function allows us to do exactly that there are many available loss functions and the nature of our problem should dictate our choice of loss function in this tutorial we ll use a simple sum of sqaures error as our loss function that is the sum of squares error is simply the sum of the difference between each predicted value and the actual value the difference is squared so that we measure the absolute value of the difference our goal in training is to find the best set of weights and biases that minimizes the loss function now that we ve measured the error of our prediction loss we need to find a way to propagate the error back and to update our weights and biases in order to know the appropriate amount to adjust the weights and biases by we need to know the derivative of the loss function with respect to the weights and biases recall from calculus that the derivative of a function is simply the slope of the function if we have the derivative we can simply update the weights and biases by increasing reducing with it refer to the diagram above this is known as gradient descent however we can t directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function does not contain the weights and biases therefore we need the chain rule to help us calculate it phew that was ugly but it allows us to get what we needed the derivative slope of the loss function with respect to the weights so that we can adjust the weights accordingly now that we have that let s add the backpropagation function into our python code for a deeper understanding of the application of calculus and the chain rule in backpropagation i strongly recommend this tutorial by blue brown now that we have our complete python code for doing feedforward and backpropagation let s apply our neural network on an example and see how well it does our neural network should learn the ideal set of weights to represent this function note that it isn t exactly trivial for us to work out the weights just by inspection alone let s train the neural network for iterations and see what happens looking at the loss per iteration graph below we can clearly see the loss monotonically decreasing towards a minimum this is consistent with the gradient descent algorithm that we ve discussed earlier let s look at the final prediction output from the neural network after iterations we did it our feedforward and backpropagation algorithm trained the neural network successfully and the predictions converged on the true values note that there s a slight difference between the predictions and the actual values this is desirable as it prevents overfitting and allows the neural network to generalize better to unseen data fortunately for us our journey isn t over there s still much to learn about neural networks and deep learning for example i ll be writing more on these topics soon so do follow me on medium and keep and eye out for them i ve certainly learnt a lot writing my own neural network from scratch although deep learning libraries such as tensorflow and keras makes it easy to build deep nets without fully understanding the inner workings of a neural network i find that it s beneficial for aspiring data scientist to gain a deeper understanding of neural networks this exercise has been a great investment of my time and i hope that it ll be useful for you as well from a quick cheer to a standing ovation clap to show how much you enjoyed this story graduate student in machine learning georgia tech linkedin https www linkedin com in jamesloy sharing concepts ideas and codes
Chintan Trivedi,1200,8,https://towardsdatascience.com/using-deep-q-learning-in-fifa-18-to-perfect-the-art-of-free-kicks-f2e4e979ee66?source=---------7----------------,Using Deep Q-Learning in FIFA 18 to perfect the art of free-kicks,a code tutorial in tensorflow that uses reinforcement learning to take free kicks in my previous article i presented an ai bot trained to play the game of fifa using supervised learning technique with this approach the bot quickly learnt the basics of the game like passing and shooting however the training data required to improve it further quickly became cumbersome to gather and provided little to no improvements making this approach very time consuming for this sake i decided to switch to reinforcement learning as suggested by almost everyone who commented on that article in this article i ll provide a short description of what reinforcement learning is and how i applied it to this game a big challenge in implementing this is that we do not have access to the game s code so we can only make use of what we see on the game screen due to this reason i was unable to train the ai on the full game but could find a work around to implement it for skill games in practice mode for this tutorial i will be trying to teach the bot to take yard free kicks but you can modify it to play other skill games as well let s start with understanding the reinforcement learning technique and how we can formulate our free kick problem to fit this technique contrary to supervised learning we do not need to manually label the training data in reinforcement learning instead we interact with our environment and observe the outcome of our interaction we repeat this process multiple times gaining examples of positive and negative experiences which acts as our training data thus we learn by experimentation and not imitation let s say our environment is in a particular state s and upon taking an action a it changes to state s for this particular action the immediate reward you observe in the environment is r any set of actions that follow this action will have their own immediate rewards until you stop interacting due to a positive or a negative experience these are called future rewards thus for the current state s we will try to estimate out of all actions possible which action will fetch us the maximum immediate future reward denoted by q s a called the q function this gives us q s a r q s a which denotes the expected final reward by taking action a in state s here is a discount factor to account for uncertainty in predicting the future thus we want to trust the present a bit more than the future deep q learning is a special type of reinforcement learning technique where the q function is learnt by a deep neural network given the environment s state as an image input to this network it tries to predict the expected final reward for all possible actions like a regression problem the action with the maximum predicted q value is chosen as our action to be taken in the environment hence the name deep q learning note if we had a performance meter in kick off mode of fifa like there is in the practice mode we might have been able to formulate this problem for playing the entire game and not restrict ourselves to just taking free kicks that or we need access to game s internal code which we don t have anyways let s make the most of what we do have while the bot has not mastered all different kinds of free kicks it has learnt some situations very well it almost always hits the target in absence of wall of players but struggles in its presence also when it hasn t encountered a situation frequently in training like not facing the goal it behaves bonkers however with every training epoch this behavior was noticed to decrease on an average as shown in the figure above the average goal scoring rate grows from to on an average after training for epochs this means the current bot scores about half of the free kicks it attempts for reference a human would average around do consider that fifa tends to behave non deterministically which makes learning very difficult more results in video format can be found on my youtube channel with the video embedded below please subscribe to my channel if you wish to keep track of all my projects we shall implement this in python using tools like tensorflow keras for deep learning and pytesseract for ocr the git link is provided below with the requirements setup instructions in the repository description i would recommend below gists of code only for the purpose of understanding this tutorial since some lines have been removed for brevity please use the full code from git while running it let s go over the main parts of the code we do not have any readymade api available that gives us access to the code so let s make our own api instead we ll use game s screenshots to observe the state simulated key presses to take action in the game environment and optical character recognition to read our reward in the game we have three main methods in our fifa class observe act get reward and an additional method is over to check if the free kick has been taken or not throughout the training process we want to store all our experiences and observed rewards we will use this as the training data for our q learning model so for every action we take we store the experience s a r s along with a game over flag the target label that our model will try to learn is the final reward for each action which is a real number for our regression problem now that we can interact with the game and store our interactions in memory let s start training our q learning model for this we will attain a balance between exploration taking a random action in the game and exploitation taking action predicted by our model this way we can perform trial and error to obtain different experiences in the game the parameter epsilon is used for this purpose which is an exponentially decreasing factor that balances exploration and exploitation in the beginning when we know nothing we want to do more exploration but as number of epochs increases and we learn more we want to do more exploitation and less exploration hence the decaying value of the epsilon parameter for this tutorial i have only trained the model for epochs due to time and performance constraints but in the future i would like to push it to at least epochs at the heart of the q learning process is a layered dense fully connected network with relu activation it takes the dimensional feature map as input state and outputs q values for each possible action the action with the maximum predicted q value is the desired action to be taken as per the network s policy for the given state this is the starting point of execution of this code but you ll have to make sure the game fifa is running in windowed mode on a second display and you load up the free kick practice mode under skill games shooting menu make sure the game controls are in sync with the keys you have hard coded in the fifa py script overall i think the results are quite satisfactory even though it fails to reach human level of performance switching from supervised to reinforcement technique for learning helps ease the pain of collecting training data given enough time to explore it performs very well in problems like learning how to play simple games however reinforcement setting seems to fail when it encounters unfamiliar situations which makes me believe formulating it as a regression problem cannot extrapolate information as well as formulating it as a classification problem in supervised setting perhaps a combination of the two could address the weaknesses of both these approaches maybe that s where we ll see the best results in building ai for games something for me to try in the future i would like to acknowledge this tutorial of deep q learning and this git repository of gaming with python for providing majority of the code with the exception of the fifa custom api most of the code s backbone has come from these sources thanks to these guys thank you for reading if you liked this tutorial please follow me on medium github or subscribe to my youtube channel from a quick cheer to a standing ovation clap to show how much you enjoyed this story data scientist ai enthusiast blogger youtuber chelsea fc fanatic also looking to build my virtual clone before i die sharing concepts ideas and codes
Abhishek Parbhakar,1700,3,https://towardsdatascience.com/why-data-scientists-love-gaussian-6e7a7b726859?source=---------8----------------,Why Data Scientists love Gaussian? – Towards Data Science,for deep learning machine learning engineers out of all the probabilistic models in the world gaussian distribution model simply stands out even if you have never worked on an ai project there is a significant chance that you have come across the gaussian model gaussian distribution model often identified with its iconic bell shaped curve also referred as normal distribution is so popular mainly because of three reasons incredible number of processes in nature and social sciences naturally follows the gaussian distribution even when they don t the gaussian gives the best model approximation for these processes some examples include central limit theorem states that when we add large number of independent random variables irrespective of the original distribution of these variables their normalized sum tends towards a gaussian distribution for example the distribution of total distance covered in an random walk tends towards a gaussian probability distribution the theorem s implications include that large number of scientific and statistical methods that have been developed specifically for gaussian models can also be applied to wide range of problems that may involve any other types of distributions the theorem can also been seen as a explanation why many natural phenomena follow gaussian distribution unlike many other distribution that changes their nature on transformation a gaussian tends to remain a gaussian for every gaussian model approximation there may exist a complex multi parameter distribution that gives better approximation but still gaussian is preferred because it makes the math a lot simpler gaussian distribution is named after great mathematician and physicist carl friedrich gauss from a quick cheer to a standing ovation clap to show how much you enjoyed this story finding equilibria among ai philosophy and economics sharing concepts ideas and codes
Leon Zhou,184,6,https://towardsdatascience.com/the-best-words-cf6fc2333c31?source=---------9----------------,The Best Words – Towards Data Science,uttered in the heat of a campaign rally in south carolina on december this statement was just another of a growing collection of trumpisms by our now president donald j trump these statements both made donald more beloved by his supporters as their relatable president while also a cause of ridicule by seemingly everyone else regardless of one s personal views of the man it cannot be denied donald has a way of speaking that is well so uniquely him his smatterings of superlatives and apparent disregard for the constraints of traditional sentence structure are just a few of the things that make his speech instantly recognizable from that of his predecessors or peers it was this unique style that interested me and i set out to try and capture it using machine learning to generate text that looked and sounded like something donald trump might say to learn president trump s style i first had to gather sufficient examples of it i focused my efforts on two primary sources the obvious first place to look for words by donald trump was his twitter feed the current president is unique in his use of the platform as a direct and unfiltered connection to the american people furthermore as a figure of interest his words have naturally been collected and organized for posterity saving me the hassle of using the ever changing and restrictive twitter api all in all there were a little under tweets available for my use in addition to his online persona however i also wanted to gain a glimpse into his more formal role as president for this i turned to the white house briefing statements archive with the help of some python tools i was able to quickly amass a table of about transcripts of speeches and other remarks by the president these transcripts covered a variety of events such as meetings with foreign dignitaries round tables with congressional members and awards presentations unlike with the tweets where every word was written or dictated by trump himself these transcripts involved other politicians and inquisitive reporters separating donald s words from those of others seemed to be a daunting task enter regular expressions a boring name for a powerful and decidedly not boring tool regular expressions allow you to specify a pattern to search for this pattern can contain any number of very specific constraints wildcards or other restrictions to return exactly what you want and no more with some trial and error i was able to generate a complex regular expression to only return words the president spoke leaving and discarding any other words or annotations typically one of the first steps in working with text is to normalize it the extent and complexity of this normalization varies according to one s needs ranging from simply removing punctuation or capital letters to reducing all variants of a word to a base root an example of this workflow can be seen here for me however the specific idiosyncrasies and patterns that would be lost in normalization were exactly what i needed to preserve so in hopes of making my generated text just that much more believable and authentic i elected to bypass most of the standard normalization workflow before diving into a deep learning model i was curious to explore another frequently used text generation method the markov chain markov chains have been the go to for joke text generation for a long time a quick search will reveal ones for star trek past presidents the simpsons and many others the quick and dirty of the markov chain is that it only cares about the current word in determining what should come next this algorithm looks at every single time a specific word appears and every word that comes immediately after it the next word is selected randomly with a probability proportional to its frequency let me illustrate with a quick example donald trump says the word taxes if in real life of the time after he says taxes he follows up with the word bigly the markov chain will choose the next word to be bigly of the time but sometimes he doesn t say bigly sometimes he ends the sentence or moves on to a different word the chain will most likely choose bigly but there s a chance it ll go for any of the other available options thus introducing some variety in our generated text and repeat ad nauseam or until the end of the sentence this is great for quick and dirty applications but it s easy to see where it can go wrong as the markov chain only ever cares about the current word it can easily be sidetracked a sentence that started off talking about the domestic economy could just as easily end talking about the apprentice with my limited text data set most of my markov chain outputs were nonsensical but occasionally there were some flashes of brilliance and hilarity for passably real text however i needed something more sophisticated recurrent neural networks rnns have established themselves as the architecture of choice for many text or sequence based applications the detailed inner workings of rnns are outside the scope of this post but a strong relatively beginner friendly introduction may be found here the distinguishing feature of these neural units is that they have an internal memory of sorts word choice and grammar depend heavily on surrounding context so this memory is extremely useful in creating a coherent thought by keeping track of tense subjects and objects and so on the downside of these types of networks is that they are extraordinarily computationally expensive on my piddly laptop running the entirety of my text through the model once would take over an hour and considering i d need to do so about times this was no good this is where cloud computing comes in a number of established tech companies offer cloud services the largest being amazon google and microsoft on a heavy gpu computing instance that one hour plus per cycle time became ninety seconds an over x reduction in time can you tell if this following statement is real or not this was text generated off of trump s endorsement of the republican gubernatorial candidate but it might pass as something that trump tweeted in the run up to the general election the more complex neural networks i implemented with hidden fully connected layers before and after the recurrent layer were capable of generating internally consistent text given any seed of characters or less less complex networks stumbled a little on consistency but still captured the tonal feel of president trump s speech while not quite producing text at a level capable of fooling you or me consistently this attempt opened my eyes to the power of rnns in short order these networks learned spelling some aspects of grammar and in some instances how to use hashtags and hyperlinks imagine what a better designed network with more text to learn from and time to learn might produce if you re interested in looking at the code behind these models you can find the repository here and don t hesitate to reach out with any questions or feedback you may have from a quick cheer to a standing ovation clap to show how much you enjoyed this story i am a data scientist with a background in chemical engineering and biotech i am also homeless and live in my car but that s another thing entirely hire me sharing concepts ideas and codes
Dr. GP Pulipaka,2,6,https://medium.com/@gp_pulipaka/3-ways-to-apply-latent-semantic-analysis-on-large-corpus-text-on-macos-terminal-jupyterlab-colab-7b4dc3e1622?source=---------3----------------,"3 Ways to Apply Latent Semantic Analysis on Large-Corpus Text on macOS Terminal, JupyterLab, and...",latent semantic analysis works on large scale datasets to generate representations to discover the insights through natural language processing there are different approaches to perform the latent semantic analysis at multiple levels such as document level phrase level and sentence level primarily semantic analysis can be summarized into lexical semantics and the study of combining individual words into paragraphs or sentences the lexical semantics classifies and decomposes the lexical items applying lexical semantic structures has different contexts to identify the differences and similarities between the words a generic term in a paragraph or a sentence is hypernym and hyponymy provides the meaning of the relationship between instances of the hyponyms homonyms contain similar syntax or similar spelling with similar structuring with different meanings homonyms are not related to each other book is an example for homonym it can mean for someone to read something or an act of making a reservation with similar spelling form and syntax however the definition is different polysemy is another phenomenon of the words where a single word could be associated with multiple related senses and distinct meanings the word polysemy is a greek word which means many signs python provides nltk library to perform tokenization of the words by chopping the words in larger chunks into phrases or meaningful strings processing words through tokenization produce tokens word lemmatization converts words from the current inflected form into the base form latent semantic analysis applying latent semantic analysis on large datasets of text and documents represents the contextual meaning through mathematical and statistical computation methods on large corpus of text many times latent semantic analysis overtook human scores and subject matter tests conducted by humans the accuracy of latent semantic analysis is high as it reads through machine readable documents and texts at a web scale latent semantic analysis is a technique that applies singular value decomposition and principal component analysis pca the document can be represented with z x y matrix a the rows of the matrix represent the document in the collection the matrix a can represent numerous hundred thousands of rows and columns on a typical large corpus text document applying singular value decomposition develops a set of operations dubbed matrix decomposition natural language processing in python with nltk library applies a low rank approximation to the term document matrix later the low rank approximation aids in indexing and retrieving the document known as latent semantic indexing by clustering the number of words in the document brief overview of linear algebra the a with z x y matrix contains the real valued entries with non negative values for the term document matrix determining the rank of the matrix comes with the number of linearly independent columns or rows in the the matrix the rank of a z y a square c x c represented as diagonal matrix where off diagonal entries are zero examining the matrix if all the c diagonal matrices are one the identity matrix of the dimension c represented by ic for the square z x z matrix a with a vector k which contains not all zeroes for the matrix decomposition applies on the square matrix factored into the product of matrices from eigenvectors this allows to reduce the dimensionality of the words from multi dimensions to two dimensions to view on the plot the dimensionality reduction techniques with principal component analysis and singular value decomposition holds critical relevance in natural language processing the zipfian nature of the frequency of the words in a document makes it difficult to determine the similarity of the words in a static stage hence eigen decomposition is a by product of singular value decomposition as the input of the document is highly asymmetrical the latent semantic analysis is a particular technique in semantic space to parse through the document and identify the words with polysemy with nlkt library the resources such as punkt and wordnet have to be downloaded from nltk deep learning at scale with google colab notebooks training machine learning or deep learning models on cpus could take hours and could be pretty expensive in terms of the programming language efficiency with time and energy of the computer resources google built colab notebooks environment for research and development purposes it runs entirely on the cloud without requiring any additional hardware or software setup for each machine it s entirely equivalent of a jupyter notebook that aids the data scientists to share the colab notebooks by storing on google drive just like any other google sheets or documents in a collaborative environment there are no additional costs associated with enabling gpu at runtime for acceleration on the runtime there are some challenges of uploading the data into colab unlike jupyter notebook that can access the data directly from the local directory of the machine in colab there are multiple options to upload the files from the local file system or a drive can be mounted to load the data through drive fuse wrapper once this step is complete it shows the following log without errors the next step would be generating the authentication tokens to authenticate the google credentials for the drive and colab if it shows successful retrieval of access token then colab is all set at this stage the drive is not mounted yet it will show false when accessing the contents of the text file once the drive is mounted colab has access to the datasets from google drive once the files are accessible the python can be executed similar to executing in jupyter environment colab notebook also displays the results similar to what we see on jupyter notebook pycharm ide the program can be run compiled on pycharm ide environment and run on pycharm or can be executed from osx terminal results from osx terminal jupyter notebook on standalone machine jupyter notebook gives a similar output running the latent semantic analysis on the local machine references gorrell g generalized hebbian algorithm for incremental singular value decomposition in natural language processing retrieved from https www aclweb org anthology e hardeniya n natural language processing python and nltk birmingham england packt publishing landauer t k foltz p w laham d university of colorado at boulder an introduction to latent semantic analysis retrieved from http lsa colorado edu papers dp lsaintro pdf stackoverflow mounting google drive on google colab retrieved from https stackoverflow com questions mounting google drive on google colab stanford university matrix decompositions and latent semantic indexing retrieved from https nlp stanford edu ir book html htmledition matrix decompositions and latent semantic indexing html from a quick cheer to a standing ovation clap to show how much you enjoyed this story ganapathi pulipaka founder and ceo deepsingularity bestselling author big data iot startups sap machinelearning deeplearning datascience
Erick Muzart Fonseca dos Santos,16,2,https://medium.com/deeplearningbrasilia/o-grupo-de-estudo-em-deep-learning-de-bras%C3%ADlia-est%C3%A1-planejando-o-pr%C3%B3ximo-ciclo-de-encontros-do-4861851ec0ff?source=---------5----------------,O Grupo de Estudo em Deep Learning de Brasília está planejando o próximo ciclo de encontros do...,o grupo de estudo em deep learning de brasi lia esta planejando o pro ximo ciclo de encontros do grupo que deve iniciar se a partir do meio de junho de ainda ha tempo para manifestar suas prefere ncias para participar do grupo para tal favor preencher o seguinte questiona rio para que possamos agregar as prefere ncias de nossa comunidade e selecionar as opc o es que melhor atenderem a todos https goo gl forms h k sd dxw diit agradecemos se puder divulgar o grupo junto a sua rede de contatos com interesse nos temas de aprendizado automa tico e deep learning para que possamos iniciar o pro ximo ciclo ja com o ma ximo de interessados desde o primeiro dia seguem abaixo alguns dos resultados iniciais do grupo quanto aos resultados inciais do questiona rio segue uma si ntese das primeiras respostas dentre os to picos de mais interesse destacam se o deep learning o machine learning o aplicac o es de deep learning em projetos o processamento de linguagem natural prefere ncia por curso o machine learning da fast ai o deep learning parte da fast ai o deep learning parte da fast ai atenciosamente organizac a o do grupo de estudo em deep learning de brasi lia from a quick cheer to a standing ovation clap to show how much you enjoyed this story publicac o es dos membros do grupo de estudo em deep learning de brasi lia
Chris Kalahiki,30,15,https://towardsdatascience.com/beethoven-picasso-and-artificial-intelligence-caf644fc72f9?source=---------6----------------,"Beethoven, Picasso, and Artificial Intelligence – Towards Data Science",when people think of the greatest artists who ve ever lived they probably think of names like beethoven or picasso no one would ever think of a computer as a great artist but what if one day that was indeed the case could computers learn to create incredible drawings like the mona lisa perhaps one day a robot will be capable of composing the next great symphony some experts believe this to be the case in fact some of the greatest minds in artificial intelligence are diligently working to develop programs that can create drawing and music independently from humans the use of artificial intelligence in the field of art has even been picked up by tech giants the likes of google the projects that are included in this paper could have drastic implications in our everyday lives they may also change the way we view art they also showcase the incredible advancement that has been made in the field of artificial intelligence image recognition is not as far as the research goes nor is the ability to generate music in the styling of the great artists of our past although these topics will be touched upon we will focus on several more advanced achievements such as text descriptions being turned into images and generating art and music that is totally original each of these projects bring something new and innovative to the table and show us exactly how the art space is a great place to further explore applications of artificial intelligence we will be discussing problems that have been faced in these projects and how they have been overcome the future of ai looks bright let s look at what the future may hold in doing this we may be able to better understand the impact that artificial intelligence can have in an area that is driven by human creativity machines must be educated they learn from instruction how do we lead machines away from emulating what already exists and have them create new techniques no creative artist will create art today that tries to emulate the baroque or impressionist style or any other traditional style unless trying to do so ironically this problem isn t limited to paintings either music can be very structured in some respects but is also a form of art that requires vast creativity so how do we go about solving such a problem the first concept we will discuss is something called gan generative adversarial networks gans although quite complex are becoming an outdated model if artificial intelligence in the art space is to advance researchers and developers will have to work to find better methods to allow machines to generate art and music two of these such methods are presented in the form of sketch rnn and can creative adversarial networks each of these methods have their advantages over gans first let s explore what exactly a gan is below is a small excerpt explaining how a gan works generative adversarial network gan has two sub networks a generator and a discriminator the discriminator has access to a set of images training images the discriminator tries to discriminate between real images from the training set and fake images generated by the generator the generator tries to generate images similar to the training set without seeing the images the more images the generator creates the closer they get to the images from the training set the idea is that after a certain number of images are generated the gan will create images that are very similar to what we consider art this is a very impressive accomplishment to say the least but what if we take it a step further many issues associated with the gan are simply limitations on what it can do the gan is powerful but can t do quite as much as we would like for example the generator in the model described above will continue to create images closer and closer to the images given to the discriminator that it isn t producing original art could a gan be trained to draw alongside a user it s not likely the model wouldn t be able to turn a text based description of an image into an actual picture either as impressive as the gan may be we would all agree that it can be improved each of the shortcoming mentioned have actually been addressed and to an extent solved let s look at how this is done sketch rnn is a recurrent neural network model developed by google the goal of sketch rnn is to help machines learn to create art in a manner similar to the way a human may learn it has been used in a google ai experiment to be able to sketch alongside a user while doing so it can provide the users with suggestions and even complete the user s sketch when they decide to take a break sketch rnn is exposed to a massive number of sketches provided through a dataset of vector drawings obtained through another google application that we will discuss later each of these sketches are tagged to let the program know what object is in the sketch the data set represents the sketch as a set of pen strokes this allows sketch rnn to then learn what aspects each sketch of a certain object has in common if a user begins to draw a cat sketch rnn could then show the user other common features that could be on the cat this model could have many new creative applications the decoder only model trained on various classes can assist the creative process of an artist by suggesting many possible ways of finishing a sketch the sketch rnn team even believes that given a more complex dataset the applications could be used in an educational sense to teach users how to draw these applications of sketch rnn couldn t be nearly as easily achieved with gan alone another method used to improve upon gan is the creative adversarial network in their paper regarding adversarial networks generating art several researchers discuss a new way of generating art through cans the idea is that the can has two adversary networks one the generator has no access to any art it has no basis to go off of when generating images the other network the discriminator is trained to classify the images generated as being art or not when an image is generated the discriminator gives the generator two pieces of information the first is whether it believes the generated image comes from the same distributor as the pieces of art it was trained on and the other being how the discriminator can fit the generated image into one of the categories of art it was taught this technique is fantastic in that it helps the generator create images that are both emulative of past works of art in the sense that it learns what was good about those images and creative in a sense that it is taught to produce new and different artistic concepts this is a big difference from gan creating art that emulated the training images eventually the can will learn how to produce only new and innovative artwork one final future for the vanilla gan is stackgan stackgan is a text to photo realistic image synthesizer that uses stacked generative adversarial networks given a text description the stackgan is able to create images that are very much related to the given text this wouldn t be doable with a normal gan model as it would be much too difficult to generate photo realistic images from a text description even with a state of the art training database this is where stackgan comes in it breaks the problem down into parts low resolution images are generated by our stage i gan on the top of our stage i gan we stack stage ii gan to generate realistic high resolution images conditioned on stage i results and text descriptions it is through the conditioning on stage i results and text descriptions that stage ii gan can find details that stage i gan may have missed and create higher resolution images by breaking the problem down into smaller subproblems the stackgan can tackle problems that aren t possible with a regular gan on the next page is an image showing the difference between a regular gan and each step of the stackgan it is through advancements like these that have been made in recent years that we can continue to push the boundaries of what ai can do we have just seen three ways to improve upon a concept that was already quite complex and innovative each of these advancements have a practical everyday use as we continue to improve on artificial intelligence techniques we will able to do more and more in regard to not just art and music but a wide variety of tasks to improve our lives images aren t the only type of art that artificial intelligence can impact though its effect on music is being explored as we speak we will now explore some specific cases and their impact on both music and artificial intelligence in doing this we should be able to see how art can do as much for ai as ai does for it both fields benefit heavily from the types of projects that we are exploring here could a machine ever be able to create a piece of music the likes of johann sebastian bach in a project known as deepbach several researchers looked to create pieces similar to bach s chorales the beauty of deepbach is that it is able to generate coherent musical phrases and provides for instance varied reharmonizations of melodies without plagiarism what this means it that deepbach can create music with correct structure and be original it is just in the style of bach it isn t just a mashup of his works deepbach is creating new content the developers of deepbach went on to test whether their product could actually fool listeners as part of the experiment over people were asked to vote whether pieces presented to them were in fact composed by bach the subjects had varying degrees of musical expertise the results showed that as the model for deepbach s complexity increased the subjects had more and more trouble distinguishing the chorales of bach from those of deepbach this experiment shows us that through the use of artificial intelligence and machine learning it is quite possible to recreate original works in the likeness of the greats but is that the limit to what artificial intelligence can do in the field of art and music deepbach has achieved something that would have been unheard of in the not so distant past but it certainly isn t the fullest extent of what ai can do to benefit the field of music what if we want to create new and innovative music maybe ai can change the way music is created all together there must be projects that do more to push the envelope as a matter of fact that is exactly what the team behind magenta look to do magenta is a project being conducted by the google brain team and lead by douglas eck eck has been working for google since but that isn t where his interest in music began eck helped found brain music and sound an international laboratory for brain music and sound research he was also involved at the mcgill centre for interdisciplinary research in music media and technology and was an associate professor in computer science at the university of montreal magenta s goal is to be a research project to advance the state of the art in machine intelligence for music and art generation it is an open source project that uses tensorflow magenta aims to learn how to generate art and music in a way that is indeed generative it must go past just emulating existing music this is distinctly different that projects along the line of deepbach which set out to emulate existing music in a way that wasn t plagiarizing existing pieces of music eck and company realize that art is about capturing elements of surprise and drawing attention to certain aspects this leads to perhaps the biggest challenge combining generation attention and surprise to tell a compelling story so much of machine generated music and art is good in small chunks but lacks any sort of long term narrative arc such a perspective gives computer generated music more substance and helps it to become less of a gimmick one of the projects the magenta team has developed is called nsynth the idea behind nsynth is to be able to create new sounds that have never been heard before but beyond that to reimagine how music synthesis can be done unlike ordinary synthesizers that focus on a specific arrangement of oscillators or an algorithm for sample playback such as fm synthesis or granular synthesis nsynth generates sounds on an individual level to do this it uses deep neural networks google has even launched an experiment that allows users to really see what nsynth can do by allowing them to fuse together the sounds of existing instruments to create new hybrid sounds that have never been heard before as an example users can take two instruments such as a banjo and a tuba and take parts of each of their sounds to create a totally new instrument the experiment also allowed users to decide what percentage of each instrument would be used projects like magenta go above and beyond in showing us the full extent of what artificial intelligence can do in the way of generating music they explore new applications of artificial intelligence that can generate new ideas independent of humans it is the closest we have come to machine creativity although machines aren t yet able to truly think and express creativity they may soon be able to generate new and unique art and music for us to enjoy don t worry though eck doesn t intend to replace artists with ai instead he looks to provide artists with tools to create music in an entirely new way as we look ahead to a few more of the ways that ai has been used to accomplish new and innovative ideas in the art space we look at projects like quick draw and deep dream these projects showcase amazing progress in the space while pointing out some issues that researchers in ai will have to work out in the years to come quick draw is an application from the google creative lab trained to recognize quick drawings much like one would see in a game of pictionary the program can recognize simple objects such as cats and apples based on common aspects of the many pictures it was given before although the program will not get every picture right each time it is used it continues to learn from the similarities in the picture drawn and the hundreds of pictures before it the science behind quick draw uses some of the same technology that helps google translate recognize your handwriting to understand handwritings or drawings you don t just look at what the person drew you look at how they actually drew it it is presented in the form of a game with the user drawing a picture of an object chosen by the application the program then has seconds to recognize the image in each session the user is given a total of objects the images are then stored to the database used to train application this happens to be the same database we saw earlier in the sketch rnn application this image recognition is a very practical use of artificial intelligence in the realm of art and music it can do a lot to benefit us in our everyday lives but this only begins to scratch the surface of what artificial intelligence can do in this field although this is very impressive we might point out that the application doesn t truly understand what is being drawn it is just picking up on patterns in fact this distinction is part of the gap between simple ai techniques and true artificial general intelligence machines that truly understand what the objects in images are don t appear to be coming in the near future another interesting project in the art space is google s deep dream project which uses ai to create new and unique images unfortunately the deep dream generator team wouldn t go into too much detail about the technology itself mostly fearing it would be too long for an email they did however explain that convolutional neural networks train on the famous imagenet dataset those neural networks are then used to create art like images essentially deep dream takes the styling of one image and uses it to modify another image the results can be anything from a silly fusion to an artistic masterpiece this occurs when the program identifies the unique stylings of an image provided by the user and imposes those stylings onto another image that the user provides what can easily be observed through the use of deep dream is that computers aren t yet capable of truly understanding what they are doing with respect to art they can be fed complex algorithms to generate images but don t fundamentally understand what it is they are generating for example a computer may see a knife cutting through an onion and assume the knife and onion are one object the lack of an ability to truly understand the contents of an image is one dilemma that researchers have yet to solve perhaps as we continue to make advances in artificial intelligence we will be able to have machines that do truly understand what objects are in an image and even the emotions evoked by their music the only way for this to be achieved is by reaching true artificial general intelligence agi in the meantime the deep dream team believes that generative models will be able to create some really interesting pieces of art and digital content for this section we will consider where artificial intelligence could be heading in the art space we will take a look at how ai has impacted the space and in what ways it can continue to do so we will also look at ways art and music could continue to impact ai in the years to come although i don t feel that we have completely mastered the ability to emulate the great artists of our past it is just a matter of time before that problem is solved the real task to be solved is that of creating new innovations in art and music we need to work towards creation without emulation it is quite clear that we are headed in that direction through projects like can and magenta artificial general intelligence agi is not the only way to complete this task as a matter of fact even those who dispute the possibility of agi would have a hard time disputing the creation of unique works of art by a machine one path that may be taken to further improve art and music through ai is to create more advanced datasets to use in training the complex networks like sketch rnn and deep dream ai needs to be trained to be able to perform as expected that training has a huge impact on the results we get shouldn t we want to train our machines in the most beneficial way possible even developing software like sketch rnn to use the imagenet dataset used in deep dream could be huge in educating artists on techniques for drawing complex realistic images complex datasets could very well be our answer to more efficient training until our machines can think and learn like we do we will need to be very careful what data is used to train them one of the ways that art and music can help to impact ai is by providing another method of turing testing machines for those who dream of creating agi what better way to test the machine s ability that to create something that tests the full extent of human like creativity art is the truest representation of human creativity that is in fact its essence although art is probably not the ultimate end game for artificial intelligence it could be one of the best ways to test the limits of what a machine can do the day that computers can create original musical composition and create images based on descriptions given by a user could very well be the day that we stop being able to distinguish man from machine there are many benefits to using artificial intelligence in the music space some of them have already been seen in the projects we have discussed so far we have seen how artificial intelligence could be used for image recognition as well as their ability to turn our words into fantastic images we have also seen how ai can be used to synthesize new sounds that have never been heard we know that artificial intelligence can be used to create art alongside us as well as independently from us it can be taught to mimic music from the past and can create novel ideas all of these accomplishments are a part of what will drive ai research into the future who knows perhaps one day we will achieve artificial general intelligence and machines will be able to understand what is really in the images it is given maybe our computers will be able to understand how their art makes us feel there is a clear path showing us where to go from here i firmly believe that it is up to us to continue this research and test the limits of what artificial intelligence can do both in the field of art and in our everyday lives from a quick cheer to a standing ovation clap to show how much you enjoyed this story computer science student at louisiana tech university with an interest in anything ai sharing concepts ideas and codes
Adam Geitgey,14200,15,https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721?source=tag_archive---------0----------------,Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in portugue s tie ng vie t or italiano are you tired of reading endless news stories about deep learning and not really knowing what that means let s change that this time we are going to learn how to write programs that recognize objects in images using deep learning in other words we re going to explain the black magic that allows google photos to search your photos based on what is in the picture just like part and part this guide is for anyone who is curious about machine learning but has no idea where to start the goal is be accessible to anyone which means that there s a lot of generalizations and we skip lots of details but who cares if this gets anyone more interested in ml then mission accomplished if you haven t already read part and part read them now you might have seen this famous xkcd comic before the goof is based on the idea that any year old child can recognize a photo of a bird but figuring out how to make a computer recognize objects has puzzled the very best computer scientists for over years in the last few years we ve finally found a good approach to object recognition using deep convolutional neural networks that sounds like a a bunch of made up words from a william gibson sci fi novel but the ideas are totally understandable if you break them down one by one so let s do it let s write a program that can recognize birds before we learn how to recognize pictures of birds let s learn how to recognize something much simpler the handwritten number in part we learned about how neural networks can solve complex problems by chaining together lots of simple neurons we created a small neural network to estimate the price of a house based on how many bedrooms it had how big it was and which neighborhood it was in we also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems so let s modify this same neural network to recognize handwritten text but to make the job really simple we ll only try to recognize one letter the numeral machine learning only works when you have data preferably a lot of data so we need lots and lots of handwritten s to get started luckily researchers created the mnist data set of handwritten numbers for this very purpose mnist provides images of handwritten digits each as an x image here are some s from the data set the neural network we made in part only took in a three numbers as the input bedrooms sq feet etc but now we want to process images with our neural network how in the world do we feed images into a neural network instead of just numbers the answer is incredible simple a neural network takes numbers as input to a computer an image is really just a grid of numbers that represent how dark each pixel is to feed an image into our neural network we simply treat the x pixel image as an array of numbers the handle inputs we ll just enlarge our neural network to have input nodes notice that our neural network also has two outputs now instead of just one the first output will predict the likelihood that the image is an and thee second output will predict the likelihood it isn t an by having a separate output for each type of object we want to recognize we can use a neural network to classify objects into groups our neural network is a lot bigger than last time inputs instead of but any modern computer can handle a neural network with a few hundred nodes without blinking this would even work fine on your cell phone all that s left is to train the neural network with images of s and not s so it learns to tell them apart when we feed in an we ll tell it the probability the image is an is and the probability it s not an is vice versa for the counter example images here s some of our training data we can train this kind of neural network in a few minutes on a modern laptop when it s done we ll have a neural network that can recognize pictures of s with a pretty high accuracy welcome to the world of late s era image recognition it s really neat that simply feeding pixels into a neural network actually worked to build image recognition machine learning is magic right well of course it s not that simple first the good news is that our recognizer really does work well on simple images where the letter is right in the middle of the image but now the really bad news our recognizer totally fails to work when the letter isn t perfectly centered in the image just the slightest position change ruins everything this is because our network only learned the pattern of a perfectly centered it has absolutely no idea what an off center is it knows exactly one pattern and one pattern only that s not very useful in the real world real world problems are never that clean and simple so we need to figure out how to make our neural network work in cases where the isn t perfectly centered we already created a really good program for finding an centered in an image what if we just scan all around the image for possible s in smaller sections one section at a time until we find one this approach called a sliding window it s the brute force solution it works well in some limited cases but it s really inefficient you have to check the same image over and over looking for objects of different sizes we can do better than this when we trained our network we only showed it s that were perfectly centered what if we train it with more data including s in all different positions and sizes all around the image we don t even need to collect new training data we can just write a script to generate new images with the s in all kinds of different positions in the image using this technique we can easily create an endless supply of training data more data makes the problem harder for our neural network to solve but we can compensate for that by making our network bigger and thus able to learn more complicated patterns to make the network bigger we just stack up layer upon layer of nodes we call this a deep neural network because it has more layers than a traditional neural network this idea has been around since the late s but until recently training this large of a neural network was just too slow to be useful but once we figured out how to use d graphics cards which were designed to do matrix multiplication really fast instead of normal computer processors working with large neural networks suddenly became practical in fact the exact same nvidia geforce gtx video card that you use to play overwatch can be used to train neural networks incredibly quickly but even though we can make our neural network really big and train it quickly with a d graphics card that still isn t going to get us all the way to a solution we need to be smarter about how we process images into our neural network think about it it doesn t make sense to train a network to recognize an at the top of a picture separately from training it to recognize an at the bottom of a picture as if those were two totally different objects there should be some way to make the neural network smart enough to know that an anywhere in the picture is the same thing without all that extra training luckily there is as a human you intuitively know that pictures have a hierarchy or conceptual structure consider this picture as a human you instantly recognize the hierarchy in this picture most importantly we recognize the idea of a child no matter what surface the child is on we don t have to re learn the idea of child for every possible surface it could appear on but right now our neural network can t do this it thinks that an in a different part of the image is an entirely different thing it doesn t understand that moving an object around in the picture doesn t make it something different this means it has to re learn the identify of each object in every possible position that sucks we need to give our neural network understanding of translation invariance an is an no matter where in the picture it shows up we ll do this using a process called convolution the idea of convolution is inspired partly by computer science and partly by biology i e mad scientists literally poking cat brains with weird probes to figure out how cats process images instead of feeding entire images into our neural network as one grid of numbers we re going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture here s how it s going to work step by step similar to our sliding window search above let s pass a sliding window over the entire original image and save each result as a separate tiny picture tile by doing this we turned our original image into equally sized tiny image tiles earlier we fed a single image into a neural network to see if it was an we ll do the exact same thing here but we ll do it for each individual image tile however there s one big twist we ll keep the same neural network weights for every single tile in the same original image in other words we are treating every image tile equally if something interesting appears in any given tile we ll mark that tile as interesting we don t want to lose track of the arrangement of the original tiles so we save the result from processing each tile into a grid in the same arrangement as the original image it looks like this in other words we ve started with a large image and we ended with a slightly smaller array that records which sections of our original image were the most interesting the result of step was an array that maps out which parts of the original image are the most interesting but that array is still pretty big to reduce the size of the array we downsample it using an algorithm called max pooling it sounds fancy but it isn t at all we ll just look at each x square of the array and keep the biggest number the idea here is that if we found something interesting in any of the four input tiles that makes up each x grid square we ll just keep the most interesting bit this reduces the size of our array while keeping the most important bits so far we ve reduced a giant image down into a fairly small array guess what that array is just a bunch of numbers so we can use that small array as input into another neural network this final neural network will decide if the image is or isn t a match to differentiate it from the convolution step we call it a fully connected network so from start to finish our whole five step pipeline looks like this our image processing pipeline is a series of steps convolution max pooling and finally a fully connected network when solving problems in the real world these steps can be combined and stacked as many times as you want you can have two three or even ten convolution layers you can throw in max pooling wherever you want to reduce the size of your data the basic idea is to start with a large image and continually boil it down step by step until you finally have a single result the more convolution steps you have the more complicated features your network will be able to learn to recognize for example the first convolution step might learn to recognize sharp edges the second convolution step might recognize beaks using it s knowledge of sharp edges the third step might recognize entire birds using it s knowledge of beaks etc here s what a more realistic deep convolutional network like you would find in a research paper looks like in this case they start a x pixel image apply convolution and max pooling twice apply convolution more times apply max pooling and then have two fully connected layers the end result is that the image is classified into one of categories so how do you know which steps you need to combine to make your image classifier work honestly you have to answer this by doing a lot of experimentation and testing you might have to train networks before you find the optimal structure and parameters for the problem you are solving machine learning involves a lot of trial and error now finally we know enough to write a program that can decide if a picture is a bird or not as always we need some data to get started the free cifar data set contains pictures of birds and pictures of things that are not birds but to get even more data we ll also add in the caltech ucsd birds data set that has another bird pics here s a few of the birds from our combined data set and here s some of the non bird images this data set will work fine for our purposes but low res images is still pretty small for real world applications if you want google level performance you need millions of large images in machine learning having more data is almost always more important that having better algorithms now you know why google is so happy to offer you unlimited photo storage they want your sweet sweet data to build our classifier we ll use tflearn tflearn is a wrapper around google s tensorflow deep learning library that exposes a simplified api it makes building convolutional neural networks as easy as writing a few lines of code to define the layers of our network here s the code to define and train the network if you are training with a good video card with enough ram like an nvidia geforce gtx ti or better this will be done in less than an hour if you are training with a normal cpu it might take a lot longer as it trains the accuracy will increase after the first pass i got accuracy after just passes it was already up to after or so passes it capped out around accuracy and additional training didn t help so i stopped it there congrats our program can now recognize birds in images now that we have a trained neural network we can use it here s a simple script that takes in a single image file and predicts if it is a bird or not but to really see how effective our network is we need to test it with lots of images the data set i created held back images for validation when i ran those images through the network it predicted the correct answer of the time that seems pretty good right well it depends our network claims to be accurate but the devil is in the details that could mean all sorts of different things for example what if of our training images were birds and the other were not birds a program that guessed not a bird every single time would be accurate but it would also be useless we need to look more closely at the numbers than just the overall accuracy to judge how good a classification system really is we need to look closely at how it failed not just the percentage of the time that it failed instead of thinking about our predictions as right and wrong let s break them down into four separate categories using our validation set of images here s how many times our predictions fell into each category why do we break our results down like this because not all mistakes are created equal imagine if we were writing a program to detect cancer from an mri image if we were detecting cancer we d rather have false positives than false negatives false negatives would be the worse possible case that s when the program told someone they definitely didn t have cancer but they actually did instead of just looking at overall accuracy we calculate precision and recall metrics precision and recall metrics give us a clearer picture of how well we did this tells us that of the time we guessed bird we were right but it also tells us that we only found of the actual birds in the data set in other words we might not find every bird but we are pretty sure about it when we do find one now that you know the basics of deep convolutional networks you can try out some of the examples that come with tflearn to get your hands dirty with different neural network architectures it even comes with built in data sets so you don t even have to find your own images you also know enough now to start branching and learning about other areas of machine learning why not learn how to use algorithms to train computers how to play atari games next if you liked this article please consider signing up for my machine learning is fun email list i ll only email you when i have something new and awesome to share it s the best way to find out when i write more articles like this you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part part and part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Adam Geitgey,15200,13,https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78?source=tag_archive---------1----------------,Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning,update this article is part of a series check out the full series part part part part part part part and part you can also read this article in portugue s tie ng vie t or italiano have you noticed that facebook has developed an uncanny ability to recognize your friends in your photographs in the old days facebook used to make you to tag your friends in photos by clicking on them and typing in their name now as soon as you upload a photo facebook tags everyone for you like magic this technology is called face recognition facebook s algorithms are able to recognize your friends faces after they have been tagged only a few times it s pretty amazing technology facebook can recognize faces with accuracy which is pretty much as good as humans can do let s learn how modern face recognition works but just recognizing your friends would be too easy we can push this tech to the limit to solve a more challenging problem telling will ferrell famous actor apart from chad smith famous rock musician so far in part and we ve used machine learning to solve isolated problems that have only one step estimating the price of a house generating new data based on existing data and telling if an image contains a certain object all of those problems can be solved by choosing one machine learning algorithm feeding in data and getting the result but face recognition is really a series of several related problems as a human your brain is wired to do all of this automatically and instantly in fact humans are too good at recognizing faces and end up seeing faces in everyday objects computers are not capable of this kind of high level generalization at least not yet so we have to teach them how to do each step in this process separately we need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step in other words we will chain together several machine learning algorithms let s tackle this problem one step at a time for each step we ll learn about a different machine learning algorithm i m not going to explain every single algorithm completely to keep this from turning into a book but you ll learn the main ideas behind each one and you ll learn how you can build your own facial recognition system in python using openface and dlib the first step in our pipeline is face detection obviously we need to locate the faces in a photograph before we can try to tell them apart if you ve used any camera in the last years you ve probably seen face detection in action face detection is a great feature for cameras when the camera can automatically pick out faces it can make sure that all the faces are in focus before it takes the picture but we ll use it for a different purpose finding the areas of the image we want to pass on to the next step in our pipeline face detection went mainstream in the early s when paul viola and michael jones invented a way to detect faces that was fast enough to run on cheap cameras however much more reliable solutions exist now we re going to use a method invented in called histogram of oriented gradients or just hog for short to find faces in an image we ll start by making our image black and white because we don t need color data to find faces then we ll look at every single pixel in our image one at a time for every single pixel we want to look at the pixels that directly surrounding it our goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it then we want to draw an arrow showing in which direction the image is getting darker if you repeat that process for every single pixel in the image you end up with every pixel being replaced by an arrow these arrows are called gradients and they show the flow from light to dark across the entire image this might seem like a random thing to do but there s a really good reason for replacing the pixels with gradients if we analyze pixels directly really dark images and really light images of the same person will have totally different pixel values but by only considering the direction that brightness changes both really dark images and really bright images will end up with the same exact representation that makes the problem a lot easier to solve but saving the gradient for every single pixel gives us way too much detail we end up missing the forest for the trees it would be better if we could just see the basic flow of lightness darkness at a higher level so we could see the basic pattern of the image to do this we ll break up the image into small squares of x pixels each in each square we ll count up how many gradients point in each major direction how many point up point up right point right etc then we ll replace that square in the image with the arrow directions that were the strongest the end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way to find faces in this hog image all we have to do is find the part of our image that looks the most similar to a known hog pattern that was extracted from a bunch of other training faces using this technique we can now easily find faces in any image if you want to try this step out yourself using python and dlib here s code showing how to generate and view hog representations of images whew we isolated the faces in our image but now we have to deal with the problem that faces turned different directions look totally different to a computer to account for this we will try to warp each picture so that the eyes and lips are always in the sample place in the image this will make it a lot easier for us to compare faces in the next steps to do this we are going to use an algorithm called face landmark estimation there are lots of ways to do this but we are going to use the approach invented in by vahid kazemi and josephine sullivan the basic idea is we will come up with specific points called landmarks that exist on every face the top of the chin the outside edge of each eye the inner edge of each eyebrow etc then we will train a machine learning algorithm to be able to find these specific points on any face here s the result of locating the face landmarks on our test image now that we know were the eyes and mouth are we ll simply rotate scale and shear the image so that the eyes and mouth are centered as best as possible we won t do any fancy d warps because that would introduce distortions into the image we are only going to use basic image transformations like rotation and scale that preserve parallel lines called affine transformations now no matter how the face is turned we are able to center the eyes and mouth are in roughly the same position in the image this will make our next step a lot more accurate if you want to try this step out yourself using python and dlib here s the code for finding face landmarks and here s the code for transforming the image using those landmarks now we are to the meat of the problem actually telling faces apart this is where things get really interesting the simplest approach to face recognition is to directly compare the unknown face we found in step with all the pictures we have of people that have already been tagged when we find a previously tagged face that looks very similar to our unknown face it must be the same person seems like a pretty good idea right there s actually a huge problem with that approach a site like facebook with billions of users and a trillion photos can t possibly loop through every previous tagged face to compare it to every newly uploaded picture that would take way too long they need to be able to recognize faces in milliseconds not hours what we need is a way to extract a few basic measurements from each face then we could measure our unknown face the same way and find the known face with the closest measurements for example we might measure the size of each ear the spacing between the eyes the length of the nose etc if you ve ever watched a bad crime show like csi you know what i am talking about ok so which measurements should we collect from each face to build our known face database ear size nose length eye color something else it turns out that the measurements that seem obvious to us humans like eye color don t really make sense to a computer looking at individual pixels in an image researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself deep learning does a better job than humans at figuring out which parts of a face are important to measure the solution is to train a deep convolutional neural network just like we did in part but instead of training the network to recognize pictures objects like we did last time we are going to train it to generate measurements for each face the training process works by looking at face images at a time then the algorithm looks at the measurements it is currently generating for each of those three images it then tweaks the neural network slightly so that it makes sure the measurements it generates for and are slightly closer while making sure the measurements for and are slightly further apart after repeating this step millions of times for millions of images of thousands of different people the neural network learns to reliably generate measurements for each person any ten different pictures of the same person should give roughly the same measurements machine learning people call the measurements of each face an embedding the idea of reducing complicated raw data like a picture into a list of computer generated numbers comes up a lot in machine learning especially in language translation the exact approach for faces we are using was invented in by researchers at google but many similar approaches exist this process of training a convolutional neural network to output face embeddings requires a lot of data and computer power even with an expensive nvidia telsa video card it takes about hours of continuous training to get good accuracy but once the network has been trained it can generate measurements for any face even ones it has never seen before so this step only needs to be done once lucky for us the fine folks at openface already did this and they published several trained networks which we can directly use thanks brandon amos and team so all we need to do ourselves is run our face images through their pre trained network to get the measurements for each face here s the measurements for our test image so what parts of the face are these numbers measuring exactly it turns out that we have no idea it doesn t really matter to us all that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person if you want to try this step yourself openface provides a lua script that will generate embeddings all images in a folder and write them to a csv file you run it like this this last step is actually the easiest step in the whole process all we have to do is find the person in our database of known people who has the closest measurements to our test image you can do that by using any basic machine learning classification algorithm no fancy deep learning tricks are needed we ll use a simple linear svm classifier but lots of classification algorithms could work all we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match running this classifier takes milliseconds the result of the classifier is the name of the person so let s try out our system first i trained a classifier with the embeddings of about pictures each of will ferrell chad smith and jimmy falon then i ran the classifier on every frame of the famous youtube video of will ferrell and chad smith pretending to be each other on the jimmy fallon show it works and look how well it works for faces in different poses even sideways faces let s review the steps we followed now that you know how this all works here s instructions from start to finish of how run this entire face recognition pipeline on your own computer update you can still follow the steps below to use openface however i ve released a new python based face recognition library called face recognition that is much easier to install and use so i d recommend trying out face recognition first instead of continuing below i even put together a pre configured virtual machine with face recognition opencv tensorflow and lots of other deep learning tools pre installed you can download and run it on your computer very easily give the virtual machine a shot if you don t want to install all these libraries yourself original openface instructions if you liked this article please consider signing up for my machine learning is fun newsletter you can also follow me on twitter at ageitgey email me directly or find me on linkedin i d love to hear from you if i can help you or your team with machine learning now continue on to machine learning is fun part from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in computers and machine learning likes to write about it
Arthur Juliani,9000,6,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=tag_archive---------2----------------,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,for this tutorial in my reinforcement learning series we are going to be exploring a family of rl algorithms called q learning algorithms these are a little different than the policy based algorithms that will be looked at in the the following tutorials parts instead of starting with a complex and unwieldy deep neural network we will begin by implementing a simple lookup table version of the algorithm and then show how to implement a neural network equivalent using tensorflow given that we are going back to basics it may be best to think of this as part of the series it will hopefully give an intuition into what is really happening in q learning that we can then build on going forward when we eventually combine the policy gradient and q learning approaches to build state of the art rl agents if you are more interested in policy networks or already have a grasp on q learning feel free to start the tutorial series here instead unlike policy gradient methods which attempt to learn functions which directly map an observation to an action q learning attempts to learn the value of being in a given state and taking a specific action there while both approaches ultimately allow us to take intelligent actions given a situation the means of getting to that action differ significantly you may have heard about deepq networks which can play atari games these are really just larger and more complex implementations of the q learning algorithm we are going to discuss here for this tutorial we are going to be attempting to solve the frozenlake environment from the openai gym for those unfamiliar the openai gym provides an easy way for people to experiment with their learning agents in an array of provided toy games the frozenlake environment consists of a x grid of blocks each one either being the start block the goal block a safe frozen block or a dangerous hole the objective is to have an agent learn to navigate from the start to the goal without moving onto a hole at any given time the agent can choose to move either up down left or right the catch is that there is a wind which occasionally blows the agent onto a space they didn t choose as such perfect performance every time is impossible but learning to avoid the holes and reach the goal are certainly still doable the reward at every step is except for entering the goal which provides a reward of thus we will need an algorithm that learns long term expected rewards this is exactly what q learning is designed to provide in it s simplest implementation q learning is a table of values for every state row and action column possible in the environment within each cell of the table we learn a value for how good it is to take a given action within a given state in the case of the frozenlake environment we have possible states one for each block and possible actions the four directions of movement giving us a x table of q values we start by initializing the table to be uniform all zeros and then as we observe the rewards we obtain for various actions we update the table accordingly we make updates to our q table using something called the bellman equation which states that the expected long term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state in this way we reuse our own q table when estimating how to update our table for future actions in equation form the rule looks like this this says that the q value for a given state s and action a should represent the current reward r plus the maximum discounted future reward expected according to our own table for the next state s we would end up in the discount variable allows us to decide how important the possible future rewards are compared to the present reward by updating in this way the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state below is a python walkthrough of the q table algorithm implemented in the frozenlake environment thanks to praneet d for finding the optimal hyperparameters for this approach now you may be thinking tables are great but they don t really scale do they while it is easy to have a x table for a simple grid world the number of possible states in any modern game or real world environment is nearly infinitely larger for most interesting problems tables simply don t work we instead need some way to take a description of our state and produce q values for actions without a table that is where neural networks come in by acting as a function approximator we can take any number of possible states that can be represented as a vector and learn to map them to q values in the case of the frozenlake example we will be using a one layer network which takes the state encoded in a one hot vector x and produces a vector of q values one for each action such a simple network acts kind of like a glorified table with the network weights serving as the old cells the key difference is that we can easily expand the tensorflow network with added layers activation functions and different input types whereas all that is impossible with a regular table the method of updating is a little different as well instead of directly updating our table with a network we will be using backpropagation and a loss function our loss function will be sum of squares loss where the difference between the current predicted q values and the target value is computed and the gradients passed through the network in this case our q target for the chosen action is the equivalent to the q value computed in equation above below is the tensorflow walkthrough of implementing our simple q network while the network learns to solve the frozenlake problem it turns out it doesn t do so quite as efficiently as the q table while neural networks allow for greater flexibility they do so at the cost of stability when it comes to q learning there are a number of possible extensions to our simple q network which allow for greater performance and more robust learning two tricks in particular are referred to as experience replay and freezing target networks those improvements and other tweaks were the key to getting atari playing deep q networks and we will be exploring those additions in the future for more info on the theory behind q learning see this great post by tambet matiisen i hope this tutorial has been helpful for those curious about how to implement simple q learning algorithms if this post has been valuable to you please consider donating to help support future tutorials articles and implementations any contribution is greatly appreciated if you d like to follow my work on deep learning ai and cognitive science follow me on medium arthur juliani or on twitter awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning unity d cognitive neuroscience phd student exploring frontier technology through the lens of artificial intelligence data science and the shape of things to come
Dhruv Parthasarathy,4300,12,https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4?source=tag_archive---------5----------------,A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN,at athelas we use convolutional neural networks cnns for a lot more than just classification in this post we ll see how cnns can be used with great results in image instance segmentation ever since alex krizhevsky geoff hinton and ilya sutskever won imagenet in convolutional neural networks cnns have become the gold standard for image classification in fact since then cnns have improved to the point where they now outperform humans on the imagenet challenge while these results are impressive image classification is far simpler than the complexity and diversity of true human visual understanding in classification there s generally an image with a single object as the focus and the task is to say what that image is see above but when we look at the world around us we carry out far more complex tasks we see complicated sights with multiple overlapping objects and different backgrounds and we not only classify these different objects but also identify their boundaries differences and relations to one another can cnns help us with such complex tasks namely given a more complicated image can we use cnns to identify the different objects in the image and their boundaries as has been shown by ross girshick and his peers over the last few years the answer is conclusively yes through this post we ll cover the intuition behind some of the main techniques used in object detection and segmentation and see how they ve evolved from one implementation to the next in particular we ll cover r cnn regional cnn the original application of cnns to this problem along with its descendants fast r cnn and faster r cnn finally we ll cover mask r cnn a paper released recently by facebook research that extends such object detection techniques to provide pixel level segmentation here are the papers referenced in this post inspired by the research of hinton s lab at the university of toronto a small team at uc berkeley led by professor jitendra malik asked themselves what today seems like an inevitable question object detection is the task of finding the different objects in an image and classifying them as seen in the image above the team comprised of ross girshick a name we ll see again jeff donahue and trevor darrel found that this problem can be solved with krizhevsky s results by testing on the pascal voc challenge a popular object detection challenge akin to imagenet they write let s now take a moment to understand how their architecture regions with cnns r cnn works understanding r cnn the goal of r cnn is to take in an image and correctly identify where the main objects via a bounding box in the image but how do we find out where these bounding boxes are r cnn does what we might intuitively do as well propose a bunch of boxes in the image and see if any of them actually correspond to an object r cnn creates these bounding boxes or region proposals using a process called selective search which you can read about here at a high level selective search shown in the image above looks at the image through windows of different sizes and for each size tries to group together adjacent pixels by texture color or intensity to identify objects once the proposals are created r cnn warps the region to a standard square size and passes it through to a modified version of alexnet the winning submission to imagenet that inspired r cnn as shown above on the final layer of the cnn r cnn adds a support vector machine svm that simply classifies whether this is an object and if so what object this is step in the image above improving the bounding boxes now having found the object in the box can we tighten the box to fit the true dimensions of the object we can and this is the final step of r cnn r cnn runs a simple linear regression on the region proposal to generate tighter bounding box coordinates to get our final result here are the inputs and outputs of this regression model so to summarize r cnn is just the following steps r cnn works really well but is really quite slow for a few simple reasons in ross girshick the first author of r cnn solved both these problems leading to the second algorithm in our short history fast r cnn let s now go over its main insights fast r cnn insight roi region of interest pooling for the forward pass of the cnn girshick realized that for each image a lot of proposed regions for the image invariably overlapped causing us to run the same cnn computation again and again times his insight was simple why not run the cnn just once per image and then find a way to share that computation across the proposals this is exactly what fast r cnn does using a technique known as roipool region of interest pooling at its core roipool shares the forward pass of a cnn for an image across its subregions in the image above notice how the cnn features for each region are obtained by selecting a corresponding region from the cnn s feature map then the features in each region are pooled usually using max pooling so all it takes us is one pass of the original image as opposed to fast r cnn insight combine all models into one network the second insight of fast r cnn is to jointly train the cnn classifier and bounding box regressor in a single model where earlier we had different models to extract image features cnn classify svm and tighten bounding boxes regressor fast r cnn instead used a single network to compute all three you can see how this was done in the image above fast r cnn replaced the svm classifier with a softmax layer on top of the cnn to output a classification it also added a linear regression layer parallel to the softmax layer to output bounding box coordinates in this way all the outputs needed came from one single network here are the inputs and outputs to this overall model even with all these advancements there was still one remaining bottleneck in the fast r cnn process the region proposer as we saw the very first step to detecting the locations of objects is generating a bunch of potential bounding boxes or regions of interest to test in fast r cnn these proposals were created using selective search a fairly slow process that was found to be the bottleneck of the overall process in the middle a team at microsoft research composed of shaoqing ren kaiming he ross girshick and jian sun found a way to make the region proposal step almost cost free through an architecture they creatively named faster r cnn the insight of faster r cnn was that region proposals depended on features of the image that were already calculated with the forward pass of the cnn first step of classification so why not reuse those same cnn results for region proposals instead of running a separate selective search algorithm indeed this is just what the faster r cnn team achieved in the image above you can see how a single cnn is used to both carry out region proposals and classification this way only one cnn needs to be trained and we get region proposals almost for free the authors write here are the inputs and outputs of their model how the regions are generated let s take a moment to see how faster r cnn generates these region proposals from cnn features faster r cnn adds a fully convolutional network on top of the features of the cnn creating what s known as the region proposal network the region proposal network works by passing a sliding window over the cnn feature map and at each window outputting k potential bounding boxes and scores for how good each of those boxes is expected to be what do these k boxes represent intuitively we know that objects in an image should fit certain common aspect ratios and sizes for instance we know that we want some rectangular boxes that resemble the shapes of humans likewise we know we won t see many boxes that are very very thin in such a way we create k such common aspect ratios we call anchor boxes for each such anchor box we output one bounding box and score per position in the image with these anchor boxes in mind let s take a look at the inputs and outputs to this region proposal network we then pass each such bounding box that is likely to be an object into fast r cnn to generate a classification and tightened bounding boxes so far we ve seen how we ve been able to use cnn features in many interesting ways to effectively locate different objects in an image with bounding boxes can we extend such techniques to go one step further and locate exact pixels of each object instead of just bounding boxes this problem known as image segmentation is what kaiming he and a team of researchers including girshick explored at facebook ai using an architecture known as mask r cnn much like fast r cnn and faster r cnn mask r cnn s underlying intuition is straight forward given that faster r cnn works so well for object detection could we extend it to also carry out pixel level segmentation mask r cnn does this by adding a branch to faster r cnn that outputs a binary mask that says whether or not a given pixel is part of an object the branch in white in the above image as before is just a fully convolutional network on top of a cnn based feature map here are its inputs and outputs but the mask r cnn authors had to make one small adjustment to make this pipeline work as expected roialign realigning roipool to be more accurate when run without modifications on the original faster r cnn architecture the mask r cnn authors realized that the regions of the feature map selected by roipool were slightly misaligned from the regions of the original image since image segmentation requires pixel level specificity unlike bounding boxes this naturally led to inaccuracies the authors were able to solve this problem by cleverly adjusting roipool to be more precisely aligned using a method known as roialign imagine we have an image of size x and a feature map of size x let s imagine we want features the region corresponding to the top left x pixels in the original image see above how might we select these pixels from the feature map we know each pixel in the original image corresponds to pixels in the feature map to select pixels from the original image we just select pixels in roipool we would round this down and select pixels causing a slight misalignment however in roialign we avoid such rounding instead we use bilinear interpolation to get a precise idea of what would be at pixel this at a high level is what allows us to avoid the misalignments caused by roipool once these masks are generated mask r cnn combines them with the classifications and bounding boxes from faster r cnn to generate such wonderfully precise segmentations if you re interested in trying out these algorithms yourselves here are relevant repositories faster r cnn mask r cnn in just years we ve seen how the research community has progressed from krizhevsky et al s original result to r cnn and finally all the way to such powerful results as mask r cnn seen in isolation results like mask r cnn seem like incredible leaps of genius that would be unapproachable yet through this post i hope you ve seen how such advancements are really the sum of intuitive incremental improvements through years of hard work and collaboration each of the ideas proposed by r cnn fast r cnn faster r cnn and finally mask r cnn were not necessarily quantum leaps yet their sum products have led to really remarkable results that bring us closer to a human level understanding of sight what particularly excites me is that the time between r cnn and mask r cnn was just three years with continued funding focus and support how much further can computer vision improve over the next three years if you see any errors or issues in this post please contact me at dhruv getathelas com and i ll immediately correct them if you re interested in applying such techniques come join us at athelas where we apply computer vision to blood diagnostics daily other posts we ve written thanks to bharath ramsundar pranav ramkrishnan tanay tandon and oliver cameron for help with this post from a quick cheer to a standing ovation clap to show how much you enjoyed this story dhruvp vp eng athelas mit math and cs undergrad mit cs masters previously director of ai programs udacity blood diagnostics through deep learning http athelas com
Sebastian Heinz,4400,13,https://medium.com/mlreview/a-simple-deep-learning-model-for-stock-price-prediction-using-tensorflow-30505541d877?source=tag_archive---------6----------------,A simple deep learning model for stock price prediction using TensorFlow,for a recent hackathon that we did at statworx some of our team members scraped minutely s p data from the google finance api the data consisted of index as well as stock prices of the s p s constituents having this data at hand the idea of developing a deep learning model for predicting the s p index based on the constituents prices one minute ago came immediately on my mind playing around with the data and building the deep learning model with tensorflow was fun and so i decided to write my first medium com story a little tensorflow tutorial on predicting s p stock prices what you will read is not an in depth tutorial but more a high level introduction to the important building blocks and concepts of tensorflow models the python code i ve created is not optimized for efficiency but understandability the dataset i ve used can be downloaded from here mb our team exported the scraped stock data from our scraping server as a csv file the dataset contains n minutes of data ranging from april to august on stocks as well as the total s p index price index and stocks are arranged in wide format the data was already cleaned and prepared meaning missing stock and index prices were locf ed last observation carried forward so that the file did not contain any missing values a quick look at the s p time series using pyplot plot data sp note this is actually the lead of the s p index meaning its value is shifted minute into the future this operation is necessary since we want to predict the next minute of the index and not the current minute the dataset was split into training and test data the training data contained of the total dataset the data was not shuffled but sequentially sliced the training data ranges from april to approx end of july the test data ends end of august there are a lot of different approaches to time series cross validation such as rolling forecasts with and without refitting or more elaborate concepts such as time series bootstrap resampling the latter involves repeated samples from the remainder of the seasonal decomposition of the time series in order to simulate samples that follow the same seasonal pattern as the original time series but are not exact copies of its values most neural network architectures benefit from scaling the inputs sometimes also the output why because most common activation functions of the network s neurons such as tanh or sigmoid are defined on the or interval respectively nowadays rectified linear unit relu activations are commonly used activations which are unbounded on the axis of possible activation values however we will scale both the inputs and targets anyway scaling can be easily accomplished in python using sklearn s minmaxscaler remark caution must be undertaken regarding what part of the data is scaled and when a common mistake is to scale the whole dataset before training and test split are being applied why is this a mistake because scaling invokes the calculation of statistics e g the min max of a variable when performing time series forecasting in real life you do not have information from future observations at the time of forecasting therefore calculation of scaling statistics has to be conducted on training data and must then be applied to the test data otherwise you use future information at the time of forecasting which commonly biases forecasting metrics in a positive direction tensorflow is a great piece of software and currently the leading deep learning and neural network computation framework it is based on a c low level backend but is usually controlled via python there is also a neat tensorflow library for r maintained by rstudio tensorflow operates on a graph representation of the underlying computational task this approach allows the user to specify mathematical operations as elements in a graph of data variables and operators since neural networks are actually graphs of data and mathematical operations tensorflow is just perfect for neural networks and deep learning check out this simple example stolen from our deep learning introduction from our blog in the figure above two numbers are supposed to be added those numbers are stored in two variables a and b the two values are flowing through the graph and arrive at the square node where they are being added the result of the addition is stored into another variable c actually a b and c can be considered as placeholders any numbers that are fed into a and b get added and are stored into c this is exactly how tensorflow works the user defines an abstract representation of the model neural network through placeholders and variables afterwards the placeholders get filled with real data and the actual computations take place the following code implements the toy example from above in tensorflow after having imported the tensorflow library two placeholders are defined using tf placeholder they correspond to the two blue circles on the left of the image above afterwards the mathematical addition is defined via tf add the result of the computation is c with placeholders set up the graph can be executed with any integer value for a and b of course the former problem is just a toy example the required graphs and computations in a neural network are much more complex as mentioned before it all starts with placeholders we need two placeholders in order to fit our model x contains the network s inputs the stock prices of all s p constituents at time t t and y the network s outputs the index value of the s p at time t t the shape of the placeholders correspond to none n stocks with none meaning that the inputs are a dimensional matrix and the outputs are a dimensional vector it is crucial to understand which input and output dimensions the neural net needs in order to design it properly the none argument indicates that at this point we do not yet know the number of observations that flow through the neural net graph in each batch so we keep if flexible we will later define the variable batch size that controls the number of observations per training batch besides placeholders variables are another cornerstone of the tensorflow universe while placeholders are used to store input and target data in the graph variables are used as flexible containers within the graph that are allowed to change during graph execution weights and biases are represented as variables in order to adapt during training variables need to be initialized prior to model training we will get into that a litte later in more detail the model consists of four hidden layers the first layer contains neurons slightly more than double the size of the inputs subsequent hidden layers are always half the size of the previous layer which means and finally neurons a reduction of the number of neurons for each subsequent layer compresses the information the network identifies in the previous layers of course other network architectures and neuron configurations are possible but are out of scope for this introduction level article it is important to understand the required variable dimensions between input hidden and output layers as a rule of thumb in multilayer perceptrons mlps the type of networks used here the second dimension of the previous layer is the first dimension in the current layer for weight matrices this might sound complicated but is essentially just each layer passing its output as input to the next layer the biases dimension equals the second dimension of the current layer s weight matrix which corresponds the number of neurons in this layer after definition of the required weight and bias variables the network topology the architecture of the network needs to be specified hereby placeholders data and variables weighs and biases need to be combined into a system of sequential matrix multiplications furthermore the hidden layers of the network are transformed by activation functions activation functions are important elements of the network architecture since they introduce non linearity to the system there are dozens of possible activation functions out there one of the most common is the rectified linear unit relu which will also be used in this model the image below illustrates the network architecture the model consists of three major building blocks the input layer the hidden layers and the output layer this architecture is called a feedforward network feedforward indicates that the batch of data solely flows from left to right other network architectures such as recurrent neural networks also allow data flowing backwards in the network the cost function of the network is used to generate a measure of deviation between the network s predictions and the actual observed training targets for regression problems the mean squared error mse function is commonly used mse computes the average squared deviation between predictions and targets basically any differentiable function can be implemented in order to compute a deviation measure between predictions and targets however the mse exhibits certain properties that are advantageous for the general optimization problem to be solved the optimizer takes care of the necessary computations that are used to adapt the network s weight and bias variables during training those computations invoke the calculation of so called gradients that indicate the direction in which the weights and biases have to be changed during training in order to minimize the network s cost function the development of stable and speedy optimizers is a major field in neural network an deep learning research here the adam optimizer is used which is one of the current default optimizers in deep learning development adam stands for adaptive moment estimation and can be considered as a combination between two other popular optimizers adagrad and rmsprop initializers are used to initialize the network s variables before training since neural networks are trained using numerical optimization techniques the starting point of the optimization problem is one the key factors to find good solutions to the underlying problem there are different initializers available in tensorflow each with different initialization approaches here i use the tf variance scaling initializer which is one of the default initialization strategies note that with tensorflow it is possible to define multiple initialization functions for different variables within the graph however in most cases a unified initialization is sufficient after having defined the placeholders variables initializers cost functions and optimizers of the network the model needs to be trained usually this is done by minibatch training during minibatch training random data samples of n batch size are drawn from the training data and fed into the network the training dataset gets divided into n batch size batches that are sequentially fed into the network at this point the placeholders x and y come into play they store the input and target data and present them to the network as inputs and targets a sampled data batch of x flows through the network until it reaches the output layer there tensorflow compares the models predictions against the actual observed targets y in the current batch afterwards tensorflow conducts an optimization step and updates the networks parameters corresponding to the selected learning scheme after having updated the weights and biases the next batch is sampled and the process repeats itself the procedure continues until all batches have been presented to the network one full sweep over all batches is called an epoch the training of the network stops once the maximum number of epochs is reached or another stopping criterion defined by the user applies during the training we evaluate the networks predictions on the test set the data which is not learned but set aside for every th batch and visualize it additionally the images are exported to disk and later combined into a video animation of the training process see below the model quickly learns the shape und location of the time series in the test data and is able to produce an accurate prediction after some epochs nice one can see that the networks rapidly adapts to the basic shape of the time series and continues to learn finer patterns of the data this also corresponds to the adam learning scheme that lowers the learning rate during model training in order not to overshoot the optimization minimum after epochs we have a pretty close fit to the test data the final test mse equals it is very low because the target is scaled the mean absolute percentage error of the forecast on the test set is equal to which is pretty good note that this is just a fit to the test data no actual out of sample metrics in a real world scenario please note that there are tons of ways of further improving this result design of layers and neurons choosing different initialization and activation schemes introduction of dropout layers of neurons early stopping and so on furthermore different types of deep learning models such as recurrent neural networks might achieve better performance on this task however this is not the scope of this introductory post the release of tensorflow was a landmark event in deep learning research its flexibility and performance allows researchers to develop all kinds of sophisticated neural network architectures as well as other ml algorithms however flexibility comes at the cost of longer time to model cycles compared to higher level apis such as keras or mxnet nonetheless i am sure that tensorflow will make its way to the de facto standard in neural network and deep learning development in research and practical applications many of our customers are already using tensorflow or start developing projects that employ tensorflow models also our data science consultants at statworx are heavily using tensorflow for deep learning and neural net research and development let s see what google has planned for the future of tensorflow one thing that is missing at least in my opinion is a neat graphical user interface for designing and developing neural net architectures with tensorflow backend maybe this is something google is already working on if you have any comments or questions on my first medium story feel free to comment below i will try to answer them also feel free to use my code or share this story with your peers on social platforms of your choice update i ve added both the python script as well as a zipped dataset to a github repository feel free to clone and fork lastly follow me on twitter linkedin from a quick cheer to a standing ovation clap to show how much you enjoyed this story ceo statworx doing data science stats and ml for over a decade food wine and cocktail enthusiast check our website https www statworx com highlights from machine learning research projects and learning materials from and for ml scientists engineers an enthusiasts
Max Pechyonkin,23000,8,https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b?source=tag_archive---------7----------------,Understanding Hinton’s Capsule Networks. Part I: Intuition.,part i intuition you are reading it now part ii how capsules workpart iii dynamic routing between capsulespart iv capsnet architecture quick announcement about our new publication ai we are getting the best writers together to talk about the theory practice and business of ai and machine learning follow it to stay up to date on the latest trends last week geoffrey hinton and his team published two papers that introduced a completely new type of neural network based on so called capsules in addition to that the team published an algorithm called dynamic routing between capsules that allows to train such a network for everyone in the deep learning community this is huge news and for several reasons first of all hinton is one of the founders of deep learning and an inventor of numerous models and algorithms that are widely used today secondly these papers introduce something completely new and this is very exciting because it will most likely stimulate additional wave of research and very cool applications in this post i will explain why this new architecture is so important as well as intuition behind it in the following posts i will dive into technical details however before talking about capsules we need to have a look at cnns which are the workhorse of today s deep learning cnns convolutional neural networks are awesome they are one of the reasons deep learning is so popular today they can do amazing things that people used to think computers would not be capable of doing for a long long time nonetheless they have their limits and they have fundamental drawbacks let us consider a very simple and non technical example imagine a face what are the components we have the face oval two eyes a nose and a mouth for a cnn a mere presence of these objects can be a very strong indicator to consider that there is a face in the image orientational and relative spatial relationships between these components are not very important to a cnn how do cnns work the main component of a cnn is a convolutional layer its job is to detect important features in the image pixels layers that are deeper closer to the input will learn to detect simple features such as edges and color gradients whereas higher layers will combine simple features into more complex features finally dense layers at the top of the network will combine very high level features and produce classification predictions an important thing to understand is that higher level features combine lower level features as a weighted sum activations of a preceding layer are multiplied by the following layer neuron s weights and added before being passed to activation nonlinearity nowhere in this setup there is pose translational and rotational relationship between simpler features that make up a higher level feature cnn approach to solve this issue is to use max pooling or successive convolutional layers that reduce spacial size of the data flowing through the network and therefore increase the field of view of higher layer s neurons thus allowing them to detect higher order features in a larger region of the input image max pooling is a crutch that made convolutional networks work surprisingly well achieving superhuman performance in many areas but do not be fooled by its performance while cnns work better than any model before them max pooling nonetheless is losing valuable information hinton himself stated that the fact that max pooling is working so well is a big mistake and a disaster of course you can do away with max pooling and still get good results with traditional cnns but they still do not solve the key problem in the example above a mere presence of eyes a mouth and a nose in a picture does not mean there is a face we also need to know how these objects are oriented relative to each other computer graphics deals with constructing a visual image from some internal hierarchical representation of geometric data note that the structure of this representation needs to take into account relative positions of objects that internal representation is stored in computer s memory as arrays of geometrical objects and matrices that represent relative positions and orientation of these objects then special software takes that representation and converts it into an image on the screen this is called rendering inspired by this idea hinton argues that brains in fact do the opposite of rendering he calls it inverse graphics from visual information received by eyes they deconstruct a hierarchical representation of the world around us and try to match it with already learned patterns and relationships stored in the brain this is how recognition happens and the key idea is that representation of objects in the brain does not depend on view angle so at this point the question is how do we model these hierarchical relationships inside of a neural network the answer comes from computer graphics in d graphics relationships between d objects can be represented by a so called pose which is in essence translation plus rotation hinton argues that in order to correctly do classification and object recognition it is important to preserve hierarchical pose relationships between object parts this is the key intuition that will allow you to understand why capsule theory is so important it incorporates relative relationships between objects and it is represented numerically as a d pose matrix when these relationships are built into internal representation of data it becomes very easy for a model to understand that the thing that it sees is just another view of something that it has seen before consider the image below you can easily recognize that this is the statue of liberty even though all the images show it from different angles this is because internal representation of the statue of liberty in your brain does not depend on the view angle you have probably never seen these exact pictures of it but you still immediately knew what it was for a cnn this task is really hard because it does not have this built in understanding of d space but for a capsnet it is much easier because these relationships are explicitly modeled the paper that uses this approach was able to cut error rate by as compared to the previous state of the art which is a huge improvement another benefit of the capsule approach is that it is capable of learning to achieve state of the art performance by only using a fraction of the data that a cnn would use hinton mentions this in his famous talk about what is wrongs with cnns in this sense the capsule theory is much closer to what the human brain does in practice in order to learn to tell digits apart the human brain needs to see only a couple of dozens of examples hundreds at most cnns on the other hand need tens of thousands of examples to achieve very good performance which seems like a brute force approach that is clearly inferior to what we do with our brains the idea is really simple there is no way no one has come up with it before and the truth is hinton has been thinking about this for decades the reason why there were no publications is simply because there was no technical way to make it work before one of the reasons is that computers were just not powerful enough in the pre gpu based era before around another reason is that there was no algorithm that allowed to implement and successfully learn a capsule network in the same fashion the idea of artificial neurons was around since s but it was not until mid s when backpropagation algorithm showed up and allowed to successfully train deep networks in the same fashion the idea of capsules itself is not that new and hinton has mentioned it before but there was no algorithm up until now to make it work this algorithm is called dynamic routing between capsules this algorithm allows capsules to communicate with each other and create representations similar to scene graphs in computer graphics capsules introduce a new building block that can be used in deep learning to better model hierarchical relationships inside of internal knowledge representation of a neural network intuition behind them is very simple and elegant hinton and his team proposed a way to train such a network made up of capsules and successfully trained it on a simple data set achieving state of the art performance this is very encouraging nonetheless there are challenges current implementations are much slower than other modern deep learning models time will show if capsule networks can be trained quickly and efficiently in addition we need to see if they work well on more difficult data sets and in different domains in any case the capsule network is a very interesting and already working model which will definitely get more developed over time and contribute to further expansion of deep learning application domain this concludes part one of the series on capsule networks in the part ii more technical part i will walk you through the capsnet s internal workings step by step you can follow me on twitter let s also connect on linkedin from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning the ai revolution is here navigate the ever changing industry with our thoughtfully written articles whether your a researcher engineer or entrepreneur
Slav Ivanov,3900,17,https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415?source=tag_archive---------8----------------,"The $1700 great Deep Learning box: Assembly, setup and benchmarks",updated april uses cuda cudnn and tensorflow after years of using a thin client in the form of increasingly thinner macbooks i had gotten used to it so when i got into deep learning dl i went straight for the brand new at the time amazon p cloud servers no upfront cost the ability to train many models simultaneously and the general coolness of having a machine learning model out there slowly teaching itself however as time passed the aws bills steadily grew larger even as i switched to x cheaper spot instances also i didn t find myself training more than one model at a time instead i d go to lunch workout etc while the model was training and come back later with a clear head to check on it but eventually the model complexity grew and took longer to train i d often forget what i did differently on the model that had just completed its day training nudged by the great experiences of the other folks on the fast ai forum i decided to settle down and to get a dedicated dl box at home the most important reason was saving time while prototyping models if they trained faster the feedback time would be shorter thus it would be easier for my brain to connect the dots between the assumptions i had for the model and its results then i wanted to save money i was using amazon web services aws which offered p instances with nvidia k gpus lately the aws bills were around month with a tendency to get larger also it is expensive to store large datasets like imagenet and lastly i haven t had a desktop for over years and wanted to see what has changed in the meantime spoiler alert mostly nothing what follows are my choices inner monologue and gotchas from choosing the components to benchmarking a sensible budget for me would be about years worth of my current compute spending at month for aws this put it at around for the whole thing you can check out all the components used the pc part picker site is also really helpful in detecting if some of the components don t play well together the gpu is the most crucial component in the box it will train these deep networks fast shortening the feedback cycle disclosure the following are affiliate links to help me pay for well more gpus the choice is between a few of nvidia s cards gtx gtx ti gtx gtx ti and finally the titan x the prices might fluctuate especially because some gpus are great for cryptocurrency mining wink wink on performance side gtx ti and titan x are similar roughly speaking the gtx is about faster than gtx and gtx ti is about faster than gtx the new gtx ti is very close in performance to gtx tim dettmers has a great article on picking a gpu for deep learning which he regularly updates as new cards come on the market here are the things to consider when picking a gpu considering all of this i picked the gtx ti mainly for the training speed boost i plan to add a second ti soonish even though the gpu is the mvp in deep learning the cpu still matters for example data preparation is usually done on the cpu the number of cores and threads per core is important if we want to parallelize all that data prep to stay on budget i picked a mid range cpu the intel i it s relatively cheap but good enough to not slow things down edit as a few people have pointed out probably the biggest gotcha that is unique to dl multi gpu is to pay attention to the pcie lanes supported by the cpu motherboard by andrej karpathy we want to have each gpu have pcie lanes so it eats data as fast as possible gb s for pcie this means that for two cards we need pcie lanes however the cpu i have picked has only lanes so gpus would run in x mode instead of x this might be a bottleneck leading to less than ideal utilization of the graphics cards thus a cpu with lines is recommended edit however tim dettmers points out that having lanes per card should only decrease performance by for two gpus so currently my recommendation is go with pcie lanes per video card unless it gets too expensive for you otherwise lanes should do as well a good solution with to have for a double gpu machine would be an intel xeon processor like the e v pcie lanes or if you want to splurge go for a higher end processor like the desktop i k memory ram it s nice to have a lot of memory if we are to be working with rather big datasets i got sticks of gb for a total of gb of ram and plan to buy another gb later following jeremy howard s advice i got a fast ssd disk to keep my os and current data on and then a slow spinning hdd for those huge datasets like imagenet ssd i remember when i got my first macbook air years ago how blown away was i by the ssd speed to my delight a new generation of ssd called nvme has made its way to market in the meantime a gb mydigitalssd nvme drive was a great deal this baby copies files at gigabytes per second hdd tb seagate while ssds have been getting fast hdd have been getting cheap to somebody who has used macbooks with gb disk for the last years having this much space feels almost obscene the one thing that i kept in mind when picking a motherboard was the ability to support two gtx ti both in the number of pci express lanes the minimum is x and the physical size of cards also make sure it s compatible with the chosen cpu an asus tuf z did it for me msi x a sli plus should work great if you got an intel xeon cpu rule of thumb power supply should provide enough juice for the cpu and the gpus plus watts extra the intel i processor uses w and the gpus ti need w each so i got a deepcool w gold psu currently unavailable evga gq is similar the gold here refers to the power efficiency i e how much of the power consumed is wasted as heat the case should be the same form factor as the motherboard also having enough leds to embarrass a burner is a bonus a friend recommended the thermaltake n case which i promptly got no leds sadly here is how much i spent on all the components your costs may vary gtx ti cpu ram ssd hdd motherboard psu case total adding tax and fees this nicely matches my preset budget of if you don t have much experience with hardware and fear you might break something a professional assembly might be the best option however this was a great learning opportunity that i couldn t pass even though i ve had my share of hardware related horror stories the first and important step is to read the installation manuals that came with each component especially important for me as i ve done this before once or twice and i have just the right amount of inexperience to mess things up this is done before installing the motherboard in the case next to the processor there is a lever that needs to be pulled up the processor is then placed on the base double check the orientation finally the lever comes down to fix the cpu in place but i had a quite the difficulty doing this once the cpu was in position the lever wouldn t go down i actually had a more hardware capable friend of mine video walk me through the process turns out the amount of force required to get the lever locked down was more than what i was comfortable with next is fixing the fan on top of the cpu the fan legs must be fully secured to the motherboard consider where the fan cable will go before installing the processor i had came with thermal paste if yours doesn t make sure to put some paste between the cpu and the cooling unit also replace the paste if you take off the fan i put the power supply unit psu in before the motherboard to get the power cables snugly placed in case back side pretty straight forward carefully place it and screw it in a magnetic screwdriver was really helpful then connect the power cables and the case buttons and leds just slide it in the m slot and screw it in piece of cake the memory proved quite hard to install requiring too much effort to properly lock in a few times i almost gave up thinking i must be doing it wrong eventually one of the sticks clicked in and the other one promptly followed at this point i turned the computer on to make sure it works to my relief it started right away finally the gpu slid in effortlessly pins of power later and it was running nb do not plug your monitor in the external card right away most probably it needs drivers to function see below finally it s complete now that we have the hardware in place only the soft part remains out with the screwdriver in with the keyboard note on dual booting if you plan to install windows because you know for benchmarks totally not for gaming it would be wise to do windows first and linux second i didn t and had to reinstall ubuntu because windows messed up the boot partition livewire has a detailed article on dual boot most dl frameworks are designed to work on linux first and eventually support other operating systems so i went for ubuntu my default linux distribution an old gb usb drive was laying around and worked great for the installation unetbootin osx or rufus windows can prepare the linux thumb drive the default options worked fine during the ubuntu install at the time of writing ubuntu was just released so i opted for the previous version whose quirks are much better documented online ubuntu server or desktop the server and desktop editions of ubuntu are almost identical with the notable exception of the visual interface called x not being installed with server i installed the desktop and disabled autostarting x so that the computer would boot it in terminal mode if needed one could launch the visual desktop later by typing startx let s get our install up to date from jeremy howard s excellent install gpu script to deep learn on our machine we need a stack of technologies to use our gpu download cuda from nvidia or just run the code below updated to specify version of cuda thanks to zhanwenchen for the tip if you need to add later versions of cuda click here after cuda has been installed the following code will add the cuda installation to the path variable now we can verify that cuda has been installed successfully by running this should have installed the display driver as well for me nvidia smi showed err as the device name so i installed the latest nvidia drivers as of may to fix it removing cuda nvidia drivers if at any point the drivers or cuda seem broken as they did for me multiple times it might be better to start over by running since version tensorflow supports cudnn so we install that to download cudnn one needs to register for a free developer account after downloading install with the following anaconda is a great package manager for python i ve moved to python so will be using the anaconda version the popular dl framework by google installation validate tensorfow install to make sure we have our stack running smoothly i like to run the tensorflow mnist example we should see the loss decreasing during training keras is a great high level neural networks framework an absolute pleasure to work with installation can t be easier too pytorch is a newcomer in the world of dl frameworks but its api is modeled on the successful torch which was written in lua pytorch feels new and exciting mostly great although some things are still to be implemented we install it by running jupyter is a web based ide for python which is ideal for data sciency tasks it s installed with anaconda so we just configure and test it now if we open http localhost we should see a jupyter screen run jupyter on boot rather than running the notebook every time the computer is restarted we can set it to autostart on boot we will use crontab to do this which we can edit by running crontab e then add the following after the last line in the crontab file i use my old trusty macbook air for development so i d like to be able to log into the dl box both from my home network also when on the run ssh key it s way more secure to use a ssh key to login instead of a password digital ocean has a great guide on how to setup this ssh tunnel if you want to access your jupyter notebook from another computer the recommended way is to use ssh tunneling instead of opening the notebook to the world and protecting with a password let s see how we can do this then to connect over ssh tunnel run the following script on the client to test this open a browser and try http localhost from the remote machine your jupyter notebook should appear setup out of network access finally to access the dl box from the outside world we need things setting up out of network access depends on the router network setup so i m not going into details now that we have everything running smoothly let s put it to the test we ll be comparing the newly built box to an aws p xlarge instance which is what i ve used so far for dl the tests are computer vision related meaning convolutional networks with a fully connected model thrown in we time training models on aws p instance gpu k aws p virtual cpu the gtx ti and intel i cpu andres hernandez points out that my comparison does not use tensorflow that is optimized for these cpus which would have helped the them perform better check his insightful comment for more details the hello world of computer vision the mnist database consists of handwritten digits we run the keras example on mnist which uses multilayer perceptron mlp the mlp means that we are using only fully connected layers not convolutions the model is trained for epochs on this dataset which achieves over accuracy out of the box we see that the gtx ti is times faster than the k on aws p in training the model this is rather surprising as these cards should have about the same performance i believe this is because of the virtualization or underclocking of the k on aws the cpus perform times slower than the gpus as we will see later it s a really good result for the processors this is due to the small model which fails to fully utilize the parallel processing power of the gpus interestingly the desktop intel i achieves x speedup over the virtual cpu on amazon a vgg net will be finetuned for the kaggle dogs vs cats competition in this competition we need to tell apart pictures of dogs and cats running the model on cpus for the same number of batches wasn t feasible therefore we finetune for batches epoch on the gpus and batches on the cpus the code used is on github the ti is times faster that the aws gpu k the difference in the cpus performance is about the same as the previous experiment i is x faster however it s absolutely impractical to use cpus for this task as the cpus were taking x more time on this large model that includes convolutional layers and a couple semi wide fully connected layers on top a gan generative adversarial network is a way to train a model to generate images gan achieves this by pitting two networks against each other a generator which learns to create better and better images and a discriminator that tries to tell which images are real and which are dreamt up by the generator the wasserstein gan is an improvement over the original gan we will use a pytorch implementation that is very similar to the one by the wgan author the models are trained for steps and the loss is all over the place which is often the case with gans cpus aren t considered the gtx ti finishes x faster than the aws p k which is in line with the previous results the final benchmark is on the original style transfer paper gatys et al implemented on tensorflow code available style transfer is a technique that combines the style of one image a painting for example and the content of another image check out my previous post for more details on how style transfer works the gtx ti outperforms the aws k by a factor of this time the cpus are times slower than graphics cards the slowdown is less than on the vgg finetuning task but more than on the mnist perceptron experiment the model uses mostly the earlier layers of the vgg network and i suspect this was too shallow to fully utilize the gpus the dl box is in the next room and a large model is training on it was it a wise investment time will tell but it is beautiful to watch the glowing leds in the dark and to hear its quiet hum as models are trying to squeeze out that extra accuracy percentage point from a quick cheer to a standing ovation clap to show how much you enjoyed this story entrepreneur hacker machine learning deep learning and other types of learning
Stefan Kojouharov,14200,7,https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?source=tag_archive---------9----------------,"Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data",over the past few months i have been collecting ai cheat sheets from time to time i share them with friends and colleagues and recently i have been getting asked a lot so i decided to organize and share the entire collection to make things more interesting and give context i added descriptions and or excerpts for each major topic this is the most complete list and the big o is at the very end enjoy this machine learning cheat sheet will help you find the right estimator for the job which is the most difficult part the flowchart will help you check the documentation and rough guide of each estimator that will help you to know more about the problems and how to solve it scikit learn formerly scikits learn is a free software machine learning library for the python programming language it features various classification regression and clustering algorithms including support vector machines random forests gradient boosting k means and dbscan and is designed to interoperate with the python numerical and scientific libraries numpy and scipy in may google announced the second generation of the tpu as well as the availability of the tpus in google compute engine the second generation tpus deliver up to teraflops of performance and when organized into clusters of tpus provide up to petaflops in google s tensorflow team decided to support keras in tensorflow s core library chollet explained that keras was conceived to be an interface rather than an end to end machine learning framework it presents a higher level more intuitive set of abstractions that make it easy to configure neural networks regardless of the backend scientific computing library numpy targets the cpython reference implementation of python which is a non optimizing bytecode interpreter mathematical algorithms written for this version of python often run much slower than compiled equivalents numpy address the slowness problem partly by providing multidimensional arrays and functions and operators that operate efficiently on arrays requiring rewriting some code mostly inner loops using numpy the name pandas is derived from the term panel data an econometrics term for multidimensional structured data sets the term data wrangler is starting to infiltrate pop culture in the movie kong skull island one of the characters played by actor marc evan jackson is introduced as steve woodward our data wrangler scipy builds on the numpy array object and is part of the numpy stack which includes tools like matplotlib pandas and sympy and an expanding set of scientific computing libraries this numpy stack has similar users to other applications such as matlab gnu octave and scilab the numpy stack is also sometimes referred to as the scipy stack matplotlib is a plotting library for the python programming language and its numerical mathematics extension numpy it provides an object oriented api for embedding plots into applications using general purpose gui toolkits like tkinter wxpython qt or gtk there is also a procedural pylab interface based on a state machine like opengl designed to closely resemble that of matlab though its use is discouraged scipy makes use of matplotlib pyplot is a matplotlib module which provides a matlab like interface matplotlib is designed to be as usable as matlab with the ability to use python with the advantage that it is free if you like this list you can let me know here stefan is the founder of chatbot s life a chatbot media and consulting firm chatbot s life has grown to over k views per month and has become the premium place to learn about bots ai online chatbot s life has also consulted many of the top bot companies like swelly instavest outbrain neargroup and a number of enterprises big o algorithm cheat sheet http bigocheatsheet com bokeh cheat sheet https s amazonaws com assets datacamp com blog assets python bokeh cheat sheet pdf data science cheat sheet https www datacamp com community tutorials python data science cheat sheet basics data wrangling cheat sheet https www rstudio com wp content uploads data wrangling cheatsheet pdf data wrangling https en wikipedia org wiki data wrangling ggplot cheat sheet https www rstudio com wp content uploads ggplot cheatsheet pdf keras cheat sheet https www datacamp com community blog keras cheat sheet gs drkenms keras https en wikipedia org wiki keras machine learning cheat sheet https ai icymi email new machinelearning cheat sheet by emily barry abdsc machine learning cheat sheet https docs microsoft com en in azure machine learning machine learning algorithm cheat sheet ml cheat sheet http peekaboo vision blogspot com machine learning cheat sheet for scikit html matplotlib cheat sheet https www datacamp com community blog python matplotlib cheat sheet gs uekyspy matpotlib https en wikipedia org wiki matplotlib neural networks cheat sheet http www asimovinstitute org neural network zoo neural networks graph cheat sheet http www asimovinstitute org blog neural networks https www quora com where can find a cheat sheet for neural network numpy cheat sheet https www datacamp com community blog python numpy cheat sheet gs ak zbge numpy https en wikipedia org wiki numpy pandas cheat sheet https www datacamp com community blog python pandas cheat sheet gs oundfxm pandas https en wikipedia org wiki pandas software pandas cheat sheet https www datacamp com community blog pandas cheat sheet python gs hpforic pyspark cheat sheet https www datacamp com community blog pyspark cheat sheet python gs l j zxq scikit cheat sheet https www datacamp com community blog scikit learn cheat sheet scikit learn https en wikipedia org wiki scikit learn scikit learn cheat sheet http peekaboo vision blogspot com machine learning cheat sheet for scikit html scipy cheat sheet https www datacamp com community blog python scipy cheat sheet gs jdsg oi scipy https en wikipedia org wiki scipy tesorflow cheat sheet https www altoros com tensorflow cheat sheet html tensor flow https en wikipedia org wiki tensorflow from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder of chatbots life i help companies create great chatbots ai systems and share my insights along the way latest news info and tutorials on artificial intelligence machine learning deep learning big data and what it means for humanity
Netflix Technology Blog,99,11,https://medium.com/netflix-techblog/distributed-neural-networks-with-gpus-in-the-aws-cloud-ccf71e82056b?source=tag_archive---------0----------------,Distributed Neural Networks with GPUs in the AWS Cloud,by alex chen justin basilico and xavier amatriain as we have described previously on this blog at netflix we are constantly innovating by looking for better ways to find the best movies and tv shows for our members when a new algorithmic technique such as deep learning shows promising results in other domains e g image recognition neuro imaging language models and speech recognition it should not come as a surprise that we would try to figure out how to apply such techniques to improve our product in this post we will focus on what we have learned while building infrastructure for experimenting with these approaches at netflix we hope that this will be useful for others working on similar algorithms especially if they are also leveraging the amazon web services aws infrastructure however we will not detail how we are using variants of artificial neural networks for personalization since it is an active area of research many researchers have pointed out that most of the algorithmic techniques used in the trendy deep learning approaches have been known and available for some time much of the more recent innovation in this area has been around making these techniques feasible for real world applications this involves designing and implementing architectures that can execute these techniques using a reasonable amount of resources in a reasonable amount of time the first successful instance of large scale deep learning made use of cpu cores in machines in order to train an artificial neural network in a matter of days while that was a remarkable milestone the required infrastructure cost and computation time are still not practical andrew ng and his team addressed this issue in follow up work their implementation used gpus as a powerful yet cheap alternative to large clusters of cpus using this architecture they were able to train a model times larger in a few days using only machines in another study schwenk et al showed that training these models on gpus can improve performance dramatically even when comparing to high end multicore cpus given our well known approach and leadership in cloud computing we sought out to implement a large scale neural network training system that leveraged both the advantages of gpus and the aws cloud we wanted to use a reasonable number of machines to implement a powerful machine learning solution using a neural network approach we also wanted to avoid needing special machines in a dedicated data center and instead leverage the full on demand computing power we can obtain from aws in architecting our approach for leveraging computing power in the cloud we sought to strike a balance that would make it fast and easy to train neural networks by looking at the entire training process for computing resources we have the capacity to use many gpu cores cpu cores and aws instances which we would like to use efficiently for an application such as this we typically need to train not one but multiple models either from different datasets or configurations e g different international regions for each configuration we need to perform hyperparameter tuning where each combination of parameters requires training a separate neural network in our solution we take the approach of using gpu based parallelism for training and using distributed computation for handling hyperparameter tuning and different configurations some of you might be thinking that the scenario described above is not what people think of as a distributed machine learning in the traditional sense for instance in the work by ng et al cited above they distribute the learning algorithm itself between different machines while that approach might make sense in some cases we have found that to be not always the norm especially when a dataset can be stored on a single instance to understand why we first need to explain the different levels at which a model training process can be distributed in a standard scenario we will have a particular model with multiple instances those instances might correspond to different partitions in your problem space a typical situation is to have different models trained for different countries or regions since the feature distribution and even the item space might be very different from one region to the other this represents the first initial level at which we can decide to distribute our learning process we could have for example a separate machine train each of the countries where netflix operates since each region can be trained entirely independently however as explained above training a single instance actually implies training and testing several models each corresponding to a different combinations of hyperparameters this represents the second level at which the process can be distributed this level is particularly interesting if there are many parameters to optimize and you have a good strategy to optimize them like bayesian optimization with gaussian processes the only communication between runs are hyperparameter settings and test evaluation metrics finally the algorithm training itself can be distributed while this is also interesting it comes at a cost for example training ann is a comparatively communication intensive process given that you are likely to have thousands of cores available in a single gpu instance it is very convenient if you can squeeze the most out of that gpu and avoid getting into costly across machine communication scenarios this is because communication within a machine using memory is usually much faster than communication over a network the following pseudo code below illustrates the three levels at which an algorithm training process like us can be distributed in this post we will explain how we addressed level and distribution in our use case note that one of the reasons we did not need to address level distribution is because our model has millions of parameters compared to the billions in the original paper by ng before we addressed distribution problem though we had to make sure the gpu based parallel training was efficient we approached this by first getting a proof of concept to work on our own development machines and then addressing the issue of how to scale and use the cloud as a second stage we started by using a lenovo s workstation with a nvidia quadro gpu this gpu has cores and provides a useful baseline for our experiments especially considering that we planned on using a more powerful machine and gpu in the aws cloud our first attempt to train our neural network model took hours we then ran the same code to train the model in on a ec s cg xlarge instance which has a more powerful tesla m with cores however the training time jumped from to over hours profiling showed that most of the time was spent on the function calls to nvidia performance primitive library e g nppsmulc f i nppsexp f i calling the npps functions repeatedly took x more system time on the cg instance than in the lenovo s while we tried to uncover the root cause we worked our way around the issue by reimplementing the npps functions using the customized cuda kernel e g replace nppsmulc f i function with replacing all npps functions in this way for the neural network code reduced the total training time on the cg instance from over hours to just minutes when training on million samples training million samples took seconds of gpu time using the same approach on the lenovo s the total training time also reduced from hours to hours this makes us believe that the implementation of these functions is suboptimal regardless of the card specifics while we were implementing this hack we also worked with the aws team to find a principled solution that would not require a kernel patch in doing so we found that the performance degradation was related to the nvreg checkpciconfigspace parameter of the kernel according to redhat setting this parameter to disables very slow accesses to the pci configuration space in a virtualized environment such as the aws cloud these accesses cause a trap in the hypervisor that results in even slower access nvreg checkpciconfigspace is a parameter of kernel module nvidia current that can be set using we tested the effect of changing this parameter using a benchmark that calls mulc repeatedly x times below are the results runtime in sec on our cg xlarge instances as you can see disabling accesses to pci space had a spectacular effect in the original npps functions decreasing the runtime by the effect was significant even in our optimized kernel functions saving almost in runtime however it is important to note that even when the pci access is disabled our customized functions performed almost better than the default ones we should also point out that there are other options which we have not explored so far but could be useful for others first we could look at optimizing our code by applying a kernel fusion trick that combines several computation steps into one kernel to reduce the memory access finally we could think about using theano the gpu match compiler in python which is supposed to also improve performance in these cases while our initial work was done using cg xlarge ec instances we were interested in moving to the new ec gpu g xlarge instance type which has a grid k gpu gk chip with cores currently our application is also bounded by gpu memory bandwidth and the grid k s memory bandwidth is gb sec which is an improvement over the tesla m s at gb sec of course using a gpu with faster memory would also help e g titan s memory bandwidth is gb sec we repeated the same comparison between the default npps functions and our customized ones with and without pci space access on the g xlarge instances one initial surprise was that we measured worse performance for npps on the g instances than the cg when pci space access was enabled however disabling it improved performance between and compared to the cg instances again our kernelmulc customized functions are over better with benchmark times under a second thus switching to g with the right configuration allowed us to run our experiments faster or alternatively larger experiments in the same amount of time once we had optimized the single node training and testing operations we were ready to tackle the issue of hyperparameter optimization if you are not familiar with this concept here is a simple explanation most machine learning algorithms have parameters to tune which are called often called hyperparameters to distinguish them from model parameters that are produced as a result of the learning algorithm for example in the case of a neural network we can think about optimizing the number of hidden units the learning rate or the regularization weight in order to tune these you need to train and test several different combinations of hyperparameters and pick the best one for your final model a naive approach is to simply perform an exhaustive grid search over the different possible combinations of reasonable hyperparameters however when faced with a complex model where training each one is time consuming and there are many hyperparameters to tune it can be prohibitively costly to perform such exhaustive grid searches luckily you can do better than this by thinking of parameter tuning as an optimization problem in itself one way to do this is to use a bayesian optimization approach where an algorithm s performance with respect to a set of hyperparameters is modeled as a sample from a gaussian process gaussian processes are a very effective way to perform regression and while they can have trouble scaling to large problems they work well when there is a limited amount of data like what we encounter when performing hyperparameter optimization we use package spearmint to perform bayesian optimization and find the best hyperparameters for the neural network training algorithm we hook up spearmint with our training algorithm by having it choose the set of hyperparameters and then training a neural network with those parameters using our gpu optimized code this model is then tested and the test metric results used to update the next hyperparameter choices made by spearmint we ve squeezed high performance from our gpu but we only have gpu cards per machine so we would like to make use of the distributed computing power of the aws cloud to perform the hyperparameter tuning for all configurations such as different models per international region to do this we use the distributed task queue celery to send work to each of the gpus each worker process listens to the task queue and runs the training on one gpu this allows us for example to tune train and update several models daily for all international regions although the spearmint celery system is working we are currently evaluating more complete and flexible solutions using htcondor or starcluster htcondor can be used to manage the workflow of any directed acyclic graph dag it handles input output file transfer and resource management in order to use condor we need each compute node register into the manager with a given classad e g slot has gpu true stard attrs has gpu then the user can submit a job with a configuration requirements has gpu so that the job only runs on aws instances that have an available gpu the main advantage of using condor is that it also manages the distribution of the data needed for the training of the different models condor also allows us to run the spearmint bayesian optimization on the manager instead of having to run it on each of the workers another alternative is to use starcluster which is an open source cluster computing framework for aws ec developed at mit starcluster runs on the oracle grid engine formerly sun grid engine in a fault tolerant way and is fully supported by spearmint finally we are also looking into integrating spearmint with jobman in order to better manage the hyperparameter search workflow figure below illustrates the generalized setup using spearmint plus celery condor or starcluster implementing bleeding edge solutions such as using gpus to train large scale neural networks can be a daunting endeavour if you need to do it in your own custom infrastructure the cost and the complexity might be overwhelming levering the public aws cloud can have obvious benefits provided care is taken in the customization and use of the instance resources by sharing our experience we hope to make it much easier and straightforward for others to develop similar applications we are always looking for talented researchers and engineers to join our team so if you are interested in solving these types of problems please take a look at some of our open positions on the netflix jobs page originally published at techblog netflix com on february from a quick cheer to a standing ovation clap to show how much you enjoyed this story learn more about how netflix designs builds and operates our systems and engineering organizations learn about netflix s world class engineering efforts company culture product developments and more
Francesco Gadaleta,3,4,https://hackernoon.com/gradient-descent-vs-coordinate-descent-9b5657f1c59f?source=tag_archive---------1----------------,Gradient descent vs coordinate descent – Hacker Noon,when it comes to function minimization it s time to open a book of optimization and linear algebra i am currently working on variable selection and lasso based solutions in genetics what lasso does is basically minimizing the loss function and an penalty in order to set to zero some regression coefficients and select only those covariates that are really associated with the response pheew the shortest summary of lasso ever we all know that provided the function to be minimized is convex a good direction to follow in order to find a local minimum is towards the negative gradient of the function now my question is how good or bad is following the negative gradient with respect to a coordinate descent approach that loops across all dimensions and minimizes along each there is no better way to try this with real code and start measuring hence i wrote some code that implements both gradient descent and coordinate descent the comparison might not be completely fair because the learning rate in the gradient descent procedure is fixed at which in some cases might be slower indeed but even with some tuning maybe with some linear search or adaptive learning rates it s quite common to see that coordinate descent overcomes its brother gradient descent many times this occurs much more often when the number of covariates becomes very high as in many computational biology problems in the figure below i plot the analytical solution in red the gradient descent minimisation in blue and the coordinate descent in green across a number of iterations a small explanation is probably necessary to read the function that performs coordinate descent for a more mathematical explanation refer to the original post coordinate descent will update each variable in a round robin fashion despite the learning rate of the gradient descent procedure which could indeed speed up convergence the comparison between the two is fair at least in terms of complexity coordinate descent needs to perform operations for each coordinate update gradient descent performs the same number of operations the r code that performs this comparison and generates the plot above is given below feel free to download this code remember to cite me and send me some cookies happy descent originally published at worldofpiggy com on may from a quick cheer to a standing ovation clap to show how much you enjoyed this story machine learning math crypto blockchain fitchain io how hackers start their afternoons
Milo Spencer-Harper,2200,3,https://medium.com/technology-invention-and-more/how-to-build-a-multi-layered-neural-network-in-python-53ec3d1d326a?source=tag_archive---------0----------------,How to build a multi-layered neural network in Python,in my last blog post thanks to an excellent blog post by andrew trask i learned how to build a neural network for the first time it was super simple lines of python code modelling the behaviour of a single neuron but what if we are faced with a more difficult problem can you guess what the should be the trick is to notice that the third column is irrelevant but the first two columns exhibit the behaviour of a xor gate if either the first column or the second column is then the output is however if both columns are or both columns are then the output is so the correct answer is however this would be too much for our single neuron to handle this is considered a nonlinear pattern because there is no direct one to one relationship between the inputs and the output instead we must create an additional hidden layer consisting of four neurons layer this layer enables the neural network to think about combinations of inputs you can see from the diagram that the output of layer feeds into layer it is now possible for the neural network to discover correlations between the output of layer and the output in the training set as the neural network learns it will amplify those correlations by adjusting the weights in both layers in fact image recognition is very similar there is no direct relationship between pixels and apples but there is a direct relationship between combinations of pixels and apples the process of adding more layers to a neural network so it can think about combinations is called deep learning ok are we ready for the python code first i ll give you the code and then i ll explain further also available here https github com miloharper multi layer neural network this code is an adaptation from my previous neural network so for a more comprehensive explanation it s worth looking back at my earlier blog post what s different this time is that there are multiple layers when the neural network calculates the error in layer it propagates the error backwards to layer adjusting the weights as it goes this is called back propagation ok let s try running it using the terminal command python main py you should get a result that looks like this first the neural network assigned herself random weights to her synaptic connections then she trained herself using the training set then she considered a new situation that she hadn t seen before and predicted the correct answer is so she was pretty close you might have noticed that as my neural network has become smarter i ve inadvertently personified her by using she instead of it that s pretty cool but the computer is doing lots of matrix multiplication behind the scenes which is hard to visualise in my next blog post i ll visually represent our neural network with an animated diagram of her neurons and synaptic connections so we can see her thinking from a quick cheer to a standing ovation clap to show how much you enjoyed this story studied economics at oxford university founder of www moju io interested in politics and ai technology trends and new invention follow this collection to update the latest trend update as a collection editor i don t have any permission to add your articles in the wild please submit your article and i will approve also follow this collection please
Jim Fleming,294,3,https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f?source=tag_archive---------1----------------,Loading a TensorFlow graph with the C++ API – Jim Fleming – Medium,check out the related post loading tensorflow graphs from node js using the c api the current documentation around loading a graph with c is pretty sparse so i spent some time setting up a barebones example in the tensorflow repo there are more involved examples such as building a graph in c however the c api for constructing graphs is not as complete as the python api many features including automatic gradient computation are not available from c yet another example in the repo demonstrates defining your own operations but most users will never need this i imagine the most common use case for the c api is for loading pre trained graphs to be standalone or embedded in other applications be aware there are some caveats to this approach that i ll cover at the end let s start by creating a minimal tensorflow graph and write it out as a protobuf file make sure to assign names to your inputs and operations so they re easier to assign when we execute the graph later the node s do have default names but they aren t very useful variable or mul here s an example created with jupyter let s create a new folder like tensorflow tensorflow my project name for your binary or library to live i m going to call the project loader since it will be loading a graph inside this project folder we ll create a new file called my project name cc e g loader cc if you re curious the cc extension is essentially the same as cpp but is preferred by google s code guidelines inside loader cc we re going to do a few things now we create a build file for our project this tells bazel what to compile inside we want to define a cc binary for our program you can also use the linkshared option on the binary to produce a shared library or the cc library rule if you re going to link it using bazel here s the final directory structure you could also call bazel run loader to run the executable directly however the working directory for bazel run is buried in a temporary folder and readbinaryproto looks in the current working directory for relative paths and that should be all we need to do to compile and run c code for tensorflow the last thing to cover are the caveats i mentioned hopefully someone can shed some light on these last points so we can begin to embed tensorflow graphs in applications if you are that person message me on twitter or email if you d like help deploying tensorflow in production i do consulting from a quick cheer to a standing ovation clap to show how much you enjoyed this story cto and lead ml engineer at fomoro focused on machine learning and applying cutting edge research for businesses previously rdio what i m working on
Milo Spencer-Harper,1800,4,https://medium.com/deep-learning-101/how-to-generate-a-video-of-a-neural-network-learning-in-python-62f5c520e85c?source=tag_archive---------2----------------,Video of a neural network learning – Deep Learning 101 – Medium,as part of my quest to learn about ai i generated a video of a neural network learning many of the examples on the internet use matrices grids of numbers to represent a neural network this method is favoured because it is however it s difficult to understand what is happening from a learning perspective being able to visually see a neural network is hugely beneficial the video you are about to see shows a neural network trying to solve this pattern can you work it out it s the same problem i posed in my previous blog post the trick is to notice that the third column is irrelevant but the first two columns exhibit the behaviour of a xor gate if either the first column or the second column is then the output is however if both columns are or both columns are then the output is so the correct answer is our neural network will cycle through these examples times to speed up the video i will only show you of these cycles pausing for a second on each frame why the number it ensures the video lasts exactly as long as the music each time she considers an example in the training set you will see her think you will see her neurons and her synaptic connections glow she will then calculate the error the difference between the output and the desired output she will then propagate this error backwards adjusting her synaptic connections green synaptic connections represent positive weights a signal flowing through this synapse will excite the next neuron to fire red synaptic connections represent negative weights a signal flowing through this synapse will inhibit the next neuron from firing thicker synapses represent stronger connections larger weights in the beginning her synaptic weights are randomly assigned notice how some synapses are green positive and others are red negative if these synapses turn out to be beneficial in calculating the right answer she will strengthen them over time however if they are unhelpful these synapses will wither it s even possible for a synapse which was originally positive to become negative and vice versa an example of this is the first synapse into the output neuron early on in the video it turns from red to green in the beginning her brain looks like this did you notice that all her neurons are dark this is because she isn t currently thinking about anything the numbers to the right of each neuron represent the level of neural activity and vary between and ok now she is going to think about the pattern we saw earlier watch the video carefully to see her synapses grow thicker as she learns did you notice how i slowed the video down at the beginning by skipping only a small number of cycles when i first shot the video i didn t do this however i realised that learning is subject to the law of diminishing returns the neural network changes more rapidly during the initial stage of training which is why i slowed this bit down now that she has learned about the pattern using the examples in the training set let s examine her brain again do you see how she has strengthened some of her synapses at the expense of others for instance do you remember how the third column in the training set is irrelevant in determining the answer you can see she has discovered this because the synapses coming out of her third input neuron have almost withered away relative to the others let s give her a new situation to think about you can see her neural pathways light up she has estimated the correct answer is so she was very close pretty cool traditional computer programs can t learn but neural networks can learn and adapt to new situations just like the human mind how did i do it i used the python library matplotlib which provides methods for drawing and animation i created the glow effects using alpha transparency you can view my full source code here thanks for reading if you enjoyed reading this article please click the heart icon to recommend from a quick cheer to a standing ovation clap to show how much you enjoyed this story studied economics at oxford university founder of www moju io interested in politics and ai fundamentals and latest developments in deeplearning
Christian Hernandez,364,7,https://medium.com/crossing-the-pond/into-the-age-of-context-f0aed15171d7?source=tag_archive---------3----------------,Into the Age of Context – Crossing the Pond – Medium,i spent most of my early career proclaiming that this was the year of mobile the year of mobile was actually when the iphone launched and accelerated a revolution around mobile computing as the economist recently put it just eight years later apple s iphone exemplifies the early st century s defining technology it s not a question of whether smartphones have become our primary computing interaction device it s a question of by how much relative to other interaction mediums so let s agree that we are currently living in the era of mobile looking forward to the next year though i personally believe we will move from the era of mobile to the age of context credit to robert scoble and shel israel for their book with that same term let me first define what i mean by age of context in the age of context personal data ex calendar and email location and time is integrated with publicly available data ex traffic data pollution level and app level data ex uber surge pricing number of steps tracked by my fitbit to intelligently drive me towards an action ex getting me to walk to my next meeting instead of ordering a car it is an age in which we and the devices and sensors around us generate massive reams of data and in which self teaching algorithms drill into that data to derive insight and recommend or auto generate an action it is an era in which our biological computational capacity and actions are enhanced and improved by digital services the age of context is being brought about by a number of technology trends which have been accelerating in a parallel and are now coming together the first and most obvious trend is the proliferation of supercomputers in our pockets industry analysts forecast billion phones will be shipped by these devices carry not only a growing amount of processing power but also the ecosystem of applications and services which integrate with sensors and functionality on the device to allow us to literally remote control our life in the evolution from the current era of mobile to the future age of context the supercomputers in our pocket evolve from information delivery and application interaction layers to notification context aware action drivers smartphones will soon be complemented by wearable computing devices be that the apple watch or a future evolution of google glass these new form factors are ideally suited for an era in which data needs to be compiled into succinct notifications and action enablers in the last years the web has evolved into a social web on top of which identities and deep insight into each of us powers services and experiences it allows goodreads to associate books with my identity vivino to determine that i like earthy red wines unilever to best target me for an ad on facebook and netflix to mine my data to then commission a show it knows i will like this identity layer is now being overlayed with a financial layer in which associated with my digital identity i also have a secure digital payment mechanism this transactional financial layer will begin to enable seamless transactions in the age of context the starbuck app will know that i usually emerge from the tube at am and walk to their local store to order a tall americano extra shot at as i reach street level my phone or watch or wearable computing device will know where i am close to starbucks and to the office know my routine have my payment information stored and simply generate an action driver that says tall americano extra shot order a few minutes later i can pick up my coffee which has already been paid for these services are already possible today a parallel and accelerating trend which will power the age of context is the proliferation of intelligent and connected sensors around us call that internet of things or call it simply a democratization and consumerization of devices that capture data for now and act on data eventually while the end number varies industry analysts all believe the number of connected devices starts to get very big very fast gartner predicts that by there will be billion connected devices with the vast majority of those being consumer centric today my jawbone is a fairly basic data collection device it knows that i walked steps and slept too little but it doesn t drive me to action other than providing me with a visualization of the data in the age of context this will change as larger and larger data sets of sensor data combined with other data combined with intelligent analytics allows data to become actionable in the future my jawbone won t simply count my steps it will also be able to integrate with other data sets to generate personal health insights it will have tracked over time that my blood pressure rises every morning at after i have consumed the third coffee of the day comparing my blood rate to thousands of others of my age range and demographic background it will know that the levels are unhealthy and it will help me take a conscious decision not to consume that extra coffee through a notification data will derive insight and that insight will hopefully drive action one could argue that the parallel trends of mobile sensors and the social web are already mainstream what then is bringing them together to weave the age of context the glue is data the massive amounts of data the growing number of internet users and connected devices generate each day more critically the cost of storing this data has dropped to nearly zero deloitte estimated that in the cost of storing a gigabyte of data was and that by the cost had dropped to but data by itself is just bits and bytes the second key trend that is weaving the age of context is the breakthroughs in algorithms and models to analyze this data in close to real time for the age of context to come about systems must know how to query and how to act on all the possible contextual data points to drive the simplified actions outlined in the examples above the advances and investment into machine learning and ai are the final piece of the puzzle needed to turn data from information to action the most visible example of the age of context today is google now google has a lot of information about me it knows what work is as i spend most of the time there between am and pm it knows what home is as i spend most of the evenings there since i use google apps it knows what my first meeting is since i search for duke basketball on a regular basis it knows i care about the scores since i usually take the tube and google has access to the london tfl data it knows that i will be late to my next meeting but even though google now recently opened up its api to third party developers it is still fairly google biased and google optimized for the age of context to thrive the platforms that power it must be interlinked across data and applications whether this age comes about through intelligent agents like siri or viv or the character from her or a meta app layer sitting across vertical apps and services is still unclear the missing piece for much of this to come about is a common meta language for vertical and punctual apps to share data and actions this common language will likely be an evolution of the various deep linking standards being developed facebook has a flavour android has a flavour and a myriad of startups have flavours an emerging standard will not only enable the age of context but also probably crown the champion of this new era as the standard will also own the interactions the interlinkages and the paths to monetization across devices and experiences the trends above are all happening around us the standards and algorithms are all being built by brilliant minds across the world the interface layers and devices are already with us the age of context is being created at an accelerating pace and i can t wait to see what gets built and how our day to day lives are enhanced by this new era thanks to john henderson for his feedback and thoughts on this post from a quick cheer to a standing ovation clap to show how much you enjoyed this story co founder and managing partner whitestarvc former product and mobile guy at smallish companies that became big salvadoran born londoner ygl of the wef stories from the white star capital team and our portfolio companies on entrepreneurship and scaling globally
Venture Scanner,207,5,https://medium.com/@VentureScanner/the-state-of-artificial-intelligence-in-six-visuals-8bc6e9bf8f32?source=tag_archive---------4----------------,The State of Artificial Intelligence in Six Visuals,we cover many emerging markets in the startup ecosystem previously we published posts that summarized financial technology internet of things bitcoin and martech in six visuals this week we do the same with artificial intelligence ai at this time we are tracking ai companies across categories with a combined funding amount of billion to see all of our ai related posts check out our blog the six artificial intelligence visuals below help make sense of this dynamic market deep learning machine learning applications machine learning is the technology of computer algorithms that operate based on its learnings from existing data deep learning is a subset of machine learning that focuses on deeply layered neural networks the following companies utilize deep learning machine learning technology in a specific way or use case in their products computer vision image recognition computer vision is the method of processing and analyzing images to understand and produce information from them image recognition is the process of scanning images to identify objects and faces the following companies either build computer vision image recognition technology or utilize it as the core offering in their products deep learning machine learning general machine learning is the technology of computer algorithms that operate based on its learning from existing data deep learning is a subset of machine learning that focuses on deeply layered neural networks the following companies either build deep learning machine learning technology or utilize it as the core offering of their products natural language processing natural language processing is the method through which computers process human language input and convert into understandable representations to derive meaning from them the following companies either build natural language processing technology or utilize it as the core offering in their products excluding all speech recognition companies smart robots smart robot companies build robots that can learn from their experience and act and react autonomously based on the conditions of their environment virtual personal assistants virtual personal assistants are software agents that use artificial intelligence to perform tasks and services for an individual such as customer service etc natural language processing speech recognition speech recognition is a subset of natural language processing that focuses on processing a sound clip of human speech and deriving meaning from it computer vision image recognition computer vision is the method of processing and analyzing images to understand and produce information from them image recognition is the process of scanning images to identify objects and faces the following companies utilize computer vision image recognition technology in a specific way or use case in their products recommendation engines and collaborative filtering recommendation engines are systems that predict the preferences and interests of users for certain items movies restaurants and deliver personalized recommendations to them collaborative filtering is a method of predicting a user s preferences and interests by collecting the preference information from many other similar users gesture control gesture control is the process through which humans interact and communicate with computers with their gestures which are recognized and interpreted by the computers video automatic content recognition video automatic content recognition is the process through which the computer compares a sampling of video content with a source content file to identify what the content is through its unique characteristics context aware computing context aware computing is the process through which computers become aware of their environment and their context of use such as location orientation lighting and adapt their behavior accordingly speech to speech transition speech to speech translation is the process through which human speech in one language is processed by the computer and translated into another language instantly the bar graph above summarizes the number of companies in each artificial intelligence category to show which are dominating the current market currently the deep learning machine learning applications category is leading the way with a total of companies followed by natural language processing speech recognition with companies the bar graph above summarizes the average company funding per artificial intelligence category again the deep learning machine learning applications category leads the way with an average of m per funded company the sem category includes companies that help marketers with managing and scaling their paid search programs the graph above compares total venture funding in artificial intelligence to the number of companies in each category deep learning machine learning applications seems to be the category with the most traction the following infographic is an updated heat map indicating where artificial intelligence startups exist across countries currently the united states is leading the way with companies the united kingdom is in second with companies followed by canada with the bar graph above summarizes artificial intelligence by median age of category the speech recognition and video content recognition categories have the highest median age at years followed by computer vision general at years as artificial intelligence continues to develop so too will its moving parts we hope this post provides some big picture clarity on this booming industry venture scanner enables corporations to research identify and connect with the most innovative technologies and companies we do this through a unique combination of our data technology and expert analysts if you have any questions reach out to info venturescanner com from a quick cheer to a standing ovation clap to show how much you enjoyed this story technology and analyst powered research firm visit us at www venturescanner com
Illia Polosukhin,108,3,https://medium.com/@ilblackdragon/tensorflow-tutorial-part-2-9ffe47049c92?source=tag_archive---------5----------------,Tensorflow Tutorial — Part 2 – Illia Polosukhin – Medium,in the previous part of this tutorial i introduced a bit of tensorflow and scikit flow and showed how to build a simple logistic regression model on titanic dataset in this part let s go deeper and try multi layer fully connected neural networks writing your custom model to plug into the scikit flow and top it with trying out convolutional networks of course there is not much point of yet another linear logistic regression framework an idea behind tensorflow and many other deep learning frameworks is to be able to connect differentiable parts of the model together and optimize them given the same cost or loss function scikit flow already implements a convenient wrapper around tensorflow api for creating many layers of fully connected units so it s simple to start with deep model by just swapping classifier in our previous model to the tensorflowdnnclassifier and specify hidden units per layer this will create layers of fully connected units with and hidden units respectively with default rectified linear unit activations we will be able to customize this setup in the next part i didn t play much with hyperparameters but previous dnn model actually yielded worse accuracy then a logistic regression we can explore if this is due to overfitting on under fitting in a separate post for the sake of this example i though want to show how to switch to the custom model where you can have more control this model is very similar to the previous one but we changed the activation function from a rectified linear unit to a hyperbolic tangent rectified linear unit and hyperbolic tangent are most popular activation functions for neural networks as you can see creating a custom model is as easy as writing a function that takes x and y inputs which are tensors and returns two tensors predictions and loss this is where you can start learning tensorflow apis to create parts of sub graph what kind of tensorflow tutorial would this be without an example of digit recognition this is just an example how you can try different types of datasets and models not limiting to only floating number features here we take digits dataset and write a custom model we ve created conv model function that given tensor x and y runs d convolutional layer with the most simple max pooling just maximum the result is passed as features to skflow models logistic regression which handles classification to required number of classes by attaching softmax over classes and computing cross entropy loss it s easy now to modify this code to add as many layers as you want some of the state of the art image recognition models are hundred layers of convolutions max pooling dropout and etc the part is expanding the model for titanic dataset with handling categorical variables ps thanks to vlad frolov for helping with missing articles and pointing mistakes in the draft from a quick cheer to a standing ovation clap to show how much you enjoyed this story co founder near ai teaching machines to code i m tweeting as ilblackdragon
Derrick Harris,124,9,https://medium.com/s-c-a-l-e/how-baidu-mastered-mandarin-with-deep-learning-and-lots-of-data-1d94032564a5?source=tag_archive---------6----------------,Baidu explains how it’s mastering Mandarin with deep learning,on aug at the international neural network society conference on big data in san francisco baidu senior research engineer awni hannun presented on a new model that the chinese search giant has developed for handling voice queries in mandarin the model which is accurate percent of the time in tests is based on a powerful deep learning system called deep speech that baidu first unveiled in december in this lightly edited interview hannun explains why his new research is important why mandarin is such a tough language to learn and where we can expect to see future advances in deep learning methods scale how accurate is deep speech at translating mandarin awni hannun it has a percent character error rate which essentially means that it gets wrong out of characters to put that in context this is in my opinion and to the best of our lab s knowledge the best system at transcribing mandarin voice queries in the world in fact we ran an experiment where we had a few people at the lab who speak chinese transcribe some of the examples that we were testing the system on it turned out that our system was better at transcribing examples than they were if we restricted it to transcribing without the help of the internet and such things what is it about mandarin that makes it such a challenge compared with other languages there are a couple of differences with mandarin that made us think it would be very difficult to have our english speech system work well with it one is that it s a tonal language so when you say a word in a different pitch it changes the meaning of the word which is definitely not the case in english in traditional speech recognition it s actually a desirable property that there is some pitch invariance which essentially means that it tries to ignore pitch when it does the transcription so you have to change a bunch of things to get a system to work with mandarin or any chinese for that matter however for us it was not the case that we had to change a whole bunch of things because our pipeline is much simpler than the traditional speech pipeline we don t do a whole lot of pre processing on the audio in order to make it pitch invariant but rather just let the model learn what s relevant from the data to most effectively transcribe it properly it was actually able to do that fine in mandarin without having to change the input the other thing that is very different about chinese mandarin in this case is the character set the english alphabet is letters whereas in chinese it s something like different characters our system directly outputs a character at a time as it s building its transcription so we speculated it would be very challenging to have to do that on characters at each step versus that s a challenge we were able to overcome just by using characters that people commonly say which is a smaller subset baidu has been handling a fairly high volume of voice searches for a while now how is the deep speech system better than the previous system for handling queries in mandarin baidu has a very active system for voice search in mandarin and it works pretty well i think in terms of total query activity it s still a relatively small percentage we want to make that share larger or at least enable people to use it more by making the accuracy of the system better can you describe the difference between a search based system like deep speech and something like microsoft s skype translate which is also based on deep learning typically the way it s done is there are three modules in the pipeline the first is a speech transcription module the second is the machine translation module and the third would be the speech synthesis module what we re talking about specifically is just the speech transcription module and i m sure microsoft has one as part of skype translate our system is different than that system in that it s more what we call end to end rather than having a lot of human engineered components that have been developed over decades of speech research by looking at the system and saying what what features are important or which phonemes the model should predict we just have some input data which is an audio wav file on which we do very little pre processing and then we have a big deep neural network that outputs directly to characters we give it enough data that it s able to learn what s relevant from the input to correctly transcribe the output with as little human intervention as possible one thing that s pleasantly surprising to us is that we had to do very little changing to it other than scaling it and giving it the right data to make this system we showed in december that worked really well on english work remarkably well in chinese as well what s the usual timeline to get this type of system from r d into production it s not an easy process but i think it s easier than the process of getting a model to be very accurate in the sense that it s more of an engineering problem than a research problem we re actively working on that now and i m hopeful our research system will be in production in the near term baidu has plans and products in other areas including wearables and other embedded forms of speech recognition does the work you re doing on search relate to these other initiatives we want to build a speech system that can be used as the interface to any smart device not just voice search it turns out that voice search is a very important part of baidu s ecosystem so that s one place we can have a lot of impact right now is the pace of progress and significant advances in deep learning as fast it seems i think right now it does feel like the pace is increasing because people are recognizing that if you take tasks where you have some input and are trying to produce some output you can apply deep learning to that task if it was some old machine learning task such as machine translation or speech recognition which has been heavily engineered for the past several decades you can make significant advances if you try to simplify that pipeline with deep learning and increase the amount of data we re just on the crest of that in particular processing sequential data with deep learning is something that we re just figuring out how to do really well we ve come up with models that seem to work well and we re at the point where we re going to start squeezing a lot of performance out of these models and then you ll see that right and left benchmarks will be dropping when it comes to sequential data beyond that i don t know it s possible we ll start to plateau or we ll start inventing new architectures to do new tasks i think the moral of this story is where there s a lot of data and where it makes sense to use a deep learning model success is with high probability going to happen that s why it feels like progress is happening so rapidly right now it really becomes a story of how can we get right data when deep learning is involved that becomes the big challenge architecturally deep speech runs on a powerful gpu based system where are the opportunities to move deep learning algorithms onto smaller systems such as smartphones in order to offload processing from baidu s or anyone else s servers that s something i think about a lot actually and i think the future is bright in that regard it s certainly the case that deep learning models are getting bigger and bigger but typically it also has also been the case that the size and expressivity of the model is more necessary during training than it is during testing there has been a lot of work that shows that if you take a model that has been trained at say bit floating point precision and then compress it to bit fixed point precision it works just as well at test time or it works almost as well you can reduce it by a factor of four and still have it work just as well there s also a lot of work in compressing existing models like how can we take a giant model that we ve trained to soak up a lot of data and then say train another much smaller model to duplicate what that large model does but that small model we can actually put into an embedded device somewhere often the hard part is in training the system in those cases it needs to be really big and the servers have to be really beefy but i do think there s a lot of promising work with which we can make the models a lot smaller and there s a future in terms of embedding them in different places of course something like search has to go back to cloud servers unless you ve somehow indexed the whole web on your smartphone right yeah that would be challenging for some additional context on just how powerful a system deep speech is and why baidu puts so much emphasis on systems architecture for its deep learning efforts consider this explanation offered by baidu systems research scientist bryan catanzaro from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder editor writer of architecht day job is running content at replicated formerly at gigaom mesosphere fortune what s next in computing told by the people behind the software
Kyle McDonald,109,6,https://medium.com/@kcimc/comparing-artificial-artists-7d889428fce4?source=tag_archive---------7----------------,Comparing Artificial Artists – Kyle McDonald – Medium,last wednesday a neural algorithm of artistic style was posted to arxiv featuring some of the most compelling imagery generated by deep convolutional neural networks dcnns since google research s deepdream post on sunday kai sheng tai posted the first public implementation i immediately stopped working on my implementation and started playing with his unfortunately his results don t quite match the paper and it s unclear why i m just getting started with this topic so as i learn i want to share my understanding of the algorithm here along with some results i got from testing his code in two parts the paper describes an algorithm for rendering a photo in the style of a given painting instead of trying to match the activations exactly try to match the correlation of the activations they call this style reconstruction and depending on the layer you reconstruct you get varying levels of abstraction the correlation feature they use is called a gram matrix the dot product between the vectorized feature activation matrix and its transpose if this sounds confusing see the footnotes finally instead of optimizing for just one of these things they optimize for both simultaneously the style of one image and the content of another image here is an attempt to recreate the results from the paper using kai s implementation not quite the same and possibly explained by a few differences between kai s implementation and the original paper as a final comparison consider the images andrej karpathy posted from his own implementation the same large scale high level features are missing here just like in the style reconstruction of seated nude above beside s kai s i ve seen one more implementation from a phd student named satoshi a brief example in python with chainer i haven t spent as much time with it as i had to adapt it to run on my cpu due to lack of memory but i did notice after running tu bingen in the style of the starry night with a e ratio and iterations it seems to converge on something matching the general structure but lacking the overall palette i d like to understand this algorithm well enough to generalize it to other media mainly thinking about sound right now so if you have an insights or other implementations please share them in the comments i ve started testing another implementation that popped up this morning from justin johnson his follow the original paper very closely except for using unequal weights when balancing different layers used for style reconstruction all the following examples were run for iterations with the default ratio of e justin switched his implementation to use l bfgs and equally weighted layers and to my eyes this matches the results in the original paper here are his results for one of the harder content style pairs other implementations that look great but i haven t tested enough the definition of the gram matrix confused me at first so i wrote it out as code using a literal translation of equation in the paper you would write in python with numpy it turns out that the original description is computed more efficiently than this literal translation for example kai writes in lua with torch satoshi computes it for all the layers simultaneously in python with chainer or again in python with numpy and caffe layers from a quick cheer to a standing ovation clap to show how much you enjoyed this story artist working with code
Jim Fleming,165,4,https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa?source=tag_archive---------8----------------,Highway Networks with TensorFlow – Jim Fleming – Medium,this week i implemented highway networks to get an intuition for how they work highway networks inspired by lstms are a method of constructing networks with hundreds even thousands of layers let s see how we construct them using tensorflow tl dr fully connected highway repo and convolutional highway repo for comparison let s start with a standard fully connected or dense layer we need a weight matrix and a bias vector then we ll compute the following for the layer output here s what a dense layer looks like as a graph in tensorboard for the highway layer what we want are two gates that control the flow of information the transform gate controls how much of the activation we pass through and the carry gate controls how much of the unmodified input we pass through otherwise the layer largely resembles a dense layer with a few additions what happens is that when the transform gate is we pass through our activation h and suppress the carry gate since it will be when the carry gate is we pass through the unmodified input x while the activation is suppressed here s what the highway layer graph looks in tensorboard using a highway layer in a network is also straightforward one detail to keep in mind is that consecutive highway layers must be the same size but you can use fully connected layers to change dimensionality this becomes especially complicated in convolutional layers where each layer can change the output dimensions we can use padding same to maintain each layers dimensionality otherwise by simply using hyperparameters from the tensorflow docs i e no hyperparameter search the fully connected highway network performed much better than a fully connected network using mnist as my simple trial now that we have a highway network i wanted to answer a few questions that came up for me while reading the paper for instance how deep will the network converge the paper briefly mentions layers can we train with layers on mnist yes also reaching around accuracy try it out with a carry bias around for mnist from the paper the network will only utilize layers anyway the network can probably even go deeper since the it s just learning to carry the last layers or so we can t do much useful at or past layers so that seems sufficient for now what happens if you set very low or very high carry biases in either extreme the network simply fails to converge in a reasonable amount of time in the case of low biases more positive the network starts as if the carry gates aren t present at all in the case of high biases more negative we re putting more emphasis on carrying and the network can take a long time to overcome that otherwise the biases don t seem to need to be exact at least on this simple example when in doubt start with high biases more negative since it s easier to learn to overcome carrying than without carry gates which is just a plain network overall i was happy with how easy highway networks were to implement they re fully differentiable with only a single additional hyperparameter for the initial carry bias one downside is that highway layers do require additional parameters for the transform weights and biases however since we can go deeper the layers do not need to be as wide which can compensate here s are the complete notebooks if you want to play with the code fully connected highway repo and convolutional highway repo follow me on twitter for more posts like these if you d like building very deep networks in production i do consulting from a quick cheer to a standing ovation clap to show how much you enjoyed this story cto and lead ml engineer at fomoro focused on machine learning and applying cutting edge research for businesses previously rdio what i m working on
Nathan Benaich,264,10,https://medium.com/@NathanBenaich/investing-in-artificial-intelligence-a-vc-perspective-afaf6adc82ea?source=tag_archive---------9----------------,Investing in Artificial Intelligence – Nathan Benaich – Medium,my expanded talking points from a presentation i gave at the re work investing in deep learning dinner in london on st december tl dr check out the slides here it s my belief that artificial intelligence is one of the most exciting and transformative opportunities of our time there s a few reasons why that s so consumers worldwide carry billion smartphones they re increasingly addicted to these devices and of the world is online kpcb this means we re creating new data assets that never existed before user behavior preferences interests knowledge connections the costs of compute and storage are both plummeting by orders of magnitude while the computational capacity of today s processors is growing we ve seen improvements in learning methods architectures and software infrastructure the pace of innovation can therefore only be accelerating indeed we don t fully appreciate what tomorrow will look and feel like ai driven products are already out in the wild and improving the performance of search engines recommender systems e g e commerce music ad serving and financial trading amongst others companies with the resources to invest in ai are already creating an impetus for others to follow suit or risk not having a competitive seat at the table together therefore the community has a better understanding and is equipped with more capable tools with which to build learning systems for a wide range of increasingly complex tasks more on this discussion here a key consideration in my view is that the open sourcing of technologies by large incumbents google microsoft intel ibm and the range of companies productising technologies for cheap means that technical barriers are eroding fast what ends up moving the needle are proprietary data access creation experienced talent and addictive products operational commercial financial there are two big factors that make involving the user in an ai driven product paramount machines don t yet recapitulate human cognition in order to pick up where software falls short we need to call on the user for help buyers users of software products have more choice today than ever as such they re often fickle avg day retention for apps is returning expected value out of the box is key to building habits hyperparameter optimisation can help here are some great examples of products which prove that involving the user in the loop improves performance we can even go a step further i think by explaining how machine generated results are obtained for example ibm watson surfaces relevant literature when supporting a patient diagnosis in the oncology clinic doing so improves user satisfaction and helps build confidence in the system to encourage longer term use and investment remember it s generally hard for us to trust something we don t truly understand to put this discussion into context let s first look at the global vc market q q saw bn invested a volume higher than each of the full year totals for of the last years nvca we re likely to breach bn by year end there are circa companies working in the ai field most of which tackle problems in business intelligence finance and security q saw a flurry of deals into ai companies started by well respected and achieved academics vicarious scaled inference metamind and sentient technologies so far we ve seen circa deals into ai companies defined as businesses whose description includes keywords artificial intelligence machine learning computer vision nlp data science neural network deep learning from jan st thru st dec cb insights in the uk companies like ravelin signal and gluru raised seed rounds circa bn was invested albeit bloated by large venture debt or credit lines for consumer business loan providers avant m debt credit zestfinance m debt liftforward m credit and argon credit m credit importantly of deals were m in size and of the cash was invested into us companies vs in europe of rounds were in the us the exit market has seen m a transactions and ipo adgorithms on the lse six events were for european companies in asia and the rest were accounted for by american companies the largest transactions were tellapart twitter m m raised elastica blue coat systems m m raised and supersonicads ironsource m m raised which return solid multiples of invested capital the remaining transactions were mostly for talent given that median team size at the time of the acquisition was ppl median altogether ai investments will have accounted for circa of total vc investments for that s higher than the claimed in but still tracking far behind competing categories like adtech mobile and bi software the key takeaway points are a the financing and exit markets for ai companies are still nascent as exemplified by the small rounds and low deal volumes and b the vast majority of activity takes place in the us businesses must therefore have exposure to this market i spent a number of summers in university and years in grad school researching the genetic factors governing the spread of cancer around the body a key takeaway i left with is the following therapeutic development is a very challenging expensive lengthy regulated and ultimately offers a transient solution to treating disease instead i truly believe that what we need to improve healthcare outcomes is granular and longitudinal monitoring of physiology and lifestyle this should enable early detection of health conditions in near real time drive down cost of care over a patient s lifetime while consequently improving outcomes consider the digitally connected lifestyles we lead today the devices some of us interact with on a daily basis are able to track our movements vital signs exercise sleep and even reproductive health we re disconnected for fewer hours of the day than we re online and i think we re less apprehensive to storing various data types in the cloud where they can be accessed with consent by rd parties sure the news might paint a different but the fact is that we re still using the web and it s wealth of products on a population level therefore we have the chance to interrogate data sets that have never before existed from these we could glean insights into how nature and nurture influence the genesis and development of disease that s huge look at today s clinical model a patient presents into the hospital when they feel something is wrong the doctor has to conduct a battery of tests to derive a diagnosis these tests address a single often late stage time point at which moment little can be done to reverse damage e g in the case of cancer now imagine the future in a world of continuous non invasive monitoring of physiology and lifestyle we could predict disease onset and outcome understand which condition a patient likely suffers from and how they ll respond to various therapeutic modalities there s loads of applications for artificial intelligence here intelligence sensors signal processing anomaly detection multivariate classifiers deep learning on molecular interactions some companies are already hacking away at this problem a point worth noting is that the uk has a slight leg up on the data access front initiatives like the uk biobank k patient records genomics england k genomes sequenced hipsci stem cells and the nhs care data programme are leading the way in creating centralised data repositories for public health and therapeutic research cheers for pointing out hari arul could businesses ever conceivably run themselves ai enabled automation of knowledge work could cut employment costs by tn by baml coupled to the efficiency gains worth tn driven by robots i reckon there s a chance for near complete automation of core repetitive businesses functions in the future think of all the productised saas tools that are available off the shelf for crm marketing billing payments logistics web development customer interactions finance hiring and bi then consider tools like zapier or tray io which help connect applications and program business logic these could be further expanded by leveraging contextual data points that inform decision making perhaps we could eventually re image the new ebay where you ll have fully automated inventory procurement pricing listing generation translation recommendations transaction processing customer interaction packaging fulfilment and shipping of course probably a ways off i m bullish on the value to be created with artificial intelligence across our personal and professional lives i think there s currently low vc risk tolerance for this sector especially given shortening investment horizons for value to be created more support is needed for companies driving long term innovation especially that far less is occurring within universities vc was born to fund moonshots we must remember that access to technology will over time become commoditised it s therefore key to understand your use case your user the value you bring and how it s experience and assessed this gets to the point of finding a strategy to build a sustainable advantage such that others find it hard to replicate your offering aspects of this strategy may in fact be non ai and non technical in nature e g the user experience layer thanks for highlighting this hari arul as such there s a renewed focused on core principles build a solution to an unsolved poorly served high value persistent problem for consumers or businesses finally you must have exposure to the us market where the lion s share of value is created and realised we have an opportunity to catalyse the growth of the ai sector in europe but not without keeping close tabs on what works doesn t work across the pond first hand working in the space we d love to get to know you sign up to my newsletter covering ai news and analysis from the tech world research lab and private public company market i m an investor at playfair capital a london based investment firm focusing on early stage technology companies that change the way we live work and play we invest across europe and the us and our focus is on core technologies and user experiences of our portfolio is ai mapillary duedil jukedeck seldon clarify gluru and ravelin we want to take risk on technologists creating new markets or reinventing existing ones from a quick cheer to a standing ovation clap to show how much you enjoyed this story advancing human progress with intelligent systems venture partner pointninecap former scientist photographer perpetual foodie nathan ai ldn ai twentybn
Tal Perry,2600,17,https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02?source=tag_archive---------3----------------,Deep Learning the Stock Market – Tal Perry – Medium,update took me a while but here is an ipython notebook with a rough implementation in the past few months i ve been fascinated with deep learning especially its applications to language and text i ve spent the bulk of my career in financial technologies mostly in algorithmic trading and alternative data services you can see where this is going i wrote this to get my ideas straight in my head while i ve become a deep learning enthusiast i don t have too many opportunities to brain dump an idea in most of its messy glory i think that a decent indication of a clear thought is the ability to articulate it to people not from the field i hope that i ve succeeded in doing that and that my articulation is also a pleasurable read why nlp is relevant to stock prediction in many nlp problems we end up taking a sequence and encoding it into a single fixed size representation then decoding that representation into another sequence for example we might tag entities in the text translate from english to french or convert audio frequencies to text there is a torrent of work coming out in these areas and a lot of the results are achieving state of the art performance in my mind the biggest difference between the nlp and financial analysis is that language has some guarantee of structure it s just that the rules of the structure are vague markets on the other hand don t come with a promise of a learnable structure that such a structure exists is the assumption that this project would prove or disprove rather it might prove or disprove if i can find that structure assuming the structure is there the idea of summarizing the current state of the market in the same way we encode the semantics of a paragraph seems plausible to me if that doesn t make sense yet keep reading it will you shall know a word by the company it keeps firth j r there is tons of literature on word embeddings richard socher s lecture is a great place to start in short we can make a geometry of all the words in our language and that geometry captures the meaning of words and relationships between them you may have seen the example of king man woman queen or something of the sort embeddings are cool because they let us represent information in a condensed way the old way of representing words was holding a vector a big list of numbers that was as long as the number of words we know and setting a in a particular place if that was the current word we are looking at that is not an efficient approach nor does it capture any meaning with embeddings we can represent all of the words in a fixed number of dimensions seems to be plenty works great and then leverage their higher dimensional geometry to understand them the picture below shows an example an embedding was trained on more or less the entire internet after a few days of intensive calculations each word was embedded in some high dimensional space this space has a geometry concepts like distance and so we can ask which words are close together the authors inventors of that method made an example here are the words that are closest to frog but we can embed more than just words we can do say stock market embeddings market vec the first word embedding algorithm i heard about was word vec i want to get the same effect for the market though i ll be using a different algorithm my input data is a csv the first column is the date and there are columns corresponding to the high low open closing price of stocks that is my input vector is dimensional which is too big so the first thing i m going to do is stuff it into a lower dimensional space say because i liked the movie taking something in dimensions and stuffing it into a dimensional space my sound hard but its actually easy we just need to multiply matrices a matrix is a big excel spreadsheet that has numbers in every cell and no formatting problems imagine an excel table with columns and rows and when we basically bang it against the vector a new vector comes out that is only of size i wish that s how they would have explained it in college the fanciness starts here as we re going to set the numbers in our matrix at random and part of the deep learning is to update those numbers so that our excel spreadsheet changes eventually this matrix spreadsheet i ll stick with matrix from now on will have numbers in it that bang our original dimensional vector into a concise dimensional summary of itself we re going to get a little fancier here and apply what they call an activation function we re going to take a function and apply it to each number in the vector individually so that they all end up between and or and infinity it depends why it makes our vector more special and makes our learning process able to understand more complicated things how so what what i m expecting to find is that that new embedding of the market prices the vector into a smaller space captures all the essential information for the task at hand without wasting time on the other stuff so i d expect they d capture correlations between other stocks perhaps notice when a certain sector is declining or when the market is very hot i don t know what traits it will find but i assume they ll be useful now what lets put aside our market vectors for a moment and talk about language models andrej karpathy wrote the epic post the unreasonable effectiveness of recurrent neural networks if i d summarize in the most liberal fashion the post boils down to and then as a punchline he generated a bunch of text that looks like shakespeare and then he did it again with the linux source code and then again with a textbook on algebraic geometry so i ll get back to the mechanics of that magic box in a second but let me remind you that we want to predict the future market based on the past just like he predicted the next word based on the previous one where karpathy used characters we re going to use our market vectors and feed them into the magic black box we haven t decided what we want it to predict yet but that is okay we won t be feeding its output back into it either going deeper i want to point out that this is where we start to get into the deep part of deep learning so far we just have a single layer of learning that excel spreadsheet that condenses the market now we re going to add a few more layers and stack them to make a deep something that s the deep in deep learning so karpathy shows us some sample output from the linux source code this is stuff his black box wrote notice that it knows how to open and close parentheses and respects indentation conventions the contents of the function are properly indented and the multi line printk statement has an inner indentation that means that this magic box understands long range dependencies when it s indenting within the print statement it knows it s in a print statement and also remembers that it s in a function or at least another indented scope that s nuts it s easy to gloss over that but an algorithm that has the ability to capture and remember long term dependencies is super useful because we want to find long term dependencies in the market inside the magical black box what s inside this magical black box it is a type of recurrent neural network rnn called an lstm an rnn is a deep learning algorithm that operates on sequences like sequences of characters at every step it takes a representation of the next character like the embeddings we talked about before and operates on the representation with a matrix like we saw before the thing is the rnn has some form of internal memory so it remembers what it saw previously it uses that memory to decide how exactly it should operate on the next input using that memory the rnn can remember that it is inside of an intended scope and that is how we get properly nested output text a fancy version of an rnn is called a long short term memory lstm lstm has cleverly designed memory that allows it to so an lstm can see a and say to itself oh yeah that s important i should remember that and when it does it essentially remembers an indication that it is in a nested scope once it sees the corresponding it can decide to forget the original opening brace and thus forget that it is in a nested scope we can have the lstm learn more abstract concepts by stacking a few of them on top of each other that would make us deep again now each output of the previous lstm becomes the inputs of the next lstm and each one goes on to learn higher abstractions of the data coming in in the example above and this is just illustrative speculation the first layer of lstms might learn that characters separated by a space are words the next layer might learn word types like static void action new function the next layer might learn the concept of a function and its arguments and so on it s hard to tell exactly what each layer is doing though karpathy s blog has a really nice example of how he did visualize exactly that connecting market vec and lstms the studious reader will notice that karpathy used characters as his inputs not embeddings technically a one hot encoding of characters but lars eidnes actually used word embeddings when he wrote auto generating clickbait with recurrent neural network the figure above shows the network he used ignore the softmax part we ll get to it later for the moment check out how on the bottom he puts in a sequence of words vectors at the bottom and each one remember a word vector is a representation of a word in the form of a bunch of numbers like we saw in the beginning of this post lars inputs a sequence of word vectors and each one of them we re going to do the same thing with one difference instead of word vectors we ll input marketvectors those market vectors we described before to recap the marketvectors should contain a summary of what s happening in the market at a given point in time by putting a sequence of them through lstms i hope to capture the long term dynamics that have been happening in the market by stacking together a few layers of lstms i hope to capture higher level abstractions of the market s behavior what comes out thus far we haven t talked at all about how the algorithm actually learns anything we just talked about all the clever transformations we ll do on the data we ll defer that conversation to a few paragraphs down but please keep this part in mind as it is the se up for the punch line that makes everything else worthwhile in karpathy s example the output of the lstms is a vector that represents the next character in some abstract representation in eidnes example the output of the lstms is a vector that represents what the next word will be in some abstract space the next step in both cases is to change that abstract representation into a probability vector that is a list that says how likely each character or word respectively is likely to appear next that s the job of the softmax function once we have a list of likelihoods we select the character or word that is the most likely to appear next in our case of predicting the market we need to ask ourselves what exactly we want to market to predict some of the options that i thought about were and are regression problems where we have to predict an actual number instead of the likelihood of a specific event like the letter n appearing or the market going up those are fine but not what i want to do and are fairly similar they both ask to predict an event in technical jargon a class label an event could be the letter n appearing next or it could be moved up while not going down more than in the last minutes the trade off between and is that is much more common and thus easier to learn about while is more valuable as not only is it an indicator of profit but also has some constraint on risk is the one we ll continue with for this article because it s similar to and but has mechanics that are easier to follow the vix is sometimes called the fear index and it represents how volatile the stocks in the s p are it is derived by observing the implied volatility for specific options on each of the stocks in the index sidenote why predict the vix what makes the vix an interesting target is that back to our lstm outputs and the softmax how do we use the formulations we saw before to predict changes in the vix a few minutes in the future for each point in our dataset we ll look what happened to the vix minutes later if it went up by more than without going down more than during that time we ll output a otherwise a then we ll get a sequence that looks like we want to take the vector that our lstms output and squish it so that it gives us the probability of the next item in our sequence being a the squishing happens in the softmax part of the diagram above technically since we only have class now we use a sigmoid so before we get into how this thing learns let s recap what we ve done so far how does this thing learn now the fun part everything we did until now was called the forward pass we d do all of those steps while we train the algorithm and also when we use it in production here we ll talk about the backward pass the part we do only while in training that makes our algorithm learn so during training not only did we prepare years worth of historical data we also prepared a sequence of prediction targets that list of and that showed if the vix moved the way we want it to or not after each observation in our data to learn we ll feed the market data to our network and compare its output to what we calculated comparing in our case will be simple subtraction that is we ll say that our model s error is or in english the square root of the square of the difference between what actually happened and what we predicted here s the beauty that s a differential function that is we can tell by how much the error would have changed if our prediction would have changed a little our prediction is the outcome of a differentiable function the softmax the inputs to the softmax the lstms are all mathematical functions that are differentiable now all of these functions are full of parameters those big excel spreadsheets i talked about ages ago so at this stage what we do is take the derivative of the error with respect to every one of the millions of parameters in all of those excel spreadsheets we have in our model when we do that we can see how the error will change when we change each parameter so we ll change each parameter in a way that will reduce the error this procedure propagates all the way to the beginning of the model it tweaks the way we embed the inputs into marketvectors so that our marketvectors represent the most significant information for our task it tweaks when and what each lstm chooses to remember so that their outputs are the most relevant to our task it tweaks the abstractions our lstms learn so that they learn the most important abstractions for our task which in my opinion is amazing because we have all of this complexity and abstraction that we never had to specify anywhere it s all inferred mathamagically from the specification of what we consider to be an error what s next now that i ve laid this out in writing and it still makes sense to me i want so if you ve come this far please point out my errors and share your inputs other thoughts here are some mostly more advanced thoughts about this project what other things i might try and why it makes sense to me that this may actually work liquidity and efficient use of capital generally the more liquid a particular market is the more efficient that is i think this is due to a chicken and egg cycle whereas a market becomes more liquid it is able to absorb more capital moving in and out without that capital hurting itself as a market becomes more liquid and more capital can be used in it you ll find more sophisticated players moving in this is because it is expensive to be sophisticated so you need to make returns on a large chunk of capital in order to justify your operational costs a quick corollary is that in less liquid markets the competition isn t quite as sophisticated and so the opportunities a system like this can bring may not have been traded away the point being were i to try and trade this i would try and trade it on less liquid segments of the market that is maybe the tase instead of the s p this stuff is new the knowledge of these algorithms the frameworks to execute them and the computing power to train them are all new at least in the sense that they are available to the average joe such as myself i d assume that top players have figured this stuff out years ago and have had the capacity to execute for as long but as i mention in the above paragraph they are likely executing in liquid markets that can support their size the next tier of market participants i assume have a slower velocity of technological assimilation and in that sense there is or soon will be a race to execute on this in as yet untapped markets multiple time frames while i mentioned a single stream of inputs in the above i imagine that a more efficient way to train would be to train market vectors at least on multiple time frames and feed them in at the inference stage that is my lowest time frame would be sampled every seconds and i d expect the network to learn dependencies that stretch hours at most i don t know if they are relevant or not but i think there are patterns on multiple time frames and if the cost of computation can be brought low enough then it is worthwhile to incorporate them into the model i m still wrestling with how best to represent these on the computational graph and perhaps it is not mandatory to start with marketvectors when using word vectors in nlp we usually start with a pretrained model and continue adjusting the embeddings during training of our model in my case there are no pretrained market vector available nor is tehre a clear algorithm for training them my original consideration was to use an auto encoder like in this paper but end to end training is cooler a more serious consideration is the success of sequence to sequence models in translation and speech recognition where a sequence is eventually encoded as a single vector and then decoded into a different representation like from speech to text or from english to french in that view the entire architecture i described is essentially the encoder and i haven t really laid out a decoder but i want to achieve something specific with the first layer the one that takes as input the dimensional vector and outputs a dimensional one i want it to find correlations or relations between various stocks and compose features about them the alternative is to run each input through an lstm perhaps concatenate all of the output vectors and consider that output of the encoder stage i think this will be inefficient as the interactions and correlations between instruments and their features will be lost and thre will be x more computation required on the other hand such an architecture could naively be paralleled across multiple gpus and hosts which is an advantage cnns recently there has been a spur of papers on character level machine translation this paper caught my eye as they manage to capture long range dependencies with a convolutional layer rather than an rnn i haven t given it more than a brief read but i think that a modification where i d treat each stock as a channel and convolve over channels first like in rgb images would be another way to capture the market dynamics in the same way that they essentially encode semantic meaning from characters from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder of https lighttag io platform to annotate text for nlp google developer expert in ml i do deep learning on text for a living and for fun
Andrej Karpathy,9200,7,https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b?source=tag_archive---------4----------------,Yes you should understand backprop – Andrej Karpathy – Medium,when we offered cs n deep learning class at stanford we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level the students had to implement the forward and the backward pass of each layer in raw numpy inevitably some students complained on the class message boards this is seemingly a perfectly sensible appeal if you re never going to write backward passes once the class is over why practice writing them are we just torturing the students for our own amusement some easy answers could make arguments along the lines of it s worth knowing what s under the hood as an intellectual curiosity or perhaps you might want to improve on the core algorithm later but there is a much stronger and practical argument which i wanted to devote a whole post to the problem with backpropagation is that it is a leaky abstraction in other words it is easy to fall into the trap of abstracting away the learning process believing that you can simply stack arbitrary layers together and backprop will magically make them work on your data so lets look at a few explicit examples where this is not the case in quite unintuitive ways we re starting off easy here at one point it was fashionable to use sigmoid or tanh non linearities in the fully connected layers the tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non linearities can saturate and entirely stop learning your training loss will be flat and refuse to go down for example a fully connected layer with sigmoid non linearity computes using raw numpy if your weight matrix w is initialized too large the output of the matrix multiply could have a very large range e g numbers between and which will make all outputs in the vector z almost binary either or but if that is the case z z which is local gradient of the sigmoid non linearity will in both cases become zero vanish making the gradient for both x and w be zero the rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule another non obvious fun fact about sigmoid is that its local gradient z z achieves a maximum at when z that means that every time the gradient signal flows through a sigmoid gate its magnitude always diminishes by one quarter or more if you re using basic sgd this would make the lower layers of a network train much slower than the higher ones tldr if you re using sigmoids or tanh non linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn t cause them to be fully saturated see a longer explanation in this cs n lecture video another fun non linearity is the relu which thresholds neurons at zero from below the forward and backward pass for a fully connected layer that uses relu would at the core include if you stare at this for a while you ll see that if a neuron gets clamped to zero in the forward pass i e z it doesn t fire then its weights will get zero gradient this can lead to what is called the dead relu problem where if a relu neuron is unfortunately initialized such that it never fires or if a neuron s weights ever get knocked off with a large update during training into this regime then this neuron will remain permanently dead it s like permanent irrecoverable brain damage sometimes you can forward the entire training set through a trained network and find that a large fraction e g of your neurons were zero the entire time tldr if you understand backpropagation and your network has relus you re always nervous about dead relus these are neurons that never turn on for any example in your entire training set and will remain permanently dead neurons can also die during training usually as a symptom of aggressive learning rates see a longer explanation in cs n lecture video vanilla rnns feature another good example of unintuitive effects of backpropagation i ll copy paste a slide from cs n that has a simplified rnn that does not take any input x and only computes the recurrence on the hidden state equivalently the input x could always be zero this rnn is unrolled for t time steps when you stare at what the backward pass is doing you ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix the recurrence matrix whh interspersed with non linearity backprop what happens when you take one number a and start multiplying it by some other number b i e a b b b b b b this sequence either goes to zero if b or explodes to infinity when b the same thing happens in the backward pass of an rnn except b is a matrix and not just a number so we have to reason about its largest eigenvalue instead tldr if you understand backpropagation and you re using rnns you are nervous about having to do gradient clipping or you prefer to use an lstm see a longer explanation in this cs n lecture video lets look at one more the one that actually inspired this post yesterday i was browsing for a deep q learning implementation in tensorflow to see how others deal with computing the numpy equivalent of q a where a is an integer vector turns out this trivial operation is not supported in tf anyway i searched dqn tensorflow clicked the first link and found the core code here is an excerpt if you re familiar with dqn you can see that there is the target q t which is just reward gamma argmax a q s a and then there is q acted which is q s a of the action that was taken the authors here subtract the two into variable delta which they then want to minimize on line with the l loss with tf reduce mean tf square so far so good the problem is on line the authors are trying to be robust to outliers so if the delta is too large they clip it with tf clip by value this is well intentioned and looks sensible from the perspective of the forward pass but it introduces a major bug if you think about the backward pass the clip by value function has a local gradient of zero outside of the range min delta to max delta so whenever the delta is above min max delta the gradient becomes exactly zero during backprop the authors are clipping the raw q delta when they are likely trying to clip the gradient for added robustness in that case the correct thing to do is to use the huber loss in place of tf square it s a bit gross in tensorflow because all we want to do is clip the gradient if it is above a threshold but since we can t meddle with the gradients directly we have to do it in this round about way of defining the huber loss in torch this would be much more simple i submitted an issue on the dqn repo and this was promptly fixed backpropagation is a leaky abstraction it is a credit assignment scheme with non trivial consequences if you try to ignore how it works under the hood because tensorflow automagically makes my networks learn you will not be ready to wrestle with the dangers it presents and you will be much less effective at building and debugging neural networks the good news is that backpropagation is not that difficult to understand if presented properly i have relatively strong feelings on this topic because it seems to me that of backpropagation materials out there present it all wrong filling pages with mechanical math instead i would recommend the cs n lecture on backprop which emphasizes intuition yay for shameless self advertising and if you can spare the time as a bonus work through the cs n assignments which get you to write backprop manually and help you solidify your understanding that s it for now i hope you ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing also i m aware that this post has unintentionally turned into several cs n ads apologies for that from a quick cheer to a standing ovation clap to show how much you enjoyed this story director of ai at tesla previously research scientist at openai and phd student at stanford i like to train deep neural nets on large datasets
Erik Hallström,2500,7,https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767?source=tag_archive---------5----------------,How to build a Recurrent Neural Network in TensorFlow (1/7),in this tutorial i ll explain how to build a simple working recurrent neural network in tensorflow this is the first in a series of seven parts where various aspects and techniques of building recurrent neural networks in tensorflow are covered a short introduction to tensorflow is available here for now let s get started with the rnn it is short for recurrent neural network and is basically a neural network that can be used when your data is treated as a sequence where the particular order of the data points matter more importantly this sequence can be of arbitrary length the most straight forward example is perhaps a time series of numbers where the task is to predict the next value given previous values the input to the rnn at every time step is the current value as well as a state vector which represent what the network has seen at time steps before this state vector is the encoded memory of the rnn initially set to zero the best and most comprehensive article explaining rnn s i ve found so far is this article by researchers at ucsd highly recommended for now you only need to understand the basics read it until the modern rnn architectures section that will be covered later although this article contains some explanations it is mostly focused on the practical part how to build it you are encouraged to look up more theory on the internet there are plenty of good explanations we will build a simple echo rnn that remembers the input data and then echoes it after a few time steps first let s set some constants we ll need what they mean will become clear in a moment now generate the training data the input is basically a random binary vector the output will be the echo of the input shifted echo step steps to the right notice the reshaping of the data into a matrix with batch size rows neural networks are trained by approximating the gradient of loss function with respect to the neuron weights by looking at only a small subset of the data also known as a mini batch the theoretical reason for doing this is further elaborated in this question the reshaping takes the whole dataset and puts it into a matrix that later will be sliced up into these mini batches tensorflow works by first building up a computational graph that specifies what operations will be done the input and output of this graph is typically multidimensional arrays also known as tensors the graph or parts of it can then be executed iteratively in a session this can either be done on the cpu gpu or even a resource on a remote server the two basic tensorflow data structures that will be used in this example are placeholders and variables on each run the batch data is fed to the placeholders which are starting nodes of the computational graph also the rnn state is supplied in a placeholder which is saved from the output of the previous run the weights and biases of the network are declared as tensorflow variables which makes them persistent across runs and enables them to be updated incrementally for each batch the figure below shows the input data matrix and the current batch batchx placeholder is in the dashed rectangle as we will see later this batch window is slided truncated backprop length steps to the right at each run hence the arrow in our example below batch size truncated backprop length and total series length note that these numbers are just for visualization purposes the values are different in the code the series order index is shown as numbers in a few of the data points now it s time to build the part of the graph that resembles the actual rnn computation first we want to split the batch data into adjacent time steps as you can see in the picture below that is done by unpacking the columns axis of the batch into a python list the rnn will simultaneously be training on different parts in the time series steps to to and to in the current batch example the reason for using the variable names plural series is to emphasize that the variable is a list that represent a time series with multiple entries at each step the fact that the training is done on three places simultaneously in our time series requires us to save three instances of states when propagating forward that has already been accounted for as you see that the init state placeholder has batch size rows next let s build the part of the graph that does the actual rnn computation notice the concatenation on line what we actually want to do is calculate the sum of two affine transforms current input wa current state wb in the figure below by concatenating those two tensors you will only use one matrix multiplication the addition of the bias b is broadcasted on all samples in the batch you may wonder the variable name truncated backprop length is supposed to mean when a rnn is trained it is actually treated as a deep neural network with reoccurring weights in every layer these layers will not be unrolled to the beginning of time that would be too computationally expensive and are therefore truncated at a limited number of time steps in our sample schematics above the error is backpropagated three steps in our batch this is the final part of the graph a fully connected softmax layer from the state to the output that will make the classes one hot encoded and then calculating the loss of the batch the last line is adding the training functionality tensorflow will perform back propagation for us automatically the computation graph is executed once for each mini batch and the network weights are updated incrementally notice the api call to sparse softmax cross entropy with logits it automatically calculates the softmax internally and then computes the cross entropy in our example the classes are mutually exclusive they are either zero or one which is the reason for using the sparse softmax you can read more about it in the api the usage is to havelogits is of shape batch size num classes and labels of shape batch size there is a visualization function so we can se what s going on in the network as we train it will plot the loss over the time show training input training output and the current predictions by the network on different sample series in a training batch it s time to wrap up and train the network in tensorflow the graph is executed in a session new data is generated on each epoch not the usual way to do it but it works in this case since everything is predictable you can see that we are moving truncated backprop length steps forward on each iteration line but it is possible have different strides this subject is further elaborated in this article the downside with doing this is that truncated backprop length need to be significantly larger than the time dependencies three steps in our case in order to encapsulate the relevant training data otherwise there might a lot of misses as you can see on the figure below also realize that this is just simple example to explain how a rnn works this functionality could easily be programmed in just a few lines of code the network will be able to exactly learn the echo behavior so there is no need for testing data the program will update the plot as training progresses shown in the picture below blue bars denote a training input signal binary one red bars show echos in the training output and green bars are the echos the net is generating the different bar plots show different sample series in the current batch our algorithm will fairly quickly learn the task the graph in the top left corner shows the output of the loss function but why are there spikes in the curve think of it for a moment answer is below the reason for the spikes is that we are starting on a new epoch and generating new data since the matrix is reshaped the first element on each row is adjacent to the last element in the previous row the first few elements on all rows except the first have dependencies that will not be included in the state so the net will always perform badly on the first batch this is the whole runnable program just copy paste and run after each part in the article series the whole runnable program will be presented if a line is referenced by number these are the line numbers that we mean in the next post in this series we will be simplify the computational graph creation by using the native tensorflow rnn api from a quick cheer to a standing ovation clap to show how much you enjoyed this story studied engineering physics and in machine learning at royal institute of technology in stockholm also been living in taiwan interested in deep learning
Stefan Kojouharov,1500,23,https://chatbotslife.com/ultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c?source=tag_archive---------6----------------,Ultimate Guide to Leveraging NLP & Machine Learning for your Chatbot,code snippets and github included over the past few months i have been collecting the best resources on nlp and how to apply nlp and deep learning to chatbots every once in awhile i would run across an exception piece of content and i quickly started putting together a master list soon i found myself sharing this list and some of the most useful articles with developers and other people in bot community in process my list became a guide and after some urging i have decided to share it or at least a condensed version of it for length reasons this guide is mostly based on the work done by denny britz who has done a phenomenal job exploring the depths of deep learning for bots code snippets and github included without further ado let us begin chatbots are a hot topic and many companies are hoping to develop bots to have natural conversations indistinguishable from human ones and many are claiming to be using nlp and deep learning techniques to make this possible but with all the hype around ai it s sometimes difficult to tell fact from fiction in this series i want to go over some of the deep learning techniques that are used to build conversational agents starting off by explaining where we are right now what s possible and what will stay nearly impossible for at least a little while retrieval based models easier use a repository of predefined responses and some kind of heuristic to pick an appropriate response based on the input and context the heuristic could be as simple as a rule based expression match or as complex as an ensemble of machine learning classifiers these systems don t generate any new text they just pick a response from a fixed set generative models harder don t rely on pre defined responses they generate new responses from scratch generative models are typically based on machine translation techniques but instead of translating from one language to another we translate from an input to an output response both approaches have some obvious pros and cons due to the repository of handcrafted responses retrieval based methods don t make grammatical mistakes however they may be unable to handle unseen cases for which no appropriate predefined response exists for the same reasons these models can t refer back to contextual entity information like names mentioned earlier in the conversation generative models are smarter they can refer back to entities in the input and give the impression that you re talking to a human however these models are hard to train are quite likely to make grammatical mistakes especially on longer sentences and typically require huge amounts of training data deep learning techniques can be used for both retrieval based or generative models but research seems to be moving into the generative direction deep learning architectures likesequence to sequence are uniquely suited for generating text and researchers are hoping to make rapid progress in this area however we re still at the early stages of building generative models that work reasonably well production systems are more likely to be retrieval based for now long vs short conversations the longer the conversation the more difficult to automate it on one side of the spectrum areshort text conversations easier where the goal is to create a single response to a single input for example you may receive a specific question from a user and reply with an appropriate answer then there are long conversations harder where you go through multiple turns and need to keep track of what has been said customer support conversations are typically long conversational threads with multiple questions in an open domain harder setting the user can take the conversation anywhere there isn t necessarily have a well defined goal or intention conversations on social media sites like twitter and reddit are typically open domain they can go into all kinds of directions the infinite number of topics and the fact that a certain amount of world knowledge is required to create reasonable responses makes this a hard problem in a closed domain easier setting the space of possible inputs and outputs is somewhat limited because the system is trying to achieve a very specific goal technical customer support or shopping assistants are examples of closed domain problems these systems don t need to be able to talk about politics they just need to fulfill their specific task as efficiently as possible sure users can still take the conversation anywhere they want but the system isn t required to handle all these cases and the users don t expect it to there are some obvious and not so obvious challenges when building conversational agents most of which are active research areas to produce sensible responses systems may need to incorporate both linguistic context andphysical context in long dialogs people keep track of what has been said and what information has been exchanged that s an example of linguistic context the most common approach is toembed the conversation into a vector but doing that with long conversations is challenging experiments in building end to end dialogue systems using generative hierarchical neural network models and attention with intention for a neural network conversation model both go into that direction one may also need to incorporate other kinds of contextual data such as date time location or information about a user when generating responses the agent should ideally produce consistent answers to semantically identical inputs for example you want to get the same reply to how old are you and what is your age this may sound simple but incorporating such fixed knowledge or personality into models is very much a research problem many systems learn to generate linguistic plausible responses but they are not trained to generate semantically consistent ones usually that s because they are trained on a lot of data from multiple different users models like that in a persona based neural conversation model are making first steps into the direction of explicitly modeling a personality the ideal way to evaluate a conversational agent is to measure whether or not it is fulfilling its task e g solve a customer support problem in a given conversation but such labels are expensive to obtain because they require human judgment and evaluation sometimes there is no well defined goal as is the case with open domain models common metrics such as bleuthat are used for machine translation and are based on text matching aren t well suited because sensible responses can contain completely different words or phrases in fact in how not to evaluate your dialogue system an empirical study of unsupervised evaluation metrics for dialogue response generation researchers find that none of the commonly used metrics really correlate with human judgment a common problem with generative systems is that they tend to produce generic responses like that s great or i don t know that work for a lot of input cases early versions of google s smart reply tended to respond with i love you to almost anything that s partly a result of how these systems are trained both in terms of data and in terms of actual training objective algorithm some researchers have tried to artificially promote diversity through various objective functions however humans typically produce responses that are specific to the input and carry an intention because generative systems and particularly open domain systems aren t trained to have specific intentions they lack this kind of diversity given all the cutting edge research right now where are we and how well do these systems actually work let s consider our taxonomy again a retrieval based open domain system is obviously impossible because you can never handcraft enough responses to cover all cases a generative open domain system is almost artificial general intelligence agi because it needs to handle all possible scenarios we re very far away from that as well but a lot of research is going on in that area this leaves us with problems in restricted domains where both generative and retrieval based methods are appropriate the longer the conversations and the more important the context the more difficult the problem becomes in a recent interview andrew ng now chief scientist of baidu puts it well many companies start off by outsourcing their conversations to human workers and promise that they can automate it once they ve collected enough data that s likely to happen only if they are operating in a pretty narrow domain like a chat interface to call an uber for example anything that s a bit more open domain like sales emails is beyond what we can currently do however we can also use these systems to assist human workers by proposing and correcting responses that s much more feasible grammatical mistakes in production systems are very costly and may drive away users that s why most systems are probably best off using retrieval based methods that are free of grammatical errors and offensive responses if companies can somehow get their hands on huge amounts of data then generative models become feasible but they must be assisted by other techniques to prevent them from going off the rails like microsoft s tay did the code and data for this tutorial is on github the vast majority of production systems today are retrieval based or a combination of retrieval based and generative google s smart reply is a good example generative models are an active area of research but we re not quite there yet if you want to build a conversational agent today your best bet is most likely a retrieval based model in this post we ll work with the ubuntu dialog corpus paper github the ubuntu dialog corpus udc is one of the largest public dialog datasets available it s based on chat logs from the ubuntu channels on a public irc network the paper goes into detail on how exactly the corpus was created so i won t repeat that here however it s important to understand what kind of data we re working with so let s do some exploration first the training data consists of examples positive label and negative label each example consists of a context the conversation up to this point and an utterance a response to the context a positive label means that an utterance was an actual response to a context and a negative label means that the utterance wasn t it was picked randomly from somewhere in the corpus here is some sample data note that the dataset generation script has already done a bunch of preprocessing for us it hastokenized stemmed and lemmatized the output using the nltk tool the script also replaced entities like names locations organizations urls and system paths with special tokens this preprocessing isn t strictly necessary but it s likely to improve performance by a few percent the average context is words long and the average utterance is words long check out the jupyter notebook to see the data analysis the data set comes with test and validations sets the format of these is different from that of the training data each record in the test validation set consists of a context a ground truth utterance the real response and incorrect utterances called distractors the goal of the model is to assign the highest score to the true utterance and lower scores to wrong utterances the are various ways to evaluate how well our model does a commonly used metric is recall k recall k means that we let the model pick the k best responses out of the possible responses true and distractors if the correct one is among the picked ones we mark that test example as correct so a larger k means that the task becomes easier if we set k we get a recall of because we only have responses to pick from if we set k the model has only one chance to pick the right response at this point you may be wondering how the distractors were chosen in this data set the distractors were picked at random however in the real world you may have millions of possible responses and you don t know which one is correct you can t possibly evaluate a million potential responses to pick the one with the highest score that d be too expensive google ssmart reply uses clustering techniques to come up with a set of possible responses to choose from first or if you only have a few hundred potential responses in total you could just evaluate all of them before starting with fancy neural network models let s build some simple baseline models to help us understand what kind of performance we can expect we ll use the following function to evaluate our recall k metric here y is a list of our predictions sorted by score in descending order and y test is the actual label for example a y of would mean that the utterance number got the highest score and utterance got the lowest score remember that we have utterances for each test example and the first one index is always the correct one because the utterance column comes before the distractor columns in our data intuitively a completely random predictor should get a score of for recall a score of for recall and so on let s see if that s the case great seems to work of course we don t just want a random predictor another baseline that was discussed in the original paper is a tf idf predictor tf idf stands for term frequency inverse document frequency and it measures how important a word in a document is relative to the whole corpus without going into too much detail you can find many tutorials about tf idf on the web documents that have similar content will have similar tf idf vectors intuitively if a context and a response have similar words they are more likely to be a correct pair at least more likely than random many libraries out there such as scikit learn come with built in tf idf functions so it s very easy to use let s build a tf idf predictor and see how well it performs we can see that the tf idf model performs significantly better than the random model it s far from perfect though the assumptions we made aren t that great first of all a response doesn t necessarily need to be similar to the context to be correct secondly tf idf ignores word order which can be an important signal with a neural network model we can do a bit better the deep learning model we will build in this post is called a dual encoder lstm network this type of network is just one of many we could apply to this problem and it s not necessarily the best one you can come up with all kinds of deep learning architectures that haven t been tried yet it s an active research area for example the seq seq model often used in machine translation would probably do well on this task the reason we are going for the dual encoder is because it has been reported to give decent performance on this data set this means we know what to expect and can be sure that our implementation is correct applying other models to this problem would be an interesting project the dual encoder lstm we ll build looks like this paper it roughly works as follows to train the network we also need a loss cost function we ll use the binary cross entropy loss common for classification problems let s call our true label for a context response pair y this can be either actual response or incorrect response let s call our predicted probability from above y then the cross entropy loss is calculated as l y ln y y ln y the intuition behind this formula is simple if y we are left with l ln y which penalizes a prediction far away from and if y we are left with l ln y which penalizes a prediction far away from for our implementation we ll use a combination of numpy pandas tensorflow and tf learn a combination of high level convenience functions for tensorflow the dataset originally comes in csv format we could work directly with csvs but it s better to convert our data into tensorflow s proprietary example format quick side note there s alsotf sequenceexample but it doesn t seem to be supported by tf learn yet the main benefit of this format is that it allows us to load tensors directly from the input files and let tensorflow handle all the shuffling batching and queuing of inputs as part of the preprocessing we also create a vocabulary this means we map each word to an integer number e g cat may become the tfrecord files we will generate store these integer numbers instead of the word strings we will also save the vocabulary so that we can map back from integers to words later on each example contains the following fields the preprocessing is done by the prepare data py python script which generates files train tfrecords validation tfrecords and test tfrecords you can run the script yourself or download the data files here in order to use tensorflow s built in support for training and evaluation we need to create an input function a function that returns batches of our input data in fact because our training and test data have different formats we need different input functions for them the input function should return a batch of features and labels if available something along the lines of because we need different input functions during training and evaluation and because we hate code duplication we create a wrapper called create input fn that creates an input function for the appropriate mode it also takes a few other parameters here s the definition we re using the complete code can be found in udc inputs py on a high level the function does the following we already mentioned that we want to use the recall k metric to evaluate our model luckily tensorflow already comes with many standard evaluation metrics that we can use including recall k to use these metrics we need to create a dictionary that maps from a metric name to a function that takes the predictions and label as arguments above we use functools partial to convert a function that takes arguments to one that only takes arguments don t let the name streaming sparse recall at k confuse you streaming just means that the metric is accumulated over multiple batches and sparse refers to the format of our labels this brings is to an important point what exactly is the format of our predictions during evaluation during training we predict the probability of the example being correct but during evaluation our goal is to score the utterance and distractors and pick the best one we don t simply predict correct incorrect this means that during evaluation each example should result in a vector of scores e g where the scores correspond to the true response and the distractors respectively each utterance is scored independently so the probabilities don t need to add up to because the true response is always element in array the label for each example is the example above would be counted as classified incorrectly by recall because the third distractor got a probability of while the true response only got it would be scored as correct by recall however before writing the actual neural network code i like to write the boilerplate code for training and evaluating the model that s because as long as you adhere to the right interfaces it s easy to swap out what kind of network you are using let s assume we have a model functionmodel fn that takes as inputs our batched features labels and mode train or evaluation and returns the predictions then we can write general purpose code to train our model as follows here we create an estimator for our model fn two input functions for training and evaluation data and our evaluation metrics dictionary we also define a monitor that evaluates our model every flags eval every steps during training finally we train the model the training runs indefinitely but tensorflow automatically saves checkpoint files in model dir so you can stop the training at any time a more fancy technique would be to use early stopping which means you automatically stop training when a validation set metric stops improving i e you are starting to overfit you can see the full code in udc train py two things i want to mention briefly is the usage of flags this is a way to give command line parameters to the program similar to python s argparse hparams is a custom object we create in hparams py that holds hyperparameters nobs we can tweak of our model this hparams object is given to the model when we instantiate it now that we have set up the boilerplate code around inputs parsing evaluation and training it s time to write code for our dual lstm neural network because we have different formats of training and evaluation data i ve written a create model fn wrapper that takes care of bringing the data into the right format for us it takes a model impl argument which is a function that actually makes predictions in our case it s the dual encoder lstm we described above but we could easily swap it out for some other neural network let s see what that looks like the full code is in dual encoder py given this we can now instantiate our model function in the main routine in udc train py that we defined earlier that s it we can now run python udc train py and it should start training our networks occasionally evaluating recall on our validation data you can choose how often you want to evaluate using the eval every switch to get a complete list of all available command line flags that we defined using tf flags and hparams you can run python udc train py help info tensorflow results after steps sec batch recall at recall at recall at recall at loss after you ve trained the model you can evaluate it on the test set using python udc test py model dir model dir from training e g python udc test py model dir github chatbot retrieval runs this will run the recall k evaluation metrics on the test set instead of the validation set note that you must call udc test py with the same parameters you used during training so if you trained with embedding size you need to call the test script with the same after training for about steps around an hour on a fast gpu our model gets the following results on the test set while recall is close to our tfidf model recall and recall are significantly better suggesting that our neural network assigns higher scores to the correct answers the original paper reported and for recall recall and recall respectively but i haven t been able to reproduce scores quite as high perhaps additional data preprocessing or hyperparameter optimization may bump scores up a bit more you can modify and run udc predict py to get probability scores for unseen data for example python udc predict py model dir runs outputs you could imagine feeding in potential responses to a context and then picking the one with the highest score in this post we ve implemented a retrieval based neural network model that can assign scores to potential responses given a conversation context there is still a lot of room for improvement however one can imagine that other neural networks do better on this task than a dual lstm encoder there is also a lot of room for hyperparameter optimization or improvements to the preprocessing step the code and data for this tutorial is on github so check it out denny s blogs http blog dennybritz com http www wildml com mark clark https www linkedin com in markwclark i hope you have found this condensed nlp guide helpful i wanted to publish a longer version imagine if this was x longer however i don t want to scare the readers away as someone who develops the front end of bots user experience personality flow etc i find it extremely helpful to the understand the stack know the technological pros and cons and so to be able to effectively design around nlp nlu limitations ultimately a lot of the issues bots face today eg context can be designed around effectively if you have any suggestions on regarding this article and how it can be improved feel free to drop me a line creator of bots including smart notes bot founder of chatbot s life where we help companies create great chatbots and share our insights along the way want to talk bots best way to chat directly and see my latest projects is via my personal bot stefan s bot currently i m consulting a number of companies on their chatbot projects to get feedback on your chatbot project or to start a chatbot project contact me from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder of chatbots life i help companies create great chatbots ai systems and share my insights along the way best place to learn about chatbots we share the latest bot news info ai nlp tools tutorials more
Arthur Juliani,3500,8,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=tag_archive---------7----------------,Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C),in this article i want to provide a tutorial on implementing the asynchronous advantage actor critic a c algorithm in tensorflow we will use it to solve a simple challenge in a d doom environment with the holidays right around the corner this will be my final post for the year and i hope it will serve as a culmination of all the previous topics in the series if you haven t yet or are new to deep learning and reinforcement learning i suggest checking out the earlier entries in the series before going through this post in order to understand all the building blocks which will be utilized here if you have been following the series thank you i have learned so much about rl in the past year and am happy to have shared it with everyone through this article series so what is a c the a c algorithm was released by google s deepmind group earlier this year and it made a splash by essentially obsoleting dqn it was faster simpler more robust and able to achieve much better scores on the standard battery of deep rl tasks on top of all that it could work in continuous as well as discrete action spaces given this it has become the go to deep rl algorithm for new challenging problems with complex state and action spaces in fact openai just released a version of a c as their universal starter agent for working with their new and very diverse set of universe environments asynchronous advantage actor critic is quite a mouthful let s start by unpacking the name and from there begin to unpack the mechanics of the algorithm itself asynchronous unlike dqn where a single agent represented by a single neural network interacts with a single environment a c utilizes multiple incarnations of the above in order to learn more efficiently in a c there is a global network and multiple worker agents which each have their own set of network parameters each of these agents interacts with it s own copy of the environment at the same time as the other agents are interacting with their environments the reason this works better than having a single agent beyond the speedup of getting more work done is that the experience of each agent is independent of the experience of the others in this way the overall experience available for training becomes more diverse actor critic so far this series has focused on value iteration methods such as q learning or policy iteration methods such as policy gradient actor critic combines the benefits of both approaches in the case of a c our network will estimate both a value function v s how good a certain state is to be in and a policy s a set of action probability outputs these will each be separate fully connected layers sitting at the top of the network critically the agent uses the value estimate the critic to update the policy the actor more intelligently than traditional policy gradient methods advantage if we think back to our implementation of policy gradient the update rule used the discounted returns from a set of experiences in order to tell the agent which of its actions were good and which were bad the network was then updated in order to encourage and discourage actions appropriately the insight of using advantage estimates rather than just discounted returns is to allow the agent to determine not just how good its actions were but how much better they turned out to be than expected intuitively this allows the algorithm to focus on where the network s predictions were lacking if you recall from the dueling q network architecture the advantage function is as follow since we won t be determining the q values directly in a c we can use the discounted returns r as an estimate of q s a to allow us to generate an estimate of the advantage in this tutorial we will go even further and utilize a slightly different version of advantage estimation with lower variance referred to as generalized advantage estimation in the process of building this implementation of the a c algorithm i used as reference the quality implementations by dennybritz and openai both of which i highly recommend if you d like to see alternatives to my code here each section embedded here is taken out of context for instructional purposes and won t run on its own to view and run the full functional a c implementation see my github repository the general outline of the code architecture is the a c algorithm begins by constructing the global network this network will consist of convolutional layers to process spatial dependencies followed by an lstm layer to process temporal dependencies and finally value and policy output layers below is example code for establishing the network graph itself next a set of worker agents each with their own network and environment are created each of these workers are run on a separate processor thread so there should be no more workers than there are threads on your cpu from here we go asynchronous each worker begins by setting its network parameters to those of the global network we can do this by constructing a tensorflow op which sets each variable in the local worker network to the equivalent variable value in the global network each worker then interacts with its own copy of the environment and collects experience each keeps a list of experience tuples observation action reward done value that is constantly added to from interactions with the environment once the worker s experience history is large enough we use it to determine discounted return and advantage and use those to calculate value and policy losses we also calculate an entropy h of the policy this corresponds to the spread of action probabilities if the policy outputs actions with relatively similar probabilities then entropy will be high but if the policy suggests a single action with a large probability then entropy will be low we use the entropy as a means of improving exploration by encouraging the model to be conservative regarding its sureness of the correct action a worker then uses these losses to obtain gradients with respect to its network parameters each of these gradients are typically clipped in order to prevent overly large parameter updates which can destabilize the policy a worker then uses the gradients to update the global network parameters in this way the global network is constantly being updated by each of the agents as they interact with their environment once a successful update is made to the global network the whole process repeats the worker then resets its own network parameters to those of the global network and the process begins again to view the full and functional code see the github repository here the robustness of a c allows us to tackle a new generation of reinforcement learning challenges one of which is d environments we have come a long way from multi armed bandits and grid worlds and in this tutorial i have set up the code to allow for playing through the first vizdoom challenge vizdoom is a system to allow for rl research using the classic doom game engine the maintainers of vizdoom recently created a pip package so installing it is as simple as pip install vizdoom once it is installed we will be using the basic wad environment which is provided in the github repository and needs to be placed in the working directory the challenge consists of controlling an avatar from a first person perspective in a single square room there is a single enemy on the opposite side of the room which appears in a random location each episode the agent can only move to the left or right and fire a gun the goal is to shoot the enemy as quickly as possible using as few bullets as possible the agent has time steps per episode to shoot the enemy shooting the enemy yields a reward of and each time step as well as each shot yields a small penalty after about episodes per worker agent the network learns a policy to quickly solve the challenge feel free to adjust parameters such as learning rate clipping magnitude update frequency etc to attempt to achieve ever greater performance or utilize a c in your own rl tasks i hope this tutorial has been helpful to those new to a c and asynchronous reinforcement learning now go forth and build ais there are a lot of moving parts in a c so if you discover a bug or find a better way to do something please don t hesitate to bring it up here or in the github i am more than happy to incorporate changes and feedback to improve the algorithm if you d like to follow my writing on deep learning ai and cognitive science follow me on medium arthur juliani or on twitter awjuliani if this post has been valuable to you please consider donating to help support future tutorials articles and implementations any contribution is greatly appreciated more from my simple reinforcement learning with tensorflow series from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning unity d cognitive neuroscience phd student exploring frontier technology through the lens of artificial intelligence data science and the shape of things to come
Alexandr Honchar,1910,7,https://medium.com/machine-learning-world/neural-networks-for-algorithmic-trading-part-one-simple-time-series-forecasting-f992daa1045a?source=tag_archive---------8----------------,Neural networks for algorithmic trading. Simple time series forecasting,ciao people this is first part of my experiments on application of deep learning to finance in particular to algorithmic trading i want to implement trading system from scratch based only on deep learning approaches so for any problem we have here price prediction trading strategy risk management we gonna use different variations of artificial neural networks anns and check how well they can handle this now i plan to work on next sections i highly recommend you to check out code and ipython notebook in this repository in this first part i want to show how mlps cnns and rnns can be used for financial time series prediction in this part we are not going to use any feature engineering let s just consider historical dataset of s p index price movements we have information from to about open close high low prices for every day in the year and volume of trades first we will try just to predict close price in the end of the next day second we will try to predict return close price open price download the dataset from yahoo finance or from this repository we will consider our problem as regression problem trying to forecast exactly close price or return next day binary classification problem price will go up or down for training nns we gonna use framework keras first let s prepare our data for training we want to predict t value based on n previous days information for example having close prices from past days on the market we want to predict what price will be tomorrow on the st day we use first of time series as training set consider it as historical data and last as testing set for model evaluation here is example of loading splitting into training samples and preprocessing of raw input data it will be just hidden layer perceptron number of hidden neurons is chosen empirically we will work on hyperparameters optimization in next sections between two hidden layers we add one dropout layer to prevent overfitting important thing is dense activation linear and mse in compile section we want one output that can be in any range we predict real value and our loss function is defined as mean squared error let s see what happens if we just pass chunks of days close prices and predict price on st day final mse but it s not very representative information below is plot of predictions for first points of test dataset black line is actual data blue one predicted we can clearly see that our algorithm is not even close by value but can learn the trend let s scale our data using sklearn s method preprocessing scale to have our time series zero mean and unit variance and train the same mlp now we have mse but it is on scaled data on the plot below you can see actual scaled time series black and our forecast blue for it for using this model in real world we should return back to unscaled time series we can do it by multiplying or prediction by standard deviation of time series we used to make prediction unscaled time steps and add it s mean value mse in this case equals here is the plot of restored predictions red and real data green not bad isn t it but let s try more sophisticated algorithms for this problem i am not going to dive into theory of convolutional neural networks you can check out this amazing resourses let s define layer convolutional neural network combination of convolution and max pooling layers with one fully connected layer and the same output as earlier let s check out results mses for scaled and restored data are plots are below even looking on mse on scaled data this network learned much worse most probably deeper architecture needs more data for training or it just overfitted due to too high number of filters or layers we will consider this issue later as recurrent architecture i want to use two stacked lstm layers read more about lstms here plots of forecasts are below mses rnn forecasting looks more like moving average model it can t learn and predict all fluctuations so it s a bit unexpectable result but we can see that mlps work better for this time series forecasting let s check out what will happen if we swith from regression to classification problem now we will use not close prices but daily return close price open price and we want to predict if close price is higher or lower than open price based on last days returns code is changed just a bit we change our last dense layer to have output or and add softmax output to expect probabilistic output to load binary outputs change in the code following line also we change loss function to binary cross entopy and add accuracy metrics oh it s not better than random guessing accuracy let s try something better check out the results below we can see that treating financial time series prediction as regression problem is better approach it can learn the trend and prices close to the actual what was surprising for me that mlps are treating sequence data better as cnns or rnns which are supposed to work better with time series i explain it with pretty small dataset k time stamps and dummy hyperparameters choice you can reproduce results and get better using code from repository i think we can get better results both in regression and classification using different features not only scaled time series like some technical indicators volume of sales also we can try more frequent data let s say minute by minute ticks to have more training data all these things i m going to do later so stay tuned from a quick cheer to a standing ovation clap to show how much you enjoyed this story ai entrepreneur blogger and researcher making machines work learn and like but humans create discover and love the best about machine learning computer vision deep learning natural language processing and other
Arthur Juliani,1700,8,https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df?source=tag_archive---------9----------------,Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond,welcome to the latest installment of my reinforcement learning series in this tutorial we will be walking through the creation of a deep q network it will be built upon the simple one layer q network we created in part so i would recommend reading that first if you are new to reinforcement learning while our ordinary q network was able to barely perform as well as the q table in a simple game environment deep q networks are much more capable in order to transform an ordinary q network into a dqn we will be making the following improvements it was these three innovations that allowed the google deepmind team to achieve superhuman performance on dozens of atari games using their dqn agent we will be walking through each individual improvement and showing how to implement it we won t stop there though the pace of deep learning research is extremely fast and the dqn of is no longer the most advanced agent around anymore i will discuss two simple additional improvements to the dqn architecture double dqn and dueling dqn that allow for improved performance stability and faster training time in the end we will have a network that can tackle a number of challenging atari games and we will demonstrate how to train the dqn to learn a basic navigation task since our agent is going to be learning to play video games it has to be able to make sense of the game s screen output in a way that is at least similar to how humans or other intelligent animals are able to instead of considering each pixel independently convolutional layers allow us to consider regions of an image and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network in this way they act similarly to human receptive fields indeed there is a body of research showing that convolutional neural network learn representations that are similar to those of the primate visual cortex as such they are ideal for the first few elements within our network in tensorflow we can utilize the tf contrib layers convolution d function to easily create a convolutional layer we write for function as follows here num outs refers to how many filters we would like to apply to the previous layer kernel size refers to how large a window we would like to slide over the previous layer stride refers to how many pixels we want to skip as we slide the window across the layer finally padding refers to whether we want our window to slide over just the bottom layer valid or add padding around it same in order to ensure that the convolutional layer has the same dimensions as the previous layer for more information see the tensorflow documentation the second major addition to make dqns work is experience replay the basic idea is that by storing an agent s experiences and then randomly drawing batches of them to train the network we can more robustly learn to perform well in the task by keeping the experiences we draw random we prevent the network from only learning about what it is immediately doing in the environment and allow it to learn from a more varied array of past experiences each of these experiences are stored as a tuple of state action reward next state the experience replay buffer stores a fixed number of recent memories and as new ones come in old ones are removed when the time comes to train we simply draw a uniform batch of random memories from the buffer and train our network with them for our dqn we will build a simple class that handles storing and retrieving memories the third major addition to the dqn that makes it unique is the utilization of a second network during the training procedure this second network is used to generate the target q values that will be used to compute the loss for every action during training why not use just use one network for both estimations the issue is that at every step of training the q network s values shift and if we are using a constantly shifting set of values to adjust our network values then the value estimations can easily spiral out of control the network can become destabilized by falling into feedback loops between the target and estimated q values in order to mitigate that risk the target network s weights are fixed and only periodically or slowly updated to the primary q networks values in this way training can proceed in a more stable manner instead of updating the target network periodically and all at once we will be updating it frequently but slowly this technique was introduced in another deepmind paper earlier this year where they found that it stabilized the training process with the additions above we have everything we need to replicate the dwn of but the world moves fast and a number of improvements above and beyond the dqn architecture described by deepmind have allowed for even greater performance and stability before training your new dqn on your favorite atari game i would suggest checking the newer additions out i will provide a description and some code for two of them double dqn and dueling dqn both are simple to implement and by combining both techniques we can achieve better performance with faster training times the main intuition behind double dqn is that the regular dqn often overestimates the q values of the potential actions to take in a given state while this would be fine if all actions were always overestimates equally there was reason to believe this wasn t the case you can easily imagine that if certain suboptimal actions regularly were given higher q values than optimal actions the agent would have a hard time ever learning the ideal policy in order to correct for this the authors of ddqn paper propose a simple trick instead of taking the max over q values when computing the target q value for our training step we use our primary network to chose an action and our target network to generate the target q value for that action by decoupling the action choice from the target q value generation we are able to substantially reduce the overestimation and train faster and more reliably below is the new ddqn equation for updating the target value in order to explain the reasoning behind the architecture changes that dueling dqn makes we need to first explain some a few additional reinforcement learning terms the q values that we have been discussing so far correspond to how good it is to take a certain action given a certain state this can be written as q s a this action given state can actually be decomposed into two more fundamental notions of value the first is the value function v s which says simple how good it is to be in any given state the second is the advantage function a a which tells how much better taking a certain action would be compared to the others we can then think of q as being the combination of v and a more formally the goal of dueling dqn is to have a network that separately computes the advantage and value functions and combines them back into a single q function only at the final layer it may seem somewhat pointless to do this at first glance why decompose a function that we will just put back together the key to realizing the benefit is to appreciate that our reinforcement learning agent may not need to care about both value and advantage at any given time for example imagine sitting outside in a park watching the sunset it is beautiful and highly rewarding to be sitting there no action needs to be taken and it doesn t really make sense to think of the value of sitting there as being conditioned on anything beyond the environmental state you are in we can achieve more robust estimates of state value by decoupling it from the necessity of being attached to specific actions now that we have learned all the tricks to get the most out of our dqn let s actually try it on a game environment while the dqn we have described above could learn atari games with enough training getting the network to perform well on those games takes at least a day of training on a powerful machine for educational purposes i have built a simple game environment which our dqn learns to master in a couple hours on a moderately powerful machine i am using a gtx in the environment the agent controls a blue square and the goal is to navigate to the green squares reward while avoiding the red squares reward at the start of each episode all squares are randomly placed within a x grid world the agent has steps to achieve as large a reward as possible because they are randomly positioned the agent needs to do more than simply learn a fixed path as was the case in the frozenlake environment from tutorial instead the agent must learn a notion of spatial relationships between the blocks and indeed it is able to do just that the game environment outputs x x color images and uses function calls as similar to the openai gym as possible in doing so it should be easy to modify this code to work on any of the openai atari games i encourage those with the time and computing resources necessary to try getting the agent to perform well in an atari game the hyperparameters may need some tuning but it is definitely possible good luck if this post has been valuable to you please consider donating to help support future tutorials articles and implementations any contribution is greatly appreciated if you d like to follow my work on deep learning ai and cognitive science follow me on medium arthur juliani or on twitter awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning unity d cognitive neuroscience phd student
Vishal Maini,32000,10,https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12?source=tag_archive---------0----------------,A Beginner’s Guide to AI/ML 🤖👶 – Machine Learning for Humans – Medium,part why machine learning matters the big picture of artificial intelligence and machine learning past present and future part supervised learning learning with an answer key introducing linear regression loss functions overfitting and gradient descent part supervised learning ii two methods of classification logistic regression and svms part supervised learning iii non parametric learners k nearest neighbors decision trees random forests introducing cross validation hyperparameter tuning and ensemble models part unsupervised learning clustering k means hierarchical dimensionality reduction principal components analysis pca singular value decomposition svd part neural networks deep learning why where and how deep learning works drawing inspiration from the brain convolutional neural networks cnns recurrent neural networks rnns real world applications part reinforcement learning exploration and exploitation markov decision processes q learning policy learning and deep reinforcement learning the value learning problem appendix the best machine learning resources a curated list of resources for creating your machine learning curriculum this guide is intended to be accessible to anyone basic concepts in probability statistics programming linear algebra and calculus will be discussed but it isn t necessary to have prior knowledge of them to gain value from this series artificial intelligence will shape our future more powerfully than any other innovation this century anyone who does not understand it will soon find themselves feeling left behind waking up in a world full of technology that feels more and more like magic the rate of acceleration is already astounding after a couple of ai winters and periods of false hope over the past four decades rapid advances in data storage and computer processing power have dramatically changed the game in recent years in google trained a conversational agent ai that could not only convincingly interact with humans as a tech support helpdesk but also discuss morality express opinions and answer general facts based questions the same year deepmind developed an agent that surpassed human level performance at atari games receiving only the pixels and game score as inputs soon after in deepmind obsoleted their own achievement by releasing a new state of the art gameplay method called a c meanwhile alphago defeated one of the best human players at go an extraordinary achievement in a game dominated by humans for two decades after machines first conquered chess many masters could not fathom how it would be possible for a machine to grasp the full nuance and complexity of this ancient chinese war strategy game with its possible board positions there are only atoms in the universe in march openai created agents that invented their own language to cooperate and more effectively achieve their goal soon after facebook reportedly successfully training agents to negotiate and even lie just a few days ago as of this writing on august openai reached yet another incredible milestone by defeating the world s top professionals in v matches of the online multiplayer game dota much of our day to day technology is powered by artificial intelligence point your camera at the menu during your next trip to taiwan and the restaurant s selections will magically appear in english via the google translate app today ai is used to design evidence based treatment plans for cancer patients instantly analyze results from medical tests to escalate to the appropriate specialist immediately and conduct scientific research for drug discovery in everyday life it s increasingly commonplace to discover machines in roles traditionally occupied by humans really don t be surprised if a little housekeeping delivery bot shows up instead of a human next time you call the hotel desk to send up some toothpaste in this series we ll explore the core machine learning concepts behind these technologies by the end you should be able to describe how they work at a conceptual level and be equipped with the tools to start building similar applications yourself artificial intelligence is the study of agents that perceive the world around them form plans and make decisions to achieve their goals its foundations include mathematics logic philosophy probability linguistics neuroscience and decision theory many fields fall under the umbrella of ai such as computer vision robotics machine learning and natural language processing machine learning is a subfield of artificial intelligence its goal is to enable computers to learn on their own a machine s learning algorithm enables it to identify patterns in observed data build models that explain the world and predict things without having explicit pre programmed rules and models the technologies discussed above are examples of artificial narrow intelligence ani which can effectively perform a narrowly defined task meanwhile we re continuing to make foundational advances towards human level artificial general intelligence agi also known as strong ai the definition of an agi is an artificial intelligence that can successfully perform any intellectual task that a human being can including learning planning and decision making under uncertainty communicating in natural language making jokes manipulating people trading stocks or reprogramming itself and this last one is a big deal once we create an ai that can improve itself it will unlock a cycle of recursive self improvement that could lead to an intelligence explosion over some unknown time period ranging from many decades to a single day you may have heard this point referred to as the singularity the term is borrowed from the gravitational singularity that occurs at the center of a black hole an infinitely dense one dimensional point where the laws of physics as we understand them start to break down a recent report by the future of humanity institute surveyed a panel of ai researchers on timelines for agi and found that researchers believe there is a chance of ai outperforming humans in all tasks in years grace et al we ve personally spoken with a number of sane and reasonable ai practitioners who predict much longer timelines the upper limit being never and others whose timelines are alarmingly short as little as a few years the advent of greater than human level artificial superintelligence asi could be one of the best or worst things to happen to our species it carries with it the immense challenge of specifying what ais will want in a way that is friendly to humans while it s impossible to say what the future holds one thing is certain is a good time to start understanding how machines think to go beyond the abstractions of a philosopher in an armchair and intelligently shape our roadmaps and policies with respect to ai we must engage with the details of how machines see the world what they want their potential biases and failure modes their temperamental quirks just as we study psychology and neuroscience to understand how humans learn decide act and feel machine learning is at the core of our journey towards artificial general intelligence and in the meantime it will change every industry and have a massive impact on our day to day lives that s why we believe it s worth understanding machine learning at least at a conceptual level and we designed this series to be the best place to start you don t necessarily need to read the series cover to cover to get value out of it here are three suggestions on how to approach it depending on your interests and how much time you have vishal most recently led growth at upstart a lending platform that utilizes machine learning to price credit automate the borrowing process and acquire users he spends his time thinking about startups applied cognitive science moral philosophy and the ethics of artificial intelligence samer is a master s student in computer science and engineering at ucsd and co founder of conigo labs prior to grad school he founded tablescribe a business intelligence tool for smbs and spent two years advising fortune companies at mckinsey samer previously studied computer science and ethics politics and economics at yale most of this series was written during a day trip to the united kingdom in a frantic blur of trains planes cafes pubs and wherever else we could find a dry place to sit our aim was to solidify our own understanding of artificial intelligence machine learning and how the methods therein fit together and hopefully create something worth sharing in the process and now without further ado let s dive into machine learning with part supervised learning more from machine learning for humans a special thanks to jonathan eng edoardo conti grant schneider sunny kumar stephanie he tarun wadhwa and sachin maini series editor for their significant contributions and feedback from a quick cheer to a standing ovation clap to show how much you enjoyed this story research comms deepmindai previously upstart yale trueventurestec demystifying artificial intelligence machine learning discussions on safe and intentional application of ai for positive social impact
Tim Anglade,7000,23,https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3?source=tag_archive---------1----------------,"How HBO’s Silicon Valley built “Not Hotdog” with mobile TensorFlow, Keras & React Native",the hbo show silicon valley released a real ai app that identifies hotdogs and not hotdogs like the one shown on season s th episode the app is now available on android as well as ios to achieve this we designed a bespoke neural architecture that runs directly on your phone and trained it with tensorflow keras nvidia gpus while the use case is farcical the app is an approachable example of both deep learning and edge computing all ai work is powered by the user s device and images are processed without ever leaving their phone this provides users with a snappier experience no round trip to the cloud offline availability and better privacy this also allows us to run the app at a cost of even under the load of a million users providing significant savings compared to traditional cloud based ai approaches the app was developed in house by the show by a single developer running on a single laptop attached gpu using hand curated data in that respect it may provide a sense of what can be achieved today with a limited amount of time resources by non technical companies individual developers and hobbyists alike in that spirit this article attempts to give a detailed overview of steps involved to help others build their own apps if you haven t seen the show or tried the app you should the app lets you snap a picture and then tells you whether it thinks that image is of a hotdog or not it s a straightforward use case that pays homage to recent ai research and applications in particular imagenet while we ve probably dedicated more engineering resources to recognizing hotdogs than anyone else the app still fails in horrible and or subtle ways conversely it s also sometimes able to recognize hotdogs in complex situations according to engadget it s incredible i ve had more success identifying food with the app in minutes than i have had tagging and identifying songs with shazam in the past two years have you ever found yourself reading hacker news thinking they raised a m series a for that i could build it in one weekend this app probably feels a lot like that and the initial prototype was indeed built in a single weekend using google cloud platform s vision api and react native but the final app we ended up releasing on the app store required months of additional part time work to deliver meaningful improvements that would be difficult for an outsider to appreciate we spent weeks optimizing overall accuracy training time inference time iterating on our setup tooling so we could have a faster development iterations and spent a whole weekend optimizing the user experience around ios android permissions don t even get me started on that one all too often technical blog posts or academic papers skip over this part preferring to present the final chosen solution in the interest of helping others learn from our mistake choices we will present an abridged view of the approaches that didn t work for us before we describe the final architecture we ended up shipping in the next section we chose react native to build the prototype as it would give us an easy sandbox to experiment with and would help us quickly support many devices the experience ended up being a good one and we kept react native for the remainder of the project it didn t always make things easy and the design for the app was purposefully limited but in the end react native got the job done the other main component we used for the prototype google cloud s vision api was quickly abandoned there were main factors for these reasons we started experimenting with what s trendily called edge computing which for our purposes meant that after training our neural network on our laptop we would export it and embed it directly into our mobile app so that the neural network execution phase or inference would run directly inside the user s phone through a chance encounter with pete warden of the tensorflow team we had become aware of its ability to run tensorflow directly embedded on an ios device and started exploring that path after react native tensorflow became the second fixed part of our stack it only took a day of work to integrate tensorflow s objective c camera example in our react native shell it took slightly longer to use their transfer learning script which helps you retrain the inception architecture to deal with a more specific image problem inception is the name of a family of neural architectures built by google to deal with image recognition problems inception is available pre trained which means the training phase has been completed and the weights are set most often for image recognition networks they have been trained on imagenet a dataset containing over different types of objects hotdogs are one of them however much like google cloud s vision api imagenet training rewards breadth as much as depth here and out of the box accuracy on a single one of the categories can be lacking as such retraining also called transfer learning aims to take a full trained neural net and retrain it to perform better on the specific problem you d like to handle this usually involves some degree of forgetting either by excising entire layers from the stack or by slowly erasing the network s ability to distinguish a type of object e g chairs in favor of better accuracy at recognizing the one you care about i e hotdogs while the network inception in this case may have been trained on the m images contained in imagenet we were able to retrain it on a just a few thousand hotdog images to get drastically enhanced hotdog recognition the big advantage of transfer learning are you will get better results much faster and with less data than if you train from scratch a full training might take months on multiple gpus and require millions of images while retraining can conceivably be done in hours on a laptop with a couple thousand images one of the biggest challenges we encountered was understanding exactly what should count as a hotdog and what should not defining what a hotdog is ends up being surprisingly difficult do cut up sausages count and if so which kinds and subject to cultural interpretation similarly the open world nature of our problem meant we had to deal with an almost infinite number of inputs while certain computer vision problems have relatively limited inputs say x rays of bolts with or without a mechanical default we had to prepare the app to be fed selfies nature shots and any number of foods suffice to say this approach was promising and did lead to some improved results however it had to be abandoned for a couple of reasons first the nature of our problem meant a strong imbalance in training data there are many more examples of things that are not hotdogs than things that are hotdogs in practice this means that if you train your algorithm on hotdog images and non hotdog images and it recognizes of the former but of the latter it will still score accuracy by default this was not straightforward to solve out of the box using tensorflow s retrain tool and basically necessitated setting up a deep learning model from scratch import weights and train in a more controlled manner at this point we decided to bite the bullet and get something started with keras a deep learning library that provides nicer easier to use abstractions on top of tensorflow including pretty awesome training tools and a class weights option which is ideal to deal with this sort of dataset imbalance we were dealing with we used that opportunity to try other popular neural architectures like vgg but one problem remained none of them could comfortably fit on an iphone they consumed too much memory which led to app crashes and would sometime takes up to seconds to compute which was not ideal from a ux standpoint many things were attempted to mitigate that but in the end it these architectures were just too big to run efficiently on mobile to give you a context out of time this was roughly the mid way point of the project by that time the ui was done and very little of it was going to change but in hindsight the neural net was at best done we had a good sense of challenges a good dataset but lines of the final neural architecture had been written none of our neural code could reliably run on mobile and even our accuracy was going to improve drastically in the weeks to come the problem directly ahead of us was simple if inception and vgg were too big was there a simpler pre trained neural network we could retrain at the suggestion of the always excellent jeremy p howard where has that guy been all our life we explored xception enet and squeezenet we quickly settled on squeezenet due to its explicit positioning as a solution for embedded deep learning and the availability of a pre trained keras model on github yay open source so how big of a difference does this make an architecture like vgg uses about million parameters essentially the number of numbers necessary to model the neurons and values between them inception is already a massive improvement requiring only million parameters squeezenet in comparison only requires million this has two advantages there are tradeoffs of course during this phase we started experimenting with tuning the neural network architecture in particular we started using batch normalization and trying different activation functions after adding batch normalization and elu to squeezenet we were able to train neural network that achieve accuracy when training from scratch however they were relatively brittle meaning the same network would overfit in some cases or underfit in others when confronted to real life testing even adding more examples to the dataset and playing with data augmentation failed to deliver a network that met expectations so while this phase was promising and for the first time gave us a functioning app that could work entirely on an iphone in less than a second we eventually moved to our th final architecture our final architecture was spurred in large part by the publication on april of google s mobilenets paper promising a new neural architecture with inception like accuracy on simple problems like ours with only m or so parameters this meant it sat in an interesting sweet spot between a squeezenet that had maybe been overly simplistic for our purposes and the possibly overwrought elephant trying to squeeze in a tutu of using inception or vgg on mobile the paper introduced some capacity to tune the size complexity of network specifically to trade memory cpu consumption against accuracy which was very much top of mind for us at the time with less than a month to go before the app had to launch we endeavored to reproduce the paper s results this was entirely anticlimactic as within a day of the paper being published a keras implementation was already offered publicly on github by refik can malli a student at istanbul technical university whose work we had already benefitted from when we took inspiration from his excellent keras squeezenet implementation the depth openness of the deep learning community and the presence of talented minds like r c is what makes deep learning viable for applications today but they also make working in this field more thrilling than any tech trend we ve been involved with our final architecture ended up making significant departures from the mobilenets architecture or from convention in particular so how does this stack work exactly deep learning often gets a bad rap for being a black box and while it s true many components of it can be mysterious the networks we use often leak information about how some of their magic work we can look at the layers of this stack and how they activate on specific input images giving us a sense of each layer s ability to recognize sausage buns or other particularly salient hotdog features data quality was of the utmost importance a neural network can only be as good as the data that trained it and improving training set quality was probably one of the top things we spent time on during this project the key things we did to improve this were the final composition of our dataset was k images of which only k were hotdogs there are only so many hotdogs you can look at but there are many not hotdogs to look at the imbalance was dealt with by saying a keras class weight of in favor of hotdogs of the remaining k images most were of food with just k photos of non food items to help the network generalize a bit more and not get tricked into seeing a hotdog if presented with an image of a human in a red outfit our data augmentation rules were as follows these numbers were derived intuitively based on experiments and our understanding of the real life usage of our app as opposed to careful experimentation the final key to our data pipeline was using patrick rodriguez s multiprocess image data generator for keras while keras does have a built in multi threaded and multiprocess implementation we found patrick s library to be consistently faster in our experiments for reasons we did not have time to investigate this library cut our training time to a third of what it used to be the network was trained using a macbook pro and attached external gpu egpu specifically an nvidia gtx ti we d probably buy a ti if we were starting today we were able to train the network on batches of images at a time the network was trained for a total of epochs meaning we ran all k images through the network times this took about hours we trained the network in phases while learning rates were identified by running the linear experiment recommended by the clr paper they seem to intuitively make sense in that the max for each phase is within a factor of of the previous minimum which is aligned with the industry standard recommendation of halving your learning rate if your accuracy plateaus during training in the interest of time we performed some training runs on a paperspace p instance running ubuntu in those cases we were able to double the batch size and found that optimal learning rates for each phase were roughly double as well even having designed a relatively compact neural architecture and having trained it to handle situations it may find in a mobile context we had a lot of work left to make it run properly trying to run a top of the line neural net architecture out of the box can quickly burns hundreds megabytes of ram which few mobile devices can spare today beyond network optimizations it turns out the way you handle images or even load tensorflow itself can have a huge impact on how quickly your network runs how little ram it uses and how crash free the experience will be for your users this was maybe the most mysterious part of this project relatively little information can be found about it possibly due to the dearth of production deep learning applications running on mobile devices as of today however we must commend the tensorflow team and particularly pete warden andrew harp and chad whipkey for the existing documentation and their kindness in answering our inquiries instead of using tensorflow on ios we looked at using apple s built in deep learning libraries instead bnns mpscnn and later on coreml we would have designed the network in keras trained it with tensorflow exported all the weight values re implemented the network with bnns or mpscnn or imported it via coreml and loaded the parameters into that new implementation however the biggest obstacle was that these new apple libraries are only available on ios and we wanted to support older versions of ios as ios adoption and these frameworks continue to improve there may not be a case for using tensorflow on device in the near future if you think injecting javascript into your app on the fly is cool try injecting neural nets into your app the last production trick we used was to leverage codepush and apple s relatively permissive terms of service to live inject new versions of our neural networks after submission to the app store while this was mostly done to help us quickly deliver accuracy improvements to our users after release you could conceivably use this approach to drastically expand or alter the feature set of your app without going through an app store review again there are a lot of things that didn t work or we didn t have time to do and these are the ideas we d investigate in the future finally we d be remiss not to mention the obvious and important influence of user experience developer experience and built in biases in developing an ai app each probably deserve their own post or their own book but here are the very concrete impacts of these things in our experience ux user experience is arguably more critical at every stage of the development of an ai app than for a traditional application there are no deep learning algorithms that will give you perfect results right now but there are many situations where the right mix of deep learning ux will lead to results that are indistinguishable from perfect proper ux expectations are irreplaceable when it comes to setting developers on the right path to design their neural networks setting the proper expectations for users when they use the app and gracefully handling the inevitable ai failures building ai apps without a ux first mindset is like training a neural net without stochastic gradient descent you will end up stuck in the local minima of the uncanny valley on your way to building the perfect ai use case dx developer experience is extremely important as well because deep learning training time is the new horsing around while waiting for your program to compile we suggest you heavily favor dx first hence keras as it s always possible to optimize runtime for later runs manual gpu parallelization multi process data augmentation tensorflow pipeline even re implementing for caffe pytorch even projects with relatively obtuse apis documentation like tensorflow greatly improve dx by providing a highly tested highly used well maintained environment for training running neural networks for the same reason it s hard to beat both the cost as well as the flexibility of having your own local gpu for development being able to look at edit images locally edit code with your preferred tool without delays greatly improves the development quality speed of building ai projects most ai apps will hit more critical cultural biases than ours but as an example even our straightforward use case caught us flat footed with built in biases in our initial dataset that made the app unable to recognize french style hotdogs asian hotdogs and more oddities we did not have immediate personal experience with it s critical to remember that ai do not make better decisions than humans they are infected by the same human biases we fall prey to via the training sets humans provide thanks to mike judge alec berg clay tarver todd silverstein jonathan dotan lisa schomas amy solomon dorothy street rich toyon and all the writers of the show the app would simply not exist without them meaghan dana david jay and everyone at hbo scale venture partners gitlab rachel thomas and jeremy howard fast ai for all that they have taught me and for kindly reviewing a draft of this post check out their free online deep learning course it s awesome jp simard for his help on ios and finally the tensorflow team r machinelearning for their help inspiration and thanks to everyone who used shared the app it made staring at pictures of hotdogs for months on end totally worth it from a quick cheer to a standing ovation clap to show how much you enjoyed this story a i startups hbo s silicon valley get in touch timanglade gmail com
Dhruv Parthasarathy,4300,12,https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4?source=tag_archive---------2----------------,A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN,at athelas we use convolutional neural networks cnns for a lot more than just classification in this post we ll see how cnns can be used with great results in image instance segmentation ever since alex krizhevsky geoff hinton and ilya sutskever won imagenet in convolutional neural networks cnns have become the gold standard for image classification in fact since then cnns have improved to the point where they now outperform humans on the imagenet challenge while these results are impressive image classification is far simpler than the complexity and diversity of true human visual understanding in classification there s generally an image with a single object as the focus and the task is to say what that image is see above but when we look at the world around us we carry out far more complex tasks we see complicated sights with multiple overlapping objects and different backgrounds and we not only classify these different objects but also identify their boundaries differences and relations to one another can cnns help us with such complex tasks namely given a more complicated image can we use cnns to identify the different objects in the image and their boundaries as has been shown by ross girshick and his peers over the last few years the answer is conclusively yes through this post we ll cover the intuition behind some of the main techniques used in object detection and segmentation and see how they ve evolved from one implementation to the next in particular we ll cover r cnn regional cnn the original application of cnns to this problem along with its descendants fast r cnn and faster r cnn finally we ll cover mask r cnn a paper released recently by facebook research that extends such object detection techniques to provide pixel level segmentation here are the papers referenced in this post inspired by the research of hinton s lab at the university of toronto a small team at uc berkeley led by professor jitendra malik asked themselves what today seems like an inevitable question object detection is the task of finding the different objects in an image and classifying them as seen in the image above the team comprised of ross girshick a name we ll see again jeff donahue and trevor darrel found that this problem can be solved with krizhevsky s results by testing on the pascal voc challenge a popular object detection challenge akin to imagenet they write let s now take a moment to understand how their architecture regions with cnns r cnn works understanding r cnn the goal of r cnn is to take in an image and correctly identify where the main objects via a bounding box in the image but how do we find out where these bounding boxes are r cnn does what we might intuitively do as well propose a bunch of boxes in the image and see if any of them actually correspond to an object r cnn creates these bounding boxes or region proposals using a process called selective search which you can read about here at a high level selective search shown in the image above looks at the image through windows of different sizes and for each size tries to group together adjacent pixels by texture color or intensity to identify objects once the proposals are created r cnn warps the region to a standard square size and passes it through to a modified version of alexnet the winning submission to imagenet that inspired r cnn as shown above on the final layer of the cnn r cnn adds a support vector machine svm that simply classifies whether this is an object and if so what object this is step in the image above improving the bounding boxes now having found the object in the box can we tighten the box to fit the true dimensions of the object we can and this is the final step of r cnn r cnn runs a simple linear regression on the region proposal to generate tighter bounding box coordinates to get our final result here are the inputs and outputs of this regression model so to summarize r cnn is just the following steps r cnn works really well but is really quite slow for a few simple reasons in ross girshick the first author of r cnn solved both these problems leading to the second algorithm in our short history fast r cnn let s now go over its main insights fast r cnn insight roi region of interest pooling for the forward pass of the cnn girshick realized that for each image a lot of proposed regions for the image invariably overlapped causing us to run the same cnn computation again and again times his insight was simple why not run the cnn just once per image and then find a way to share that computation across the proposals this is exactly what fast r cnn does using a technique known as roipool region of interest pooling at its core roipool shares the forward pass of a cnn for an image across its subregions in the image above notice how the cnn features for each region are obtained by selecting a corresponding region from the cnn s feature map then the features in each region are pooled usually using max pooling so all it takes us is one pass of the original image as opposed to fast r cnn insight combine all models into one network the second insight of fast r cnn is to jointly train the cnn classifier and bounding box regressor in a single model where earlier we had different models to extract image features cnn classify svm and tighten bounding boxes regressor fast r cnn instead used a single network to compute all three you can see how this was done in the image above fast r cnn replaced the svm classifier with a softmax layer on top of the cnn to output a classification it also added a linear regression layer parallel to the softmax layer to output bounding box coordinates in this way all the outputs needed came from one single network here are the inputs and outputs to this overall model even with all these advancements there was still one remaining bottleneck in the fast r cnn process the region proposer as we saw the very first step to detecting the locations of objects is generating a bunch of potential bounding boxes or regions of interest to test in fast r cnn these proposals were created using selective search a fairly slow process that was found to be the bottleneck of the overall process in the middle a team at microsoft research composed of shaoqing ren kaiming he ross girshick and jian sun found a way to make the region proposal step almost cost free through an architecture they creatively named faster r cnn the insight of faster r cnn was that region proposals depended on features of the image that were already calculated with the forward pass of the cnn first step of classification so why not reuse those same cnn results for region proposals instead of running a separate selective search algorithm indeed this is just what the faster r cnn team achieved in the image above you can see how a single cnn is used to both carry out region proposals and classification this way only one cnn needs to be trained and we get region proposals almost for free the authors write here are the inputs and outputs of their model how the regions are generated let s take a moment to see how faster r cnn generates these region proposals from cnn features faster r cnn adds a fully convolutional network on top of the features of the cnn creating what s known as the region proposal network the region proposal network works by passing a sliding window over the cnn feature map and at each window outputting k potential bounding boxes and scores for how good each of those boxes is expected to be what do these k boxes represent intuitively we know that objects in an image should fit certain common aspect ratios and sizes for instance we know that we want some rectangular boxes that resemble the shapes of humans likewise we know we won t see many boxes that are very very thin in such a way we create k such common aspect ratios we call anchor boxes for each such anchor box we output one bounding box and score per position in the image with these anchor boxes in mind let s take a look at the inputs and outputs to this region proposal network we then pass each such bounding box that is likely to be an object into fast r cnn to generate a classification and tightened bounding boxes so far we ve seen how we ve been able to use cnn features in many interesting ways to effectively locate different objects in an image with bounding boxes can we extend such techniques to go one step further and locate exact pixels of each object instead of just bounding boxes this problem known as image segmentation is what kaiming he and a team of researchers including girshick explored at facebook ai using an architecture known as mask r cnn much like fast r cnn and faster r cnn mask r cnn s underlying intuition is straight forward given that faster r cnn works so well for object detection could we extend it to also carry out pixel level segmentation mask r cnn does this by adding a branch to faster r cnn that outputs a binary mask that says whether or not a given pixel is part of an object the branch in white in the above image as before is just a fully convolutional network on top of a cnn based feature map here are its inputs and outputs but the mask r cnn authors had to make one small adjustment to make this pipeline work as expected roialign realigning roipool to be more accurate when run without modifications on the original faster r cnn architecture the mask r cnn authors realized that the regions of the feature map selected by roipool were slightly misaligned from the regions of the original image since image segmentation requires pixel level specificity unlike bounding boxes this naturally led to inaccuracies the authors were able to solve this problem by cleverly adjusting roipool to be more precisely aligned using a method known as roialign imagine we have an image of size x and a feature map of size x let s imagine we want features the region corresponding to the top left x pixels in the original image see above how might we select these pixels from the feature map we know each pixel in the original image corresponds to pixels in the feature map to select pixels from the original image we just select pixels in roipool we would round this down and select pixels causing a slight misalignment however in roialign we avoid such rounding instead we use bilinear interpolation to get a precise idea of what would be at pixel this at a high level is what allows us to avoid the misalignments caused by roipool once these masks are generated mask r cnn combines them with the classifications and bounding boxes from faster r cnn to generate such wonderfully precise segmentations if you re interested in trying out these algorithms yourselves here are relevant repositories faster r cnn mask r cnn in just years we ve seen how the research community has progressed from krizhevsky et al s original result to r cnn and finally all the way to such powerful results as mask r cnn seen in isolation results like mask r cnn seem like incredible leaps of genius that would be unapproachable yet through this post i hope you ve seen how such advancements are really the sum of intuitive incremental improvements through years of hard work and collaboration each of the ideas proposed by r cnn fast r cnn faster r cnn and finally mask r cnn were not necessarily quantum leaps yet their sum products have led to really remarkable results that bring us closer to a human level understanding of sight what particularly excites me is that the time between r cnn and mask r cnn was just three years with continued funding focus and support how much further can computer vision improve over the next three years if you see any errors or issues in this post please contact me at dhruv getathelas com and i ll immediately correct them if you re interested in applying such techniques come join us at athelas where we apply computer vision to blood diagnostics daily other posts we ve written thanks to bharath ramsundar pranav ramkrishnan tanay tandon and oliver cameron for help with this post from a quick cheer to a standing ovation clap to show how much you enjoyed this story dhruvp vp eng athelas mit math and cs undergrad mit cs masters previously director of ai programs udacity blood diagnostics through deep learning http athelas com
Sebastian Heinz,4400,13,https://medium.com/mlreview/a-simple-deep-learning-model-for-stock-price-prediction-using-tensorflow-30505541d877?source=tag_archive---------3----------------,A simple deep learning model for stock price prediction using TensorFlow,for a recent hackathon that we did at statworx some of our team members scraped minutely s p data from the google finance api the data consisted of index as well as stock prices of the s p s constituents having this data at hand the idea of developing a deep learning model for predicting the s p index based on the constituents prices one minute ago came immediately on my mind playing around with the data and building the deep learning model with tensorflow was fun and so i decided to write my first medium com story a little tensorflow tutorial on predicting s p stock prices what you will read is not an in depth tutorial but more a high level introduction to the important building blocks and concepts of tensorflow models the python code i ve created is not optimized for efficiency but understandability the dataset i ve used can be downloaded from here mb our team exported the scraped stock data from our scraping server as a csv file the dataset contains n minutes of data ranging from april to august on stocks as well as the total s p index price index and stocks are arranged in wide format the data was already cleaned and prepared meaning missing stock and index prices were locf ed last observation carried forward so that the file did not contain any missing values a quick look at the s p time series using pyplot plot data sp note this is actually the lead of the s p index meaning its value is shifted minute into the future this operation is necessary since we want to predict the next minute of the index and not the current minute the dataset was split into training and test data the training data contained of the total dataset the data was not shuffled but sequentially sliced the training data ranges from april to approx end of july the test data ends end of august there are a lot of different approaches to time series cross validation such as rolling forecasts with and without refitting or more elaborate concepts such as time series bootstrap resampling the latter involves repeated samples from the remainder of the seasonal decomposition of the time series in order to simulate samples that follow the same seasonal pattern as the original time series but are not exact copies of its values most neural network architectures benefit from scaling the inputs sometimes also the output why because most common activation functions of the network s neurons such as tanh or sigmoid are defined on the or interval respectively nowadays rectified linear unit relu activations are commonly used activations which are unbounded on the axis of possible activation values however we will scale both the inputs and targets anyway scaling can be easily accomplished in python using sklearn s minmaxscaler remark caution must be undertaken regarding what part of the data is scaled and when a common mistake is to scale the whole dataset before training and test split are being applied why is this a mistake because scaling invokes the calculation of statistics e g the min max of a variable when performing time series forecasting in real life you do not have information from future observations at the time of forecasting therefore calculation of scaling statistics has to be conducted on training data and must then be applied to the test data otherwise you use future information at the time of forecasting which commonly biases forecasting metrics in a positive direction tensorflow is a great piece of software and currently the leading deep learning and neural network computation framework it is based on a c low level backend but is usually controlled via python there is also a neat tensorflow library for r maintained by rstudio tensorflow operates on a graph representation of the underlying computational task this approach allows the user to specify mathematical operations as elements in a graph of data variables and operators since neural networks are actually graphs of data and mathematical operations tensorflow is just perfect for neural networks and deep learning check out this simple example stolen from our deep learning introduction from our blog in the figure above two numbers are supposed to be added those numbers are stored in two variables a and b the two values are flowing through the graph and arrive at the square node where they are being added the result of the addition is stored into another variable c actually a b and c can be considered as placeholders any numbers that are fed into a and b get added and are stored into c this is exactly how tensorflow works the user defines an abstract representation of the model neural network through placeholders and variables afterwards the placeholders get filled with real data and the actual computations take place the following code implements the toy example from above in tensorflow after having imported the tensorflow library two placeholders are defined using tf placeholder they correspond to the two blue circles on the left of the image above afterwards the mathematical addition is defined via tf add the result of the computation is c with placeholders set up the graph can be executed with any integer value for a and b of course the former problem is just a toy example the required graphs and computations in a neural network are much more complex as mentioned before it all starts with placeholders we need two placeholders in order to fit our model x contains the network s inputs the stock prices of all s p constituents at time t t and y the network s outputs the index value of the s p at time t t the shape of the placeholders correspond to none n stocks with none meaning that the inputs are a dimensional matrix and the outputs are a dimensional vector it is crucial to understand which input and output dimensions the neural net needs in order to design it properly the none argument indicates that at this point we do not yet know the number of observations that flow through the neural net graph in each batch so we keep if flexible we will later define the variable batch size that controls the number of observations per training batch besides placeholders variables are another cornerstone of the tensorflow universe while placeholders are used to store input and target data in the graph variables are used as flexible containers within the graph that are allowed to change during graph execution weights and biases are represented as variables in order to adapt during training variables need to be initialized prior to model training we will get into that a litte later in more detail the model consists of four hidden layers the first layer contains neurons slightly more than double the size of the inputs subsequent hidden layers are always half the size of the previous layer which means and finally neurons a reduction of the number of neurons for each subsequent layer compresses the information the network identifies in the previous layers of course other network architectures and neuron configurations are possible but are out of scope for this introduction level article it is important to understand the required variable dimensions between input hidden and output layers as a rule of thumb in multilayer perceptrons mlps the type of networks used here the second dimension of the previous layer is the first dimension in the current layer for weight matrices this might sound complicated but is essentially just each layer passing its output as input to the next layer the biases dimension equals the second dimension of the current layer s weight matrix which corresponds the number of neurons in this layer after definition of the required weight and bias variables the network topology the architecture of the network needs to be specified hereby placeholders data and variables weighs and biases need to be combined into a system of sequential matrix multiplications furthermore the hidden layers of the network are transformed by activation functions activation functions are important elements of the network architecture since they introduce non linearity to the system there are dozens of possible activation functions out there one of the most common is the rectified linear unit relu which will also be used in this model the image below illustrates the network architecture the model consists of three major building blocks the input layer the hidden layers and the output layer this architecture is called a feedforward network feedforward indicates that the batch of data solely flows from left to right other network architectures such as recurrent neural networks also allow data flowing backwards in the network the cost function of the network is used to generate a measure of deviation between the network s predictions and the actual observed training targets for regression problems the mean squared error mse function is commonly used mse computes the average squared deviation between predictions and targets basically any differentiable function can be implemented in order to compute a deviation measure between predictions and targets however the mse exhibits certain properties that are advantageous for the general optimization problem to be solved the optimizer takes care of the necessary computations that are used to adapt the network s weight and bias variables during training those computations invoke the calculation of so called gradients that indicate the direction in which the weights and biases have to be changed during training in order to minimize the network s cost function the development of stable and speedy optimizers is a major field in neural network an deep learning research here the adam optimizer is used which is one of the current default optimizers in deep learning development adam stands for adaptive moment estimation and can be considered as a combination between two other popular optimizers adagrad and rmsprop initializers are used to initialize the network s variables before training since neural networks are trained using numerical optimization techniques the starting point of the optimization problem is one the key factors to find good solutions to the underlying problem there are different initializers available in tensorflow each with different initialization approaches here i use the tf variance scaling initializer which is one of the default initialization strategies note that with tensorflow it is possible to define multiple initialization functions for different variables within the graph however in most cases a unified initialization is sufficient after having defined the placeholders variables initializers cost functions and optimizers of the network the model needs to be trained usually this is done by minibatch training during minibatch training random data samples of n batch size are drawn from the training data and fed into the network the training dataset gets divided into n batch size batches that are sequentially fed into the network at this point the placeholders x and y come into play they store the input and target data and present them to the network as inputs and targets a sampled data batch of x flows through the network until it reaches the output layer there tensorflow compares the models predictions against the actual observed targets y in the current batch afterwards tensorflow conducts an optimization step and updates the networks parameters corresponding to the selected learning scheme after having updated the weights and biases the next batch is sampled and the process repeats itself the procedure continues until all batches have been presented to the network one full sweep over all batches is called an epoch the training of the network stops once the maximum number of epochs is reached or another stopping criterion defined by the user applies during the training we evaluate the networks predictions on the test set the data which is not learned but set aside for every th batch and visualize it additionally the images are exported to disk and later combined into a video animation of the training process see below the model quickly learns the shape und location of the time series in the test data and is able to produce an accurate prediction after some epochs nice one can see that the networks rapidly adapts to the basic shape of the time series and continues to learn finer patterns of the data this also corresponds to the adam learning scheme that lowers the learning rate during model training in order not to overshoot the optimization minimum after epochs we have a pretty close fit to the test data the final test mse equals it is very low because the target is scaled the mean absolute percentage error of the forecast on the test set is equal to which is pretty good note that this is just a fit to the test data no actual out of sample metrics in a real world scenario please note that there are tons of ways of further improving this result design of layers and neurons choosing different initialization and activation schemes introduction of dropout layers of neurons early stopping and so on furthermore different types of deep learning models such as recurrent neural networks might achieve better performance on this task however this is not the scope of this introductory post the release of tensorflow was a landmark event in deep learning research its flexibility and performance allows researchers to develop all kinds of sophisticated neural network architectures as well as other ml algorithms however flexibility comes at the cost of longer time to model cycles compared to higher level apis such as keras or mxnet nonetheless i am sure that tensorflow will make its way to the de facto standard in neural network and deep learning development in research and practical applications many of our customers are already using tensorflow or start developing projects that employ tensorflow models also our data science consultants at statworx are heavily using tensorflow for deep learning and neural net research and development let s see what google has planned for the future of tensorflow one thing that is missing at least in my opinion is a neat graphical user interface for designing and developing neural net architectures with tensorflow backend maybe this is something google is already working on if you have any comments or questions on my first medium story feel free to comment below i will try to answer them also feel free to use my code or share this story with your peers on social platforms of your choice update i ve added both the python script as well as a zipped dataset to a github repository feel free to clone and fork lastly follow me on twitter linkedin from a quick cheer to a standing ovation clap to show how much you enjoyed this story ceo statworx doing data science stats and ml for over a decade food wine and cocktail enthusiast check our website https www statworx com highlights from machine learning research projects and learning materials from and for ml scientists engineers an enthusiasts
Max Pechyonkin,23000,8,https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b?source=tag_archive---------4----------------,Understanding Hinton’s Capsule Networks. Part I: Intuition.,part i intuition you are reading it now part ii how capsules workpart iii dynamic routing between capsulespart iv capsnet architecture quick announcement about our new publication ai we are getting the best writers together to talk about the theory practice and business of ai and machine learning follow it to stay up to date on the latest trends last week geoffrey hinton and his team published two papers that introduced a completely new type of neural network based on so called capsules in addition to that the team published an algorithm called dynamic routing between capsules that allows to train such a network for everyone in the deep learning community this is huge news and for several reasons first of all hinton is one of the founders of deep learning and an inventor of numerous models and algorithms that are widely used today secondly these papers introduce something completely new and this is very exciting because it will most likely stimulate additional wave of research and very cool applications in this post i will explain why this new architecture is so important as well as intuition behind it in the following posts i will dive into technical details however before talking about capsules we need to have a look at cnns which are the workhorse of today s deep learning cnns convolutional neural networks are awesome they are one of the reasons deep learning is so popular today they can do amazing things that people used to think computers would not be capable of doing for a long long time nonetheless they have their limits and they have fundamental drawbacks let us consider a very simple and non technical example imagine a face what are the components we have the face oval two eyes a nose and a mouth for a cnn a mere presence of these objects can be a very strong indicator to consider that there is a face in the image orientational and relative spatial relationships between these components are not very important to a cnn how do cnns work the main component of a cnn is a convolutional layer its job is to detect important features in the image pixels layers that are deeper closer to the input will learn to detect simple features such as edges and color gradients whereas higher layers will combine simple features into more complex features finally dense layers at the top of the network will combine very high level features and produce classification predictions an important thing to understand is that higher level features combine lower level features as a weighted sum activations of a preceding layer are multiplied by the following layer neuron s weights and added before being passed to activation nonlinearity nowhere in this setup there is pose translational and rotational relationship between simpler features that make up a higher level feature cnn approach to solve this issue is to use max pooling or successive convolutional layers that reduce spacial size of the data flowing through the network and therefore increase the field of view of higher layer s neurons thus allowing them to detect higher order features in a larger region of the input image max pooling is a crutch that made convolutional networks work surprisingly well achieving superhuman performance in many areas but do not be fooled by its performance while cnns work better than any model before them max pooling nonetheless is losing valuable information hinton himself stated that the fact that max pooling is working so well is a big mistake and a disaster of course you can do away with max pooling and still get good results with traditional cnns but they still do not solve the key problem in the example above a mere presence of eyes a mouth and a nose in a picture does not mean there is a face we also need to know how these objects are oriented relative to each other computer graphics deals with constructing a visual image from some internal hierarchical representation of geometric data note that the structure of this representation needs to take into account relative positions of objects that internal representation is stored in computer s memory as arrays of geometrical objects and matrices that represent relative positions and orientation of these objects then special software takes that representation and converts it into an image on the screen this is called rendering inspired by this idea hinton argues that brains in fact do the opposite of rendering he calls it inverse graphics from visual information received by eyes they deconstruct a hierarchical representation of the world around us and try to match it with already learned patterns and relationships stored in the brain this is how recognition happens and the key idea is that representation of objects in the brain does not depend on view angle so at this point the question is how do we model these hierarchical relationships inside of a neural network the answer comes from computer graphics in d graphics relationships between d objects can be represented by a so called pose which is in essence translation plus rotation hinton argues that in order to correctly do classification and object recognition it is important to preserve hierarchical pose relationships between object parts this is the key intuition that will allow you to understand why capsule theory is so important it incorporates relative relationships between objects and it is represented numerically as a d pose matrix when these relationships are built into internal representation of data it becomes very easy for a model to understand that the thing that it sees is just another view of something that it has seen before consider the image below you can easily recognize that this is the statue of liberty even though all the images show it from different angles this is because internal representation of the statue of liberty in your brain does not depend on the view angle you have probably never seen these exact pictures of it but you still immediately knew what it was for a cnn this task is really hard because it does not have this built in understanding of d space but for a capsnet it is much easier because these relationships are explicitly modeled the paper that uses this approach was able to cut error rate by as compared to the previous state of the art which is a huge improvement another benefit of the capsule approach is that it is capable of learning to achieve state of the art performance by only using a fraction of the data that a cnn would use hinton mentions this in his famous talk about what is wrongs with cnns in this sense the capsule theory is much closer to what the human brain does in practice in order to learn to tell digits apart the human brain needs to see only a couple of dozens of examples hundreds at most cnns on the other hand need tens of thousands of examples to achieve very good performance which seems like a brute force approach that is clearly inferior to what we do with our brains the idea is really simple there is no way no one has come up with it before and the truth is hinton has been thinking about this for decades the reason why there were no publications is simply because there was no technical way to make it work before one of the reasons is that computers were just not powerful enough in the pre gpu based era before around another reason is that there was no algorithm that allowed to implement and successfully learn a capsule network in the same fashion the idea of artificial neurons was around since s but it was not until mid s when backpropagation algorithm showed up and allowed to successfully train deep networks in the same fashion the idea of capsules itself is not that new and hinton has mentioned it before but there was no algorithm up until now to make it work this algorithm is called dynamic routing between capsules this algorithm allows capsules to communicate with each other and create representations similar to scene graphs in computer graphics capsules introduce a new building block that can be used in deep learning to better model hierarchical relationships inside of internal knowledge representation of a neural network intuition behind them is very simple and elegant hinton and his team proposed a way to train such a network made up of capsules and successfully trained it on a simple data set achieving state of the art performance this is very encouraging nonetheless there are challenges current implementations are much slower than other modern deep learning models time will show if capsule networks can be trained quickly and efficiently in addition we need to see if they work well on more difficult data sets and in different domains in any case the capsule network is a very interesting and already working model which will definitely get more developed over time and contribute to further expansion of deep learning application domain this concludes part one of the series on capsule networks in the part ii more technical part i will walk you through the capsnet s internal workings step by step you can follow me on twitter let s also connect on linkedin from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning the ai revolution is here navigate the ever changing industry with our thoughtfully written articles whether your a researcher engineer or entrepreneur
Slav Ivanov,3900,17,https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415?source=tag_archive---------5----------------,"The $1700 great Deep Learning box: Assembly, setup and benchmarks",updated april uses cuda cudnn and tensorflow after years of using a thin client in the form of increasingly thinner macbooks i had gotten used to it so when i got into deep learning dl i went straight for the brand new at the time amazon p cloud servers no upfront cost the ability to train many models simultaneously and the general coolness of having a machine learning model out there slowly teaching itself however as time passed the aws bills steadily grew larger even as i switched to x cheaper spot instances also i didn t find myself training more than one model at a time instead i d go to lunch workout etc while the model was training and come back later with a clear head to check on it but eventually the model complexity grew and took longer to train i d often forget what i did differently on the model that had just completed its day training nudged by the great experiences of the other folks on the fast ai forum i decided to settle down and to get a dedicated dl box at home the most important reason was saving time while prototyping models if they trained faster the feedback time would be shorter thus it would be easier for my brain to connect the dots between the assumptions i had for the model and its results then i wanted to save money i was using amazon web services aws which offered p instances with nvidia k gpus lately the aws bills were around month with a tendency to get larger also it is expensive to store large datasets like imagenet and lastly i haven t had a desktop for over years and wanted to see what has changed in the meantime spoiler alert mostly nothing what follows are my choices inner monologue and gotchas from choosing the components to benchmarking a sensible budget for me would be about years worth of my current compute spending at month for aws this put it at around for the whole thing you can check out all the components used the pc part picker site is also really helpful in detecting if some of the components don t play well together the gpu is the most crucial component in the box it will train these deep networks fast shortening the feedback cycle disclosure the following are affiliate links to help me pay for well more gpus the choice is between a few of nvidia s cards gtx gtx ti gtx gtx ti and finally the titan x the prices might fluctuate especially because some gpus are great for cryptocurrency mining wink wink on performance side gtx ti and titan x are similar roughly speaking the gtx is about faster than gtx and gtx ti is about faster than gtx the new gtx ti is very close in performance to gtx tim dettmers has a great article on picking a gpu for deep learning which he regularly updates as new cards come on the market here are the things to consider when picking a gpu considering all of this i picked the gtx ti mainly for the training speed boost i plan to add a second ti soonish even though the gpu is the mvp in deep learning the cpu still matters for example data preparation is usually done on the cpu the number of cores and threads per core is important if we want to parallelize all that data prep to stay on budget i picked a mid range cpu the intel i it s relatively cheap but good enough to not slow things down edit as a few people have pointed out probably the biggest gotcha that is unique to dl multi gpu is to pay attention to the pcie lanes supported by the cpu motherboard by andrej karpathy we want to have each gpu have pcie lanes so it eats data as fast as possible gb s for pcie this means that for two cards we need pcie lanes however the cpu i have picked has only lanes so gpus would run in x mode instead of x this might be a bottleneck leading to less than ideal utilization of the graphics cards thus a cpu with lines is recommended edit however tim dettmers points out that having lanes per card should only decrease performance by for two gpus so currently my recommendation is go with pcie lanes per video card unless it gets too expensive for you otherwise lanes should do as well a good solution with to have for a double gpu machine would be an intel xeon processor like the e v pcie lanes or if you want to splurge go for a higher end processor like the desktop i k memory ram it s nice to have a lot of memory if we are to be working with rather big datasets i got sticks of gb for a total of gb of ram and plan to buy another gb later following jeremy howard s advice i got a fast ssd disk to keep my os and current data on and then a slow spinning hdd for those huge datasets like imagenet ssd i remember when i got my first macbook air years ago how blown away was i by the ssd speed to my delight a new generation of ssd called nvme has made its way to market in the meantime a gb mydigitalssd nvme drive was a great deal this baby copies files at gigabytes per second hdd tb seagate while ssds have been getting fast hdd have been getting cheap to somebody who has used macbooks with gb disk for the last years having this much space feels almost obscene the one thing that i kept in mind when picking a motherboard was the ability to support two gtx ti both in the number of pci express lanes the minimum is x and the physical size of cards also make sure it s compatible with the chosen cpu an asus tuf z did it for me msi x a sli plus should work great if you got an intel xeon cpu rule of thumb power supply should provide enough juice for the cpu and the gpus plus watts extra the intel i processor uses w and the gpus ti need w each so i got a deepcool w gold psu currently unavailable evga gq is similar the gold here refers to the power efficiency i e how much of the power consumed is wasted as heat the case should be the same form factor as the motherboard also having enough leds to embarrass a burner is a bonus a friend recommended the thermaltake n case which i promptly got no leds sadly here is how much i spent on all the components your costs may vary gtx ti cpu ram ssd hdd motherboard psu case total adding tax and fees this nicely matches my preset budget of if you don t have much experience with hardware and fear you might break something a professional assembly might be the best option however this was a great learning opportunity that i couldn t pass even though i ve had my share of hardware related horror stories the first and important step is to read the installation manuals that came with each component especially important for me as i ve done this before once or twice and i have just the right amount of inexperience to mess things up this is done before installing the motherboard in the case next to the processor there is a lever that needs to be pulled up the processor is then placed on the base double check the orientation finally the lever comes down to fix the cpu in place but i had a quite the difficulty doing this once the cpu was in position the lever wouldn t go down i actually had a more hardware capable friend of mine video walk me through the process turns out the amount of force required to get the lever locked down was more than what i was comfortable with next is fixing the fan on top of the cpu the fan legs must be fully secured to the motherboard consider where the fan cable will go before installing the processor i had came with thermal paste if yours doesn t make sure to put some paste between the cpu and the cooling unit also replace the paste if you take off the fan i put the power supply unit psu in before the motherboard to get the power cables snugly placed in case back side pretty straight forward carefully place it and screw it in a magnetic screwdriver was really helpful then connect the power cables and the case buttons and leds just slide it in the m slot and screw it in piece of cake the memory proved quite hard to install requiring too much effort to properly lock in a few times i almost gave up thinking i must be doing it wrong eventually one of the sticks clicked in and the other one promptly followed at this point i turned the computer on to make sure it works to my relief it started right away finally the gpu slid in effortlessly pins of power later and it was running nb do not plug your monitor in the external card right away most probably it needs drivers to function see below finally it s complete now that we have the hardware in place only the soft part remains out with the screwdriver in with the keyboard note on dual booting if you plan to install windows because you know for benchmarks totally not for gaming it would be wise to do windows first and linux second i didn t and had to reinstall ubuntu because windows messed up the boot partition livewire has a detailed article on dual boot most dl frameworks are designed to work on linux first and eventually support other operating systems so i went for ubuntu my default linux distribution an old gb usb drive was laying around and worked great for the installation unetbootin osx or rufus windows can prepare the linux thumb drive the default options worked fine during the ubuntu install at the time of writing ubuntu was just released so i opted for the previous version whose quirks are much better documented online ubuntu server or desktop the server and desktop editions of ubuntu are almost identical with the notable exception of the visual interface called x not being installed with server i installed the desktop and disabled autostarting x so that the computer would boot it in terminal mode if needed one could launch the visual desktop later by typing startx let s get our install up to date from jeremy howard s excellent install gpu script to deep learn on our machine we need a stack of technologies to use our gpu download cuda from nvidia or just run the code below updated to specify version of cuda thanks to zhanwenchen for the tip if you need to add later versions of cuda click here after cuda has been installed the following code will add the cuda installation to the path variable now we can verify that cuda has been installed successfully by running this should have installed the display driver as well for me nvidia smi showed err as the device name so i installed the latest nvidia drivers as of may to fix it removing cuda nvidia drivers if at any point the drivers or cuda seem broken as they did for me multiple times it might be better to start over by running since version tensorflow supports cudnn so we install that to download cudnn one needs to register for a free developer account after downloading install with the following anaconda is a great package manager for python i ve moved to python so will be using the anaconda version the popular dl framework by google installation validate tensorfow install to make sure we have our stack running smoothly i like to run the tensorflow mnist example we should see the loss decreasing during training keras is a great high level neural networks framework an absolute pleasure to work with installation can t be easier too pytorch is a newcomer in the world of dl frameworks but its api is modeled on the successful torch which was written in lua pytorch feels new and exciting mostly great although some things are still to be implemented we install it by running jupyter is a web based ide for python which is ideal for data sciency tasks it s installed with anaconda so we just configure and test it now if we open http localhost we should see a jupyter screen run jupyter on boot rather than running the notebook every time the computer is restarted we can set it to autostart on boot we will use crontab to do this which we can edit by running crontab e then add the following after the last line in the crontab file i use my old trusty macbook air for development so i d like to be able to log into the dl box both from my home network also when on the run ssh key it s way more secure to use a ssh key to login instead of a password digital ocean has a great guide on how to setup this ssh tunnel if you want to access your jupyter notebook from another computer the recommended way is to use ssh tunneling instead of opening the notebook to the world and protecting with a password let s see how we can do this then to connect over ssh tunnel run the following script on the client to test this open a browser and try http localhost from the remote machine your jupyter notebook should appear setup out of network access finally to access the dl box from the outside world we need things setting up out of network access depends on the router network setup so i m not going into details now that we have everything running smoothly let s put it to the test we ll be comparing the newly built box to an aws p xlarge instance which is what i ve used so far for dl the tests are computer vision related meaning convolutional networks with a fully connected model thrown in we time training models on aws p instance gpu k aws p virtual cpu the gtx ti and intel i cpu andres hernandez points out that my comparison does not use tensorflow that is optimized for these cpus which would have helped the them perform better check his insightful comment for more details the hello world of computer vision the mnist database consists of handwritten digits we run the keras example on mnist which uses multilayer perceptron mlp the mlp means that we are using only fully connected layers not convolutions the model is trained for epochs on this dataset which achieves over accuracy out of the box we see that the gtx ti is times faster than the k on aws p in training the model this is rather surprising as these cards should have about the same performance i believe this is because of the virtualization or underclocking of the k on aws the cpus perform times slower than the gpus as we will see later it s a really good result for the processors this is due to the small model which fails to fully utilize the parallel processing power of the gpus interestingly the desktop intel i achieves x speedup over the virtual cpu on amazon a vgg net will be finetuned for the kaggle dogs vs cats competition in this competition we need to tell apart pictures of dogs and cats running the model on cpus for the same number of batches wasn t feasible therefore we finetune for batches epoch on the gpus and batches on the cpus the code used is on github the ti is times faster that the aws gpu k the difference in the cpus performance is about the same as the previous experiment i is x faster however it s absolutely impractical to use cpus for this task as the cpus were taking x more time on this large model that includes convolutional layers and a couple semi wide fully connected layers on top a gan generative adversarial network is a way to train a model to generate images gan achieves this by pitting two networks against each other a generator which learns to create better and better images and a discriminator that tries to tell which images are real and which are dreamt up by the generator the wasserstein gan is an improvement over the original gan we will use a pytorch implementation that is very similar to the one by the wgan author the models are trained for steps and the loss is all over the place which is often the case with gans cpus aren t considered the gtx ti finishes x faster than the aws p k which is in line with the previous results the final benchmark is on the original style transfer paper gatys et al implemented on tensorflow code available style transfer is a technique that combines the style of one image a painting for example and the content of another image check out my previous post for more details on how style transfer works the gtx ti outperforms the aws k by a factor of this time the cpus are times slower than graphics cards the slowdown is less than on the vgg finetuning task but more than on the mnist perceptron experiment the model uses mostly the earlier layers of the vgg network and i suspect this was too shallow to fully utilize the gpus the dl box is in the next room and a large model is training on it was it a wise investment time will tell but it is beautiful to watch the glowing leds in the dark and to hear its quiet hum as models are trying to squeeze out that extra accuracy percentage point from a quick cheer to a standing ovation clap to show how much you enjoyed this story entrepreneur hacker machine learning deep learning and other types of learning
Stefan Kojouharov,14200,7,https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?source=tag_archive---------6----------------,"Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data",over the past few months i have been collecting ai cheat sheets from time to time i share them with friends and colleagues and recently i have been getting asked a lot so i decided to organize and share the entire collection to make things more interesting and give context i added descriptions and or excerpts for each major topic this is the most complete list and the big o is at the very end enjoy this machine learning cheat sheet will help you find the right estimator for the job which is the most difficult part the flowchart will help you check the documentation and rough guide of each estimator that will help you to know more about the problems and how to solve it scikit learn formerly scikits learn is a free software machine learning library for the python programming language it features various classification regression and clustering algorithms including support vector machines random forests gradient boosting k means and dbscan and is designed to interoperate with the python numerical and scientific libraries numpy and scipy in may google announced the second generation of the tpu as well as the availability of the tpus in google compute engine the second generation tpus deliver up to teraflops of performance and when organized into clusters of tpus provide up to petaflops in google s tensorflow team decided to support keras in tensorflow s core library chollet explained that keras was conceived to be an interface rather than an end to end machine learning framework it presents a higher level more intuitive set of abstractions that make it easy to configure neural networks regardless of the backend scientific computing library numpy targets the cpython reference implementation of python which is a non optimizing bytecode interpreter mathematical algorithms written for this version of python often run much slower than compiled equivalents numpy address the slowness problem partly by providing multidimensional arrays and functions and operators that operate efficiently on arrays requiring rewriting some code mostly inner loops using numpy the name pandas is derived from the term panel data an econometrics term for multidimensional structured data sets the term data wrangler is starting to infiltrate pop culture in the movie kong skull island one of the characters played by actor marc evan jackson is introduced as steve woodward our data wrangler scipy builds on the numpy array object and is part of the numpy stack which includes tools like matplotlib pandas and sympy and an expanding set of scientific computing libraries this numpy stack has similar users to other applications such as matlab gnu octave and scilab the numpy stack is also sometimes referred to as the scipy stack matplotlib is a plotting library for the python programming language and its numerical mathematics extension numpy it provides an object oriented api for embedding plots into applications using general purpose gui toolkits like tkinter wxpython qt or gtk there is also a procedural pylab interface based on a state machine like opengl designed to closely resemble that of matlab though its use is discouraged scipy makes use of matplotlib pyplot is a matplotlib module which provides a matlab like interface matplotlib is designed to be as usable as matlab with the ability to use python with the advantage that it is free if you like this list you can let me know here stefan is the founder of chatbot s life a chatbot media and consulting firm chatbot s life has grown to over k views per month and has become the premium place to learn about bots ai online chatbot s life has also consulted many of the top bot companies like swelly instavest outbrain neargroup and a number of enterprises big o algorithm cheat sheet http bigocheatsheet com bokeh cheat sheet https s amazonaws com assets datacamp com blog assets python bokeh cheat sheet pdf data science cheat sheet https www datacamp com community tutorials python data science cheat sheet basics data wrangling cheat sheet https www rstudio com wp content uploads data wrangling cheatsheet pdf data wrangling https en wikipedia org wiki data wrangling ggplot cheat sheet https www rstudio com wp content uploads ggplot cheatsheet pdf keras cheat sheet https www datacamp com community blog keras cheat sheet gs drkenms keras https en wikipedia org wiki keras machine learning cheat sheet https ai icymi email new machinelearning cheat sheet by emily barry abdsc machine learning cheat sheet https docs microsoft com en in azure machine learning machine learning algorithm cheat sheet ml cheat sheet http peekaboo vision blogspot com machine learning cheat sheet for scikit html matplotlib cheat sheet https www datacamp com community blog python matplotlib cheat sheet gs uekyspy matpotlib https en wikipedia org wiki matplotlib neural networks cheat sheet http www asimovinstitute org neural network zoo neural networks graph cheat sheet http www asimovinstitute org blog neural networks https www quora com where can find a cheat sheet for neural network numpy cheat sheet https www datacamp com community blog python numpy cheat sheet gs ak zbge numpy https en wikipedia org wiki numpy pandas cheat sheet https www datacamp com community blog python pandas cheat sheet gs oundfxm pandas https en wikipedia org wiki pandas software pandas cheat sheet https www datacamp com community blog pandas cheat sheet python gs hpforic pyspark cheat sheet https www datacamp com community blog pyspark cheat sheet python gs l j zxq scikit cheat sheet https www datacamp com community blog scikit learn cheat sheet scikit learn https en wikipedia org wiki scikit learn scikit learn cheat sheet http peekaboo vision blogspot com machine learning cheat sheet for scikit html scipy cheat sheet https www datacamp com community blog python scipy cheat sheet gs jdsg oi scipy https en wikipedia org wiki scipy tesorflow cheat sheet https www altoros com tensorflow cheat sheet html tensor flow https en wikipedia org wiki tensorflow from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder of chatbots life i help companies create great chatbots ai systems and share my insights along the way latest news info and tutorials on artificial intelligence machine learning deep learning big data and what it means for humanity
Vishal Maini,8000,13,https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab?source=tag_archive---------7----------------,"Machine Learning for Humans, Part 2.1: Supervised Learning",how much money will we make by spending more dollars on digital advertising will this loan applicant pay back the loan or not what s going to happen to the stock market tomorrow in supervised learning problems we start with a data set containing training examples with associated correct labels for example when learning to classify handwritten digits a supervised learning algorithm takes thousands of pictures of handwritten digits along with labels containing the correct number each image represents the algorithm will then learn the relationship between the images and their associated numbers and apply that learned relationship to classify completely new images without labels that the machine hasn t seen before this is how you re able to deposit a check by taking a picture with your phone to illustrate how supervised learning works let s examine the problem of predicting annual income based on the number of years of higher education someone has completed expressed more formally we d like to build a model that approximates the relationship f between the number of years of higher education x and corresponding annual income y one method for predicting income would be to create a rigid rules based model for how income and education are related for example i d estimate that for every additional year of higher education annual income increases by you could come up with a more complex model by including some rules about degree type years of work experience school tiers etc for example if they completed a bachelor s degree or higher give the income estimate a x multiplier but this kind of explicit rules based programming doesn t work well with complex data imagine trying to design an image classification algorithm made of if then statements describing the combinations of pixel brightnesses that should be labeled cat or not cat supervised machine learning solves this problem by getting the computer to do the work for you by identifying patterns in the data the machine is able to form heuristics the primary difference between this and human learning is that machine learning runs on computer hardware and is best understood through the lens of computer science and statistics whereas human pattern matching happens in a biological brain while accomplishing the same goals in supervised learning the machine attempts to learn the relationship between income and education from scratch by running labeled training data through a learning algorithm this learned function can be used to estimate the income of people whose income y is unknown as long as we have years of education x as inputs in other words we can apply our model to the unlabeled test data to estimate y the goal of supervised learning is to predict y as accurately as possible when given new examples where x is known and y is unknown in what follows we ll explore several of the most common approaches to doing so the rest of this section will focus on regression in part we ll dive deeper into classification methods regression predicts a continuous target variable y it allows you to estimate a value such as housing prices or human lifespan based on input data x here target variable means the unknown variable we care about predicting and continuous means there aren t gaps discontinuities in the value that y can take on a person s weight and height are continuous values discrete variables on the other hand can only take on a finite number of values for example the number of kids somebody has is a discrete variable predicting income is a classic regression problem your input data x includes all relevant information about individuals in the data set that can be used to predict income such as years of education years of work experience job title or zip code these attributes are called features which can be numerical e g years of work experience or categorical e g job title or field of study you ll want as many training observations as possible relating these features to the target output y so that your model can learn the relationship f between x and y the data is split into a training data set and a test data set the training set has labels so your model can learn from these labeled examples the test set does not have labels i e you don t yet know the value you re trying to predict it s important that your model can generalize to situations it hasn t encountered before so that it can perform well on the test data in our trivially simple d example this could take the form of a csv file where each row contains a person s education level and income add more columns with more features and you ll have a more complex but possibly more accurate model how do we build models that make accurate useful predictions in the real world we do so by using supervised learning algorithms now let s get to the fun part getting to know the algorithms we ll explore some of the ways to approach regression and classification and illustrate key machine learning concepts throughout draw the line yes this counts as machine learning first we ll focus on solving the income prediction problem with linear regression since linear models don t work well with image recognition tasks this is the domain of deep learning which we ll explore later we have our data set x and corresponding target values y the goal of ordinary least squares ols regression is to learn a linear model that we can use to predict a new y given a previously unseen x with as little error as possible we want to guess how much income someone earns based on how many years of education they received linear regression is a parametric method which means it makes an assumption about the form of the function relating x and y we ll cover examples of non parametric methods later our model will be a function that predicts y given a specific x is the y intercept and is the slope of our line i e how much income increases or decreases with one additional year of education our goal is to learn the model parameters in this case and that minimize error in the model s predictions to find the best parameters graphically in two dimensions this results in a line of best fit in three dimensions we would draw a plane and so on with higher dimensional hyperplanes mathematically we look at the difference between each real data point y and our model s prediction y square these differences to avoid negative numbers and penalize larger differences and then add them up and take the average this is a measure of how well our data fits the line for a simple problem like this we can compute a closed form solution using calculus to find the optimal beta parameters that minimize our loss function but as a cost function grows in complexity finding a closed form solution with calculus is no longer feasible this is the motivation for an iterative approach called gradient descent which allows us to minimize a complex loss function put on a blindfold take a step downhill you ve found the bottom when you have nowhere to go but up gradient descent will come up over and over again especially in neural networks machine learning libraries like scikit learn and tensorflow use it in the background everywhere so it s worth understanding the details the goal of gradient descent is to find the minimum of our model s loss function by iteratively getting a better and better approximation of it imagine yourself walking through a valley with a blindfold on your goal is to find the bottom of the valley how would you do it a reasonable approach would be to touch the ground around you and move in whichever direction the ground is sloping down most steeply take a step and repeat the same process continually until the ground is flat then you know you ve reached the bottom of a valley if you move in any direction from where you are you ll end up at the same elevation or further uphill going back to mathematics the ground becomes our loss function and the elevation at the bottom of the valley is the minimum of that function let s take a look at the loss function we saw in regression we see that this is really a function of two variables and all the rest of the variables are determined since x y and n are given during training we want to try to minimize this function the function is f z to begin gradient descent you make some guess of the parameters and that minimize the function next you find the partial derivatives of the loss function with respect to each beta parameter dz d dz d a partial derivative indicates how much total loss is increased or decreased if you increase or by a very small amount put another way how much would increasing your estimate of annual income assuming zero higher education increase the loss i e inaccuracy of your model you want to go in the opposite direction so that you end up walking downhill and minimizing loss similarly if you increase your estimate of how much each incremental year of education affects income how much does this increase loss z if the partial derivative dz is a negative number then increasing is good because it will reduce total loss if it s a positive number you want to decrease if it s zero don t change because it means you ve reached an optimum keep doing that until you reach the bottom i e the algorithm converged and loss has been minimized there are lots of tricks and exceptional cases beyond the scope of this series but generally this is how you find the optimal parameters for your parametric model overfitting sherlock your explanation of what just happened is too specific to the situation regularization don t overcomplicate things sherlock i ll punch you for every extra word hyperparameter here s the strength with which i will punch you for every extra word a common problem in machine learning is overfitting learning a function that perfectly explains the training data that the model learned from but doesn t generalize well to unseen test data overfitting happens when a model overlearns from the training data to the point that it starts picking up idiosyncrasies that aren t representative of patterns in the real world this becomes especially problematic as you make your model increasingly complex underfitting is a related issue where your model is not complex enough to capture the underlying trend in the data remember that the only thing we care about is how the model performs on test data you want to predict which emails will be marked as spam before they re marked not just build a model that is accurate at reclassifying the emails it used to build itself in the first place hindsight is the real question is whether the lessons learned will help in the future the model on the right has zero loss for the training data because it perfectly fits every data point but the lesson doesn t generalize it would do a horrible job at explaining a new data point that isn t yet on the line two ways to combat overfitting use more training data the more you have the harder it is to overfit the data by learning too much from any single training example use regularization add in a penalty in the loss function for building a model that assigns too much explanatory power to any one feature or allows too many features to be taken into account the first piece of the sum above is our normal cost function the second piece is a regularization term that adds a penalty for large beta coefficients that give too much explanatory power to any specific feature with these two elements in place the cost function now balances between two priorities explaining the training data and preventing that explanation from becoming overly specific the lambda coefficient of the regularization term in the cost function is a hyperparameter a general setting of your model that can be increased or decreased i e tuned in order to improve performance a higher lambda value will more harshly penalize large beta coefficients that could lead to potential overfitting to decide the best value of lambda you d use a method called cross validation which involves holding out a portion of the training data during training and then seeing how well your model explains the held out portion we ll go over this in more depth here s what we covered in this section in the next section part supervised learning ii we ll talk about two foundational methods of classification logistic regression and support vector machines for a more thorough treatment of linear regression read chapters of an introduction to statistical learning the book is available for free online and is an excellent resource for understanding machine learning concepts with accompanying exercises for more practice to actually implement gradient descent in python check out this tutorial and here is a more mathematically rigorous description of the same concepts in practice you ll rarely need to implement gradient descent from scratch but understanding how it works behind the scenes will allow you to use it more effectively and understand why things break when they do more from machine learning for humans from a quick cheer to a standing ovation clap to show how much you enjoyed this story research comms deepmindai previously upstart yale trueventurestec demystifying artificial intelligence machine learning discussions on safe and intentional application of ai for positive social impact
Arvind N,9500,8,https://towardsdatascience.com/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153?source=tag_archive---------8----------------,Thoughts after taking the Deeplearning.ai courses – Towards Data Science,update feb nd when this blog post was written only courses had been released all courses in this specialization are now out i will have a follow up blog post soon between a full time job and a toddler at home i spend my spare time learning about the ideas in cognitive science ai once in a while a great paper video course comes out and you re instantly hooked andrew ng s new deeplearning ai course is like that shane carruth or rajnikanth movie that one yearns for naturally as soon as the course was released on coursera i registered and spent the past evenings binge watching the lectures working through quizzes and programming assignments dl practitioners and ml engineers typically spend most days working at an abstract keras or tensorflow level but it s nice to take a break once in a while to get down to the nuts and bolts of learning algorithms and actually do back propagation by hand it is both fun and incredibly useful andrew ng s new adventure is a bottom up approach to teaching neural networks powerful non linearity learning algorithms at a beginner mid level in classic ng style the course is delivered through a carefully chosen curriculum neatly timed videos and precisely positioned information nuggets andrew picks up from where his classic ml course left off and introduces the idea of neural networks using a single neuron logistic regression and slowly adding complexity more neurons and layers by the end of the weeks course a student is introduced to all the core ideas required to build a dense neural network such as cost loss functions learning iteratively using gradient descent and vectorized parallel python numpy implementations andrew patiently explains the requisite math and programming concepts in a carefully planned order and a well regulated pace suitable for learners who could be rusty in math coding lectures are delivered using presentation slides on which andrew writes using digital pens it felt like an effective way to get the listener to focus i felt comfortable watching videos at x or x speed quizzes are placed at the end of each lecture sections and are in the multiple choice question format if you watch the videos once you should be able to quickly answer all the quiz questions you can attempt quizzes multiple times and the system is designed to keep your highest score programming assignments are done via jupyter notebooks powerful browser based applications assignments have a nice guided sequential structure and you are not required to write more than lines of code in each section if you understand the concepts like vectorization intuitively you can complete most programming sections with just line of code after the assignment is coded it takes button click to submit your code to the automated grading system which returns your score in a few minutes some assignments have time restrictions say three attempts in hours etc jupyter notebooks are well designed and work without any issues instructions are precise and it feels like a polished product anyone interested in understanding what neural networks are how they work how to build them and the tools available to bring your ideas to life if your math is rusty there is no need to worry andrew explains all the required calculus and provides derivatives at every occasion so that you can focus on building the network and concentrate on implementing your ideas in code if your programming is rusty there is a nice coding assignment to teach you numpy but i recommend learning python first on codecademy let me explain this with an analogy assume you are trying to learn how to drive a car jeremy s fast ai course puts you in the drivers seat from the get go he teaches you to move the steering wheel press the brake accelerator etc then he slowly explains more details about how the car works why rotating the wheel makes the car turn why pressing the brake pedal makes you slow down and stop etc he keeps getting deeper into the inner workings of the car and by the end of the course you know how the internal combustion engine works how the fuel tank is designed etc the goal of the course is to get you driving you can choose to stop at any point after you can drive reasonably well there is no need to learn how to build repair the car andrew s dl course does all of this but in the complete opposite order he teaches you about internal combustion engine first he keeps adding layers of abstraction and by the end of the course you are driving like an f racer the fast ai course mainly teaches you the art of driving while andrew s course primarily teaches you the engineering behind the car if you have not done any machine learning before this don t take this course first the best starting point is andrew s original ml course on coursera after you complete that course please try to complete part of jeremy howard s excellent deep learning course jeremy teaches deep learning top down which is essential for absolute beginners once you are comfortable creating deep neural networks it makes sense to take this new deeplearning ai course specialization which fills up any gaps in your understanding of the underlying details and concepts andrew stresses on the engineering aspects of deep learning and provides plenty of practical tips to save time and money the third course in the dl specialization felt incredibly useful for my role as an architect leading engineering teams jargon is handled well andrew explains that an empirical process trial error he is brutally honest about the reality of designing and training deep nets at some point i felt he might have as well just called deep learning as glorified curve fitting squashes all hype around dl and ai andrew makes restrained careful comments about proliferation of ai hype in the mainstream media and by the end of the course it is pretty clear that dl is nothing like the terminator wonderful boilerplate code that just works out of the box excellent course structure nice consistent and useful notation andrew strives to establish a fresh nomenclature for neural nets and i feel he could be quite successful in this endeavor style of teaching that is unique to andrew and carries over from ml i could feel the same excitement i felt in when i took his original ml course the interviews with deep learning heroes are refreshing it is motivating and fun to hear personal stories and anecdotes i wish that he d said concretely more often good tools are important and will help you accelerate your learning pace i bought a digital pen after seeing andrew teach with one it helped me work more efficiently there is a psychological reason why i recommend the fast ai course before this one once you find your passion you can learn uninhibited you just get that dopamine rush each time you score full points don t be scared by dl jargon hyperparameters settings architecture topology style etc or the math symbols if you take a leap of faith and pay attention to the lectures andrew shows why the symbols and notation are actually quite useful they will soon become your tools of choice and you will wield them with style thanks for reading and best wishes update thanks for the overwhelmingly positive response many people are asking me to explain gradient descent and the differential calculus i hope this helps from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in strong ai sharing concepts ideas and codes
Blaise Aguera y Arcas,8700,15,https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477?source=tag_archive---------0----------------,Do algorithms reveal sexual orientation or just expose our stereotypes?,by blaise agu era y arcas alexander todorov and margaret mitchell a study claiming that artificial intelligence can infer sexual orientation from facial images caused a media uproar in the fall of the economist featured this work on the cover of their september th magazine on the other hand two major lgbtq organizations the human rights campaign and glaad immediately labeled it junk science michal kosinski who co authored the study with fellow researcher yilun wang initially expressed surprise calling the critiques knee jerk reactions however he then proceeded to make even bolder claims that such ai algorithms will soon be able to measure the intelligence political orientation and criminal inclinations of people from their facial images alone kosinski s controversial claims are nothing new last year two computer scientists from china posted a non peer reviewed paper online in which they argued that their ai algorithm correctly categorizes criminals with nearly accuracy from a government id photo alone technology startups had also begun to crop up claiming that they can profile people s character from their facial images these developments had prompted the three of us to collaborate earlier in the year on a medium essay physiognomy s new clothes to confront claims that ai face recognition reveals deep character traits we described how the junk science of physiognomy has roots going back into antiquity with practitioners in every era resurrecting beliefs based on prejudice using the new methodology of the age in the th century this included anthropology and psychology in the th genetics and statistical analysis and in the st artificial intelligence in late the paper motivating our physiognomy essay seemed well outside the mainstream in tech and academia but as in other areas of discourse what recently felt like a fringe position must now be addressed head on kosinski is a faculty member of stanford s graduate school of business and this new study has been accepted for publication in the respected journal of personality and social psychology much of the ensuing scrutiny has focused on ethics implicitly assuming that the science is valid we will focus on the science the authors trained and tested their sexual orientation detector using images from public profiles on a us dating website composite images of the lesbian gay and straight men and women in the sample reveal a great deal about the information available to the algorithm clearly there are differences between these four composite faces wang and kosinski assert that the key differences are in physiognomy meaning that a sexual orientation tends to go along with a characteristic facial structure however we can immediately see that some of these differences are more superficial for example the average straight woman appears to wear eyeshadow while the average lesbian does not glasses are clearly visible on the gay man and to a lesser extent on the lesbian while they seem absent in the heterosexual composites might it be the case that the algorithm s ability to detect orientation has little to do with facial structure but is due rather to patterns in grooming presentation and lifestyle we conducted a survey of americans using amazon s mechanical turk crowdsourcing platform to see if we could independently confirm these patterns asking yes no questions such as do you wear eyeshadow do you wear glasses and do you have a beard as well as questions about gender and sexual orientation the results show that lesbians indeed use eyeshadow much less than straight women do gay men and women do both wear glasses more and young opposite sex attracted men are considerably more likely to have prominent facial hair than their gay or same sex attracted peers breaking down the answers by the age of the respondent can provide a richer and clearer view of the data than any single statistic in the following figures we show the proportion of women who answer yes to do you ever use makeup top and do you wear eyeshadow bottom averaged over year age intervals the blue curves represent strictly opposite sex attracted women a nearly identical set to those who answered yes to are you heterosexual or straight the cyan curve represents women who answer yes to either or both of are you sexually attracted to women and are you romantically attracted to women and the red curve represents women who answer yes to are you homosexual gay or lesbian the shaded regions around each curve show confidence intervals the patterns revealed here are intuitive it won t be breaking news to most that straight women tend to wear more makeup and eyeshadow than same sex attracted and even more so lesbian identifying women on the other hand these curves also show us how often these stereotypes are violated that same sex attracted men of most ages wear glasses significantly more than exclusively opposite sex attracted men do might be a bit less obvious but this trend is equally clear a proponent of physiognomy might be tempted to guess that this is somehow related to differences in visual acuity between these populations of men however asking the question do you like how you look in glasses reveals that this is likely more of a stylistic choice same sex attracted women also report wearing glasses more as well as liking how they look in glasses more across a range of ages one can also see how opposite sex attracted women under the age of wear contact lenses significantly more than same sex attracted women despite reporting that they have a vision defect at roughly the same rate further illustrating how the difference is driven by an aesthetic preference similar analysis shows that young same sex attracted men are much less likely to have hairy faces than opposite sex attracted men serious facial hair in our plots is defined as answering yes to having a goatee beard or moustache but no to stubble overall opposite sex attracted men in our sample are more likely to have serious facial hair than same sex attracted men and for men under the age of who are overrepresented on dating websites this rises to wang and kosinski speculate in their paper that the faintness of the beard and moustache in their gay male composite might be connected with prenatal underexposure to androgens male hormones resulting in a feminizing effect hence sparser facial hair the fact that we see a cohort of same sex attracted men in their s who have just as much facial hair as opposite sex attracted men suggests a different story in which fashion trends and cultural norms play the dominant role in choices about facial hair among men not differing exposure to hormones early in development the authors of the paper additionally note that the heterosexual male composite appears to have darker skin than the other three composites our survey confirms that opposite sex attracted men consistently self report having a tan face yes to is your face tan slightly more often than same sex attracted men once again wang and kosinski reach for a hormonal explanation writing while the brightness of the facial image might be driven by many factors previous research found that testosterone stimulates melanocyte structure and function leading to a darker skin however a simpler answer is suggested by the responses to the question do you work outdoors overall opposite sex attracted men are more likely to work outdoors and among men under this rises to previous research has found that increased exposure to sunlight leads to darker skin none of these results prove that there is no physiological basis for sexual orientation in fact ample evidence shows us that orientation runs much deeper than a choice or a lifestyle in a critique aimed in part at fraudulent conversion therapy programs united states surgeon general david satcher wrote in a report sexual orientation is usually determined by adolescence if not earlier and there is no valid scientific evidence that sexual orientation can be changed it follows that if we dig deeply enough into human physiology and neuroscience we will eventually find reliable correlates and maybe even the origins of sexual orientation in our survey we also find some evidence of outwardly visible correlates of orientation that are not cultural perhaps most strikingly very tall women are overrepresented among lesbian identifying respondents however while this is interesting it s very far from a good predictor of women s sexual orientation makeup and eyeshadow do much better the way wang and kosinski measure the efficacy of their ai gaydar is equivalent to choosing a straight and a gay or lesbian face image both from data held out during the training process and asking how often the algorithm correctly guesses which is which performance would be no better than random chance for women guessing that the taller of the two is the lesbian achieves only accuracy barely above random chance this is because despite the statistically meaningful overrepresentation of tall women among the lesbian population the great majority of lesbians are not unusually tall by contrast the performance measures in the paper for gay men and for lesbian women seem impressive consider however that we can achieve comparable results with trivial models based only on a handful of yes no survey questions about presentation for example for pairs of women one of whom is lesbian the following not exactly superhuman algorithm is on average accurate if neither or both women wear eyeshadow flip a coin otherwise guess that the one who wears eyeshadow is straight and the other lesbian adding six more yes no questions about presentation do you ever use makeup do you have long hair do you have short hair do you ever use colored lipstick do you like how you look in glasses and do you work outdoors as additional signals raises the performance to given how many more details about presentation are available in a face image performance no longer seems so impressive several studies including a recent one in the journal of sex research have shown that human judges gaydar is no more reliable than a coin flip when the judgement is based on pictures taken under well controlled conditions head pose lighting glasses makeup etc it s better than chance if these variables are not controlled for because a person s presentation especially if that person is out involves social signaling we signal our orientation and many other kinds of status presumably in order to attract the kind of attention we want and to fit in with people like us wang and kosinski argue against this interpretation on the grounds that their algorithm works on facebook selfies of openly gay men as well as dating website selfies the issue however is not whether the images come from a dating website or facebook but whether they are self posted or taken under standardized conditions most people present themselves in ways that have been calibrated over many years of media consumption observing others looking in the mirror and gauging social reactions in one of the earliest gaydar studies using social media participants could categorize gay men with about accuracy but when the researchers used facebook images of gay and heterosexual men posted by their friends still far from a perfect control the accuracy dropped to if subtle biases in image quality expression and grooming can be picked up on by humans these biases can also be detected by an ai algorithm while wang and kosinski acknowledge grooming and style they believe that the chief differences between their composite images relate to face shape arguing that gay men s faces are more feminine narrower jaws longer noses larger foreheads while lesbian faces are more masculine larger jaws shorter noses smaller foreheads as with less facial hair on gay men and darker skin on straight men they suggest that the mechanism is gender atypical hormonal exposure during development this echoes a widely discredited th century model of homosexuality sexual inversion more likely heterosexual men tend to take selfies from slightly below which will have the apparent effect of enlarging the chin shortening the nose shrinking the forehead and attenuating the smile see our selfies below this view emphasizes dominance or perhaps more benignly an expectation that the viewer will be shorter on the other hand as a wedding photographer notes in her blog when you shoot from above your eyes look bigger which is generally attractive especially for women this may be a heteronormative assessment when a face is photographed from below the nostrils are prominent while higher shooting angles de emphasize and eventually conceal them altogether looking again at the composite images we can see that the heterosexual male face has more pronounced dark spots corresponding to the nostrils than the gay male while the opposite is true for the female faces this is consistent with a pattern of heterosexual men on average shooting from below heterosexual women from above as the wedding photographer suggests and gay men and lesbian women from directly in front a similar pattern is evident in the eyebrows shooting from above makes them look more v shaped but their apparent shape becomes flatter and eventually caret shaped as the camera is lowered shooting from below also makes the outer corners of the eyes appear lower in short the changes in the average positions of facial landmarks are consistent with what we would expect to see from differing selfie angles the ambiguity between shooting angle and the real physical sizes of facial features is hard to fully disentangle from a two dimensional image both for a human viewer and for an algorithm although the authors are using face recognition technology designed to try to cancel out all effects of head pose lighting grooming and other variables not intrinsic to the face we can confirm that this doesn t work perfectly that s why multiple distinct images of a person help when grouping photos by subject in google photos and why a person may initially appear in more than one group tom white a researcher at victoria university in new zealand has experimented with the same facial recognition engine kosinski and wang use vgg face and has found that its output varies systematically based on variables like smiling and head pose when he trains a classifier based on vgg face s output to distinguish a happy expression from a neutral one it gets the answer right of the time which is significant given that the heterosexual female composite has a much more pronounced smile changes in head pose might be even more reliably detectable for test images a classifier is able to pick out the ones facing to the right with accuracy in summary we have shown how the obvious differences between lesbian or gay and straight faces in selfies relate to grooming presentation and lifestyle that is differences in culture not in facial structure these differences include we ve demonstrated that just a handful of yes no questions about these variables can do nearly as good a job at guessing orientation as supposedly sophisticated facial recognition ai further the current generation of facial recognition remains sensitive to head pose and facial expression therefore at least at this point it s hard to credit the notion that this ai is in some way superhuman at outing us based on subtle but unalterable details of our facial structure this doesn t negate the privacy concerns the authors and various commentators have raised but it emphasizes that such concerns relate less to ai per se than to mass surveillance which is troubling regardless of the technologies used even when as in the days of the stasi in east germany these were nothing but paper files and audiotapes like computers or the internal combustion engine ai is a general purpose technology that can be used to automate a great many tasks including ones that should not be undertaken in the first place we are hopeful about the confluence of new powerful ai technologies with social science but not because we believe in reviving the th century research program of inferring people s inner character from their outer appearance rather we believe ai is an essential tool for understanding patterns in human culture and behavior it can expose stereotypes inherent in everyday language it can reveal uncomfortable truths as in google s work with the geena davis institute where our face gender classifier established that men are seen and heard nearly twice as often as women in hollywood movies yet female led films outperform others at the box office making social progress and holding ourselves to account is more difficult without such hard evidence even when it only confirms our suspicions two of us margaret mitchell and blaise agu era y arcas are research scientists specializing in machine learning and ai at google agu era y arcas leads a team that includes deep learning applied to face recognition and powers face grouping in google photos alex todorov is a professor in the psychology department at princeton where he directs the social perception lab he is the author of face value the irresistible influence of first impressions this wording is based on several large national surveys which we were able to use to sanity check our numbers about of respondents identified as homosexual gay or lesbian and as heterosexual about of all genders were exclusively same sex attracted of the men were either sexually or romantically same sex attracted and of the women just under of respondents were trans and about identified with both or neither of the pronouns she and he these numbers are broadly consistent with other surveys especially when considered as a function of age the mechanical turk population skews somewhat younger than the overall population of the us and consistent with other studies our data show that younger people are far more likely to identify non heteronormatively these are wider for same sex attracted and lesbian women because they are minority populations resulting in a larger sampling error the same holds for older people in our sample for the remainder of the plots we stick to opposite sex attracted and same sex attracted as the counts are higher and the error bars therefore smaller these categories are also somewhat less culturally freighted since they rely on questions about attraction rather than identity as with eyeshadow and makeup the effects are similar and often even larger when comparing heterosexual identifying with lesbian or gay identifying people although we didn t test this explicitly slightly different rates of laser correction surgery seem a likely cause of the small but growing disparity between opposite sex attracted and same sex attracted women who answer yes to the vision defect questions as they age this finding may prompt the further question why do more opposite sex attracted men work outdoors this is not addressed by any of our survey questions but hopefully the other evidence presented here will discourage an essentialist assumption such as straight men are just more outdoorsy without the evidence of a controlled study that can support the leap from correlation to cause such explanations are a form of logical fallacy sometimes called a just so story an unverifiable narrative explanation for a cultural practice of the lesbian identified women in the sample or were over six feet and or were over out of heterosexual women women who answered yes to are you heterosexual or straight only or were over six feet and or were over they note that these figures rise to for men and for women if images are considered these results are based on the simplest possible machine learning technique a linear classifier the classifier is trained on a randomly chosen of the data with the remaining of the data held out for testing over repetitions of this procedure the error is with the same number of repetitions and holdout basing the decision on height alone gives an error of and basing it on eyeshadow alone yields a longstanding body of work e g goffman s the presentation of self in everyday life and jones and pittman s toward a general theory of strategic self presentation delves more deeply into why we present ourselves the way we do both for instrumental reasons status power attraction and because our presentation informs and is informed by how we conceive of our social selves from a quick cheer to a standing ovation clap to show how much you enjoyed this story blaise aguera y arcas leads google s ai group in seattle he founded seadragon and was one of the creators of photosynth at microsoft
David Foster,12800,11,https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188?source=tag_archive---------1----------------,How to build your own AlphaZero AI using Python and Keras,in this article i ll attempt to cover three things in march deepmind s alphago beat times world champion go player lee sedol in a series watched by over million people a machine had learnt a super human strategy for playing go a feat previously thought impossible or at the very least at least a decade away from being accomplished this in itself was a remarkable achievement however on th october deepmind took a giant leap further the paper mastering the game of go without human knowledge unveiled a new variant of the algorithm alphago zero that had defeated alphago incredibly it had done so by learning solely through self play starting tabula rasa blank state and gradually finding strategies that would beat previous incarnations of itself no longer was a database of human expert games required to build a super human ai a mere days later on th december deepmind released another paper mastering chess and shogi by self play with a general reinforcement learning algorithm showing how alphago zero could be adapted to beat the world champion programs stockfish and elmo at chess and shogi the entire learning process from being shown the games for the first time to becoming the best computer program in the world had taken under hours with this alphazero was born the general algorithm for getting good at something quickly without any prior knowledge of human expert strategy there are two amazing things about this achievement it cannot be overstated how important this is this means that the underlying methodology of alphago zero can be applied to any game with perfect information the game state is fully known to both players at all times because no prior expertise is required beyond the rules of the game this is how it was possible for deepmind to publish the chess and shogi papers only days after the original alphago zero paper quite literally all that needed to change was the input file that describes the mechanics of the game and to tweak the hyper parameters relating to the neural network and monte carlo tree search if alphazero used super complex algorithms that only a handful of people in the world understood it would still be an incredible achievement what makes it extraordinary is that a lot of the ideas in the paper are actually far less complex than previous versions at its heart lies the following beautifully simple mantra for learning doesn t that sound a lot like how you learn to play games when you play a bad move it s either because you misjudged the future value of resulting positions or you misjudged the likelihood that your opponent would play a certain move so didn t think to explore that possibility these are exactly the two aspects of gameplay that alphazero is trained to learn firstly check out the alphago zero cheat sheet for a high level understanding of how alphago zero works it s worth having that to refer to as we walk through each part of the code there s also a great article here that explains how alphazero works in more detail clone this git repository which contains the code i ll be referencing to start the learning process run the top two panels in the run ipynb jupyter notebook once it s built up enough game positions to fill its memory the neural network will begin training through additional self play and training it will gradually get better at predicting the game value and next moves from any position resulting in better decision making and smarter overall play we ll now have a look at the code in more detail and show some results that demonstrate the ai getting stronger over time n b this is my own understanding of how alphazero works based on the information available in the papers referenced above if any of the below is incorrect apologies and i ll endeavour to correct it the game that our algorithm will learn to play is connect or four in a row not quite as complex as go but there are still game positions in total the game rules are straightforward players take it in turns to enter a piece of their colour in the top of any available column the first player to get four of their colour in a row each vertically horizontally or diagonally wins if the entire grid is filled without a four in a row being created the game is drawn here s a summary of the key files that make up the codebase this file contains the game rules for connect each squares is allocated a number from to as follows the game py file gives the logic behind moving from one game state to another given a chosen action for example given the empty board and action the takeaction method return a new game state with the starting player s piece at the bottom of the centre column you can replace the game py file with any game file that conforms to the same api and the algorithm will in principal learn strategy through self play based on the rules you have given it this contains the code that starts the learning process it loads the game rules and then iterates through the main loop of the algorithm which consist of three stages there are two agents involved in this loop the best player and the current player the best player contains the best performing neural network and is used to generate the self play memories the current player then retrains its neural network on these memories and is then pitched against the best player if it wins the neural network inside the best player is switched for the neural network inside the current player and the loop starts again this contains the agent class a player in the game each player is initialised with its own neural network and monte carlo search tree the simulate method runs the monte carlo tree search process specifically the agent moves to a leaf node of the tree evaluates the node with its neural network and then backfills the value of the node up through the tree the act method repeats the simulation multiple times to understand which move from the current position is most favourable it then returns the chosen action to the game to enact the move the replay method retrains the neural network using memories from previous games this file contains the residual cnn class which defines how to build an instance of the neural network it uses a condensed version of the neural network architecture in the alphagozero paper i e a convolutional layer followed by many residual layers then splitting into a value and policy head the depth and number of convolutional filters can be specified in the config file the keras library is used to build the network with a backend of tensorflow to view individual convolutional filters and densely connected layers in the neural network run the following inside the the run ipynb notebook this contains the node edge and mcts classes that constitute a monte carlo search tree the mcts class contains the movetoleaf and backfill methods previously mentioned and instances of the edge class store the statistics about each potential move this is where you set the key parameters that influence the algorithm adjusting these variables will affect that running time neural network accuracy and overall success of the algorithm the above parameters produce a high quality connect player but take a long time to do so to speed the algorithm up try the following parameters instead contains the playmatches and playmatchesbetweenversions functions that play matches between two agents to play against your creation run the following code it s also in the run ipynb notebook when you run the algorithm all model and memory files are saved in the run folder in the root directory to restart the algorithm from this checkpoint later transfer the run folder to the run archive folder attaching a run number to the folder name then enter the run number model version number and memory version number into the initialise py file corresponding to the location of the relevant files in the run archive folder running the algorithm as usual will then start from this checkpoint an instance of the memory class stores the memories of previous games that the algorithm uses to retrain the neural network of the current player this file contains a custom loss function that masks predictions from illegal moves before passing to the cross entropy loss function the locations of the run and run archive folders log files are saved to the log folder inside the run folder to turn on logging set the values of the logger disabled variables to false inside this file viewing the log files will help you to understand how the algorithm works and see inside its mind for example here is a sample from the logger mcts file equally from the logger tourney file you can see the probabilities attached to each move during the evaluation phase training over a couple of days produces the following chart of loss against mini batch iteration number the top line is the error in the policy head the cross entropy of the mcts move probabilities against the output from the neural network the bottom line is the error in the value head the mean squared error between the actual game value and the neural network predict of the value the middle line is an average of the two clearly the neural network is getting better at predicting the value of each game state and the likely next moves to show how this results in stronger and stronger play i ran a league between players ranging from the st iteration of the neural network up to the th each pairing played twice with both players having a chance to play first here are the final standings clearly the later versions of the neural network are superior to the earlier versions winning most of their games it also appears that the learning hasn t yet saturated with further training time the players would continue to get stronger learning more and more intricate strategies as an example one clear strategy that the neural network has favoured over time is grabbing the centre column early observe the difference between the first version of the algorithm and say the th version st neural network version th neural network version this is a good strategy as many lines require the centre column claiming this early ensures your opponent cannot take advantage of this this has been learnt by the neural network without any human input there is a game py file for a game called metasquares in the games folder this involves placing x and o markers in a grid to try to form squares of different sizes larger squares score more points than smaller squares and the player with the most points when the grid is full wins if you switch the connect game py file for the metasquares game py file the same algorithm will learn how to play metasquares instead hopefully you find this article useful let me know in the comments below if you find any typos or have questions about anything in the codebase or article and i ll get back to you as soon as possible if you would like to learn more about how our company applied data science develops innovative data science solutions for businesses feel free to get in touch through our website or directly through linkedin and if you like this feel free to leave a few hearty claps applied data science is a london based consultancy that implements end to end data science solutions for businesses delivering measurable value if you re looking to do more with your data let s talk from a quick cheer to a standing ovation clap to show how much you enjoyed this story co founder of applied data science cutting edge data science news and projects
Aman Agarwal,7000,24,https://medium.freecodecamp.org/explained-simply-how-an-ai-program-mastered-the-ancient-game-of-go-62b8940a9080?source=tag_archive---------2----------------,Explained Simply: How an AI program mastered the ancient game of Go,this is about alphago google deepmind s go playing ai that shook the technology world in by defeating one of the best players in the world lee sedol go is an ancient board game which has so many possible moves at each step that future positions are hard to predict and therefore it requires strong intuition and abstract thinking to play because of this reason it was believed that only humans could be good at playing go most researchers thought that it would still take decades to build an ai which could think like that in fact i m releasing this essay today because this week march marks the two year anniversary of the alphago vs sedol match but alphago didn t stop there months later it played professional games on a go website under disguise as a player named master and won every single game against dozens of world champions of course without resting between games naturally this was a huge achievement in the field of ai and sparked worldwide discussions about whether we should be excited or worried about artificial intelligence today we are going to take the original research paper published by deepmind in the nature journal and break it down paragraph by paragraph using simple english after this essay you ll know very clearly what alphago is and how it works i also hope that after reading this you will not believe all the news headlines made by journalists to scare you about ai and instead feel excited about it worrying about the growing achievements of ai is like worrying about the growing abilities of microsoft powerpoint yes it will get better with time with new features being added to it but it can t just uncontrollably grow into some kind of hollywood monster you don t need to know how to play go to understand this paper in fact i myself have only read the first lines in wikipedia s opening paragraph about it instead surprisingly i use some examples from basic chess to explain the algorithms you just have to know what a player board game is in which each player takes turns and there is one winner at the end beyond that you don t need to know any physics or advanced math or anything this will make it more approachable for people who only just now started learning about machine learning or neural networks and especially for those who don t use english as their first language which can make it very difficult to read such papers if you have no prior knowledge of ai and neural networks you can read the deep learning section of one of my previous essays here after reading that you ll be able to get through this essay if you want to get a shallow understanding of reinforcement learning too optional reading you can find it here here s the original paper if you want to try reading it as for me hi i m aman an ai and autonomous robots engineer i hope that my work will save you a lot of time and effort if you were to study this on your own do you speak japanese ryohji ikebe has kindly written a brief memo about this essay in japanese in a series of tweets as you know the goal of this research was to train an ai program to play go at the level of world class professional human players to understand this challenge let me first talk about something similar done for chess in the early s ibm came out with the deep blue computer which defeated the great champion gary kasparov in chess he s also a very cool guy make sure to read more about him later how did deep blue play well it used a very brute force method at each step of the game it took a look at all the possible legal moves that could be played and went ahead to explore each and every move to see what would happen and it would keep exploring move after move for a while forming a kind of huge decision tree of thousands of moves and then it would come back along that tree observing which moves seemed most likely to bring a good result but what do we mean by good result well deep blue had many carefully designed chess strategies built into it by expert chess players to help it make better decisions for example how to decide whether to protect the king or get advantage somewhere else they made a specific evaluation algorithm for this purpose to compare how advantageous or disadvantageous different board positions are ibm hard coded expert chess strategies into this evaluation function and finally it chooses a carefully calculated move on the next turn it basically goes through the whole thing again as you can see this means deep blue thought about millions of theoretical positions before playing each move this was not so impressive in terms of the ai software of deep blue but rather in the hardware ibm claimed it to be one of the most powerful computers available in the market at that time it could look at million board positions per second now we come to go just believe me that this game is much more open ended and if you tried the deep blue strategy on go you wouldn t be able to play well there would be so many positions to look at at each step that it would simply be impractical for a computer to go through that hell for example at the opening move in chess there are possible moves in go the first player has possible moves and this scope of choices stays wide throughout the game this is what they mean by enormous search space moreover in go it s not so easy to judge how advantageous or disadvantageous a particular board position is at any specific point in the game you kinda have to play the whole game for a while before you can determine who is winning but let s say you magically had a way to do both of these and that s where deep learning comes in so in this research deepmind used neural networks to do both of these tasks if you haven t read about them yet here s the link again they trained a policy neural network to decide which are the most sensible moves in a particular board position so it s like following an intuitive strategy to pick moves from any position and they trained a value neural network to estimate how advantageous a particular board arrangement is for the player or in other words how likely you are to win the game from this position they trained these neural networks first with human game examples your good old ordinary supervised learning after this the ai was able to mimic human playing to a certain degree so it acted like a weak human player and then to train the networks even further they made the ai play against itself millions of times this is the reinforcement learning part with this the ai got better because it had more practice with these two networks alone deepmind s ai was able to play well against state of the art go playing programs that other researchers had built before these other programs had used an already popular pre existing game playing algorithm called the monte carlo tree search mcts more about this later but guess what we still haven t talked about the real deal deepmind s ai isn t just about the policy and value networks it doesn t use these two networks as a replacement of the monte carlo tree search instead it uses the neural networks to make the mcts algorithm work better and it got so much better that it reached superhuman levels this improved variation of mcts is alphago the ai that beat lee sedol and went down in ai history as one of the greatest breakthroughs ever so essentially alphago is simply an improved implementation of a very ordinary computer science algorithm do you understand now why ai in its current form is absolutely nothing to be scared of wow we ve spent a lot of time on the abstract alone alright to understand the paper from this point on first we ll talk about a gaming strategy called the monte carlo tree search algorithm for now i ll just explain this algorithm at enough depth to make sense of this essay but if you want to learn about it in depth some smart people have also made excellent videos and blog posts on this a short video series from udacity jeff bradberry s explanation of mcts an mcts tutorial by fullstack academy the following section is long but easy to understand i ll try my best and very important so stay with me the rest of the essay will go much quicker let s talk about the first paragraph of the essay above remember what i said about deep blue making a huge tree of millions of board positions and moves at each step of the game you had to do simulations and look at and compare each and every possible move as i said before that was a simple approach and very straightforward approach if the average software engineer had to design a game playing ai and had all the strongest computers of the world he or she would probably design a similar solution but let s think about how do humans themselves play chess let s say you re at a particular board position in the middle of the game by game rules you can do a dozen different things move this pawn here move the queen two squares here or three squares there and so on but do you really make a list of all the possible moves you can make with all your pieces and then select one move from this long list no you intuitively narrow down to a few key moves let s say you come up with sensible moves that you think make sense and then you wonder what will happen in the game if you chose one of these moves you might spend seconds considering each of these moves and their future and note that during these seconds you don t have to carefully plan out the future of each move you can just roll out a few mental moves guided by your intuition without too much careful thought well a good player would think farther and more deeply than an average player this is because you have limited time and you can t accurately predict what your opponent will do at each step in that lovely future you re cooking up in your brain so you ll just have to let your gut feeling guide you i ll refer to this part of the thinking process as rollout so take note of it so after rolling out your few sensible moves you finally say screw it and just play the move you find best then the opponent makes a move it might be a move you had already well anticipated which means you are now pretty confident about what you need to do next you don t have to spend too much time on the rollouts again or it could be that your opponent hits you with a pretty cool move that you had not expected so you have to be even more careful with your next move this is how the game carries on and as it gets closer and closer to the finishing point it would get easier for you to predict the outcome of your moves so your rollouts don t take as much time the purpose of this long story is to describe what the mcts algorithm does on a superficial level it mimics the above thinking process by building a search tree of moves and positions every time again for more details you should check out the links i mentioned earlier the innovation here is that instead of going through all the possible moves at each position which deep blue did it instead intelligently selects a small set of sensible moves and explores those instead to explore them it rolls out the future of each of these moves and compares them based on their imagined outcomes seriously this is all i think you need to understand this essay now coming back to the screenshot from the paper go is a perfect information game please read the definition in the link don t worry it s not scary and theoretically for such games no matter which particular position you are at in the game even if you have just played moves it is possible that you can correctly guess who will win or lose assuming that both players play perfectly from that point on i have no idea who came up with this theory but it is a fundamental assumption in this research project and it works so that means given a state of the game s there is a function v s which can predict the outcome let s say probability of you winning this game from to they call it the optimal value function because some board positions are more likely to result in you winning than other board positions they can be considered more valuable than the others let me say it again value probability between and of you winning the game but wait say there was a girl named foma sitting next to you while you play chess and she keeps telling you at each step if you re winning or losing you re winning you re losing nope still losing i think it wouldn t help you much in choosing which move you need to make she would also be quite annoying what would instead help you is if you drew the whole tree of all the possible moves you can make and the states that those moves would lead to and then foma would tell you for the entire tree which states are winning states and which states are losing states then you can choose moves which will keep leading you to winning states all of a sudden foma is your partner in crime not an annoying friend here foma behaves as your optimal value function v s earlier it was believed that it s not possible to have an accurate value function like foma for the game of go because the games had so much uncertainty but even if you had the wonderful foma this wonderland strategy of drawing out all the possible positions for foma to evaluate will not work very well in the real world in a game like chess or go as we said before if you try to imagine even moves into the future there can be so many possible positions that you don t have enough time to check all of them with foma so foma is not enough you need to narrow down the list of moves to a few sensible moves that you can roll out into the future how will your program do that enter lusha lusha is a skilled chess player and enthusiast who has spent decades watching grand masters play chess against each other she can look at your board position look quickly at all the available moves you can make and tell you how likely it would be that a chess expert would make any of those moves if they were sitting at your table so if you have possible moves at a point lusha will tell you the probability that each move would be picked by an expert of course a few sensible moves will have a much higher probability and other pointless moves will have very little probability she is your policy function p a s for a given state s she can give you probabilities for all the possible moves that an expert would make wow you can take lusha s help to guide you in how to select a few sensible moves and foma will tell you the likelihood of winning from each of those moves you can choose the move that both foma and lusha approve or if you want to be extra careful you can roll out the moves selected by lusha have foma evaluate them pick a few of them to roll out further into the future and keep letting foma and lusha help you predict very far into the game s future much quicker and more efficient than to go through all the moves at each step into the future this is what they mean by reducing the search space use a value function foma to predict outcomes and use a policy function lusha to give you grand master probabilities to help narrow down the moves you roll out these are called monte carlo rollouts then while you backtrack from future to present you can take average values of all the different moves you rolled out and pick the most suitable action so far this has only worked on a weak amateur level in go because the policy functions and value functions that they used to guide these rollouts weren t that great phew the first line is self explanatory in mcts you can start with an unskilled foma and unskilled lusha the more you play the better they get at predicting solid outcomes and moves narrowing the search to a beam of high probability actions is just a sophisticated way of saying lusha helps you narrow down the moves you need to roll out by assigning them probabilities that an expert would play them prior work has used this technique to achieve strong amateur level ai players even with simple or shallow as they call it policy functions yeah convolutional neural networks are great for image processing and since a neural network takes a particular input and gives an output it is essentially a function right so you can use a neural network to become a complex function so you can just pass in an image of the board position and let the neural network figure out by itself what s going on this means it s possible to create neural networks which will behave like very accurate policy and value functions the rest is pretty self explanatory here we discuss how foma and lusha were trained to train the policy network predicting for a given position which moves experts would pick you simply use examples of human games and use them as data for good old supervised learning and you want to train another slightly different version of this policy network to use for rollouts this one will be smaller and faster let s just say that since lusha is so experienced she takes some time to process each position she s good to start the narrowing down process with but if you try to make her repeat the process she ll still take a little too much time so you train a faster policy network for the rollout process i ll call it lusha s younger brother jerry i know i know enough with these names after that once you ve trained both of the slow and fast policy networks enough using human player data you can try letting lusha play against herself on a go board for a few days and get more practice this is the reinforcement learning part making a better version of the policy network then you train foma for value prediction determining the probability of you winning you let the ai practice through playing itself again and again in a simulated environment observe the end result each time and learn from its mistakes to get better and better i won t go into details of how these networks are trained you can read more technical details in the later section of the paper methods which i haven t covered here in fact the real purpose of this particular paper is not to show how they used reinforcement learning on these neural networks one of deepmind s previous papers in which they taught ai to play atari games has already discussed some reinforcement learning techniques in depth and i ve already written an explanation of that paper here for this paper as i lightly mentioned in the abstract and also underlined in the screenshot above the biggest innovation was the fact that they used rl with neural networks for improving an already popular game playing algorithm mcts rl is a cool tool in a toolbox that they used to fine tune the policy and value function neural networks after the regular supervised training this research paper is about proving how versatile and excellent this tool it is not about teaching you how to use it in television lingo the atari paper was a rl infomercial and this alphago paper is a commercial a quick note before you move on would you like to help me write more such essays explaining cool research papers if you re serious i d be glad to work with you please leave a comment and i ll get in touch with you so the first step is in training our policy nn lusha to predict which moves are likely to be played by an expert this nn s goal is to allow the ai to play similar to an expert human this is a convolutional neural network as i mentioned before it s a special kind of nn that is very useful in image processing that takes in a simplified image of a board arrangement rectifier nonlinearities are layers that can be added to the network s architecture they give it the ability to learn more complex things if you ve ever trained nns before you might have used the relu layer that s what these are the training data here was in the form of random pairs of board positions and the labels were the actions chosen by humans when they were in those positions just regular supervised learning here they use stochastic gradient ascent well this is an algorithm for backpropagation here you re trying to maximise a reward function and the reward function is just the probability of the action predicted by a human expert you want to increase this probability but hey you don t really need to think too much about this normally you train the network so that it minimises a loss function which is essentially the error difference between predicted outcome and actual label that is called gradient descent in the actual implementation of this research paper they have indeed used the regular gradient descent you can easily find a loss function that behaves opposite to the reward function such that minimising this loss will maximise the reward the policy network has layers and is called sl policy network sl supervised learning the data came from a i ll just say it s a popular website on which millions of people play go how good did this sl policy network perform it was more accurate than what other researchers had done earlier the rest of the paragraph is quite self explanatory as for the rollout policy you do remember from a few paragraphs ago how lusha the sl policy network is slow so it can t integrate well with the mcts algorithm and we trained another faster version of lusha called jerry who was her younger brother well this refers to jerry right here as you can see jerry is just half as accurate as lusha but it s thousands of times faster it will really help get through rolled out simulations of the future faster when we apply the mcts for this next section you don t have to know about reinforcement learning already but then you ll have to assume that whatever i say works if you really want to dig into details and make sure of everything you might want to read a little about rl first once you have the sl network trained in a supervised manner using human player moves with the human moves data as i said before you have to let her practice by itself and get better that s what we re doing here so you just take the sl policy network save it in a file and make another copy of it then you use reinforcement learning to fine tune it here you make the network play against itself and learn from the outcomes but there s a problem in this training style if you only forever practice against one opponent and that opponent is also only practicing with you exclusively there s not much of new learning you can do you ll just be training to practice how to beat that one player this is you guessed it overfitting your techniques play well against one opponent but don t generalize well to other opponents so how do you fix this well every time you fine tune a neural network it becomes a slightly different kind of player so you can save this version of the neural network in a list of players who all behave slightly differently right great now while training the neural network you can randomly make it play against many different older and newer versions of the opponent chosen from that list they are versions of the same player but they all play slightly differently and the more you train the more players you get to train even more with bingo in this training the only thing guiding the training process is the ultimate goal i e winning or losing you don t need to specially train the network to do things like capture more area on the board etc you just give it all the possible legal moves it can choose from and say you have to win and this is why rl is so versatile it can be used to train policy or value networks for any game not just go here they tested how accurate this rl policy network was just by itself without any mcts algorithm as you would remember this network can directly take a board position and decide how an expert would play it so you can use it to single handedly play games well the result was that the rl fine tuned network won against the sl network that was only trained on human moves it also won against other strong go playing programs must note here that even before training this rl policy network the sl policy network was already better than the state of the art and now it has further improved and we haven t even come to the other parts of the process like the value network did you know that baby penguins can sneeze louder than a dog can bark actually that s not true but i thought you d like a little joke here to distract from the scary looking equations above coming to the essay again we re done training lusha here now back to foma remember the optimal value function v s that only tells you how likely you are to win in your current board position if both players play perfectly from that point on so obviously to train an nn to become our value function we would need a perfect player which we don t have so we just use our strongest player which happens to be our rl policy network it takes the current state board state s and outputs the probability that you will win the game you play a game and get to know the outcome win or loss each of the game states act as a data sample and the outcome of that game acts as the label so by playing a move game you have data samples for value prediction lol no this approach is naive you can t use all moves from the game and add them to the dataset the training data set had to be chosen carefully to avoid overfitting each move in the game is very similar to the next one because you only move once and that gives you a new position right if you take the states at all of those moves and add them to the training data with the same label you basically have lots of kinda duplicate data and that causes overfitting to prevent this you choose only very distinct looking game states so for example instead of all moves of a game you only choose of them and add them to the training set deepmind took million positions from million different games to reduce any chances of there being duplicate data and it worked now something conceptual here there are two ways to evaluate the value of a board position one option is a magical optimal value function like the one you trained above the other option is to simply roll out into the future using your current policy lusha and look at the final outcome in this roll out obviously the real game would rarely go by your plans but deepmind compared how both of these options do you can also do a mixture of both these options we will learn about this mixing parameter a little bit later so make a mental note of this concept well your single neural network trying to approximate the optimal value function is even better than doing thousands of mental simulations using a rollout policy foma really kicked ass here when they replaced the fast rollout policy with the twice as accurate but slow rl policy lusha and did thousands of simulations with that it did better than foma but only slightly better and too slowly so foma is the winner of this competition she has proved that she can t be replaced now that we have trained the policy and value functions we can combine them with mcts and give birth to our former world champion destroyer of grand masters the breakthrough of a generation weighing two hundred and sixty eight pounds one and only alphaaaaa go in this section ideally you should have a slightly deeper understanding of the inner workings of the mcts algorithm but what you have learned so far should be enough to give you a good feel for what s going on here the only thing you should note is how we re using the policy probabilities and value estimations we combine them during roll outs to narrow down the number of moves we want to roll out at each step q s a represents the value function and u s a is a stored probability for that position i ll explain remember that the policy network uses supervised learning to predict expert moves and it doesn t just give you most likely move but rather gives you probabilities for each possible move that tell how likely it is to be an expert move this probability can be stored for each of those actions here they call it prior probability and they obviously use it while selecting which actions to explore so basically to decide whether or not to explore a particular move you consider two things first by playing this move how likely are you to win yes we already have our value network to answer this first question and the second question is how likely is it that an expert would choose this move if a move is super unlikely to be chosen by an expert why even waste time considering it this we get from the policy network then let s talk about the mixing parameter see came back to it as discussed earlier to evaluate positions you have two options one simply use the value network you have been using to evaluate states all along and two you can try to quickly play a rollout game with your current strategy assuming the other player will play similarly and see if you win or lose we saw how the value function was better than doing rollouts in general here they combine both you try giving each prediction importance or or and so on if you attach a of x to the first you ll have to attach x to the second that s what this mixing parameter means you ll see these hit and trial results later in the paper after each roll out you update your search tree with whatever information you gained during the simulation so that your next simulation is more intelligent and at the end of all simulations you just pick the best move interesting insight here remember how the rl fine tuned policy nn was better than just the sl human trained policy nn but when you put them within the mcts algorithm of alphago using the human trained nn proved to be a better choice than the fine tuned nn but in the case of the value function which you would remember uses a strong player to approximate a perfect player training foma using the rl policy works better than training her with the sl policy doing all this evaluation takes a lot of computing power we really had to bring out the big guns to be able to run these damn programs self explanatory lol our program literally blew the pants off of every other program that came before us this goes back to that mixing parameter again while evaluating positions giving equal importance to both the value function and the rollouts performed better than just using one of them the rest is self explanatory and reveals an interesting insight self explanatory self explanatory but read that red underlined sentence again i hope you can see clearly now that this line right here is pretty much the summary of what this whole research project was all about concluding paragraph let us brag a little more here because we deserve it oh and if you re a scientist or tech company and need some help in explaining your science to non technical people for marketing pr or training etc i can help you drop me a message on twitter mngrwl from a quick cheer to a standing ovation clap to show how much you enjoyed this story engineer teacher learner of foreign languages lover of history cinema and art our community publishes stories worth reading on development design and data science
Eugenio Culurciello,6400,8,https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0?source=tag_archive---------3----------------,The fall of RNN / LSTM – Towards Data Science,we fell for recurrent neural networks rnn long short term memory lstm and all their variants now it is time to drop them it is the year and lstm and rnn make a great come back from the dead we all read colah s blog and karpathy s ode to rnn but we were all young and unexperienced for a few years this was the way to solve sequence learning sequence translation seq seq which also resulted in amazing results in speech to text comprehension and the raise of siri cortana google voice assistant alexa also let us not forget machine translation which resulted in the ability to translate documents into different languages or neural machine translation but also translate images into text text into images and captioning video and well you got the idea then in the following years came resnet and attention one could then better understand that lstm were a clever bypass technique also attention showed that mlp network could be replaced by averaging networks influenced by a context vector more on this later it only took more years but today we can definitely say but do not take our words for it also see evidence that attention based networks are used more and more by google facebook salesforce to name a few all these companies have replaced rnn and variants for attention based models and it is just the beginning rnn have the days counted in all applications because they require more resources to train and run than attention based models see this post for more info remember rnn and lstm and derivatives use mainly sequential processing over time see the horizontal arrow in the diagram below this arrow means that long term information has to sequentially travel through all cells before getting to the present processing cell this means it can be easily corrupted by being multiplied many time by small numbers this is the cause of vanishing gradients to the rescue came the lstm module which today can be seen as multiple switch gates and a bit like resnet it can bypass units and thus remember for longer time steps lstm thus have a way to remove some of the vanishing gradients problems but not all of it as you can see from the figure above still we have a sequential path from older past cells to the current one in fact the path is now even more complicated because it has additive and forget branches attached to it no question lstm and gru and derivatives are able to learn a lot of longer term information see results here but they can remember sequences of s not s or s or more and one issue of rnn is that they are not hardware friendly let me explain it takes a lot of resources we do not have to train these network fast also it takes much resources to run these model in the cloud and given that the demand for speech to text is growing rapidly the cloud is not scalable we will need to process at the edge right into the amazon echo see note below for more details if sequential processing is to be avoided then we can find units that look ahead or better look back since most of the time we deal with real time causal data where we know the past and want to affect future decisions not so in translating sentences or analyzing recorded videos for example where we have all data and can reason on it more time such look back ahead units are neural attention modules which we previously explained here to the rescue and combining multiple neural attention modules comes the hierarchical neural attention encoder shown in the figure below a better way to look into the past is to use attention modules to summarize all past encoded vectors into a context vector ct notice there is a hierarchy of attention modules here very similar to the hierarchy of neural networks this is also similar to temporal convolutional network tcn reported in note below in the hierarchical neural attention encoder multiple layers of attention can look at a small portion of recent past say vectors while layers above can look at of these attention modules effectively integrating the information of x vectors this extends the ability of the hierarchical neural attention encoder to past vectors but more importantly look at the length of the path needed to propagate a representation vector to the output of the network in hierarchical networks it is proportional to log n where n are the number of hierarchy layers this is in contrast to the t steps that a rnn needs to do where t is the maximum length of the sequence to be remembered and t n this architecture is similar to a neural turing machine but lets the neural network decide what is read out from memory via attention this means an actual neural network will decide which vectors from the past are important for future decisions but what about storing to memory the architecture above stores all previous representation in memory unlike neural turning machines this can be rather inefficient think about storing the representation of every frame in a video most times the representation vector does not change frame to frame so we really are storing too much of the same what can we do is add another unit to prevent correlated data to be stored for example by not storing vectors too similar to previously stored ones but this is really a hack the best would be to be let the application guide what vectors should be saved or not this is the focus of current research studies stay tuned for more information tell your friends it is very surprising to us to see so many companies still use rnn lstm for speech to text many unaware that these networks are so inefficient and not scalable please tell them about this post about training rnn lstm rnn and lstm are difficult to train because they require memory bandwidth bound computation which is the worst nightmare for hardware designer and ultimately limits the applicability of neural networks solutions in short lstm require linear layer mlp layer per cell to run at and for each sequence time step linear layers require large amounts of memory bandwidth to be computed in fact they cannot use many compute unit often because the system has not enough memory bandwidth to feed the computational units and it is easy to add more computational units but hard to add more memory bandwidth note enough lines on a chip long wires from processors to memory etc as a result rnn lstm and variants are not a good match for hardware acceleration and we talked about this issue before here and here a solution will be compute in memory devices like the ones we work on at fwdnxt see this repository for a simple example of these techniques note hierarchical neural attention is similar to the ideas in wavenet but instead of a convolutional neural network we use hierarchical attention modules also hierarchical neural attention can be also bi directional note rnn and lstm are memory bandwidth limited problems see this for details the processing unit s need as much memory bandwidth as the number of operations s they can provide making it impossible to fully utilize them the external bandwidth is never going to be enough and a way to slightly ameliorate the problem is to use internal fast caches with high bandwidth the best way is to use techniques that do not require large amount of parameters to be moved back and forth from memory or that can be re used for multiple computation per byte transferred high arithmetic intensity note here is a paper comparing cnn to rnn temporal convolutional network tcn outperform canonical recurrent networks such as lstms across a diverse range of tasks and datasets while demonstrating longer effective memory note related to this topic is the fact that we know little of how our human brain learns and remembers sequences we often learn and recall long sequences in smaller segments such as a phone number memorized as four segments behavioral experiments suggest that humans and some animals employ this strategy of breaking down cognitive or behavioral sequences into chunks in a wide variety of tasks these chunks remind me of small convolutional or attention like networks on smaller sequences that then are hierarchically strung together like in the hierarchical neural attention encoder and temporal convolutional network tcn more studies make me think that working memory is similar to rnn networks that uses recurrent real neuron networks and their capacity is very low on the other hand both the cortex and hippocampus give us the ability to remember really long sequences of steps like where did i park my car at airport days ago suggesting that more parallel pathways may be involved to recall long sequences where attention mechanism gate important chunks and force hops in parts of the sequence that is not relevant to the final goal or task note the above evidence shows we do not read sequentially in fact we interpret characters words and sentences as a group an attention based or convolutional module perceives the sequence and projects a representation in our mind we would not be misreading this if we processed this information sequentially we would stop and notice the inconsistencies i have almost years of experience in neural networks in both hardware and software a rare combination see about me here medium webpage scholar linkedin and more if you found this article useful please consider a donation to support more tutorials and blogs any contribution can make a difference from a quick cheer to a standing ovation clap to show how much you enjoyed this story i dream and build new technology sharing concepts ideas and codes
Gary Marcus,1300,27,https://medium.com/@GaryMarcus/in-defense-of-skepticism-about-deep-learning-6e8bfd5ae0f1?source=tag_archive---------4----------------,In defense of skepticism about deep learning – Gary Marcus – Medium,in a recent appraisal of deep learning marcus i outlined ten challenges for deep learning and suggested that deep learning by itself although useful was unlikely to lead on its own to artificial general intelligence i suggested instead the deep learning be viewed not as a universal solvent but simply as one tool among many in place of pure deep learning i called for hybrid models that would incorporate not just supervised forms of deep learning but also other techniques as well such as symbol manipulation and unsupervised learning itself possibly reconceptualized i also urged the community to consider incorporating more innate structure into ai systems within a few days thousands of people had weighed in over twitter some enthusiastic e g the best discussion of deeplearning and ai i ve read in many years some not thoughtful but mostly wrong nevertheless because i think clarity around these issues is so important i ve compiled a list of fourteen commonly asked queries where does unsupervised learning fit in why didn t i say more nice things about deep learning what gives me the right to talk about this stuff in the first place what s up with asking a neural network to generalize from even numbers to odd numbers hint that s the most important one and lots more i haven t addressed literally every question i have seen but i have tried to be representative what is general intelligence thomas dietterich an eminent professor of machine learning and my most thorough and explicit critic thus far gave a nice answer that i am very comfortable with marcus wasn t very nice to deep learning he should have said more nice things about all of its vast accomplishments and he minimizes others dietterich mentioned above made both of these points writing on the first part of that true i could have said more positive things but it s not like i didn t say any or even like i forgot to mention dietterich s best example i mentioned it on the first page more generally later in the article i cited a couple of great texts and excellent blogs that have pointers to numerous examples a lot of them though would not really count as agi which was the main focus of my paper google translate for example is extremely impressive but it s not general it can t for example answer questions about what it has translated the way a human translator could the second part is more substantive is categories really very finite well yes compared to the flexibility of cognition cognitive scientists generally place the number of atomic concepts known by an individual as being on the order of and we can easily compose those into a vastly greater number of complex thoughts pets and fish are probably counted in those pet fish which is something different probably isn t counted and i can easily entertain the concept of a pet fish that is suffering from ick or note that it is always disappointing to buy a pet fish only to discover that it was infected with ick an experience that i had as a child and evidently still resent how many ideas like that i can express it s a lot more than i am not precisely sure how many visual categories a person can recognize but suspect the math is roughly similar try google images on pet fish and you do ok try it on pet fish wearing goggles and you mostly find dogs wearing goggles with a false alarm rate of over machines win over nonexpert humans on distinguishing similar dog breeds but people win by a wide margin on interpreting complex scenes like what would happen to a skydiver who was wearing a backpack rather than a parachute in focusing on category chunks the machine learning field is in my view doing itself a disservice trading a short term feeling of success for a denial of harder more open ended problems like scene and sentence comprehension that must eventually be addressed compared to the essentially infinite range of sentences and scenes we can see and comprehend of anything really is small see also note at bottom marcus says deep learning is useless but it s great for many things of course it is useful i never said otherwise only that a in its current supervised form deep learning might be approaching its limits and b that those limits would stop short from full artificial general intelligence unless maybe we started incorporating a bunch of other stuff like symbol manipulation and innateness the core of my conclusion was this one thing that i don t understand garymarcus says that dl is not good for hierarchical structures but in ylecun nature review paper says that that dl is particularly suited for exploiting such hierarchies this is an astute question from ram shankar and i should have been a lot clearer about the answer there are many different types of hierarchy one could think about deep learning is really good probably the best ever at the sort of feature wise hierarchy lecun talked about which i typically refer to as hierarchical feature detection you build lines out of pixels letters out of lines words out of letters and so forth kurzweil and hawkins have emphasized this sort of thing too and it really goes back to hubel and wiesel in neuroscience experiments and to fukushima fukushima miyake ito in ai fukushima in his neocognitron model hand wired his hierarchy of successively more abstract features lecun and many others after showed that at least in some cases you don t have to hand engineer them but you don t have to keep track of the subcomponents you encounter along the way the top level system need not explicitly encode the structure of the overall output in terms of which parts were seen along the way this is part of why a deep learning system can be fooled into thinking a pattern of a black and yellow stripes is a school bus nguyen yosinski clune that stripe pattern is strongly correlated with activation of the school bus output units which is in turn correlated with a bunch of lower level features but in a typical image recognition deep network there is no fully realized representation of a school bus as being made up of wheels a chassis windows etc virtually the whole spoofing literature can be thought of in these terms note the structural sense of hierarchy which i was discussing was different and focused around systems that can make explicit reference to the parts of larger wholes the classic illustration would be chomsky s sense of hierarchy in which a sentence is composed of increasingly complex grammatical units e g using a novel phrase like the man who mistook his hamburger for a hot dog with a larger sentence like the actress insisted that she would not be outdone by the man who mistook his hamburger for a hot dog i don t think deep learning does well here e g in discerning the relation between the actress the man and the misidentified hot dog though attempts have certainly been made even in vision the problem is not entirely licked hinton s recent capsule work sabour frosst hinton for example is an attempt to build in more robust part whole directions for image recognition by using more structured networks i see this as a good trend and one potential way to begin to address the spoofing problem but also as a reflection of trouble with the standard deep learning approach it s weird to discuss deep learning in the context of general ai general ai is not the goal of deep learning best twitter response to this came from university of quebec professor daniel lemire oh come on hinton bengio are openly going for a model of human intelligence second prize goes to a math phd at google jeremy kun who countered the dubious claim that general ai is not the goal of deep learning with if that s true then deep learning experts sure let everyone believe it is without correcting them andrew ng s recent harvard business review article which i cited implies that deep learning can do anything a person can do in a second thomas dietterich s tweet that said in part it is hard to argue that there are limits to dl jeremy howard worried that the idea that deep learning is overhyped might itself be overhyped and then suggested that every known limit had been countered deepmind s recent alphago paper see note is positioned somewhat similarly with silver et al silver et al enthusiastically reporting that in that paper s concluding discussion not one of the challenges to deep learning that i reviewed was mentioned as i will discuss in a paper coming out soon it s not actually a pure deep learning system but that s a story for another day the main reason people keep benchmarking their ai systems against humans is precisely because agi is the goal what marcus said is a problem with supervised learning not deep learning yann lecun presented a version of this in a comment on my facebook page the part about my allegedly not recognizing lecun s recent work is well odd it s true that i couldn t find a good summary article to cite when i asked lecun he told me by email that there wasn t one yet but i did mention his interest explicitly i also noted that my conclusion was positive too although i expressed reservations about current approaches to building unsupervised systems i ended optimistically what lecun s remark does get right is that many of the problems i addressed are a general problem with supervised learning not something unique to deep learning i could have been more clear about this many other supervised learning techniques face similar challenges such as problems in generalization and dependence on massive data sets relatively little of what i said is unique to deep learning in my focus on assessing deep learning at the five year resurgence mark i neglected to say that but it doesn t really help deep learning that other supervised learning techniques are in the same boat if someone could come up with a truly impressive way of using deep learning in an unsupervised way a reassessment might be required but i don t see that unsupervised learning at least as it currently pursued particularly remedies the challenges i raised e g with respect to reasoning hierarchical representations transfer robustness and interpretability it s simply a promissory note note as portland state and santa fe institute professor melanie mitchell s put it in a thus far unanswered tweet i would too in the meantime i see no principled reason to believe that unsupervised learning can solve the problems i raise unless we add in more abstract symbolic representations first deep learning is not just convolutional networks of the sort marcus critiqued it s essentially a new style of programming differentiable programming and the field is trying to work out the reusable constructs in this style we have some convolution pooling lstm gan vae memory units routing units etc tom dietterich this seemed in the context of dietterich s longer series of tweets to have been proposed as a criticism but i am puzzled by that as i am a fan of differentiable programming and said so perhaps the point was that deep learning can be taken in a broader way in any event i would not equate deep learning and differentiable programming e g approaches that i cited like neural turing machines and neural programming deep learning is a component of many differentiable systems but such systems also build in exactly the sort of elements drawn from symbol manipulation that i am and have been urging the field to integrate marcus marcus marblestone dean a marcus marblestone dean b including memory units and operations over variables and other systems like routing units stressed in the more recent two essays if integrating all this stuff into deep learning is what gets us to agi my conclusion quoted below will have turned out to be dead on now vs the future maybe deep learning doesn t work now but it s offspring will get us to agi possibly i do think that deep learning might play an important role in getting us to agi if some key things many not yet discovered are added in first but what we add matters and whether it is reasonable to call some future system an instance of deep learning per se or more sensible to call the ultimate system a such and such that uses deep learning depends on where deep learning fits into the ultimate solution maybe for example in truly adequate natural language understanding systems symbol manipulation will play an equally large role as deep learning or an even larger one part of the issue here is of course terminological a very good friend recently asked me why can t we just call anything that includes deep learning deep learning even if it includes symbol manipulation some enhancement to deep learning ought to work to which i respond why not call anything that includes symbol manipulation symbol manipulation even if it includes deep learning gradient based optimization should get its due but so should symbol manipulation which as yet is the only known tool for systematically representing and achieving high level abstraction bedrock to virtually all of the world s complex computer systems from spreadsheets to programming environments to operating systems eventually i conjecture credit will also be due to the inevitable marriage between the two hybrid systems that bring together the two great ideas of th century ai symbol processing and neural networks both initially developed in the s other new tools yet to be invented may be critical as well to a true acolyte of deep learning anything is deep learning no matter what it s incorporating and no matter how different it might be from current techniques viva imperialism if you replaced every transistor in a classic symbolic microprocessor with a neuron but kept the chip s logic entirely unchanged a true deep learning acolyte would still declare victory but we won t understand the principles driving eventual success if we lump everything together note no machine can extrapolate it s not fair to expect a neural network to generalize from even numbers to odd numbers here s a function expressed over binary digits f f f what s f if you are an ordinary human you are probably going to guess if you are neural network of the sort i discussed you probably won t if you have been told many times that hidden layers in neural networks abstract functions you should be a little bit surprised by this if you are a human you might think of the function as something like reversal easily expressed in a line of computer code if you are a neural network of a certain sort it s very hard to learn the abstraction of reversal in a way that extends from evens in that context to odds but is that impossible certainly not if you have a prior notion of an integer try another this time in decimal f f what s f none of my human readers would care that questions happens to require you to extrapolate from even numbers to odds a lot of neural networks would be flummoxed sure the function is undetermined by the sparse number of examples like all functions but it is interesting and important that most people would amid the infinite range of a priori possible inductions would alight on f and just as interesting that most standard multilayer perceptrons representing the numbers as binary digits wouldn t that s telling us something but many people in the neural network community franc ois chollet being one very salient exception don t want to listen importantly recognizing that a rule applies to any integer is roughly the same kind of generalization that allows one to recognize that a novel noun that can be used in one context can be used in a huge variety of other contexts from the first time i hear the word blicket used as an object i can guess that it will fit into a wide range of frames like i thought i saw a blicket i had a close encounter with a blicket and exceptionally large blickets frighten me etc and i can both generate and interpret such sentences without specific further training it doesn t matter whether blicket is or not similar in for example phonology to other words i have heard nor whether i pile on the adjectives or use the word as a subject or an object if most machine learning ml paradigms have a problem with this we should have problem with most ml paradigms am i being fair well yes and no it s true that i am asking neural networks to do something that violates their assumptions a neural network advocate might for example say hey wait a minute in your reversal example there are three dimensions in your input space representing the left binary digit the middle binary digit and rightmost binary digit the rightmost binary digit has only been a zero in the training there is no way a network can know what to do when you get to one in that position for example vincent lostenlan a postdoc at cornell said dietterich made essentially the same point more concisely but although both are right about why odds and evens are in this context hard for deep learning they are both wrong about the larger issues for three reasons first it can t be that people can t extrapolate you just did in two different examples at the top of this section paraphrasing chico marx who are you going to believe me or your own eyes to someone immersed deeply perhaps too deeply in contemporary machine learning my odds and evens problem seems unfair because a certain dimension the one which contains the value of in the rightmost digit hasn t been illustrated in the training regime but when you a human look at my examples above you will not be stymied by this particular gap in the training data you won t even notice it because your attention is on higher level regularities people routinely extrapolate in exactly the fashion that i have been describing like recognizing string reversal from the three training examples i gave above in a technical sense that is extrapolation and you just did it in the algebraic mind i referred to this specific kind of extrapolation as generalizing universally quantified one to one mappings outside of a space of training examples as a field we desperately need a solution to this challenge if we are ever to catch up to human learning even if it means shaking up our assumptions now it might reasonably be objected that it s not a fair fight humans manifestly depend on prior knowledge when they generalize such mappings in some sense dieterrich proposed this objection later in his tweet stream true enough but in a way that s the point neural networks of a certain sort don t have a good way of incorporating the right sort of prior knowledge in the place it is precisely because those networks don t have a way of incorporating prior knowledge like many generalizations hold for all elements of unbounded classes or odd numbers leave a remainder of one when divided by two that neural networks that lack operations over variables fail the right sort of prior knowledge that would allow neural networks to acquire and represent universally quantified one to one mappings standard neural networks can t represent such mappings except in certain limited ways convolution is a way of building in one particular such mapping prior to learning second saying that no current system deep learning or otherwise can extrapolate in the way that i have described is no excuse once again other architectures may be in the choppy water but that doesn t mean we shouldn t be trying to swim to shore if we want to get to agi we have to solve the problem put differently yes one could certainly hack together solutions to get deep learning to solve my specific number series problems by for example playing games with the input encoding schemes the real question if we want to get to agi is how to have a system learn the sort of generalizations i am describing in a general way third the claim that no current system can extrapolate turns out to be well false there are already ml systems that can extrapolate at least some functions of exactly the sort i described and you probably own one microsoft excel its flash fill function in particular gulwani powered by a very different approach to machine learning it can do certain kinds of extrapolation albeit in a narrow context by the bushel e g try typing the decimal digits in a series of rows and see if the system can extrapolate via flash fill to the eleventh item in the sequence spoiler alert it can in exactly the same way as you probably would even though there were no positive examples in the training dimension of the hundreds digit the systems learns from examples the function you want and extrapolates it piece of cake can any deep learning system do that with three training examples even with a range of experience on other small counting functions like and well maybe but only the ones that are likely do so are likely to be hybrids that build in operations over variables which are quite different from the sort of typical convolutional neural networks that most people associate with deep learning putting all this very differently one crude way to think about where we are with most ml systems that we have today note is that they just aren t designed to think outside the box they are designed to be awesome interpolators inside the box that s fine for some purposes but not others humans are better at thinking outside boxes than contemporary ai i don t think anyone can seriously doubt that but that kind of extrapolation that microsoft can do in a narrow context but that no machine can do with human like breadth is precisely what machine learning engineers really ought to be working on if they want to get to agi everybody in the field already knew this there is nothing new here well certainly not everybody as noted there were many critics who think we still don t know the limits of deep learning and others who believe that there might be some but none yet discovered that said i never said that any of my points was entirely new for virtually all i cited other scholars who had independently reached similar conclusions marcus failed to cite x definitely true the literature review was incomplete one favorite among the papers i failed to cite is shanahan s deep symbolic reinforcement garnelo arulkumaran shanahan i also can t believe i forgot richardson and domingos markov logic networks i also wish i had cited evans and edward grefenstette a great paper from deepmind and smolensky s tensor calculus work smolensky et al and work on inductive programming in various forms gulwani et al and probabilistic programming too by noah goodman goodman mansinghka roy bonawitz tenenbaum all seek to bring rules and networks close to together and older stuff by pioneers like jordan pollack smolensky et al and forbus and gentner s falkenhainer forbus gentner and hofstadter and mitchell s work on analogy and many others i am sure there is a lot more i could and should have cited overall i tried to be representative rather than fully comprehensive but i still could have done better chagrin marcus has no standing in the field he isn t a practitioner he is just a critic hesitant to raise this one but it came up in all kinds of different responses even from the mouths of certain well known professionals as ram shankar noted as a community we must circumscribe our criticism to science and merit based arguments what really matters is not my credentials which i believe do in fact qualify me to write but the validity of the arguments either my arguments are correct or they are not still for those who are curious i supply an optional mini history of some of my relevant credentials in note at the end re hierarchy what about socher s tree rnns i have written to him in hopes of having a better understanding of its current status i ve also privately pushed several other teams towards trying out tasks like lake and baroni presented pengfei et al offers some interesting discussion you could have been more critical of deep learning nobody quite said that not in exactly those words but a few came close generally privately one colleague for example pointed out that there may be some serious errors of future forecasting around the same colleague added another colleague ml researcher and author pedro domingos pointed out still other shortcomings of current deep learning methods that i didn t mention like other flexible supervised learning methods deep learning systems can be unstable in the sense that slightly changing the training data may result in large changes in the resulting model as domingos notes there s no guarantee this sort of rise and decline won t repeat itself neural networks have risen and fallen several times before all the way back to rosenblatt s first perceptron in we shouldn t mistake cyclical enthusiasm for a complete solution to intelligence which still seems to me anyway to be decades away if we want to reach agi we owe it to ourselves to be as keenly aware of challenges we face as we are of our successes there are other problems too in relying on these image sets for example in reading a draft of this paper melanie mitchell pointed me to important recent work by loghmani and colleague on assessing how deep learning does in the real world quoting from the abstract the paper analyzes the transferability of deep representations from web images to robotic data in the wild despite the promising results obtained with representations developed from web image the experiments demonstrate that object classification with real life robotic data is far from being solved and that literature is growing fast in late december there was a paper about fooling deep nets into mistaking a pair of skiers for a dog https arxiv org pdf pdf and another on a general purpose tool for building real world adversarial patches https arxiv org pdf pdf see also https arxiv org abs it s frightening to think how vulnerable deep learning can be real world contexts and for that matter consider filip pieknewski s blog on why photo trained deep learning systems have trouble transferring what they have learned to line drawings https blog piekniewski info can a deep net see a cat vision is not as solved as many people seem to think as i will explain in the forthcoming paper alphago is not actually a pure deep reinforcement learning system although the quoted passage presented it as such it s really more of a hybrid with important components that are driven by symbol manipulating algorithms along with a well engineered deep learning component alphazero by the way isn t unsupervised it s self supervised using self play and simulation as a way of generating supervised data i will have a lot more to say about that system in a forthcoming paper consider for example google search and how one might understand it google has recently added in a deep learning algorithm rankbrain to the wide array of algorithms it uses for search and google search certainly takes in data and knowledge and processes them hierarchically which according to maher ibrahim is all you need to count as being deep learning but realistically deep learning is just one cue among many the knowledge graph component for example is based instead primarily on classical ai notions of traversing ontologies by any reasonable measure google search is a hybrid with deep learning as just one strand among many calling google search as a whole a deep learning system would be grossly misleading akin to relabeling carpentry screwdrivery just because screwdrivers happen to be involved important exceptions include inductive logic programming inductive function programming the brains behind microsoft s flash fill and neural programming all are making some progress here some of these even include deep learning but they also all include structured representations and operations over variables among their primitive operations that s all i am asking for my ai experiments begin in adolescence with among other thing a latin english translator that i coded in the programming language logo in graduate school studying with steven pinker i explored the relation between language acquisition symbolic rules and neural networks i also owe a debt to my undergraduate mentor neil stillings the child language data i gathered marcus et al for my dissertation have been cited hundreds of times and were the most frequently modeled data in the s debate about neural networks and how children learned language in the late s i discovered some specific replicable problems with multilayer perceptrons marcus b marcus a based on those observation i designed a widely cited experiment published in science marcus vijayan bandi rao vishton that showed that young infants could extract algebraic rules contra jeff elman s then popular neural network all of this culminated in a mit press book marcus which lobbied for a variety of representational primitives some of which have begun to pop up in recent neural networks in particular that the use of operations over variables in the new field of differentiable programming daniluk rockta schel welbl riedel graves et al owes something to the position outlined in that book there was a strong emphasis on having memory records as well which can be seen in the memory networks being developed e g at facebook bordes usunier chopra weston the next decade saw me work on other problems including innateness marcus which i will discuss at length in the forthcoming piece about alphago and evolution marcus marcus i eventually returned to ai and cognitive modeling publishing a article on cortical computation in science marcus marblestone dean that also anticipates some of what is now happening in differentiable programming more recently i took a leave from academia to found and lead a machine learning company in by any reasonable measure that company was successful acquired by uber roughly two years after founding as co founder and ceo i put together a team of some of the very best machine learning talent in the world including zoubin ghahramani jeff clune noah goodman ken stanley and jason yosinski and played a pivotal role in developing our core intellectual property and shaping our intellectual mission a patent is pending co written by zoubin ghahramani and myself although much of what we did there remains confidential now owned by uber and not by me i can say that a large part of our efforts were addressed towards integrating deep learning with our own techniques which gave me a great deal of familiarity with joys and tribulations of tensorflow and vanishing and exploding gradients we aimed for state of the art results sometimes successfully sometimes not with sparse data using hybridized deep learning systems on a daily basis bordes a usunier n chopra s weston j large scale simple question answering with memory networks arxiv daniluk m rockta schel t welbl j riedel s frustratingly short attention spans in neural language modeling arxiv elman j l finding structure in time cognitive science evans r grefenstette e learning explanatory rules from noisy data arxiv cs ne falkenhainer b forbus k d gentner d the structure mapping engine algorithm and examples artificial intelligence fukushima k miyake s ito t neocognitron a neural network model for a mechanism of visual pattern recognition ieee transactions on systems man and cybernetics garnelo m arulkumaran k shanahan m towards deep symbolic reinforcement learning arxiv cs ai goodman n mansinghka v roy d m bonawitz k tenenbaum j b church a language for generative models arxiv preprint arxiv graves a wayne g reynolds m harley t danihelka i grabska barwin ska a et al hybrid computing using a neural network with dynamic external memory nature gulwani s automating string processing in spreadsheets using input output examples dl acm org gulwani s herna ndez orallo j kitzelmann e muggleton s h schmid u zorn b inductive programming meets the real world communications of the acm hofstadter d r mitchell m the copycat project a model of mental fluidity and analogy making advances in connectionist and neural computation theory hosseini h xiao b jaiswal m poovendran r on the limitation of convolutional neural networks in recognizing negative images arxiv cs cv hubel d h wiesel t n receptive fields of single neurones in the cat s striate cortex the journal of physiology lake b m baroni m still not systematic after all these years on the compositional skills of sequence to sequence recurrent networks arxiv loghmani m r caputo b vincze m recognizing objects in the wild where do we stand arxiv cs ro marcus g f a rethinking eliminative connectionism cogn psychol marcus g f b can connectionism save constructivism cognition marcus g f the algebraic mind integrating connectionism and cognitive science cambridge mass mit press marcus g f the birth of the mind how a tiny number of genes creates the complexities of human thought basic books marcus g f kluge the haphazard construction of the human mind boston houghton mifflin marcus g deep learning a critical appraisal arxiv marcus g f marblestone a dean t a the atoms of neural computation science marcus g f marblestone a h dean t l b frequently asked questions for the atoms of neural computation biorxiv arxiv q bio nc marcus g f the algebraic mind integrating connectionism and cognitive science cambridge mass mit press marcus g f pinker s ullman m hollander m rosen t j xu f overregularization in language acquisition monogr soc res child dev marcus g f vijayan s bandi rao s vishton p m rule learning by seven month old infants science nguyen a yosinski j clune j deep neural networks are easily fooled high confidence predictions for unrecognizable images arxiv cs cv pengfei l xipeng q xuanjing h dynamic compositional neural networks over tree structure ijcai proceedings from proceedings of the twenty sixth international joint conference on artificial intelligence ijcai ribeiro m t singh s guestrin c why should i trust you explaining the predictions of any classifier arxiv cs lg richardson m domingos p markov logic networks machine learning sabour s dffsdfdsf n hinton g e dynamic routing between capsules arxiv cs cv silver d schrittwieser j simonyan k antonoglou i huang a guez a et al mastering the game of go without human knowledge nature smolensky p lee m he x yih w t gao j deng l basic reasoning with tensor product representations arxiv cs ai from a quick cheer to a standing ovation clap to show how much you enjoyed this story ceo founder geometric intelligence acquired by uber professor of psychology and neural science nyu freelancer for the new yorker new york times
Bargava,11800,3,https://towardsdatascience.com/how-to-learn-deep-learning-in-6-months-e45e40ef7d48?source=tag_archive---------5----------------,How to learn Deep Learning in 6 months – Towards Data Science,it is quite possible to learn follow and contribute to state of art work in deep learning in about months time this article details out the steps to achieve that pre requisites you are willing to spend hours per week for the next months you have some programming skills you should be comfortable to pick up python along the way and cloud no background in python and cloud assumed some math education in the past algebra geometry etc access to internet and computer step we learn driving a car by driving not by learning how the clutch and the internal combustion engine work atleast not initially when learning deep learning we will follow the same top down approach do the fast ai course practical deep learning for coders part this takes about weeks of effort this course has a session on running the code on cloud google colaboratory has free gpu access start with that other options include paperspace aws gcp crestle and floydhub all of these are great do not start to build your own machine atleast not yet step this is the time to know some of the basics learn about calculus and linear algebra for calculus big picture of calculus provides a good overview for linear algebra gilbert strang s mit course on opencourseware is amazing once you finish the above two read the matrix calculus for deep learning step now is the time to understand the bottom up approach to deep learning do all the courses in the deep learning specialisation in coursera you need to pay to get the assignments graded but the effort is truly worth it ideally given the background you have gained so far you should be able to complete one course every week step do a capstone project this is the time where you delve deep into a deep learning library eg tensorflow pytorch mxnet and implement an architecture from scratch for a problem of your liking the first three steps are about understanding how and where to use deep learning and gaining a solid foundation this step is all about implementing a project from scratch and developing a strong foundation on the tools step now go and do fast ai s part ii course cutting edge deep learning for coders this covers more advanced topics and you will learn to read the latest research papers and make sense out of them each of the steps should take about weeks time and in about weeks since the time you started and if you followed all of the above religiously you will have a solid foundation in deep learning where to go next do the stanford s cs n and cs d courses these two are amazing courses with great depth for vision and nlp respectively they cover the latest state of art and read the deep learning book this will solidify your understanding happy deep learning create every single day from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning http impel io currently building a personalization engine http www recotap com data science trainer and mentor sharing concepts ideas and codes
Seth Weidman,2800,11,https://hackernoon.com/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef?source=tag_archive---------6----------------,The 3 Tricks That Made AlphaGo Zero Work – Hacker Noon,there were many advances in deep learning and ai in but few generated as much publicity and interest as deepmind s alphago zero this program was truly a shocking breakthrough not only did it beat the prior version of alphago the program that beat time world champion lee sedol just a year and a half earlier it was trained without any data from real human games xavier amatrain called it more significant than anything in the last years in machine learning so how did deepmind do it in this essay i ll try to give an intuitive idea of the techniques alphago zero used what made them work and what the implications for future ai research are let s start with the general approach that both alphago and alphago zero took to playing go both alphago and alphago zero evaluated the go board and chose moves using a combination of two methods alphago and alphago zero both worked by cleverly combining these two methods let s look at each one in turn go is a sufficiently complex game that computers can t simply search all possible moves using a brute force approach to find the best one indeed they can t even come close the best go programs prior to alphago overcame this by using monte carlo tree search or mcts at a high level this method involves initially exploring many possible moves on the board and then focusing this exploration over time as certain moves are found to be more likely to lead to wins than others both alphago and alphago zero use a relatively straightforward version of mcts for their lookahead simply using many of the best practices listed in the monte carlo tree search wikipedia page to properly manage the tradeoff between exploring new sequences of move or more deeply explore already explored sequences for more see the details in the search section under methods in the original alphago paper published in nature though mcts had been the core of all successful go programs prior to alphago it was deepmind s clever combination of this technique with a neural network based intuition that allowed it to surpass human performance deepmind s major innovation with alphago was to use deep neural networks to understand the state of the game and then use this understanding to intelligently guide the search of the mcts more specifically they trained networks that could look at given this information the neural networks could recommend how did deepmind train neural networks to do this here alphago and alphago zero used very different approaches we ll start first with alphago s alphago had two separately trained neural networks deepmind then combined these two neural networks with mcts that is the program s intuition with its brute force lookahead search in a very clever way it used the network that had been trained to predict moves to guide which branches of the game tree to search and used the network that had been trained to predict whether a position was winning to evaluate the positions it encountered during its search this allowed alphago to intelligently search upcoming moves and ultimately allowed it to beat lee sedol alphago zero however took this to a whole new level at a high level alphago zero works the same way as alphago specifically it plays go by using mcts based lookahead search intelligently guided by a neural network however alphago zero s neural network its intuition was trained completely differently from that of alphago let s say you have a neural network that is attempting to understand the game of go that is for every board position it is using a deep neural network to generate evaluations of what the best moves are what deepmind realized is that no matter how intelligent this neural network is whether it is completely clueless or a go master its evaluations can always be made better by mcts fundamentally mcts performs the kind of lookahead search that we would imagine a human master would perform if given enough time it intelligently guesses which variations sequences of future moves are most promising simulates those variations evaluates how good they actually are and updates its assessments of its current best moves accordingly an illustration of this is below suppose we have a neural network that is reading the board and determining that a given move results in a game being even with an evaluation of then the network intelligently looks ahead a few moves and finds a sequence of moves that can be forced from the current position that ends up resulting in an evaluation of it can then update its evaluation of the current board position to reflect that it leads to a more favorable position down the road this lookahead search therefore can always give us improved data on how good the various moves in the current position that the neural network is evaluating are this is true whether our neural network is playing at an amateur level or an expert level we can always generate improve evaluations for it by looking ahead and seeing which of its current options actually lead to better positions in addition just as in alphago we would also want our neural network to learn which moves are likely to lead to wins so also as before our agent using its mcts improved evaluations and the current state of its neural network could play games against itself winning some and losing others this data generated purely via lookahead and self play is what deepmind used to train alphago zero more specifically much was made of the fact that no games between humans were used to train alphago zero and this first trick was the reason why for a given state of a go agent it can always be made smarter by performing mcts based lookahead and using the results of that lookahead to improve the agent this is how alphago zero was able to continuously improve from when it was an amateur all the way up to when it better than the best human players the second trick was a novel neural network structure that i ll call the two headed monster alphago zero s was its neural network architecture a two headed architecture its first layers or so were layer blocks of a type often seen in modern neural net architecures these layers were followed by two heads one head that took the output of the first layers and produced probabilities of the go agent making certain moves and another that took the output of the first layers and outputted a probability of the current player winning this is quite unusual in almost all applications neural networks output a single fixed output such as the probability of an image containing a dog or a vector containing the probabilities of an image containing one of types of objects how can a net learn if it is receiving two sets of signals one on how good its evaluations of the board are and another how good the specific moves it is selecting are the answer is simple remember that neural networks are fundamentally just mathematical functions with a bunch of parameters that determine the predictions that they make we teach them by repeatedly showing them correct answers and having them update their parameters so the answers they produce more closely match these correct answers so when we use the two headed neural net to make a prediction using head we simply update the parameters that led to making that prediction namely the parameters in the body and in head similarly when we make a prediction using head we update the parameters in the body and in head this is how deepmind trained its single two headed neural network that it used to guide mcts during its search just as alphago did with two separate neural networks this trick accounted for half of alphago zero s increase in playing strength over alphago this trick is known more technically as multi task learning with hard parameter sharing sebastian ruder has a great overview here the other half of the increase in playing strength simply came from bringing the neural network architecture up to date with the latest advances in the field alphago zero used a more cutting edge neural network architecture than alphago specifically they used a residual neural network architecture instead of a purely convolutional architecture residual nets were pioneered by microsoft research in late right around the time work on the first version of alphago would have wrapped up so it both understandable that deepmind did not use them in the original alphago program interestingly as the chart below shows each of these two neural network related tricks switching from convolutional to residual architecture and using the two headed monster neural network architecture instead of separate neural networks would have resulted in about half of the increase in playing strength as was achieved when both were combined these three tricks are what enabled alphago zero to achieve its incredible performance that blew away even alpha go it is worth noting that alphago did not use any classical or even cutting edge reinforcement learning concepts no deep q learning asynchronous actor critic agents or anything else we typically associate with reinforcement learning it simply used simulations to generate training data for its neural nets to then learn from in a supervised fashion denny britz sums this idea up well in this tweet from just after when the alphago zero paper was released here s a step by step timeline of how alphago zero was trained as these self play games are happening sample positions from the most recent games along with whether the game was won or lost for each move record both a the results of the mcts evaluations of those positions how good the various moves in these positions were based on lookahead and b whether the current player won or lost the game train the neural network using both a the move evaluations produced by the mcts lookahead search and b whether the current player won or lost finally every iterations of steps evaluate the current neural network against the previous best version if it wins at least of the games begin using it to generate self play games instead of the prior version repeat steps times while the self play games are continuously being played after three days you ll have yourself an alphago zero there are many implications of deepmind s incredible achievement for the future of ai research here are a couple of key ones first the fact that self play data generated from simulations was good enough to be able to train the network suggests that simulated self play data can train agents to surpass human performance in extremely complex tasks even starting completely from scratch data generated from human experts may not be needed second the two headed monster trick seems to significantly help agents learn to perform several related tasks in many domains since it seems to prevent the agents from overfitting their behavior to any individual task deepmind seems to really like this trick and has used it and more advanced versions of it to build agents that can learn multiple tasks in several different domains many projects in robotics especially the burgeoning field of using simulations to teach robotic agents to use their limbs to accomplish tasks are using these two tricks to great effect pieter abbeel s recent nips keynote highlights many impressive new results that use these tricks along with many bleeding edge reinforcement learning techniques indeed locomotion seems like a perfect use case for the two headed monster trick in particular for example robotic agents could be simultaneously trained to hit a baseball using a bat and to throw a punch to hit a moving target since the two tasks require learning some common skills e g balance torso rotation deepmind s alphago zero was one of the most intriguing advancements in ai and deep learning in i can t wait to see what brings from a quick cheer to a standing ovation clap to show how much you enjoyed this story senior data scientist at thisismetis i write about the intersection of data science business education and society how hackers start their afternoons
Gabriel Aldamiz...,5100,11,https://hackernoon.com/how-we-grew-from-0-to-4-million-women-on-our-fashion-app-with-a-vertical-machine-learning-approach-f8b7fc0a89d7?source=tag_archive---------7----------------,"How we grew from 0 to 4 million women on our fashion app, with a vertical machine learning approach",three years ago we launched chicisimo our goal was to offer automated outfit advice today with over million women on the app we want to share how our data and machine learning approach helped us grow it s been chaotic but it is now under control if we wanted to build a human level tool to offer automated outfit advice we needed to understand people s fashion taste a friend can give us outfit advice because after seeing what we normally wear she s learnt our style how could we build a system that learns fashion taste we had previous experience with taste based projects and a background in machine learning applied to music and other sectors we saw how a collaborative filtering tool transformed the music industry from blindness to totally understanding people check out the audioscrobbler story it also made life better for those who love music and created several unicorns along the way with this background we built the following thesis online fashion will be transformed by a tool that understands taste because if you understand taste you can delight people with relevant content and a meaningful experience we also thought that outfits were the asset that would allow taste to be understood to learn what people wear or have in their closet and what style each of us like we decided we were going to build that tool to understand taste we focused on developing the correct dataset and built two assets our mobile app and our data platform from previous experience building mobile products even in symbian back then we knew it was easy to bring people to an app but difficult to retain them so we focused on small iterations to learn as fast as possible we launched an extremely early alpha of chicisimo with one key functionality we launched under another name and in another country you couldn t even upload photos but it allowed us to iterate with real data and get a lot of qualitative input at some point we launched the real chicisimo and removed this alpha from the app store we spent a long time trying to understand what our true levers of retention were and what algorithms we needed in order to match content and people three things helped with retention a identify retention levers using behavioral cohorts we use mixpanel for this we run cohorts not only over the actions that people performed but also over the value they received this was hard to conceptualize for an app such as chicisimo we thought in terms of what specific and measurable value people received measured it and run cohorts over those events and then we were able to iterate over value received not only over actions people performed we also defined and removed anti levers all those noisy things that distract from the main value and got all the relevant metrics for different time periods first session first day first week etc these super specific metrics allowed us to iterate nir eyal s book hooked how to build habit forming products discusses a framework to create habits that helped us build our model b re think the onboarding process once we knew the levers of retention we define it as the process by which new signups find the value of the app as soon as possible and before we lose them we clearly articulated to ourselves what needed to happen what and when it went something like this if people don t do action during their first minutes in their first session they will not come back so we need to change the experience to make that happen we also run tons of user tests with different types of people and observed how they perceived or mostly didn t the retention lever c define how we learn the data approach described above is key but there is much more than data when building a product people love in our case first of all we think that the what to wear problem is a very important one to solve and we truly respect it we obsess over understanding the problem and over understanding how our solution is helping or not it s our way of showing respect this leads me to one of the most surprising aspects imo of building a product the fact that regularly we access new corpuses of knowledge that we did not have before which help us improve the product significantly when we ve obtained these game changing learnings it s always been by focusing on two aspects how people relate to the problem and how people relate to the product the red arrows in the image below there are a million subtleties that happen in these two relations and we are building chicisimo by trying to understand them now we know that at any point there is something important that we don t know and therefore the question always is how can we learn sooner talking with one of my colleagues she once told me this is not about data this is about people and the truth is from day one we ve learnt significantly by having conversations with women about how they relate with the problem and with solutions we use several mechanisms having face to face conversations reading the emails we get from women without predefined questions or asking for feedback around specific topics we now use typeform and its a great tool for product insight and then we talk among ourselves and try to articulate the learnings we also seek external references we talk with other product people we play with inspiring apps and we re read articles that help us think this process is what allows us to learn and then build product and develop technology at some point we were lucky to get noticed by the app store team and we ve been featured as app of the day throughout the world view apple s description of chicisimo here on december st chicisimo was featured in a summary of apps the app store team did we are the pink c in the left image below the app got viewed by uniques thanks to this feature for a total of m times in our case app features have a conversion rate from impression to app install normally impression product page view install aso has a conversion and referrers the app aims at understanding taste so we can do a better job at suggesting outfit ideas the simple act of delivering the right content at the right time can absolutely wow people although it is an extremely difficult utility to build chicisimo content is user generated and this poses some challenges the system needs to classify different types of content automatically build the right incentives and understand how to match content and needs we soon saw that there was a lot of data coming in after thinking hey how cool we are look at all this data we have we realized it was actually a nightmare because being chaotic the data wasn t actionable this wasn t cool at all but then we decided to start giving some structure to parts of the data and we ended inventing what we called the social fashion graph the graph is a compact representation of how needs outfits and people interrelate a concept that helped us build the data platform the data platform creates a high quality dataset linked to a learning and training world our app which therefore improves with each new expression of taste we thought of outfits as playlists an outfit is a combination of items that makes sense to consume together using collaborative filtering the relations captured here allow us to offer recommendations in different areas of the app there was still a lot of noise in the data and one of the hardest things was to understand how people were expressing the same fashion need in different ways which made matching content and needs even more difficult lots of people might need ideas to go to school and express that specific need in a hundred different ways how do you capture this diversity and how do you provide structure to it we built a system to collect concepts we call them needs and captured equivalences among different ways to express the same need we ended up building a list of the world s what to wear needs which we call our ontology this really cleaned up the dataset and helped us understand what we had this understanding led to better product decisions we now understand that an outfit a need or a person can have a lot of understandable data attached if you allow people to express freely the app while having the right system behind the platform structuring data gave us control while encouraging unstructured data gave us knowledge and flexibility the end result is our current system a system that learns the meaning of an outfit how to respond to a need or the taste of an individual and i wouldn t even dare saying that this is day for us screenshot of an internal tool the amount of work we have in front of us is immense but we feel things are now under control one of the new areas we ve been working on is adding a fourth element to the social fashion graph shoppable products a system to match outfits to products automatically and to help people decide what to buy next this is pretty exciting back when we built recommender systems for music and other products it was pretty easy that s what we think now we obviously didn t think that at the time first it was easy to capture that you liked a given song then it was easy to capture the sequence in which you and others would listen to that song and therefore you could capture the correlations with this data you could do a lot however as we soon found out fashion has its own challenges there is not an easy way to match an outfit to a shoppable product think about most garments in your wardrobe most likely you won t find a link to view buy those garments online something you can do for many other products you have at home another challenge the industry is not capturing how people describe clothes or outfits so there is a strong disconnect between many ecommerces and its shoppers we think we ve solved that problem also similar ai and twiggle are working on it another challenge style is complex to capture and classify by a machine now deep learning brings a new tool to add to other mechanisms and changes everything owning the correct data set allows us to focus on the specific narrow use cases related to outfit recommendations and to focus on delivering value through the algorithms instead of spending time collecting and cleaning data now comes the fun and rewarding part so please email us if you want to join the team and help build algorithms that have real impact on people we are remote slack based people s very personal style can become as actionable as metadata and possibly as transparent as well and i think we can see the path to get there as we have a consumer product that people already love we can ship early results of these algorithms partially hidden and increase their presence as feedback improves results there are more and more researchers working of these areas you can read tangseng s paper on recommending outfits from personal closet or clothing parsing project or how edgar simo serra defines similarity between images using user provided metadata outfits are a key asset in the race to capture the billion us apparel market data is also the reason many players are taking outfits to the forefront of technology outfits are a daily habit and have proven to be great assets to attract and retain shoppers and capture their data many players are introducing a shop the look section with outfits from real people amazon zalando or google are a few examples google recently introduced a new feature called style ideas showing how a product can be worn in real life same month amazon launched its alexa echo look to help you with your outfit and alibaba s artificial intelligence personal stylist helped them achieve record sales during singles day some people think that fashion data is in the same place as music data was in ready to play a very relevant role the good news is the daily habit of deciding what to wear will not change the need to buy new clothes won t disappear either so what do you think where will we be years from now will taste data build unique online experiences what role will outfits play how will machine learning change fashion ecommerce will everything change years from now we are a small team of eight four on product and four engineers we believe in focusing on our very specific problem no one on earth can understand the problem better than us we also believe on building the complete solution ourselves while doing as few things as possible we work remote and live in slack github you can learn more about our machine learning approach here if you are a deep learning engineer or a product manager in the fashion space and want to chat temporarily access our social fashion graph please email us describing your work you can also download our ios and android apps or simply say hi hi at chicisimo com from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder ceo at chicisimo machine learning to automate outfit advise world s largest outfits app how hackers start their afternoons
Sarthak Jain,3900,10,https://medium.com/nanonets/how-to-easily-detect-objects-with-deep-learning-on-raspberrypi-225f29635c74?source=tag_archive---------8----------------,How to easily Detect Objects with Deep Learning on Raspberry Pi,disclaimer i m building nanonets com to help build ml with less data and no hardware the raspberry pi is a neat piece of hardware that has captured the hearts of a generation with m devices sold with hackers building even cooler projects on it given the popularity of deep learning and the raspberry pi camera we thought it would be nice if we could detect any object using deep learning on the pi now you will be able to detect a photobomber in your selfie someone entering harambe s cage where someone kept the sriracha or an amazon delivery guy entering your house m years of evolution have made human vision fairly evolved the human brain has of it s neurons work on processing vision as compared with percent for touch and just percent for hearing humans have two major advantages when compared with machines one is stereoscopic vision the second is an almost infinite supply of training data an infant of years has had approximately b images sampled at fps to mimic human level performance scientists broke down the visual perception task into four different categories object detection has been good enough for a variety of applications even though image segmentation is a much more precise result it suffers from the complexity of creating training data it typically takes a human annotator x more time to segment an image than draw bounding boxes this is more anecdotal and lacks a source also after detecting objects it is separately possible to segment the object from the bounding box object detection is of significant practical importance and has been used across a variety of industries some of the examples are mentioned below object detection can be used to answer a variety of questions these are the broad categories there are a variety of models architectures that are used for object detection each with trade offs between speed size and accuracy we picked one of the most popular ones yolo you only look once and have shown how it works below in under lines of code if you ignore the comments note this is pseudo code not intended to be a working example it has a black box which is the cnn part of it which is fairly standard and shown in the image below you can read the full paper here https pjreddie com media files papers yolo pdf for this task you probably need a few images per object try to capture data as close to the data you re going to finally make predictions on draw bounding boxes on the images you can use a tool like labelimg you will typically need a few people who will be working on annotating your images this is a fairly intensive and time consuming task you can read more about this at medium com nanonets nanonets how to use deep learning when you have limited data f c b cab you need a pretrained model so you can reduce the amount of data required to train without it you might need a few k images to train the model you can find a bunch of pretrained models here the process of training a model is unnecessarily difficult to simplify the process we created a docker image would make it easy to train to start training the model you can run the docker image has a run sh script that can be called with the following parameters you can find more details at to train a model you need to select the right hyper parameters finding the right parameters the art of deep learning involves a little bit of hit and try to figure out which are the best parameters to get the highest accuracy for your model there is some level of black magic associated with this along with a little bit of theory this is a great resource for finding the right parameters quantize model make it smaller to fit on a small device like the raspberry pi or mobile small devices like mobile phones and rasberry pi have very little memory and computation power training neural networks is done by applying many tiny nudges to the weights and these small increments typically need floating point precision to work though there are research efforts to use quantized representations here too taking a pre trained model and running inference is very different one of the magical qualities of deep neural networks is that they tend to cope very well with high levels of noise in their inputs why quantize neural network models can take up a lot of space on disk with the original alexnet being over mb in float format for example almost all of that size is taken up with the weights for the neural connections since there are often many millions of these in a single model the nodes and weights of a neural network are originally stored as bit floating point numbers the simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer and then compressing each float value to an eight bit integer the size of the files is reduced by code for quantization you need the raspberry pi camera live and working then capture a new image for instructions on how to install checkout this link download model once your done training the model you can download it on to your pi to export the model run then download the model onto the raspberry pi install tensorflow on the raspberry pi depending on your device you might need to change the installation a little run model for predicting on the new image the raspberry pi has constraints on both memory and compute a version of tensorflow compatible with the raspberry pi gpu is still not available therefore it is important to benchmark how much time do each of the models take to make a prediction on a new image we have removed the need to annotate images we have expert annotators who will annotate your images for you we automatically train the best model for you to achieve this we run a battery of model with different parameters to select the best for your data nanonets is entirely in the cloud and runs without using any of your hardware which makes it much easier to use since devices like the raspberry pi and mobile phones were not built to run complex compute heavy tasks you can outsource the workload to our cloud which does all of the compute for you get your free api key from http app nanonets com user api key collect the images of object you want to detect you can annotate them either using our web ui https app nanonets com objectannotation appid your model id or use open source tool like labelimg once you have dataset ready in folders images image files and annotations annotations for the image files start uploading the dataset once the images have been uploaded begin training the model the model takes hours to train you will get an email once the model is trained in the meanwhile you check the state of the model once the model is trained you can make predictions using the model from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder ceo nanonets com nanonets machine learning api
Emil Wallner,9100,25,https://medium.freecodecamp.org/how-you-can-train-an-ai-to-convert-your-design-mockups-into-html-and-css-cc7afd82fed4?source=tag_archive---------9----------------,How you can train an AI to convert your design mockups into HTML and CSS,within three years deep learning will change front end development it will increase prototyping speed and lower the barrier for building software the field took off last year when tony beltramelli introduced the pix code paper and airbnb launched sketch code currently the largest barrier to automating front end development is computing power however we can use current deep learning algorithms along with synthesized training data to start exploring artificial front end automation right now in this post we ll teach a neural network how to code a basic a html and css website based on a picture of a design mockup here s a quick overview of the process we ll build the neural network in three iterations first we ll make a bare minimum version to get a hang of the moving parts the second version html will focus on automating all the steps and explaining the neural network layers in the final version bootstrap we ll create a model that can generalize and explore the lstm layer all the code is prepared on github and floydhub in jupyter notebooks all the floydhub notebooks are inside the floydhub directory and the local equivalents are under local the models are based on beltramelli s pix code paper and jason brownlee s image caption tutorials the code is written in python and keras a framework on top of tensorflow if you re new to deep learning i d recommend getting a feel for python backpropagation and convolutional neural networks my three earlier posts on floydhub s blog will get you started let s recap our goal we want to build a neural network that will generate html css markup that corresponds to a screenshot when you train the neural network you give it several screenshots with matching html it learns by predicting all the matching html markup tags one by one when it predicts the next markup tag it receives the screenshot as well as all the correct markup tags until that point here is a simple training data example in a google sheet creating a model that predicts word by word is the most common approach today there are other approaches but that s the method we ll use throughout this tutorial notice that for each prediction it gets the same screenshot so if it has to predict words it will get the same design mockup twenty times for now don t worry about how the neural network works focus on grasping the input and output of the neural network let s focus on the previous markup say we train the network to predict the sentence i can code when it receives i then it predicts can next time it will receive i can and predict code it receives all the previous words and only has to predict the next word the neural network creates features from the data the network builds features to link the input data with the output data it has to create representations to understand what is in each screenshot the html syntax that it has predicted this builds the knowledge to predict the next tag when you want to use the trained model for real world usage it s similar to when you train the model the text is generated one by one with the same screenshot each time instead of feeding it with the correct html tags it receives the markup it has generated so far then it predicts the next markup tag the prediction is initiated with a start tag and stops when it predicts an end tag or reaches a max limit here s another example in a google sheet let s build a hello world version we ll feed a neural network a screenshot with a website displaying hello world and teach it to generate the markup first the neural network maps the design mockup into a list of pixel values from in three channels red blue and green to represent the markup in a way that the neural network understands i use one hot encoding thus the sentence i can code could be mapped like the below in the above graphic we include the start and end tag these tags are cues for when the network starts its predictions and when to stop for the input data we will use sentences starting with the first word and then adding each word one by one the output data is always one word sentences follow the same logic as words they also need the same input length instead of being capped by the vocabulary they are bound by maximum sentence length if it s shorter than the maximum length you fill it up with empty words a word with just zeros as you see words are printed from right to left this forces each word to change position for each training round this allows the model to learn the sequence instead of memorizing the position of each word in the below graphic there are four predictions each row is one prediction to the left are the images represented in their three color channels red green and blue and the previous words outside of the brackets are the predictions one by one ending with a red square to mark the end in the hello world version we use three tokens start html center h hello world h center html and end a token can be anything it can be a character word or sentence character versions require a smaller vocabulary but constrain the neural network word level tokens tend to perform best here we make the prediction floydhub is a training platform for deep learning i came across them when i first started learning deep learning and i ve used them since for training and managing my deep learning experiments you can install it and run your first model within minutes it s hands down the best option to run models on cloud gpus if you are new to floydhub do their min installation or my minute walkthrough all the notebooks are prepared inside the floydhub directory the local equivalents are under local once it s running you can find the first notebook here floydhub helloworld helloworld ipynb if you want more detailed instructions and an explanation for the flags check my earlier post in this version we ll automate many of the steps from the hello world model this section will focus on creating a scalable implementation and the moving pieces in the neural network this version will not be able to predict html from random websites but it s still a great setup to explore the dynamics of the problem if we expand the components of the previous graphic it looks like this there are two major sections first the encoder this is where we create image features and previous markup features features are the building blocks that the network creates to connect the design mockups with the markup at the end of the encoder we glue the image features to each word in the previous markup the decoder then takes the combined design and markup feature and creates a next tag feature this feature is run through a fully connected neural network to predict the next tag since we need to insert one screenshot for each word this becomes a bottleneck when training the network example instead of using the images we extract the information we need to generate the markup the information is encoded into image features this is done by using an already pre trained convolutional neural network cnn the model is pre trained on imagenet we extract the features from the layer before the final classification we end up with eight by eight pixel images known as features although they are hard to understand for us a neural network can extract the objects and position of the elements from these features in the hello world version we used a one hot encoding to represent the markup in this version we ll use a word embedding for the input and keep the one hot encoding for the output the way we structure each sentence stays the same but how we map each token is changed one hot encoding treats each word as an isolated unit instead we convert each word in the input data to lists of digits these represent the relationship between the markup tags the dimension of this word embedding is eight but often varies between depending on the size of the vocabulary the eight digits for each word are weights similar to a vanilla neural network they are tuned to map how the words relate to each other mikolov et al this is how we start developing markup features features are what the neural network develops to link the input data with the output data for now don t worry about what they are we ll dig deeper into this in the next section we ll take the word embeddings and run them through an lstm and return a sequence of markup features these are run through a time distributed dense layer think of it as a dense layer with multiple inputs and outputs in parallel the image features are first flattened regardless of how the digits were structured they are transformed into one large list of numbers then we apply a dense layer on this layer to form a high level feature these image features are then concatenated to the markup features this can be hard to wrap your mind around so let s break it down here we run the word embeddings through the lstm layer in this graphic all the sentences are padded to reach the maximum size of three tokens to mix signals and find higher level patterns we apply a timedistributed dense layer to the markup features timedistributed dense is the same as a dense layer but with multiple inputs and outputs in parallel we prepare the images we take all the mini image features and transform them into one long list the information is not changed just reorganized again to mix signals and extract higher level notions we apply a dense layer since we are only dealing with one input value we can use a normal dense layer to connect the image features to the markup features we copy the image features in this case we have three markup features thus we end up with an equal amount of image features and markup features all the sentences are padded to create three markup features since we have prepared the image features we can now add one image feature for each markup feature after sticking one image feature to each markup feature we end up with three image markup features this is the input we feed into the decoder here we use the combined image markup features to predict the next tag in the below example we use three image markup feature pairs and output one next tag feature note that the lstm layer has the sequence set to false instead of returning the length of the input sequence it only predicts one feature in our case it s a feature for the next tag it contains the information for the final prediction the dense layer works like a traditional feedforward neural network it connects the digits in the next tag feature with the final predictions say we have words in our vocabulary start hello world and end the vocabulary prediction could be the softmax activation in the dense layer distributes a probability from with the sum of all predictions equal to in this case it predicts that the th word is the next tag then you translate the one hot encoding into the mapped value say end if you can t see anything when you click these links you can right click and click on view page source here is the original website for reference in our final version we ll use a dataset of generated bootstrap websites from the pix code paper by using twitter s bootstrap we can combine html and css and decrease the size of the vocabulary we ll enable it to generate the markup for a screenshot it has not seen before we ll also dig into how it builds knowledge about the screenshot and markup instead of training it on the bootstrap markup we ll use simplified tokens that we then translate into html and css the dataset includes test screenshots and validation images for each screenshot there are on average tokens resulting in training examples by tweaking the model in the pix code paper the model can predict the web components with accuracy bleu ngram greedy search more on this later extracting features from pre trained models works well in image captioning models but after a few experiments i realized that pix code s end to end approach works better for this problem the pre trained models have not been trained on web data and are customized for classification in this model we replace the pre trained image features with a light convolutional neural network instead of using max pooling to increase information density we increase the strides this maintains the position and the color of the front end elements there are two core models that enable this convolutional neural networks cnn and recurrent neural networks rnn the most common recurrent neural network is long short term memory lstm so that s what i ll refer to there are plenty of great cnn tutorials and i covered them in my previous article here i ll focus on the lstms one of the harder things to grasp about lstms is timesteps a vanilla neural network can be thought of as two timesteps if you give it hello it predicts world but it would struggle to predict more timesteps in the below example the input has four timesteps one for each word lstms are made for input with timesteps it s a neural network customized for information in order if you unroll our model it looks like this for each downward step you keep the same weights you apply one set of weights to the previous output and another set to the new input the weighted input and output are concatenated and added together with an activation this is the output for that timestep since we reuse the weights they draw information from several inputs and build knowledge of the sequence here is a simplified version of the process for each timestep in an lstm to get a feel for this logic i d recommend building an rnn from scratch with andrew trask s brilliant tutorial the number of units in each lstm layer determines it s ability to memorize this also corresponds to the size of each output feature again a feature is a long list of numbers used to transfer information between layers each unit in the lstm layer learns to keep track of different aspects of the syntax below is a visualization of a unit that keeps tracks of the information in the row div this is the simplified markup we are using to train the bootstrap model each lstm unit maintains a cell state think of the cell state as the memory the weights and activations are used to modify the state in different ways this enables the lstm layers to fine tune which information to keep and discard for each input in addition to passing through an output feature for each input it also forwards the cell states one value for each unit in the lstm to get a feel for how the components within the lstm interact i recommend colah s tutorial jayasiri s numpy implementation and karphay s lecture and write up it s tricky to find a fair way to measure the accuracy say you compare word by word if your prediction is one word out of sync you might have accuracy if you remove one word which syncs the prediction you might end up with i used the bleu score best practice in machine translating and image captioning models it breaks the sentence into four n grams from word sequences in the below prediction cat is supposed to be code to get the final score you multiply each score with the sum is then multiplied with a sentence length penalty since the length is correct in our example it becomes our final score you could increase the number of n grams to make it harder a four n gram model is the model that best corresponds to human translations i d recommend running a few examples with the below code and reading the wiki page links to sample output front end development is an ideal space to apply deep learning it s easy to generate data and the current deep learning algorithms can map most of the logic one of the most exciting areas is applying attention to lstms this will not just improve the accuracy but enable us to visualize where the cnn puts its focus as it generates the markup attention is also key for communicating between markup stylesheets scripts and eventually the backend attention layers can keep track of variables enabling the network to communicate between programming languages but in the near feature the biggest impact will come from building a scalable way to synthesize data then you can add fonts colors words and animations step by step so far most progress is happening in taking sketches and turning them into template apps in less then two years we ll be able to draw an app on paper and have the corresponding front end in less than a second there are already two working prototypes built by airbnb s design team and uizard here are some experiments to get started getting started further experiments huge thanks to tony beltramelli and jon gold for their research and ideas and for answering questions thanks to jason brownlee for his stellar keras tutorials i included a few snippets from his tutorial in the core keras implementation and beltramelli for providing the data also thanks to qingping hou charlie harrington sai soundararaj jannes klaas claudio cabral alain demenet and dylan djian for reading drafts of this this the fourth part of a multi part blog series from emil as he learns deep learning emil has spent a decade exploring human learning he s worked for oxford s business school invested in education startups and built an education technology business last year he enrolled at ecole to apply his knowledge of human learning to machine learning if you build something or get stuck ping me below or on twitter emilwallner i d love to see what you are building this was first published as a community post on floydhub s blog from a quick cheer to a standing ovation clap to show how much you enjoyed this story i study cs at paris blog and experiment with deep learning our community publishes stories worth reading on development design and data science
Gant Laborde,1300,7,https://medium.freecodecamp.org/machine-learning-how-to-go-from-zero-to-hero-40e26f8aa6da?source=---------2----------------,Machine Learning: how to go from Zero to Hero – freeCodeCamp,if your understanding of a i and machine learning is a big question mark then this is the blog post for you here i gradually increase your awesomenessicitytm by gluing inspirational videos together with friendly text sit down and relax these videos take time and if they don t inspire you to continue to the next section fair enough however if you find yourself at the bottom of this article you ve earned your well rounded knowledge and passion for this new world where you go from there is up to you a i was always cool from moving a paddle in pong to lighting you up with combos in street fighter a i has always revolved around a programmer s functional guess at how something should behave fun but programmers aren t always gifted in programming a i as we often see just google epic game fails to see glitches in a i physics and sometimes even experienced human players regardless a i has a new talent you can teach a computer to play video games understand language and even how to identify people or things this tip of the iceberg new skill comes from an old concept that only recently got the processing power to exist outside of theory i m talking about machine learning you don t need to come up with advanced algorithms anymore you just have to teach a computer to come up with its own advanced algorithm so how does something like that even work an algorithm isn t really written as much as it is sort of bred i m not using breeding as an analogy watch this short video which gives excellent commentary and animations to the high level concept of creating the a i wow right that s a crazy process now how is it that we can t even understand the algorithm when it s done one great visual was when the a i was written to beat mario games as a human we all understand how to play a side scroller but identifying the predictive strategy of the resulting a i is insane impressed there s something amazing about this idea right the only problem is we don t know machine learning and we don t know how to hook it up to video games fortunately for you elon musk already provided a non profit company to do the latter yes in a dozen lines of code you can hook up any a i you want to countless games tasks i have two good answers on why you should care firstly machine learning ml is making computers do things that we ve never made computers do before if you want to do something new not just new to you but to the world you can do it with ml secondly if you don t influence the world the world will influence you right now significant companies are investing in ml and we re already seeing it change the world thought leaders are warning that we can t let this new age of algorithms exist outside of the public eye imagine if a few corporate monoliths controlled the internet if we don t take up arms the science won t be ours i think christian heilmann said it best in his talk on ml the concept is useful and cool we understand it at a high level but what the heck is actually happening how does this work if you want to jump straight in i suggest you skip this section and move on to the next how do i get started section if you re motivated to be a doer in ml you won t need these videos if you re still trying to grasp how this could even be a thing the following video is perfect for walking you through the logic using the classic ml problem of handwriting pretty cool huh that video shows that each layer gets simpler rather than more complicated like the function is chewing data into smaller pieces that end in an abstract concept you can get your hands dirty in interacting with this process on this site by adam harley it s cool watching data go through a trained model but you can even watch your neural network get trained one of the classic real world examples of machine learning in action is the iris data set from in a presentation i attended by javafxpert s overview on machine learning i learned how you can use his tool to visualize the adjustment and back propagation of weights to neurons on a neural network you get to watch it train the neural model even if you re not a java buff the presentation jim gives on all things machine learning is a pretty cool hour introduction into ml concepts which includes more info on many of the examples above these concepts are exciting are you ready to be the einstein of this new era breakthroughs are happening every day so get started now there are tons of resources available i ll be recommending two approaches in this approach you ll understand machine learning down to the algorithms and the math i know this way sounds tough but how cool would it be to really get into the details and code this stuff from scratch if you want to be a force in ml and hold your own in deep conversations then this is the route for you i recommend that you try out brilliant org s app always great for any science lover and take the artificial neural network course this course has no time limits and helps you learn ml while killing time in line on your phone this one costs money after level combine the above with simultaneous enrollment in andrew ng s stanford course on machine learning in weeks this is the course that jim weaver recommended in his video above i ve also had this course independently suggested to me by jen looper everyone provides a caveat that this course is tough for some of you that s a show stopper but for others that s why you re going to put yourself through it and collect a certificate saying you did this course is free you only have to pay for a certificate if you want one with those two courses you ll have a lot of work to do everyone should be impressed if you make it through because that s not simple but more so if you do make it through you ll have a deep understanding of the implementation of machine learning that will catapult you into successfully applying it in new and world changing ways if you re not interested in writing the algorithms but you want to use them to create the next breathtaking website app you should jump into tensorflow and the crash course tensorflow is the de facto open source software library for machine learning it can be used in countless ways and even with javascript here s a crash course plenty more information on available courses and rankings can be found here if taking a course is not your style you re still in luck you don t have to learn the nitty gritty of ml in order to use it today you can efficiently utilize ml as a service in many ways with tech giants who have trained models ready i would still caution you that there s no guarantee that your data is safe or even yours but the offerings of services for ml are quite attractive using an ml service might be the best solution for you if you re excited and able to upload your data to amazon microsoft google i like to think of these services as a gateway drug to advanced ml either way it s good to get started now i have to say thank you to all the aforementioned people and videos they were my inspiration to get started and though i m still a newb in the ml world i m happy to light the path for others as we embrace this awe inspiring age we find ourselves in it s imperative to reach out and connect with people if you take up learning this craft without friendly faces answers and sounding boards anything can be hard just being able to ask and get a response is a game changer add me and add the people mentioned above friendly people with friendly advice helps see i hope this article has inspired you and those around you to learn ml from a quick cheer to a standing ovation clap to show how much you enjoyed this story software consultant adjunct professor published author award winning speaker mentor organizer and immature nerd d lately full of react native tech our community publishes stories worth reading on development design and data science
James JD Sutton,2200,9,https://medium.com/coinmonks/what-is-q-from-a-laymen-given-barney-style-6387b18267d2?source=---------3----------------,What is “Q” from a laymen... – Coinmonks – Medium,a bit long but i think it might help people understand qubic a bit two takeaways i took from reading qubic rev take away one if you host a q node a node that supports the q protocol layer you can earn rewards in these manners offering pow mining rigs computer or your coffee pot pos your iota s that you hold your bandwidth that you don t use probably something to do with lifi in the future so this could be your router and lightbulbs in your house and simply the previous history of running an honest node for the system all of the above can be used to pass the resource test phase all of those resources pow pos po bandwidth and po honesty are measured and quantified your resources than essentially set you in an equivalent resource pool ie in a pool with other people of similar resource power you then earn iota s from people using the oracle system smart contract or simply who want computational power which is absolutely needed to be able to outsource the iot industry which is for sure the future so what does that mean before do you remember all of the questions iota won t work because people won t run nodes because they don t get incentives like traditional blockchains well now they can and not only that q takes every aspect of each crypto and combines it all in one pos pow pobandwidth and pohonesty more so if you have asic s you are in the asic s pool gpu s you re in the gpu pool old crappy computer you re in the old crappy computer pool you stake a lot of iota you re in the high stake iota pool etc this is the process of proving your resources to the network people will purchase resources using the qubic protocol if they want quality fast or extreme computational power they have to pay remember you the user set what you want to receive in iota for your resources economic principles if you spend a month on electricity and equipment you will only charge more than a month for your resources no one would charge less so in your pool everyone will eventually come to a quorum charging a set amount and thus the economy the users will pay for it so in essence the better the pool the more the reward you get based on economic principals in society just like blockchain i don t fully understand the exact quantitative measure of what equates to the reward such as with hash power in blockchain though it seems that once you prove your resources your machine performs the calculations that are being bought on the qubic network however if your coffee pot has a jinn chip that is ternary hardware with ternary programming abra then it can sell its resources when it s not making coffee ie proving its resources and then completing computations for buyers this is just speculative and the abra ternary language will be able to interface with binary and lower the energy consumption but a significant amount when combining abra with a ternary chip such as jinn the energy efficiency is even more one of the major bottle necks or challenges that prevents advancement in technology is the amount of battery storage within machines if we can t redesign a battery to store more power at least we can redesign the energy consumption within machine devices also your autonomous car not only can offer up its pow it can also stake the iota s it is not using in its wallet the bandwidth when it isn t working or driving and the experience honesty factor by proving its resources and then selling its computational power as it may be able to be a node in itself in addition the left over electricity it has from charging up through solar or wind power it can sell through the smart grid to neighbors or local businesses your car has multiple resources and the qubic network allows machines to offer all of their resources to their owner not just one or two as with blockchain qubics revolutionizes machinery by allowing it the machinery to sell its resources this is another building block to the ultimate vision of a machine acting in a machine economy rather than us setting this up and the fees we want to charge eventually we can create smart contracts with qubic functions which then allows machines to negotiate and earn themselves the machines will sell and buy resources themselves truly creating a machine economy and if you own the machine you earn the rewards ie income passive income take away two from the above description these are only a few use cases that i take away from reading about qubics the reality is that the community will be coming up with new use cases every day for the following year probably use cases that we can t even imagine at the present time but here is my second takeaway the qubic protocol where all this is happening miners earning people staking their iota and earning ie interest or passive income because they are hodler s and by proving their resources they sell their computational power forex financial companies using qubics for quorum oracle data smart contracts being run on the protocol scientist using computational power for medical research vw fujitsu and bosch using computational power for their iot devices etc on and on all those use cases to power to power to run the network all those functions will be conducted with zero fee transactions that take place on the tangle with real time smart contract micro payments the whole system runs on data transactions zero fee transactions by sending metadata within the transaction sent on the tangle metadata essentially i m not a techie is like the language that tells the q nodes to wake up to process data pay earn and receive and essentially run the whole q network so that is a shitload of transactions occurring at the present day the amount of transactions right now occurring from trinity speculation and trading is like a drop in the ocean compared to how many transactions the qubic network will produce it s not hard to understand the qubic network will run millions if not billions of transactions per day over the tangle and remember each transaction confirms two transactions so what does that mean more transactions mean a faster tangle a more secure tangle an infinitely scalable tangle and most importantly we can take the coo coordinator offline note there may be use cases for multiple coo s coordinators or private coo s but that is a whole other arena and i simply state this because i read someone writing such an example that went right over my head the point is q is needed to remove the coo so as everyone says why don t the dev s focus working on removing the coo wen remove coo you can see that they are working on it the qubic network will support the network because it incentives people to host nodes and earn iota also if no one uses the qubic network then it doesn t work right so making corporate partners and united nations ngo affiliates partnership with banks all of this is needed to support the qubic network so here are the building blocks to the dev s vision you need a tangle zero fee transactions that can that can send meta data you need iota a transfer means of metadata and a form of payment that can buy and sell resources ie pow pos pobandwidth and pohonesty you need the qubic network creates oracles allows for quorum based computations that powers oracles you need oracles oracles power smart contracts which is the whole shabang it will change society and change global finance you need the qubic network connects users of the network with resource providers of the network enables a machine economy and provides computational power and the most advanced smart contracts to society users of the network we need a community that the iota foundation builds from hosting ama s takes the time to talk to the community on discord and provides transparency so we all can go along on their journey of completing their vision we need global partners such as bosch vw fujitsu etc we need governments and societies such as taiwan denmark and maybe sweden and we need banking like dnb and electrical companies like elaad we need the global integration to actually use the qubic network for it to work demand drives economic principals which ultimately will pay the q node providers which will drive transactions thus scaling the network lastly you need to remove the coo and let the network grow organically this can only be done when the previous steps have been completed tangle iota qubic network oracles partners coo so removing the coo is one of the last steps after removing coo the network can just grow organically on its own without much support or help from the dev s they can then work on building applications that work on top of the qubic network this is a large challenging undertaking that is being built step by step each piece is part of a large puzzle that all comes together as for the qubic vision which is what was just released is a really large damn piece of that puzzle it just goes to show that all of this adds up to removing the coo everything the dev s and the if have been doing are working towards simply that it s all one big construct not different pieces everything ties together and the qubic network is a large friggin piece of it all their sole mission is to complete the puzzle the vision so the coo can be removed and the tangle can literally change society through the machine economy this is just my non techie understanding at the moment i have a lot more research and studying to do but damn i love it so glad to be allowed within this community and enjoy the journey with the iota foundation please clarify if i totally misunderstood anything and looking forward to hear other people s understanding lastly after writing this i re read the qubic website difficult to understand but my rough understanding is that q nodes and qubics can lie dormant listening to the tangle qubics are event driven so that when one qubic initiates another qubic may need that quorum information to activate and when the one qubic gets the result it intended to compute then the that qubic itself can activate so one qubic can initiate another qubic and so on so on like neurons firing lighting up a portion of the brain which then fires more neurons this is all done through secured data streams the tangle and the q nodes and the qubic network in a way it s a global living system with the data stream as its life blood the tangle as it s bone structure and the qubics and q nodes as its neurons for all we know in the future this global mass network can be and power ai or maybe it will grow to become one massive ai source that can help society in so many ways as i stated i m a non techie i probably haven t put out a bit of misinformation as i don t fully understand it all really i just hope to ignite curiosity so people may be inspired to put a two into the new world of the machine economy as well sometimes it is hard to see the big picture the iota foundation has been working on a vision a machine to machine economy that will change society with the tangle as a standard protocol the bone structure of it all the fact is each new development is another puzzle piece or a foundation block that stacks on top of the others in the end we have the puzzle as a whole or a great structure built upon a solid foundation https twitter com iotansea https qubic iota org https www iota org https www facebook com groups iotatangle from a quick cheer to a standing ovation clap to show how much you enjoyed this story the crypto blockchain publication educate yourself about cryptocurrency blockchain developments check tutorials on solidity and smart contracts
Justin Lee,511,10,https://medium.com/swlh/the-beginners-guide-to-conversational-commerce-96f9c7dbaefb?source=---------5----------------,The beginner’s guide to conversational commerce – The Startup – Medium,your greengrocer does it so does that guy selling sunglasses on the beach it s why the funny old french bakery around the corner s been running for years conversational marketing a buzzword a footnote a revelation everyone s talking about it but what is it at its most simple it s the act of talking and more importantly listening to your customers their problems their stories their successes forging a genuine connection and using that connection to inform your marketing decisions at its most complex conversational marketing has become synonymous with cutting edge technologies for computer based dialog processing brands have always known that one to one conversations are valuable but up until very recently it was impossible to personalize these conversations at scale in real time no longer chatbots have become a mainstay of digital marketing and every day their underlying ai becomes more sophisticated gartner predicts that by of our interactions with technology will be through conversations with smart machines in his cluetrain manifesto david weinberger reminded us that that s a hundred times more true today a successful conversational marketing strategy will pair the spark of authenticity from real conversation with the emerging technologies of the future in a article chris messina distills the concept conversational converse is the process of having a real time one to one conversation with a customer or lead it s a direct personalized dialog driven approach to nurturing long term relationships collecting data and increasing sales unlike traditional digital marketing it pulls users in instead of pushing content on them it s a discourse not a lecture despite recently picking up speed conversational marketing isn t new the concept made its first appearance in with joseph jaffe s join the conversation jaffe wanted to teach marketers to re engage their customers through community partnership and dialog in the past brands have been able to talk at their customers through email website interactions and social media not with them brands have struggled to capture keep and convert attention into sales sign ups and long term loyalty engagement was passive and results were shallow customer service was relegated to a formulaic question answer scenario that was unsatisfying for everyone involved take it from leading conversational marketing platform drift s stellar report today messaging apps have over billion monthly active users and for the first time usage rates have surpassed social networks whether it s chatting with friends on whatsapp or exchanging ideas with coworkers on slack messaging has become an integral part of our lives despite extreme app saturation the average person only uses five apps regularly and you guessed it messaging apps claim these spots boasting x better open rates than the next leading digital channel these messaging platforms have huge audiences there are over billion active monthly users on the top three messaging apps like the rise of the internet or the app economy of the past decade conversational marketing is born from current desires for real time connection and genuine value conversational marketing is an umbrella term that encompasses every dialog driven tactic from opt in email marketing to customer feedback but the engine powering recent developments is artificial intelligence ai chatbots represent the new era in conversational marketing scaleable personalized real time and data driven of course these bots aren t intended to replace human to human interactions they re there to support and enhance them helping users have the right conversations with the right people at the right time for the meantime anyway according to gartner research chatbots will account for of all customer service by chatbots are a blank canvas with the potential to be molded and infused with a persona that reflects a company s values like our very own growthbot aka a mini dharmesh shah this technology is still in its infancy so most bots follow a set of rules programmed by a human via a bot building platform the differentiator is that the chatbots carry out conversations with users using natural language ai uses first person data to learn more about each customer and deliver a hyper personalized experience reps and bots can then join forces to manage these conversations at scale let s imagine i m going to a fancy party tonight it s last minute and i ve just received a message that it s black tie but i don t have the right shoes i need to quickly find a pair that is appropriate my size coherent with the rest of my outfit a good price etc i would usually google for a shop in my area then go to browse on their website to find a pair i like but other issues would soon crop up do they have my size are the shoes smart enough are they in stock i could fill out a query on the site s contact form or give them a ring but will they answer and if they don t have the right pair they are unlikely to suggest a range of alternatives the whole process is time consuming and inefficient but suppose the brand i like has a strong conversational marketing culture instead of resorting to email i would be able to conduct the conversation in seconds on my phone instantly i m given the colours sizes and styles in stock i can pay for the right shoes with a tap of a button conversational marketing enables users to get the information they need instantly without picking up the phone or engaging with a person it s not about laziness it s about ease chris messina concludes as clara de soto cofounder of reply ai told venturebeat if users are made to toggle between various apps and platforms to get the answer they need the value of the bot is moot it needs to be native to the place they spend most time whether that s slack messenger or onsite chat but it can be tricky for brands to consolidate all their conversations in one place that s why hubspot created conversations a free multi channel tool that lets businesses have one to one conversations at scale says dharmesh shah co founder and cto of hubspot we have a much lower tolerance for mistakes with machines compared to humans of people say they won t interact with a bot again after one negative experience and if a bot seems to be able to converse in english we tend to easily overestimate how capable it is that s why it s crucial to manage your customers expectations appropriately bots are far from being autonomous and people aren t easily fooled trying to present your bot as a human agent is likely to be self defeating bots don t understand context created by preceding text and conversational nuances can easily affect their capacity to answer because bots live inside messaging apps they have the potential to invade a highly personal space making the stakes of getting it right much higher according to research people use messaging apps for customer assistance with one key goal to get their problem solved fast bots should serve one simple purpose well without getting tangled up in the conversational complications that are better left to humans the way brands and users interract is undergoing a monumental shift customers are smarter and better informed than ever before they expect personalization and transparency as a prerequisite they feel empowered by their options it s hard to fool them and even harder to gain their loyalty and most significantly they want days of the year instantaneousness to be heard to be helped right now not in half an hour not tomorrow that s why conversational marketing represents a new cornerstone in marketing but also in customer service and experience branding and sales building a bot for the sake of being on trend is not enough it needs to be part of a larger strategy where each conversation has a purpose as a long term strategy intended to facilitate lasting relationships it needs to be spearheaded towards a long term goal effective conversational marketing is an intersection of brand values user engagement and valuable dialogue it s about building your audience first selling last thanks for reading originally published at blog growthbot org from a quick cheer to a standing ovation clap to show how much you enjoyed this story head of growth for growthbot messaging conversational strategy hubspot medium s largest publication for makers subscribe to receive our top stories here https goo gl zhclji
Kai Stinchcombe,44000,11,https://medium.com/@kaistinchcombe/decentralized-and-trustless-crypto-paradise-is-actually-a-medieval-hellhole-c1ca122efdec?source=---------6----------------,Blockchain is not only crappy technology but a bad vision for the future,blockchain is not only crappy technology but a bad vision for the future its failure to achieve adoption to date is because systems built on trust norms and institutions inherently function better than the type of no need for trusted parties systems blockchain envisions that s permanent no matter how much blockchain improves it is still headed in the wrong direction this december i wrote a widely circulated article on the inapplicability of blockchain to any actual problem people objected mostly not to the technology argument but rather hoped that decentralization could produce integrity let s start with this venmo is a free service to transfer dollars and bitcoin transfers are not free yet after i wrote an article last december saying bitcoin had no use someone responded that venmo and paypal are raking in consumers money and people should switch to bitcoin what a surreal contrast between blockchain s non usefulness non adoption and the conviction of its believers it s so entirely evident that this person didn t become a bitcoin enthusiast because they were looking for a convenient free way to transfer money from one person to another and discovered bitcoin in fact i would assert that there is no single person in existence who had a problem they wanted to solve discovered that an available blockchain solution was the best way to solve it and therefore became a blockchain enthusiast the number of retailers accepting cryptocurrency as a form of payment is declining and its biggest corporate boosters like ibm nasdaq fidelity swift and walmart have gone long on press but short on actual rollout even the most prominent blockchain company ripple doesn t use blockchain in its product you read that right the company ripple decided the best way to move money across international borders was to not use ripples why all the enthusiasm for something so useless in practice people have made a number of implausible claims about the future of blockchain like that you should use it for ai in place of the type of behavior tracking that google and facebook do for example this is based on a misunderstanding of what a blockchain is a blockchain isn t an ethereal thing out there in the universe that you can put things into it s a specific data structure a linear transaction log typically replicated by computers whose owners called miners are rewarded for logging new transactions there are two things that are cool about this particular data structure one is that a change in any block invalidates every block after it which means that you can t tamper with historical transactions the second is that you only get rewarded if you re working on the same chain as everyone else so each participant has an incentive to go with the consensus the end result is a shared definitive historical record and what s more because consensus is formed by each person acting in their own interest adding a false transaction or working from a different history just means you re not getting paid and everyone else is following the rules is mathematically enforced no government or police force need come in and tell you the transaction you ve logged is false or extort bribes or bully the participants it s a powerful idea so in summary here s what blockchain the technology is let s create a very long sequence of small files each one containing a hash of the previous file some new data and the answer to a difficult math problem and divide up some money every hour among anyone willing to certify and store those files for us on their computers now here s what blockchain the metaphor is what if everyone keeps their records in a tamper proof repository not owned by anyone an illustration of the difference in walmart launched a system to track its bananas and mangoes from field to store in they abandoned it because of logistical problems getting everyone to enter the data and in they re launched it to much fanfare on blockchain if someone comes to you with the mango pickers don t like doing data entry i know let s create a very long sequence of small files each one containing a hash of the previous file is a nonsense answer but what if everyone keeps their records in a tamper proof repository not owned by anyone at least addresses the right question people treat blockchain as a futuristic integrity wand wave a blockchain at the problem and suddenly your data will be valid for almost anything people want to be valid blockchain has been proposed as a solution it s true that tampering with data stored on a blockchain is hard but it s false that blockchain is a good way to create data that has integrity to understand why this is the case let s work from the practical to the theoretical for example let s consider a widely proposed use case for blockchain buying an e book with a smart contract the goal of the blockchain is you don t trust an e book vendor and they don t trust you because you re just two individuals on the internet but because it s on blockchain you ll be able to trust the transaction in the traditional system once you pay you re hoping you ll receive the book but once the vendor has your money they don t have any incentive to deliver you re relying on visa or amazon or the government to make things fair what a recipe for being a chump in contrast on a blockchain system by executing the transaction as a record in a tamper proof repository not owned by anyone the transfer of money and digital product is automatic atomic and direct with no middleman needed to arbitrate the transaction dictate terms and take a fat cut on the way isn t that better for everybody hm perhaps you are very skilled at writing software when the novelist proposes the smart contract you take an hour or two to make sure that the contract will withdraw only an amount of money equal to the agreed upon price and that the book rather than some other file or nothing at all will actually arrive auditing software is hard the most heavily scrutinized smart contract in history had a small bug that nobody noticed that is until someone did notice it and used it to steal fifty million dollars if cryptocurrency enthusiasts putting together a m investment fund can t properly audit the software how confident are you in your e book audit perhaps you would rather write your own counteroffer software contract in case this e book author has hidden a recursion bug in their version to drain your ethereum wallet of all your life savings it s a complicated way to buy a book it s not trustless you re trusting in the software and your ability to defend yourself in a software driven world instead of trusting other people another example the purported advantages for a voting system in a weakly governed country keep your voting records in a tamper proof repository not owned by anyone sounds right yet is your afghan villager going to download the blockchain from a broadcast node and decrypt the merkle root from his linux command line to independently verify that his vote has been counted or will he rely on the mobile app of a trusted third party like the nonprofit or open source consortium administering the election or providing the software these sound like stupid examples novelists and villagers hiring e bodyguard hackers to protect them from malicious customers and nonprofits whose clever smart contracts might steal their money and votes until you realize that s actually the point instead of relying on trust or regulation in the blockchain world individuals are on purpose responsible for their own security precautions and if the software they use is malicious or buggy they should have read the software more carefully you actually see it over and over again blockchain systems are supposed to be more trustworthy but in fact they are the least trustworthy systems in the world today in less than a decade three successive top bitcoin exchanges have been hacked another is accused of insider trading the demonstration project dao smart contract got drained crypto price swings are ten times those of the world s most mismanaged currencies and bitcoin the killer app of crypto transparency is almost certainly artificially propped up by fake transactions involving billions of literally imaginary dollars blockchain systems do not magically make the data in them accurate or the people entering the data trustworthy they merely enable you to audit whether it has been tampered with a person who sprayed pesticides on a mango can still enter onto a blockchain system that the mangoes were organic a corrupt government can create a blockchain system to count the votes and just allocate an extra million addresses to their cronies an investment fund whose charter is written in software can still misallocate funds how then is trust created in the case of buying an e book even if you re buying it with a smart contract instead of auditing the software you ll rely on one of four things each of them characteristics of the old way either the author of the smart contract is someone you know of and trust the seller of the e book has a reputation to uphold you or friends of yours have bought e books from this seller in the past successfully or you re just willing to hope that this person will deal fairly in each case even if the transaction is effectuated via a smart contract in practice you re relying on trust of a counterparty or middleman not your self protective right to audit the software each man an island unto himself the contract still works but the fact that the promise is written in auditable software rather than government enforced english makes it less transparent not more transparent the same for the vote counting before blockchain can even get involved you need to trust that voter registration is done fairly that ballots are given only to eligible voters that the votes are made anonymously rather than bought or intimidated that the vote displayed by the balloting system is the same as the vote recorded and that no extra votes are given to the political cronies to cast blockchain makes none of these problems easier and many of them harder but more importantly solving them in a blockchain context requires a set of awkward workarounds that undermine the core premise so we know the entries are valid let s allow only trusted nonprofits to make entries and you re back at the good old classic ledger in fact if you look at any blockchain solution inevitably you ll find an awkward workaround to re create trusted parties in a trustless world yet absent these old way factors supposing you actually attempted to rely on blockchain s self interest self protection to build a real system you d be in a real mess eight hundred years ago in europe with weak governments unable to enforce laws and trusted counterparties few fragile and far between theft was rampant safe banking was a fantasy and personal security was at the point of the sword this is what somalia looks like now and also what it looks like to transact on the blockchain in the ideal scenario somalia on purpose that s the vision nobody wants it even the most die hard crypto enthusiasts prefer in practice to rely on trust rather than their own crypto medieval systems of bitcoins are mined by managed consortiums yet none of the consortiums use smart contracts to manage payouts instead they promise things like a long history of stable and accurate payouts sounds like a trustworthy middleman same with silk road a cryptocurrency driven online drug bazaar the key to silk road wasn t the bitcoins that was just to evade government detection it was the reputation scores that allowed people to trust criminals and the reputation scores weren t tracked on a tamper proof blockchain they were tracked by a trusted middleman if ripple silk road slush pool and the dao all prefer old way systems of creating and enforcing trust it s no wonder that the outside world had not adopted trustless systems either a decentralized tamper proof repository sounds like a great way to audit where your mango comes from how fresh it is and whether it has been sprayed with pesticides or not but actually laws on food labeling nonprofit or government inspectors an independent trusted free press empowered workers who trust whistleblower protections credible grocery stores your local nonprofit farmer s market and so on do a way better job people who actually care about food safety do not adopt blockchain because trusted is better than trustless blockchain s technology mess exposes its metaphor mess a software engineer pointing out that storing the data a sequence of small hashed files won t get the mango pickers to accurately report whether they sprayed pesticides is also pointing out why peer to peer interaction with no regulations norms middlemen or trusted parties is actually a bad way to empower people like the farmer s market or the organic labeling standard so many real ideas are hiding in plain sight do you wish there was a type of financial institution that was secure and well regulated in all the traditional ways but also has the integrity of being people powered a credit union s members elect its directors and the transaction processing revenue is divided up among the members move your money prefer a deflationary monetary policy central bankers are appointed by elected leaders want to make elections more secure and democratic help write open source voting software go out and register voters or volunteer as an election observer here or abroad wish there was a trusted e book delivery service that charged lower transaction fees and distributed more of the earnings to the authors you can already consider stated payout rates when you buy music or books buy directly from the authors or start your own e book site that s even better than what s out there projects based on the elimination of trust have failed to capture customers interest because trust is actually so damn valuable a lawless and mistrustful world where self interest is the only principle and paranoia is the only source of safety is a not a paradise but a crypto medieval hellhole as a society and as technologists and entrepreneurs in particular we re going to have to get good at cooperating at building trust and at being trustworthy instead of directing resources to the elimination of trust we should direct our resources to the creation of trust whether we use a long series of sequentially hashed files as our storage medium or not kai stinchcombe coined the terms crypto medieval futuristic integrity wand and smart mango please use freely coining terms makes you a futurist from a quick cheer to a standing ovation clap to show how much you enjoyed this story whatever the opposite of a futurist is
savedroid ICO,340,3,https://medium.com/@ico_8796/sneakpeek-the-savedroid-crypto-saving-app-part-1-your-wish-64d1f7308518?source=---------7----------------,#SNEAKPEEK The savedroid crypto saving app — Part #1: Your wish,the international beta of our brand new crypto saving app is coming soon the beta app will be launched in english language and will exclusively be available for our ico token buyers only now get ready to learn more about the savedroid crypto saving app even before its official release today we give you a very first sneak peek of one of its core features your wish with savedroid you can save up for your personal goals you want to afford in the future your own lambo or your desired moon exactly that is your wish so using the savedroid crypto saving app is not just about piling up a fortune it s all about saving up for your personal wishes which you are aspiring to fulfill but can t afford right now there are simple steps to set up your wish in less than one minute what first name your wish and select one of our illustrations to always keep you motivated to continue saving you can go small and save for your new pair of hipster sneakers or you may go big and start a crypto savings plan for your new family home everything is possible only the moon is the limit at least for now how much then set the amount you need to save up to afford your wish the amount is denominated in fiat currency as it is the prevailing means of payment by the way that makes it a lot easier for you as you don t need to do the math converting fiat to crypto and vice versa this complex task is on us when finally select the date by when you want to fulfil your wish and you are done that was easy just as easy as savedroid s other features will be to deliver on our mission to democratize crypto and bring cryptocurrencies to the masses to keep you posted on our latest product updates we have started this new sneakpeek series here we will provide you regular sneak peeks on our hottest new features stay tuned and follow our blog from a quick cheer to a standing ovation clap to show how much you enjoyed this story the savedroid ico cryptocurrencies for everyone now give power to the people join the revolution https ico savedroid com
Brandon Morelli,221,5,https://techburst.io/artificial-intelligence-top-10-articles-june-2018-4b3fa7572b46?source=---------8----------------,Artificial Intelligence Top 10 Articles — June 2018,here s what s trending this month in artificial intelligence topics include whether you re experienced with artificial intelligence or a newbie looking to learn the basics of ai there s something for everyone on this list disclosure we receive compensation from the courses we feature stars hours of video students build an ai that combines the power of data science machine learning and deep learning to create powerful ai for real world applications you will also have the chance to understand the story behind artificial intelligence learn more stars hours of video students completely understand the relationship between reinforcement learning and psychology and on a technical level apply gradient based supervised machine learning methods to reinforcement learning and implement different reinforcement learning algorithms learn more by lance ulanoff have you heard about the google duplex yet it s pretty much the talk all over the internet google ceo sundar pichai has dropped its biggest bomb when they introduced google duplex to all take a look more on this story to know more by irhum shafkat understanding convolutions can often feel a bit unnerving yet it s concept is fascinatingly powerful and highly extensible let s try to break down the mechanics of the convolution operation step by step relate and explore it s hierarchy into a more powerful one by wisewolf fund ai is already shaping the economy and in the near future its effect may be even more significant ignoring the new technology and its influence on the global economic situation is a recipe for failure read more of this article now by sam drozdov machine learning is a field of study that gives computers the ability to learn without being explicitly programmed learn the basics of machine learning and how to apply it to the products you are building right now by aman dalmia having a great opportunity to interact with great minds is one of the awesome privileges one can keep to their knowledge and motivate them to avoid the mistakes in a much better manner by simon greenman welcome to ai gold rush check out this awesome article that talks about how companies and startups make money on ai and how it helps the economic growth as well by justin lee are chatbots hype over already find out why our industry massively overestimated the initial impact chatbots would have and a lot more reasons why chatbots are not on the trend anymore by daniel jeffries ai could mean the end of all jobs for most people and that s just terrifying right check out this topic to get to know more about how will ai bring an explosion to new jobs by george seif learn more about google s automl a suite of machine learning tools that will allow one to easily train high performance deep networks without requiring the user to have any knowledge in ai by james loy understand the inner workings of deep learning through python with neural network know and train more about neural network from scratch from a quick cheer to a standing ovation clap to show how much you enjoyed this story creator of codeburstio frequently posting web development tutorials articles follow me on twitter too brandonmorelli bursts of tech to power through your day
Sarthak Jain,3900,10,https://medium.com/nanonets/how-to-easily-detect-objects-with-deep-learning-on-raspberrypi-225f29635c74?source=---------9----------------,How to easily Detect Objects with Deep Learning on Raspberry Pi,disclaimer i m building nanonets com to help build ml with less data and no hardware the raspberry pi is a neat piece of hardware that has captured the hearts of a generation with m devices sold with hackers building even cooler projects on it given the popularity of deep learning and the raspberry pi camera we thought it would be nice if we could detect any object using deep learning on the pi now you will be able to detect a photobomber in your selfie someone entering harambe s cage where someone kept the sriracha or an amazon delivery guy entering your house m years of evolution have made human vision fairly evolved the human brain has of it s neurons work on processing vision as compared with percent for touch and just percent for hearing humans have two major advantages when compared with machines one is stereoscopic vision the second is an almost infinite supply of training data an infant of years has had approximately b images sampled at fps to mimic human level performance scientists broke down the visual perception task into four different categories object detection has been good enough for a variety of applications even though image segmentation is a much more precise result it suffers from the complexity of creating training data it typically takes a human annotator x more time to segment an image than draw bounding boxes this is more anecdotal and lacks a source also after detecting objects it is separately possible to segment the object from the bounding box object detection is of significant practical importance and has been used across a variety of industries some of the examples are mentioned below object detection can be used to answer a variety of questions these are the broad categories there are a variety of models architectures that are used for object detection each with trade offs between speed size and accuracy we picked one of the most popular ones yolo you only look once and have shown how it works below in under lines of code if you ignore the comments note this is pseudo code not intended to be a working example it has a black box which is the cnn part of it which is fairly standard and shown in the image below you can read the full paper here https pjreddie com media files papers yolo pdf for this task you probably need a few images per object try to capture data as close to the data you re going to finally make predictions on draw bounding boxes on the images you can use a tool like labelimg you will typically need a few people who will be working on annotating your images this is a fairly intensive and time consuming task you can read more about this at medium com nanonets nanonets how to use deep learning when you have limited data f c b cab you need a pretrained model so you can reduce the amount of data required to train without it you might need a few k images to train the model you can find a bunch of pretrained models here the process of training a model is unnecessarily difficult to simplify the process we created a docker image would make it easy to train to start training the model you can run the docker image has a run sh script that can be called with the following parameters you can find more details at to train a model you need to select the right hyper parameters finding the right parameters the art of deep learning involves a little bit of hit and try to figure out which are the best parameters to get the highest accuracy for your model there is some level of black magic associated with this along with a little bit of theory this is a great resource for finding the right parameters quantize model make it smaller to fit on a small device like the raspberry pi or mobile small devices like mobile phones and rasberry pi have very little memory and computation power training neural networks is done by applying many tiny nudges to the weights and these small increments typically need floating point precision to work though there are research efforts to use quantized representations here too taking a pre trained model and running inference is very different one of the magical qualities of deep neural networks is that they tend to cope very well with high levels of noise in their inputs why quantize neural network models can take up a lot of space on disk with the original alexnet being over mb in float format for example almost all of that size is taken up with the weights for the neural connections since there are often many millions of these in a single model the nodes and weights of a neural network are originally stored as bit floating point numbers the simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer and then compressing each float value to an eight bit integer the size of the files is reduced by code for quantization you need the raspberry pi camera live and working then capture a new image for instructions on how to install checkout this link download model once your done training the model you can download it on to your pi to export the model run then download the model onto the raspberry pi install tensorflow on the raspberry pi depending on your device you might need to change the installation a little run model for predicting on the new image the raspberry pi has constraints on both memory and compute a version of tensorflow compatible with the raspberry pi gpu is still not available therefore it is important to benchmark how much time do each of the models take to make a prediction on a new image we have removed the need to annotate images we have expert annotators who will annotate your images for you we automatically train the best model for you to achieve this we run a battery of model with different parameters to select the best for your data nanonets is entirely in the cloud and runs without using any of your hardware which makes it much easier to use since devices like the raspberry pi and mobile phones were not built to run complex compute heavy tasks you can outsource the workload to our cloud which does all of the compute for you get your free api key from http app nanonets com user api key collect the images of object you want to detect you can annotate them either using our web ui https app nanonets com objectannotation appid your model id or use open source tool like labelimg once you have dataset ready in folders images image files and annotations annotations for the image files start uploading the dataset once the images have been uploaded begin training the model the model takes hours to train you will get an email once the model is trained in the meanwhile you check the state of the model once the model is trained you can make predictions using the model from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder ceo nanonets com nanonets machine learning api
Dr. GP Pulipaka,2,6,https://medium.com/@gp_pulipaka/3-ways-to-apply-latent-semantic-analysis-on-large-corpus-text-on-macos-terminal-jupyterlab-colab-7b4dc3e1622?source=---------5----------------,"3 Ways to Apply Latent Semantic Analysis on Large-Corpus Text on macOS Terminal, JupyterLab, and...",latent semantic analysis works on large scale datasets to generate representations to discover the insights through natural language processing there are different approaches to perform the latent semantic analysis at multiple levels such as document level phrase level and sentence level primarily semantic analysis can be summarized into lexical semantics and the study of combining individual words into paragraphs or sentences the lexical semantics classifies and decomposes the lexical items applying lexical semantic structures has different contexts to identify the differences and similarities between the words a generic term in a paragraph or a sentence is hypernym and hyponymy provides the meaning of the relationship between instances of the hyponyms homonyms contain similar syntax or similar spelling with similar structuring with different meanings homonyms are not related to each other book is an example for homonym it can mean for someone to read something or an act of making a reservation with similar spelling form and syntax however the definition is different polysemy is another phenomenon of the words where a single word could be associated with multiple related senses and distinct meanings the word polysemy is a greek word which means many signs python provides nltk library to perform tokenization of the words by chopping the words in larger chunks into phrases or meaningful strings processing words through tokenization produce tokens word lemmatization converts words from the current inflected form into the base form latent semantic analysis applying latent semantic analysis on large datasets of text and documents represents the contextual meaning through mathematical and statistical computation methods on large corpus of text many times latent semantic analysis overtook human scores and subject matter tests conducted by humans the accuracy of latent semantic analysis is high as it reads through machine readable documents and texts at a web scale latent semantic analysis is a technique that applies singular value decomposition and principal component analysis pca the document can be represented with z x y matrix a the rows of the matrix represent the document in the collection the matrix a can represent numerous hundred thousands of rows and columns on a typical large corpus text document applying singular value decomposition develops a set of operations dubbed matrix decomposition natural language processing in python with nltk library applies a low rank approximation to the term document matrix later the low rank approximation aids in indexing and retrieving the document known as latent semantic indexing by clustering the number of words in the document brief overview of linear algebra the a with z x y matrix contains the real valued entries with non negative values for the term document matrix determining the rank of the matrix comes with the number of linearly independent columns or rows in the the matrix the rank of a z y a square c x c represented as diagonal matrix where off diagonal entries are zero examining the matrix if all the c diagonal matrices are one the identity matrix of the dimension c represented by ic for the square z x z matrix a with a vector k which contains not all zeroes for the matrix decomposition applies on the square matrix factored into the product of matrices from eigenvectors this allows to reduce the dimensionality of the words from multi dimensions to two dimensions to view on the plot the dimensionality reduction techniques with principal component analysis and singular value decomposition holds critical relevance in natural language processing the zipfian nature of the frequency of the words in a document makes it difficult to determine the similarity of the words in a static stage hence eigen decomposition is a by product of singular value decomposition as the input of the document is highly asymmetrical the latent semantic analysis is a particular technique in semantic space to parse through the document and identify the words with polysemy with nlkt library the resources such as punkt and wordnet have to be downloaded from nltk deep learning at scale with google colab notebooks training machine learning or deep learning models on cpus could take hours and could be pretty expensive in terms of the programming language efficiency with time and energy of the computer resources google built colab notebooks environment for research and development purposes it runs entirely on the cloud without requiring any additional hardware or software setup for each machine it s entirely equivalent of a jupyter notebook that aids the data scientists to share the colab notebooks by storing on google drive just like any other google sheets or documents in a collaborative environment there are no additional costs associated with enabling gpu at runtime for acceleration on the runtime there are some challenges of uploading the data into colab unlike jupyter notebook that can access the data directly from the local directory of the machine in colab there are multiple options to upload the files from the local file system or a drive can be mounted to load the data through drive fuse wrapper once this step is complete it shows the following log without errors the next step would be generating the authentication tokens to authenticate the google credentials for the drive and colab if it shows successful retrieval of access token then colab is all set at this stage the drive is not mounted yet it will show false when accessing the contents of the text file once the drive is mounted colab has access to the datasets from google drive once the files are accessible the python can be executed similar to executing in jupyter environment colab notebook also displays the results similar to what we see on jupyter notebook pycharm ide the program can be run compiled on pycharm ide environment and run on pycharm or can be executed from osx terminal results from osx terminal jupyter notebook on standalone machine jupyter notebook gives a similar output running the latent semantic analysis on the local machine references gorrell g generalized hebbian algorithm for incremental singular value decomposition in natural language processing retrieved from https www aclweb org anthology e hardeniya n natural language processing python and nltk birmingham england packt publishing landauer t k foltz p w laham d university of colorado at boulder an introduction to latent semantic analysis retrieved from http lsa colorado edu papers dp lsaintro pdf stackoverflow mounting google drive on google colab retrieved from https stackoverflow com questions mounting google drive on google colab stanford university matrix decompositions and latent semantic indexing retrieved from https nlp stanford edu ir book html htmledition matrix decompositions and latent semantic indexing html from a quick cheer to a standing ovation clap to show how much you enjoyed this story ganapathi pulipaka founder and ceo deepsingularity bestselling author big data iot startups sap machinelearning deeplearning datascience
Gabriel Jiménez,50,5,https://medium.com/aimarketingassociation/chatbots-could-we-talk-edd6ccbd8f5a?source=---------7----------------,"Chatbots, could we talk? – AIMA: AI Marketing Magazine – Medium",after the euphoria for apps the trend is reversing every day we download fewer new apps and we keep with few in constant use a lot has happened since apple in proclaimed that there was an app for everything as in the next commercial the chat boom according to the report of the internet association on the habits of internet users in me xico the second social network used by mexicans is whatsapp a messaging app and the first although the report has no separate data is facebook which also includes facebook messenger as a particular fact both are from facebook as is instagram that is in position on the list the customer experience in issues such as support attention or navigation in telephone menus and the transition we have made from voice calls to text messages both for practicity and cost have catalyzed the technological development of so called virtual agents or chatbots to optimize resources and improve customer service in an environment where an immediate response is the minimum that is expected the best option to improve customer service at the lowest cost is through a chatbot but what is a chatbot it is a computer program which works either through rules or the most advanced using artificial intelligence the way to interact with them is via a chat with rules chatbots that work with rules have limited functionality respond only to specific commands if you do not write correctly what you want it does not understand it with artificial intelligence on the other hand assistants who use artificial intelligence can understand what you say in any way you write it even if you do it incorrectly abbreviated or with idiomatic expressions they are also able to improve over time learning the way people express themselves and how they ask context and memory chatbots that use artificial intelligence can resume a previous conversation or based on the context of the chat move forward in a coherent manner if for example we are looking for a movie to see in the cinema and first ask us the cinema we want to go to and then the movie then we change the movie and then the chatbot will assume that we continue talking about the same cinema unless we specify otherwise the above may seem very simple for us as people but for a chatbot to maintain a coherent and fluid conversation it is a huge achievement and one that brings great value channels a chatbot can be integrated into any chat application whether corporate your website or commercial like facebook messenger or whatsapp limitations one of the challenges faced by chatbots is the initial adoption they may fail mainly for reasons as a result of not adequately delimiting its initial scope we want to resolve all the possible issues with the chatbot that deals with complaints that supports that sells that generates interaction with customers that gives service status this causes as with any project scope creep endless requirements which makes it seem that the project never will work appropiately it is not linked to an activity that solves a business issue sometimes they apply to trivial situations or that do not have a relevant metric linked so it is impossible to measure their effectiveness and quantify their benefits to the business being a new technology we tend to think that since it has intelligence it can answer any question outside the business context for which it was defined thus also losing the initial focus and evaluating its performance outside the scope for which it was created it is important to remember that although it has artificial intelligence every bot has to have a period of learning and evolution and this takes time its process is similar to that carried by a child when it begins to learn it makes mistakes there are terms or forms of expression that it does not know but as time passes it becomes more and more ready due to the experience it acquires with each conversation the same it happens with the chatbot hand over that is why there always has to be a process to re direct a human operator to a conversation in which the chatbot is not able to respond satisfactorily in this way we keep the customer experience as a principle and we avoid frustrations to people connection with systems the chatbot can give an integral attention to clients through chat but its capacity to do it also depends on the integration that this one has with the systems of the company without this the service you provide will be incomplete and frustrating for example if we have a chatbot to schedule appointments we need that in addition to understanding what people ask you can access the agenda system to check if there is time available to schedule if you do not have it you will be limited and it will be practically useless applications the main change when using a chatbot is that instead of browsing websites we can ask to get what we want it is even possible to obtain recommendations based on questions to find the most appropriate for us benefits to know about chatbots and artificial intelligence write to me gabojimenez or linkedin com in gabrieljimenezmunoz from a quick cheer to a standing ovation clap to show how much you enjoyed this story consultative selling ai for business chatbots analytics speaker writer teacher driving the ai marketing movement
Kai Stinchcombe,44000,11,https://medium.com/@kaistinchcombe/decentralized-and-trustless-crypto-paradise-is-actually-a-medieval-hellhole-c1ca122efdec?source=tag_archive---------0----------------,Blockchain is not only crappy technology but a bad vision for the future,blockchain is not only crappy technology but a bad vision for the future its failure to achieve adoption to date is because systems built on trust norms and institutions inherently function better than the type of no need for trusted parties systems blockchain envisions that s permanent no matter how much blockchain improves it is still headed in the wrong direction this december i wrote a widely circulated article on the inapplicability of blockchain to any actual problem people objected mostly not to the technology argument but rather hoped that decentralization could produce integrity let s start with this venmo is a free service to transfer dollars and bitcoin transfers are not free yet after i wrote an article last december saying bitcoin had no use someone responded that venmo and paypal are raking in consumers money and people should switch to bitcoin what a surreal contrast between blockchain s non usefulness non adoption and the conviction of its believers it s so entirely evident that this person didn t become a bitcoin enthusiast because they were looking for a convenient free way to transfer money from one person to another and discovered bitcoin in fact i would assert that there is no single person in existence who had a problem they wanted to solve discovered that an available blockchain solution was the best way to solve it and therefore became a blockchain enthusiast the number of retailers accepting cryptocurrency as a form of payment is declining and its biggest corporate boosters like ibm nasdaq fidelity swift and walmart have gone long on press but short on actual rollout even the most prominent blockchain company ripple doesn t use blockchain in its product you read that right the company ripple decided the best way to move money across international borders was to not use ripples why all the enthusiasm for something so useless in practice people have made a number of implausible claims about the future of blockchain like that you should use it for ai in place of the type of behavior tracking that google and facebook do for example this is based on a misunderstanding of what a blockchain is a blockchain isn t an ethereal thing out there in the universe that you can put things into it s a specific data structure a linear transaction log typically replicated by computers whose owners called miners are rewarded for logging new transactions there are two things that are cool about this particular data structure one is that a change in any block invalidates every block after it which means that you can t tamper with historical transactions the second is that you only get rewarded if you re working on the same chain as everyone else so each participant has an incentive to go with the consensus the end result is a shared definitive historical record and what s more because consensus is formed by each person acting in their own interest adding a false transaction or working from a different history just means you re not getting paid and everyone else is following the rules is mathematically enforced no government or police force need come in and tell you the transaction you ve logged is false or extort bribes or bully the participants it s a powerful idea so in summary here s what blockchain the technology is let s create a very long sequence of small files each one containing a hash of the previous file some new data and the answer to a difficult math problem and divide up some money every hour among anyone willing to certify and store those files for us on their computers now here s what blockchain the metaphor is what if everyone keeps their records in a tamper proof repository not owned by anyone an illustration of the difference in walmart launched a system to track its bananas and mangoes from field to store in they abandoned it because of logistical problems getting everyone to enter the data and in they re launched it to much fanfare on blockchain if someone comes to you with the mango pickers don t like doing data entry i know let s create a very long sequence of small files each one containing a hash of the previous file is a nonsense answer but what if everyone keeps their records in a tamper proof repository not owned by anyone at least addresses the right question people treat blockchain as a futuristic integrity wand wave a blockchain at the problem and suddenly your data will be valid for almost anything people want to be valid blockchain has been proposed as a solution it s true that tampering with data stored on a blockchain is hard but it s false that blockchain is a good way to create data that has integrity to understand why this is the case let s work from the practical to the theoretical for example let s consider a widely proposed use case for blockchain buying an e book with a smart contract the goal of the blockchain is you don t trust an e book vendor and they don t trust you because you re just two individuals on the internet but because it s on blockchain you ll be able to trust the transaction in the traditional system once you pay you re hoping you ll receive the book but once the vendor has your money they don t have any incentive to deliver you re relying on visa or amazon or the government to make things fair what a recipe for being a chump in contrast on a blockchain system by executing the transaction as a record in a tamper proof repository not owned by anyone the transfer of money and digital product is automatic atomic and direct with no middleman needed to arbitrate the transaction dictate terms and take a fat cut on the way isn t that better for everybody hm perhaps you are very skilled at writing software when the novelist proposes the smart contract you take an hour or two to make sure that the contract will withdraw only an amount of money equal to the agreed upon price and that the book rather than some other file or nothing at all will actually arrive auditing software is hard the most heavily scrutinized smart contract in history had a small bug that nobody noticed that is until someone did notice it and used it to steal fifty million dollars if cryptocurrency enthusiasts putting together a m investment fund can t properly audit the software how confident are you in your e book audit perhaps you would rather write your own counteroffer software contract in case this e book author has hidden a recursion bug in their version to drain your ethereum wallet of all your life savings it s a complicated way to buy a book it s not trustless you re trusting in the software and your ability to defend yourself in a software driven world instead of trusting other people another example the purported advantages for a voting system in a weakly governed country keep your voting records in a tamper proof repository not owned by anyone sounds right yet is your afghan villager going to download the blockchain from a broadcast node and decrypt the merkle root from his linux command line to independently verify that his vote has been counted or will he rely on the mobile app of a trusted third party like the nonprofit or open source consortium administering the election or providing the software these sound like stupid examples novelists and villagers hiring e bodyguard hackers to protect them from malicious customers and nonprofits whose clever smart contracts might steal their money and votes until you realize that s actually the point instead of relying on trust or regulation in the blockchain world individuals are on purpose responsible for their own security precautions and if the software they use is malicious or buggy they should have read the software more carefully you actually see it over and over again blockchain systems are supposed to be more trustworthy but in fact they are the least trustworthy systems in the world today in less than a decade three successive top bitcoin exchanges have been hacked another is accused of insider trading the demonstration project dao smart contract got drained crypto price swings are ten times those of the world s most mismanaged currencies and bitcoin the killer app of crypto transparency is almost certainly artificially propped up by fake transactions involving billions of literally imaginary dollars blockchain systems do not magically make the data in them accurate or the people entering the data trustworthy they merely enable you to audit whether it has been tampered with a person who sprayed pesticides on a mango can still enter onto a blockchain system that the mangoes were organic a corrupt government can create a blockchain system to count the votes and just allocate an extra million addresses to their cronies an investment fund whose charter is written in software can still misallocate funds how then is trust created in the case of buying an e book even if you re buying it with a smart contract instead of auditing the software you ll rely on one of four things each of them characteristics of the old way either the author of the smart contract is someone you know of and trust the seller of the e book has a reputation to uphold you or friends of yours have bought e books from this seller in the past successfully or you re just willing to hope that this person will deal fairly in each case even if the transaction is effectuated via a smart contract in practice you re relying on trust of a counterparty or middleman not your self protective right to audit the software each man an island unto himself the contract still works but the fact that the promise is written in auditable software rather than government enforced english makes it less transparent not more transparent the same for the vote counting before blockchain can even get involved you need to trust that voter registration is done fairly that ballots are given only to eligible voters that the votes are made anonymously rather than bought or intimidated that the vote displayed by the balloting system is the same as the vote recorded and that no extra votes are given to the political cronies to cast blockchain makes none of these problems easier and many of them harder but more importantly solving them in a blockchain context requires a set of awkward workarounds that undermine the core premise so we know the entries are valid let s allow only trusted nonprofits to make entries and you re back at the good old classic ledger in fact if you look at any blockchain solution inevitably you ll find an awkward workaround to re create trusted parties in a trustless world yet absent these old way factors supposing you actually attempted to rely on blockchain s self interest self protection to build a real system you d be in a real mess eight hundred years ago in europe with weak governments unable to enforce laws and trusted counterparties few fragile and far between theft was rampant safe banking was a fantasy and personal security was at the point of the sword this is what somalia looks like now and also what it looks like to transact on the blockchain in the ideal scenario somalia on purpose that s the vision nobody wants it even the most die hard crypto enthusiasts prefer in practice to rely on trust rather than their own crypto medieval systems of bitcoins are mined by managed consortiums yet none of the consortiums use smart contracts to manage payouts instead they promise things like a long history of stable and accurate payouts sounds like a trustworthy middleman same with silk road a cryptocurrency driven online drug bazaar the key to silk road wasn t the bitcoins that was just to evade government detection it was the reputation scores that allowed people to trust criminals and the reputation scores weren t tracked on a tamper proof blockchain they were tracked by a trusted middleman if ripple silk road slush pool and the dao all prefer old way systems of creating and enforcing trust it s no wonder that the outside world had not adopted trustless systems either a decentralized tamper proof repository sounds like a great way to audit where your mango comes from how fresh it is and whether it has been sprayed with pesticides or not but actually laws on food labeling nonprofit or government inspectors an independent trusted free press empowered workers who trust whistleblower protections credible grocery stores your local nonprofit farmer s market and so on do a way better job people who actually care about food safety do not adopt blockchain because trusted is better than trustless blockchain s technology mess exposes its metaphor mess a software engineer pointing out that storing the data a sequence of small hashed files won t get the mango pickers to accurately report whether they sprayed pesticides is also pointing out why peer to peer interaction with no regulations norms middlemen or trusted parties is actually a bad way to empower people like the farmer s market or the organic labeling standard so many real ideas are hiding in plain sight do you wish there was a type of financial institution that was secure and well regulated in all the traditional ways but also has the integrity of being people powered a credit union s members elect its directors and the transaction processing revenue is divided up among the members move your money prefer a deflationary monetary policy central bankers are appointed by elected leaders want to make elections more secure and democratic help write open source voting software go out and register voters or volunteer as an election observer here or abroad wish there was a trusted e book delivery service that charged lower transaction fees and distributed more of the earnings to the authors you can already consider stated payout rates when you buy music or books buy directly from the authors or start your own e book site that s even better than what s out there projects based on the elimination of trust have failed to capture customers interest because trust is actually so damn valuable a lawless and mistrustful world where self interest is the only principle and paranoia is the only source of safety is a not a paradise but a crypto medieval hellhole as a society and as technologists and entrepreneurs in particular we re going to have to get good at cooperating at building trust and at being trustworthy instead of directing resources to the elimination of trust we should direct our resources to the creation of trust whether we use a long series of sequentially hashed files as our storage medium or not kai stinchcombe coined the terms crypto medieval futuristic integrity wand and smart mango please use freely coining terms makes you a futurist from a quick cheer to a standing ovation clap to show how much you enjoyed this story whatever the opposite of a futurist is
Dhruv Parthasarathy,4300,12,https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4?source=tag_archive---------1----------------,A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN,at athelas we use convolutional neural networks cnns for a lot more than just classification in this post we ll see how cnns can be used with great results in image instance segmentation ever since alex krizhevsky geoff hinton and ilya sutskever won imagenet in convolutional neural networks cnns have become the gold standard for image classification in fact since then cnns have improved to the point where they now outperform humans on the imagenet challenge while these results are impressive image classification is far simpler than the complexity and diversity of true human visual understanding in classification there s generally an image with a single object as the focus and the task is to say what that image is see above but when we look at the world around us we carry out far more complex tasks we see complicated sights with multiple overlapping objects and different backgrounds and we not only classify these different objects but also identify their boundaries differences and relations to one another can cnns help us with such complex tasks namely given a more complicated image can we use cnns to identify the different objects in the image and their boundaries as has been shown by ross girshick and his peers over the last few years the answer is conclusively yes through this post we ll cover the intuition behind some of the main techniques used in object detection and segmentation and see how they ve evolved from one implementation to the next in particular we ll cover r cnn regional cnn the original application of cnns to this problem along with its descendants fast r cnn and faster r cnn finally we ll cover mask r cnn a paper released recently by facebook research that extends such object detection techniques to provide pixel level segmentation here are the papers referenced in this post inspired by the research of hinton s lab at the university of toronto a small team at uc berkeley led by professor jitendra malik asked themselves what today seems like an inevitable question object detection is the task of finding the different objects in an image and classifying them as seen in the image above the team comprised of ross girshick a name we ll see again jeff donahue and trevor darrel found that this problem can be solved with krizhevsky s results by testing on the pascal voc challenge a popular object detection challenge akin to imagenet they write let s now take a moment to understand how their architecture regions with cnns r cnn works understanding r cnn the goal of r cnn is to take in an image and correctly identify where the main objects via a bounding box in the image but how do we find out where these bounding boxes are r cnn does what we might intuitively do as well propose a bunch of boxes in the image and see if any of them actually correspond to an object r cnn creates these bounding boxes or region proposals using a process called selective search which you can read about here at a high level selective search shown in the image above looks at the image through windows of different sizes and for each size tries to group together adjacent pixels by texture color or intensity to identify objects once the proposals are created r cnn warps the region to a standard square size and passes it through to a modified version of alexnet the winning submission to imagenet that inspired r cnn as shown above on the final layer of the cnn r cnn adds a support vector machine svm that simply classifies whether this is an object and if so what object this is step in the image above improving the bounding boxes now having found the object in the box can we tighten the box to fit the true dimensions of the object we can and this is the final step of r cnn r cnn runs a simple linear regression on the region proposal to generate tighter bounding box coordinates to get our final result here are the inputs and outputs of this regression model so to summarize r cnn is just the following steps r cnn works really well but is really quite slow for a few simple reasons in ross girshick the first author of r cnn solved both these problems leading to the second algorithm in our short history fast r cnn let s now go over its main insights fast r cnn insight roi region of interest pooling for the forward pass of the cnn girshick realized that for each image a lot of proposed regions for the image invariably overlapped causing us to run the same cnn computation again and again times his insight was simple why not run the cnn just once per image and then find a way to share that computation across the proposals this is exactly what fast r cnn does using a technique known as roipool region of interest pooling at its core roipool shares the forward pass of a cnn for an image across its subregions in the image above notice how the cnn features for each region are obtained by selecting a corresponding region from the cnn s feature map then the features in each region are pooled usually using max pooling so all it takes us is one pass of the original image as opposed to fast r cnn insight combine all models into one network the second insight of fast r cnn is to jointly train the cnn classifier and bounding box regressor in a single model where earlier we had different models to extract image features cnn classify svm and tighten bounding boxes regressor fast r cnn instead used a single network to compute all three you can see how this was done in the image above fast r cnn replaced the svm classifier with a softmax layer on top of the cnn to output a classification it also added a linear regression layer parallel to the softmax layer to output bounding box coordinates in this way all the outputs needed came from one single network here are the inputs and outputs to this overall model even with all these advancements there was still one remaining bottleneck in the fast r cnn process the region proposer as we saw the very first step to detecting the locations of objects is generating a bunch of potential bounding boxes or regions of interest to test in fast r cnn these proposals were created using selective search a fairly slow process that was found to be the bottleneck of the overall process in the middle a team at microsoft research composed of shaoqing ren kaiming he ross girshick and jian sun found a way to make the region proposal step almost cost free through an architecture they creatively named faster r cnn the insight of faster r cnn was that region proposals depended on features of the image that were already calculated with the forward pass of the cnn first step of classification so why not reuse those same cnn results for region proposals instead of running a separate selective search algorithm indeed this is just what the faster r cnn team achieved in the image above you can see how a single cnn is used to both carry out region proposals and classification this way only one cnn needs to be trained and we get region proposals almost for free the authors write here are the inputs and outputs of their model how the regions are generated let s take a moment to see how faster r cnn generates these region proposals from cnn features faster r cnn adds a fully convolutional network on top of the features of the cnn creating what s known as the region proposal network the region proposal network works by passing a sliding window over the cnn feature map and at each window outputting k potential bounding boxes and scores for how good each of those boxes is expected to be what do these k boxes represent intuitively we know that objects in an image should fit certain common aspect ratios and sizes for instance we know that we want some rectangular boxes that resemble the shapes of humans likewise we know we won t see many boxes that are very very thin in such a way we create k such common aspect ratios we call anchor boxes for each such anchor box we output one bounding box and score per position in the image with these anchor boxes in mind let s take a look at the inputs and outputs to this region proposal network we then pass each such bounding box that is likely to be an object into fast r cnn to generate a classification and tightened bounding boxes so far we ve seen how we ve been able to use cnn features in many interesting ways to effectively locate different objects in an image with bounding boxes can we extend such techniques to go one step further and locate exact pixels of each object instead of just bounding boxes this problem known as image segmentation is what kaiming he and a team of researchers including girshick explored at facebook ai using an architecture known as mask r cnn much like fast r cnn and faster r cnn mask r cnn s underlying intuition is straight forward given that faster r cnn works so well for object detection could we extend it to also carry out pixel level segmentation mask r cnn does this by adding a branch to faster r cnn that outputs a binary mask that says whether or not a given pixel is part of an object the branch in white in the above image as before is just a fully convolutional network on top of a cnn based feature map here are its inputs and outputs but the mask r cnn authors had to make one small adjustment to make this pipeline work as expected roialign realigning roipool to be more accurate when run without modifications on the original faster r cnn architecture the mask r cnn authors realized that the regions of the feature map selected by roipool were slightly misaligned from the regions of the original image since image segmentation requires pixel level specificity unlike bounding boxes this naturally led to inaccuracies the authors were able to solve this problem by cleverly adjusting roipool to be more precisely aligned using a method known as roialign imagine we have an image of size x and a feature map of size x let s imagine we want features the region corresponding to the top left x pixels in the original image see above how might we select these pixels from the feature map we know each pixel in the original image corresponds to pixels in the feature map to select pixels from the original image we just select pixels in roipool we would round this down and select pixels causing a slight misalignment however in roialign we avoid such rounding instead we use bilinear interpolation to get a precise idea of what would be at pixel this at a high level is what allows us to avoid the misalignments caused by roipool once these masks are generated mask r cnn combines them with the classifications and bounding boxes from faster r cnn to generate such wonderfully precise segmentations if you re interested in trying out these algorithms yourselves here are relevant repositories faster r cnn mask r cnn in just years we ve seen how the research community has progressed from krizhevsky et al s original result to r cnn and finally all the way to such powerful results as mask r cnn seen in isolation results like mask r cnn seem like incredible leaps of genius that would be unapproachable yet through this post i hope you ve seen how such advancements are really the sum of intuitive incremental improvements through years of hard work and collaboration each of the ideas proposed by r cnn fast r cnn faster r cnn and finally mask r cnn were not necessarily quantum leaps yet their sum products have led to really remarkable results that bring us closer to a human level understanding of sight what particularly excites me is that the time between r cnn and mask r cnn was just three years with continued funding focus and support how much further can computer vision improve over the next three years if you see any errors or issues in this post please contact me at dhruv getathelas com and i ll immediately correct them if you re interested in applying such techniques come join us at athelas where we apply computer vision to blood diagnostics daily other posts we ve written thanks to bharath ramsundar pranav ramkrishnan tanay tandon and oliver cameron for help with this post from a quick cheer to a standing ovation clap to show how much you enjoyed this story dhruvp vp eng athelas mit math and cs undergrad mit cs masters previously director of ai programs udacity blood diagnostics through deep learning http athelas com
Slav Ivanov,3900,17,https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415?source=tag_archive---------2----------------,"The $1700 great Deep Learning box: Assembly, setup and benchmarks",updated april uses cuda cudnn and tensorflow after years of using a thin client in the form of increasingly thinner macbooks i had gotten used to it so when i got into deep learning dl i went straight for the brand new at the time amazon p cloud servers no upfront cost the ability to train many models simultaneously and the general coolness of having a machine learning model out there slowly teaching itself however as time passed the aws bills steadily grew larger even as i switched to x cheaper spot instances also i didn t find myself training more than one model at a time instead i d go to lunch workout etc while the model was training and come back later with a clear head to check on it but eventually the model complexity grew and took longer to train i d often forget what i did differently on the model that had just completed its day training nudged by the great experiences of the other folks on the fast ai forum i decided to settle down and to get a dedicated dl box at home the most important reason was saving time while prototyping models if they trained faster the feedback time would be shorter thus it would be easier for my brain to connect the dots between the assumptions i had for the model and its results then i wanted to save money i was using amazon web services aws which offered p instances with nvidia k gpus lately the aws bills were around month with a tendency to get larger also it is expensive to store large datasets like imagenet and lastly i haven t had a desktop for over years and wanted to see what has changed in the meantime spoiler alert mostly nothing what follows are my choices inner monologue and gotchas from choosing the components to benchmarking a sensible budget for me would be about years worth of my current compute spending at month for aws this put it at around for the whole thing you can check out all the components used the pc part picker site is also really helpful in detecting if some of the components don t play well together the gpu is the most crucial component in the box it will train these deep networks fast shortening the feedback cycle disclosure the following are affiliate links to help me pay for well more gpus the choice is between a few of nvidia s cards gtx gtx ti gtx gtx ti and finally the titan x the prices might fluctuate especially because some gpus are great for cryptocurrency mining wink wink on performance side gtx ti and titan x are similar roughly speaking the gtx is about faster than gtx and gtx ti is about faster than gtx the new gtx ti is very close in performance to gtx tim dettmers has a great article on picking a gpu for deep learning which he regularly updates as new cards come on the market here are the things to consider when picking a gpu considering all of this i picked the gtx ti mainly for the training speed boost i plan to add a second ti soonish even though the gpu is the mvp in deep learning the cpu still matters for example data preparation is usually done on the cpu the number of cores and threads per core is important if we want to parallelize all that data prep to stay on budget i picked a mid range cpu the intel i it s relatively cheap but good enough to not slow things down edit as a few people have pointed out probably the biggest gotcha that is unique to dl multi gpu is to pay attention to the pcie lanes supported by the cpu motherboard by andrej karpathy we want to have each gpu have pcie lanes so it eats data as fast as possible gb s for pcie this means that for two cards we need pcie lanes however the cpu i have picked has only lanes so gpus would run in x mode instead of x this might be a bottleneck leading to less than ideal utilization of the graphics cards thus a cpu with lines is recommended edit however tim dettmers points out that having lanes per card should only decrease performance by for two gpus so currently my recommendation is go with pcie lanes per video card unless it gets too expensive for you otherwise lanes should do as well a good solution with to have for a double gpu machine would be an intel xeon processor like the e v pcie lanes or if you want to splurge go for a higher end processor like the desktop i k memory ram it s nice to have a lot of memory if we are to be working with rather big datasets i got sticks of gb for a total of gb of ram and plan to buy another gb later following jeremy howard s advice i got a fast ssd disk to keep my os and current data on and then a slow spinning hdd for those huge datasets like imagenet ssd i remember when i got my first macbook air years ago how blown away was i by the ssd speed to my delight a new generation of ssd called nvme has made its way to market in the meantime a gb mydigitalssd nvme drive was a great deal this baby copies files at gigabytes per second hdd tb seagate while ssds have been getting fast hdd have been getting cheap to somebody who has used macbooks with gb disk for the last years having this much space feels almost obscene the one thing that i kept in mind when picking a motherboard was the ability to support two gtx ti both in the number of pci express lanes the minimum is x and the physical size of cards also make sure it s compatible with the chosen cpu an asus tuf z did it for me msi x a sli plus should work great if you got an intel xeon cpu rule of thumb power supply should provide enough juice for the cpu and the gpus plus watts extra the intel i processor uses w and the gpus ti need w each so i got a deepcool w gold psu currently unavailable evga gq is similar the gold here refers to the power efficiency i e how much of the power consumed is wasted as heat the case should be the same form factor as the motherboard also having enough leds to embarrass a burner is a bonus a friend recommended the thermaltake n case which i promptly got no leds sadly here is how much i spent on all the components your costs may vary gtx ti cpu ram ssd hdd motherboard psu case total adding tax and fees this nicely matches my preset budget of if you don t have much experience with hardware and fear you might break something a professional assembly might be the best option however this was a great learning opportunity that i couldn t pass even though i ve had my share of hardware related horror stories the first and important step is to read the installation manuals that came with each component especially important for me as i ve done this before once or twice and i have just the right amount of inexperience to mess things up this is done before installing the motherboard in the case next to the processor there is a lever that needs to be pulled up the processor is then placed on the base double check the orientation finally the lever comes down to fix the cpu in place but i had a quite the difficulty doing this once the cpu was in position the lever wouldn t go down i actually had a more hardware capable friend of mine video walk me through the process turns out the amount of force required to get the lever locked down was more than what i was comfortable with next is fixing the fan on top of the cpu the fan legs must be fully secured to the motherboard consider where the fan cable will go before installing the processor i had came with thermal paste if yours doesn t make sure to put some paste between the cpu and the cooling unit also replace the paste if you take off the fan i put the power supply unit psu in before the motherboard to get the power cables snugly placed in case back side pretty straight forward carefully place it and screw it in a magnetic screwdriver was really helpful then connect the power cables and the case buttons and leds just slide it in the m slot and screw it in piece of cake the memory proved quite hard to install requiring too much effort to properly lock in a few times i almost gave up thinking i must be doing it wrong eventually one of the sticks clicked in and the other one promptly followed at this point i turned the computer on to make sure it works to my relief it started right away finally the gpu slid in effortlessly pins of power later and it was running nb do not plug your monitor in the external card right away most probably it needs drivers to function see below finally it s complete now that we have the hardware in place only the soft part remains out with the screwdriver in with the keyboard note on dual booting if you plan to install windows because you know for benchmarks totally not for gaming it would be wise to do windows first and linux second i didn t and had to reinstall ubuntu because windows messed up the boot partition livewire has a detailed article on dual boot most dl frameworks are designed to work on linux first and eventually support other operating systems so i went for ubuntu my default linux distribution an old gb usb drive was laying around and worked great for the installation unetbootin osx or rufus windows can prepare the linux thumb drive the default options worked fine during the ubuntu install at the time of writing ubuntu was just released so i opted for the previous version whose quirks are much better documented online ubuntu server or desktop the server and desktop editions of ubuntu are almost identical with the notable exception of the visual interface called x not being installed with server i installed the desktop and disabled autostarting x so that the computer would boot it in terminal mode if needed one could launch the visual desktop later by typing startx let s get our install up to date from jeremy howard s excellent install gpu script to deep learn on our machine we need a stack of technologies to use our gpu download cuda from nvidia or just run the code below updated to specify version of cuda thanks to zhanwenchen for the tip if you need to add later versions of cuda click here after cuda has been installed the following code will add the cuda installation to the path variable now we can verify that cuda has been installed successfully by running this should have installed the display driver as well for me nvidia smi showed err as the device name so i installed the latest nvidia drivers as of may to fix it removing cuda nvidia drivers if at any point the drivers or cuda seem broken as they did for me multiple times it might be better to start over by running since version tensorflow supports cudnn so we install that to download cudnn one needs to register for a free developer account after downloading install with the following anaconda is a great package manager for python i ve moved to python so will be using the anaconda version the popular dl framework by google installation validate tensorfow install to make sure we have our stack running smoothly i like to run the tensorflow mnist example we should see the loss decreasing during training keras is a great high level neural networks framework an absolute pleasure to work with installation can t be easier too pytorch is a newcomer in the world of dl frameworks but its api is modeled on the successful torch which was written in lua pytorch feels new and exciting mostly great although some things are still to be implemented we install it by running jupyter is a web based ide for python which is ideal for data sciency tasks it s installed with anaconda so we just configure and test it now if we open http localhost we should see a jupyter screen run jupyter on boot rather than running the notebook every time the computer is restarted we can set it to autostart on boot we will use crontab to do this which we can edit by running crontab e then add the following after the last line in the crontab file i use my old trusty macbook air for development so i d like to be able to log into the dl box both from my home network also when on the run ssh key it s way more secure to use a ssh key to login instead of a password digital ocean has a great guide on how to setup this ssh tunnel if you want to access your jupyter notebook from another computer the recommended way is to use ssh tunneling instead of opening the notebook to the world and protecting with a password let s see how we can do this then to connect over ssh tunnel run the following script on the client to test this open a browser and try http localhost from the remote machine your jupyter notebook should appear setup out of network access finally to access the dl box from the outside world we need things setting up out of network access depends on the router network setup so i m not going into details now that we have everything running smoothly let s put it to the test we ll be comparing the newly built box to an aws p xlarge instance which is what i ve used so far for dl the tests are computer vision related meaning convolutional networks with a fully connected model thrown in we time training models on aws p instance gpu k aws p virtual cpu the gtx ti and intel i cpu andres hernandez points out that my comparison does not use tensorflow that is optimized for these cpus which would have helped the them perform better check his insightful comment for more details the hello world of computer vision the mnist database consists of handwritten digits we run the keras example on mnist which uses multilayer perceptron mlp the mlp means that we are using only fully connected layers not convolutions the model is trained for epochs on this dataset which achieves over accuracy out of the box we see that the gtx ti is times faster than the k on aws p in training the model this is rather surprising as these cards should have about the same performance i believe this is because of the virtualization or underclocking of the k on aws the cpus perform times slower than the gpus as we will see later it s a really good result for the processors this is due to the small model which fails to fully utilize the parallel processing power of the gpus interestingly the desktop intel i achieves x speedup over the virtual cpu on amazon a vgg net will be finetuned for the kaggle dogs vs cats competition in this competition we need to tell apart pictures of dogs and cats running the model on cpus for the same number of batches wasn t feasible therefore we finetune for batches epoch on the gpus and batches on the cpus the code used is on github the ti is times faster that the aws gpu k the difference in the cpus performance is about the same as the previous experiment i is x faster however it s absolutely impractical to use cpus for this task as the cpus were taking x more time on this large model that includes convolutional layers and a couple semi wide fully connected layers on top a gan generative adversarial network is a way to train a model to generate images gan achieves this by pitting two networks against each other a generator which learns to create better and better images and a discriminator that tries to tell which images are real and which are dreamt up by the generator the wasserstein gan is an improvement over the original gan we will use a pytorch implementation that is very similar to the one by the wgan author the models are trained for steps and the loss is all over the place which is often the case with gans cpus aren t considered the gtx ti finishes x faster than the aws p k which is in line with the previous results the final benchmark is on the original style transfer paper gatys et al implemented on tensorflow code available style transfer is a technique that combines the style of one image a painting for example and the content of another image check out my previous post for more details on how style transfer works the gtx ti outperforms the aws k by a factor of this time the cpus are times slower than graphics cards the slowdown is less than on the vgg finetuning task but more than on the mnist perceptron experiment the model uses mostly the earlier layers of the vgg network and i suspect this was too shallow to fully utilize the gpus the dl box is in the next room and a large model is training on it was it a wise investment time will tell but it is beautiful to watch the glowing leds in the dark and to hear its quiet hum as models are trying to squeeze out that extra accuracy percentage point from a quick cheer to a standing ovation clap to show how much you enjoyed this story entrepreneur hacker machine learning deep learning and other types of learning
Tyler Elliot Bettilyon,17900,13,https://medium.com/@TebbaVonMathenstien/are-programmers-headed-toward-another-bursting-bubble-528e30c59a0e?source=tag_archive---------3----------------,Are Programmers Headed Toward Another Bursting Bubble?,a friend of mine recently posed a question that i ve heard many times in varying forms and forums do you think it and some lower level programming jobs are going to go the way of the dodo seems a bit like a massive job bubble that s gonna burst it s my opinion that one of the only things keeping tech and lower level computer science related jobs prestigious and well paid is ridiculous industry jargon and public ignorance about computers which are both going to go away in the next years this question is simultaneously on point about the future of technology jobs and exemplary of some pervasive misunderstandings regarding the field of software engineering while it s true that there is a great deal of ridiculous industry jargon there are equally many genuinely difficult problems waiting to be solved by those with the right skill set some software jobs are definitely going away but programmers with the right experience and knowledge will continue to be prestigious and well remunerated for many years to come as an example look at the recent explosion of ai researcher salaries and the corresponding dearth of available talent staying relevant in the ever changing technology landscape can be a challenge by looking at the technologies that are replacing programmers in the status quo we should be able to predict what jobs might disappear from the market additionally to predict how salaries and demand for specific skills might change we should consider the growing body of people learning to program as hannah pointed out public ignorance about computers is keeping wages high for those who can program and the public is becoming more computer savvy each year the fear of automation replacing jobs is neither new nor unfounded in any field and especially in technology market forces drive corporations toward automation and commodification gartner s hype cycles are one way of contextualizing this phenomenon as time goes on specific ideas and technologies push towards the plateau of productivity where they are eventually automated looking at history one must conclude that automation has the power to destroy specific job markets in diverse industries ranging from crop harvesting to automobile assembly technology advances have consistently replaced and augmented human labor to reduce costs a professor once put it this way in his compilers course take historical note of textile and steel industries do you want to build machines and tools or do you want to operate those machines in this metaphor the machine is a computer programming language this professor was really asking do you want to build websites using javascript or do you want to build the v engine that powers javascript the creation of websites is being automated by wordpress and others today v on the other hand has a growing body of competitors some of whom are solving open research questions languages will come and go how many fortran job openings are there but there will always be someone building the next language lucky for us programming language implementations are written with programming languages themselves being a machine operator in software puts you on the path to being a machine creator in a way which was not true of the steel mill workers of the past the growing number of languages interpreters and compilers shows us that every job destroying machine also brings with it new opportunities to improve those machines maintain those machines and so forth despite the growing body of jobs which no longer exist there has yet to be a moment in history where humanity has collectively said i guess there isn t any work left for us to do commodification is coming for us all not just software engineers throughout history human labor has consistently been replaced with non humans or augmented to require fewer and less skilled humans self driving cars and trucks are the flavor of the week in this grand human tradition if the cycle of creation and automation are a fact of life the natural question to answer next is which jobs and industries are at risk and which are not aws heroku and other similar hosting platforms have forever changed the role of the system administrator devops engineer internet businesses used to absolutely need their own server master someone who was well versed in linux someone who could configure a server with apache or nginx someone who could not only physically wire up the server the routers and all the other physical components but who could also configure the routing tables and all the software required to make that server accessible on the public web while there are definitely still people applying this skill set professionally aws is making some of those skills obsolete especially at the lower experience levels and on the physical side of things there are very lucrative roles within amazon and netflix and google for people with deep expertise in networking infrastructure but there is much less demand at the small to medium business scale business intelligence tools such as salesforce tableau and spotfire are also beginning to occupy spaces historically held by software engineers these systems have reduced the demand for in house database administrators but they have also increased the demand for sql as a general purpose skill they have decreased demand for in house reporting technology but increased demand for integration engineers who automate the flow of data from the business to the third party software platform s a field that was previously dominated by excel and spreadsheets is increasingly being pushed towards scripting languages like python or r and towards sql for data management some jobs have disappeared but demand for people who can write software has seen an increase overall data science is a fascinating example of commodification at a level closer to software scikit learn tensorflow and pytorch are all software libraries that make it easier for people to build machine learning applications without building the algorithms from scratch in fact it s possible to run a dataset through many different machine learning algorithms with many different parameter sets for those algorithms with little to no understanding of how those algorithms are actually implemented it s not necessarily wise to do this just possible you can bet that business intelligence companies will be trying to integrate these kinds of algorithms into their own tools over the next few years as well in many ways data science looks like web development did years ago a booming field where a little bit of knowledge can get you in the door due to a skills gap as web development bootcamps are closing and consolidating data science bootcamps are popping up in their place kaplan who bought the original web development bootcamp dev bootcamp and started a data science bootcamp metis has decided to close devbootcamp and keep metis running content management systems are among the most visible of the tools automating away the need for a software engineer squarespace and wordpress are among the most popular cms systems today these platforms are significantly reducing the value of people with a just a little bit of front end web development skill in fact the barriers for making a website and getting it online have come down so dramatically that people with zero programming experience are successfully launching websites every day those same people aren t making deeply interactive websites that serve billions of people but they absolutely do make websites for their own businesses that give customers the information they need a lovely landing page with information such as how to find the establishment and how to contact them is more than enough for a local restaurant bar or retail store if your business is not primarily an internet business it has never been easier to get a working site on the public web as a result the once thriving industry of web contractors who can quickly set up a simple website and get it online is becoming less lucrative finally it would border on hubris to ignore the physical aspect of computers in this context in the words of mike acton software is not the platform hardware is the platform software people would be wise to study at least a little computer architecture and electrical engineering a big shake up in hardware such as the arrival of consumer grade quantum computers would will change everything about professional software engineering quantum computers are still a ways off but the growing interest in gpus and the drive toward parallelization is an imminent shift cpu speeds have been stagnant for several years now and in that time a seemingly unquenchable thirst for machine learning and big data has emerged with more desire than ever to process large data sets openmp opencl go cuda and other parallel processing languages and frameworks will continue to become mainstream to be competitively fast in the near term future significant parallelization will be a requirement across the board not just in high performance niches like operating systems infrastructure and video games websites are ubiquitous the stack overflow survey reports that about of professional software engineers are working in an internet web services company the bureau of labor statistics expects growth in web development to continue much faster than average between and due to its visibility there has been a massive focus on solving the skills gap in this industry coding bootcamps teach web development almost exclusively and web development online courses have flooded udemy udacity coursera and similar marketplaces the combination of increasing automation throughout the web development technology stack and the influx of new entry level programmers with an explicit focus on web development has led some to predict a slide towards a blue collar market for software developers some have gone further suggesting that the push towards a blue collar market is a strategy architected by big tech firms others of course say we re headed for another bursting bubble change in demand for specific technologies is not news languages and frameworks are always rising and falling in technology web development in its current incarnation js is king will eventually go the way of web development of the early s remember flash what is new is that a lot of people are receiving an education explicitly and solely in the current trendy web development frameworks before you decide to label yourself a react developer remember there were people who once identified themselves as flash developers banking your career on a specific language framework or technology is a game of roulette of course it s quite difficult to predict what technologies will remain relevant but if you re going to go all in on something i suggest relying on the lindy effect and picking something like c that has already withstood the test of time the next generation will have a level of de facto tech literacy that generation x and even millennials do not have one outcome of this will be that using the next generation of cms tools will be a given these tools will get better and young workers will be better at using them this combination will definitely will bring down the value of low level it and web development skills as eager and skilled youngsters enter the job market high schools are catching on as well offering computer science and programming classes some well educated high school students will likely be entering the workforce as programming interns immediately upon graduation another big group of newcomers to programming are mbas and data analysts job listings which were once dominated by excel are starting to list sql as a nice to have and even requirement tools such as tableau spotfire salesforce and other web based metrics systems continue to replace the spreadsheet as the primary tool for report generation if this continues more data analysts will learn to use sql directly simply because it is easier than exporting the data into a spreadsheet people looking to climb the ranks and out perform their peers in these roles are taking online courses to learn about databases and statistical programming languages with these new skills they can begin to position themselves as data scientists by learning a combination of machine learning and statistical libraries look at metis curriculum as a prime example of this path finally the number of people earning computer science and software engineering degrees continues to climb purdue for example reports that applications to their cs program have doubled over five years cornell reports a similar explosion of cs graduates this trend isn t surprising given the growth and ubiquity of software it s hard for young people to imagine that computers will play a smaller role in our futures so why not study something that s going to give you job security a common argument in the industry nowadays is around the idea that the education you receive in a four year computer science program is mostly unnecessary cruft i have heard this argument repeatedly in the halls of bootcamps web development shops and online from big names in the field such as this piece by eric elliott the opposition view is popular as well with some going so far as saying all programmers should earn a master s degree like eric elliott i think it s good that there are more options than ever to break into programming and a year degree might not be the best option for many simultaneously i agree with william bain that the foundational skills which apply across programming disciplines are crucial for career longevity and that it is still hard to find that information outside of university courses i ve written previously about what skills i think aspiring engineers should learn as a foundation of a long career and joined bradfield in order to help share this knowledge coding schools of many shapes and sizes are becoming ubiquitous and for good reasons there is quite a lot you can learn about programming without getting into the minutia of big o notation obscure data structures and algorithmic trivia however while it s true that fresh graduates from stanford are competing for some jobs with fresh graduates from hack reactor it s only true in one or two sub industries code school and bootcamp graduates are not yet applying to work on embedded systems cryptography security robotics network infrastructure or ai research and development yet these fields like web development are growing quickly some programming related skills have already started their transition from rare skill to baseline expectation conversely the engineering that goes into creating beastly engines like aws is anything but common the big companies driving technology forward amazon google facebook nvidia space x and so on are typically not looking for people with a basic understanding of javascript aws serves billions of users per day to support that kind of load an aws infrastructure engineer needs a deep knowledge of network protocols computer architecture and several years of relevant experience as with any discipline there are amateurs and artisans these prestigious firms are solving research problems and building systems that are truly pushing against the boundaries of what is possible yet they still struggle to fill open roles even while basic programming skills are increasingly common people who can write algorithms to predict changes in genetic sequences that will yield a desired result are going to be highly valuable in the future people who can program satellites spacecraft and automate machinery will continue to be highly valued these are not fields that lend themselves as readily to a month intensive program as front end web development at least not without significant prior experience because computer science starts with the word computer it is assumed that young people will all have an innate understanding of it by unfortunately the ubiquity of computers has not created a new generation of people who de facto understand mathematics computer science network infrastructure electrical engineering and so on computer literacy is not the same as the study of computation despite mathematics having existed since the dawn of time there is still a relatively small portion of the population with strong statistical literacy and computer science is similarly old euclid invented several algorithms one of which is used every time you make an https request the fact that we use https every time we login to a website does not automatically imbue anyone with a knowledge of how those protocols work more established professional fields often have a bimodal wage distribution a relatively small number of practitioners make quite a lot of money and the majority of them earn a good wage but do not find themselves in the top of earners the national association for law placement collects data that can be used to visualize this phenomenon in stark clarity a huge share of law graduates make between and a good wage but hardly the salary we associate with a top professional we tend to think that all law graduates are on track to becoming partners at a law firm when really there are many paths paralegal clerk public defender judge legal services for businesses contract writing and so on computer science graduates also have many options for their professional practice from web development to embedded systems as a basic level of programming literacy continues to become an expectation rather than a nice to have i suspect a similar distribution will emerge in programming jobs while there will always be a cohort of programmers making a lot of money to push on the edges of technology there will be a growing body of middle class programmers powering the new computer centric economy the average salary for web developers will surely decrease over time that said i suspect that the number of jobs for programmers in general will only continue to grow as worker supply begins to meet demand hopefully we will see a healthy boom in a variety of middle class programming jobs there will also continue to be a top professional salary available for those programmers who are redefining what is possible regardless of which cohort of programmers you re in a career in technology means continuing your education throughout your life if you want to stay in the second cohort of programmers you may want to invest in learning how to create the machines rather than simply operate them from a quick cheer to a standing ovation clap to show how much you enjoyed this story a curious human on a quest to watch the world learn
Blaise Aguera y Arcas,8700,15,https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477?source=tag_archive---------4----------------,Do algorithms reveal sexual orientation or just expose our stereotypes?,by blaise agu era y arcas alexander todorov and margaret mitchell a study claiming that artificial intelligence can infer sexual orientation from facial images caused a media uproar in the fall of the economist featured this work on the cover of their september th magazine on the other hand two major lgbtq organizations the human rights campaign and glaad immediately labeled it junk science michal kosinski who co authored the study with fellow researcher yilun wang initially expressed surprise calling the critiques knee jerk reactions however he then proceeded to make even bolder claims that such ai algorithms will soon be able to measure the intelligence political orientation and criminal inclinations of people from their facial images alone kosinski s controversial claims are nothing new last year two computer scientists from china posted a non peer reviewed paper online in which they argued that their ai algorithm correctly categorizes criminals with nearly accuracy from a government id photo alone technology startups had also begun to crop up claiming that they can profile people s character from their facial images these developments had prompted the three of us to collaborate earlier in the year on a medium essay physiognomy s new clothes to confront claims that ai face recognition reveals deep character traits we described how the junk science of physiognomy has roots going back into antiquity with practitioners in every era resurrecting beliefs based on prejudice using the new methodology of the age in the th century this included anthropology and psychology in the th genetics and statistical analysis and in the st artificial intelligence in late the paper motivating our physiognomy essay seemed well outside the mainstream in tech and academia but as in other areas of discourse what recently felt like a fringe position must now be addressed head on kosinski is a faculty member of stanford s graduate school of business and this new study has been accepted for publication in the respected journal of personality and social psychology much of the ensuing scrutiny has focused on ethics implicitly assuming that the science is valid we will focus on the science the authors trained and tested their sexual orientation detector using images from public profiles on a us dating website composite images of the lesbian gay and straight men and women in the sample reveal a great deal about the information available to the algorithm clearly there are differences between these four composite faces wang and kosinski assert that the key differences are in physiognomy meaning that a sexual orientation tends to go along with a characteristic facial structure however we can immediately see that some of these differences are more superficial for example the average straight woman appears to wear eyeshadow while the average lesbian does not glasses are clearly visible on the gay man and to a lesser extent on the lesbian while they seem absent in the heterosexual composites might it be the case that the algorithm s ability to detect orientation has little to do with facial structure but is due rather to patterns in grooming presentation and lifestyle we conducted a survey of americans using amazon s mechanical turk crowdsourcing platform to see if we could independently confirm these patterns asking yes no questions such as do you wear eyeshadow do you wear glasses and do you have a beard as well as questions about gender and sexual orientation the results show that lesbians indeed use eyeshadow much less than straight women do gay men and women do both wear glasses more and young opposite sex attracted men are considerably more likely to have prominent facial hair than their gay or same sex attracted peers breaking down the answers by the age of the respondent can provide a richer and clearer view of the data than any single statistic in the following figures we show the proportion of women who answer yes to do you ever use makeup top and do you wear eyeshadow bottom averaged over year age intervals the blue curves represent strictly opposite sex attracted women a nearly identical set to those who answered yes to are you heterosexual or straight the cyan curve represents women who answer yes to either or both of are you sexually attracted to women and are you romantically attracted to women and the red curve represents women who answer yes to are you homosexual gay or lesbian the shaded regions around each curve show confidence intervals the patterns revealed here are intuitive it won t be breaking news to most that straight women tend to wear more makeup and eyeshadow than same sex attracted and even more so lesbian identifying women on the other hand these curves also show us how often these stereotypes are violated that same sex attracted men of most ages wear glasses significantly more than exclusively opposite sex attracted men do might be a bit less obvious but this trend is equally clear a proponent of physiognomy might be tempted to guess that this is somehow related to differences in visual acuity between these populations of men however asking the question do you like how you look in glasses reveals that this is likely more of a stylistic choice same sex attracted women also report wearing glasses more as well as liking how they look in glasses more across a range of ages one can also see how opposite sex attracted women under the age of wear contact lenses significantly more than same sex attracted women despite reporting that they have a vision defect at roughly the same rate further illustrating how the difference is driven by an aesthetic preference similar analysis shows that young same sex attracted men are much less likely to have hairy faces than opposite sex attracted men serious facial hair in our plots is defined as answering yes to having a goatee beard or moustache but no to stubble overall opposite sex attracted men in our sample are more likely to have serious facial hair than same sex attracted men and for men under the age of who are overrepresented on dating websites this rises to wang and kosinski speculate in their paper that the faintness of the beard and moustache in their gay male composite might be connected with prenatal underexposure to androgens male hormones resulting in a feminizing effect hence sparser facial hair the fact that we see a cohort of same sex attracted men in their s who have just as much facial hair as opposite sex attracted men suggests a different story in which fashion trends and cultural norms play the dominant role in choices about facial hair among men not differing exposure to hormones early in development the authors of the paper additionally note that the heterosexual male composite appears to have darker skin than the other three composites our survey confirms that opposite sex attracted men consistently self report having a tan face yes to is your face tan slightly more often than same sex attracted men once again wang and kosinski reach for a hormonal explanation writing while the brightness of the facial image might be driven by many factors previous research found that testosterone stimulates melanocyte structure and function leading to a darker skin however a simpler answer is suggested by the responses to the question do you work outdoors overall opposite sex attracted men are more likely to work outdoors and among men under this rises to previous research has found that increased exposure to sunlight leads to darker skin none of these results prove that there is no physiological basis for sexual orientation in fact ample evidence shows us that orientation runs much deeper than a choice or a lifestyle in a critique aimed in part at fraudulent conversion therapy programs united states surgeon general david satcher wrote in a report sexual orientation is usually determined by adolescence if not earlier and there is no valid scientific evidence that sexual orientation can be changed it follows that if we dig deeply enough into human physiology and neuroscience we will eventually find reliable correlates and maybe even the origins of sexual orientation in our survey we also find some evidence of outwardly visible correlates of orientation that are not cultural perhaps most strikingly very tall women are overrepresented among lesbian identifying respondents however while this is interesting it s very far from a good predictor of women s sexual orientation makeup and eyeshadow do much better the way wang and kosinski measure the efficacy of their ai gaydar is equivalent to choosing a straight and a gay or lesbian face image both from data held out during the training process and asking how often the algorithm correctly guesses which is which performance would be no better than random chance for women guessing that the taller of the two is the lesbian achieves only accuracy barely above random chance this is because despite the statistically meaningful overrepresentation of tall women among the lesbian population the great majority of lesbians are not unusually tall by contrast the performance measures in the paper for gay men and for lesbian women seem impressive consider however that we can achieve comparable results with trivial models based only on a handful of yes no survey questions about presentation for example for pairs of women one of whom is lesbian the following not exactly superhuman algorithm is on average accurate if neither or both women wear eyeshadow flip a coin otherwise guess that the one who wears eyeshadow is straight and the other lesbian adding six more yes no questions about presentation do you ever use makeup do you have long hair do you have short hair do you ever use colored lipstick do you like how you look in glasses and do you work outdoors as additional signals raises the performance to given how many more details about presentation are available in a face image performance no longer seems so impressive several studies including a recent one in the journal of sex research have shown that human judges gaydar is no more reliable than a coin flip when the judgement is based on pictures taken under well controlled conditions head pose lighting glasses makeup etc it s better than chance if these variables are not controlled for because a person s presentation especially if that person is out involves social signaling we signal our orientation and many other kinds of status presumably in order to attract the kind of attention we want and to fit in with people like us wang and kosinski argue against this interpretation on the grounds that their algorithm works on facebook selfies of openly gay men as well as dating website selfies the issue however is not whether the images come from a dating website or facebook but whether they are self posted or taken under standardized conditions most people present themselves in ways that have been calibrated over many years of media consumption observing others looking in the mirror and gauging social reactions in one of the earliest gaydar studies using social media participants could categorize gay men with about accuracy but when the researchers used facebook images of gay and heterosexual men posted by their friends still far from a perfect control the accuracy dropped to if subtle biases in image quality expression and grooming can be picked up on by humans these biases can also be detected by an ai algorithm while wang and kosinski acknowledge grooming and style they believe that the chief differences between their composite images relate to face shape arguing that gay men s faces are more feminine narrower jaws longer noses larger foreheads while lesbian faces are more masculine larger jaws shorter noses smaller foreheads as with less facial hair on gay men and darker skin on straight men they suggest that the mechanism is gender atypical hormonal exposure during development this echoes a widely discredited th century model of homosexuality sexual inversion more likely heterosexual men tend to take selfies from slightly below which will have the apparent effect of enlarging the chin shortening the nose shrinking the forehead and attenuating the smile see our selfies below this view emphasizes dominance or perhaps more benignly an expectation that the viewer will be shorter on the other hand as a wedding photographer notes in her blog when you shoot from above your eyes look bigger which is generally attractive especially for women this may be a heteronormative assessment when a face is photographed from below the nostrils are prominent while higher shooting angles de emphasize and eventually conceal them altogether looking again at the composite images we can see that the heterosexual male face has more pronounced dark spots corresponding to the nostrils than the gay male while the opposite is true for the female faces this is consistent with a pattern of heterosexual men on average shooting from below heterosexual women from above as the wedding photographer suggests and gay men and lesbian women from directly in front a similar pattern is evident in the eyebrows shooting from above makes them look more v shaped but their apparent shape becomes flatter and eventually caret shaped as the camera is lowered shooting from below also makes the outer corners of the eyes appear lower in short the changes in the average positions of facial landmarks are consistent with what we would expect to see from differing selfie angles the ambiguity between shooting angle and the real physical sizes of facial features is hard to fully disentangle from a two dimensional image both for a human viewer and for an algorithm although the authors are using face recognition technology designed to try to cancel out all effects of head pose lighting grooming and other variables not intrinsic to the face we can confirm that this doesn t work perfectly that s why multiple distinct images of a person help when grouping photos by subject in google photos and why a person may initially appear in more than one group tom white a researcher at victoria university in new zealand has experimented with the same facial recognition engine kosinski and wang use vgg face and has found that its output varies systematically based on variables like smiling and head pose when he trains a classifier based on vgg face s output to distinguish a happy expression from a neutral one it gets the answer right of the time which is significant given that the heterosexual female composite has a much more pronounced smile changes in head pose might be even more reliably detectable for test images a classifier is able to pick out the ones facing to the right with accuracy in summary we have shown how the obvious differences between lesbian or gay and straight faces in selfies relate to grooming presentation and lifestyle that is differences in culture not in facial structure these differences include we ve demonstrated that just a handful of yes no questions about these variables can do nearly as good a job at guessing orientation as supposedly sophisticated facial recognition ai further the current generation of facial recognition remains sensitive to head pose and facial expression therefore at least at this point it s hard to credit the notion that this ai is in some way superhuman at outing us based on subtle but unalterable details of our facial structure this doesn t negate the privacy concerns the authors and various commentators have raised but it emphasizes that such concerns relate less to ai per se than to mass surveillance which is troubling regardless of the technologies used even when as in the days of the stasi in east germany these were nothing but paper files and audiotapes like computers or the internal combustion engine ai is a general purpose technology that can be used to automate a great many tasks including ones that should not be undertaken in the first place we are hopeful about the confluence of new powerful ai technologies with social science but not because we believe in reviving the th century research program of inferring people s inner character from their outer appearance rather we believe ai is an essential tool for understanding patterns in human culture and behavior it can expose stereotypes inherent in everyday language it can reveal uncomfortable truths as in google s work with the geena davis institute where our face gender classifier established that men are seen and heard nearly twice as often as women in hollywood movies yet female led films outperform others at the box office making social progress and holding ourselves to account is more difficult without such hard evidence even when it only confirms our suspicions two of us margaret mitchell and blaise agu era y arcas are research scientists specializing in machine learning and ai at google agu era y arcas leads a team that includes deep learning applied to face recognition and powers face grouping in google photos alex todorov is a professor in the psychology department at princeton where he directs the social perception lab he is the author of face value the irresistible influence of first impressions this wording is based on several large national surveys which we were able to use to sanity check our numbers about of respondents identified as homosexual gay or lesbian and as heterosexual about of all genders were exclusively same sex attracted of the men were either sexually or romantically same sex attracted and of the women just under of respondents were trans and about identified with both or neither of the pronouns she and he these numbers are broadly consistent with other surveys especially when considered as a function of age the mechanical turk population skews somewhat younger than the overall population of the us and consistent with other studies our data show that younger people are far more likely to identify non heteronormatively these are wider for same sex attracted and lesbian women because they are minority populations resulting in a larger sampling error the same holds for older people in our sample for the remainder of the plots we stick to opposite sex attracted and same sex attracted as the counts are higher and the error bars therefore smaller these categories are also somewhat less culturally freighted since they rely on questions about attraction rather than identity as with eyeshadow and makeup the effects are similar and often even larger when comparing heterosexual identifying with lesbian or gay identifying people although we didn t test this explicitly slightly different rates of laser correction surgery seem a likely cause of the small but growing disparity between opposite sex attracted and same sex attracted women who answer yes to the vision defect questions as they age this finding may prompt the further question why do more opposite sex attracted men work outdoors this is not addressed by any of our survey questions but hopefully the other evidence presented here will discourage an essentialist assumption such as straight men are just more outdoorsy without the evidence of a controlled study that can support the leap from correlation to cause such explanations are a form of logical fallacy sometimes called a just so story an unverifiable narrative explanation for a cultural practice of the lesbian identified women in the sample or were over six feet and or were over out of heterosexual women women who answered yes to are you heterosexual or straight only or were over six feet and or were over they note that these figures rise to for men and for women if images are considered these results are based on the simplest possible machine learning technique a linear classifier the classifier is trained on a randomly chosen of the data with the remaining of the data held out for testing over repetitions of this procedure the error is with the same number of repetitions and holdout basing the decision on height alone gives an error of and basing it on eyeshadow alone yields a longstanding body of work e g goffman s the presentation of self in everyday life and jones and pittman s toward a general theory of strategic self presentation delves more deeply into why we present ourselves the way we do both for instrumental reasons status power attraction and because our presentation informs and is informed by how we conceive of our social selves from a quick cheer to a standing ovation clap to show how much you enjoyed this story blaise aguera y arcas leads google s ai group in seattle he founded seadragon and was one of the creators of photosynth at microsoft
Arvind N,9500,8,https://towardsdatascience.com/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153?source=tag_archive---------5----------------,Thoughts after taking the Deeplearning.ai courses – Towards Data Science,update feb nd when this blog post was written only courses had been released all courses in this specialization are now out i will have a follow up blog post soon between a full time job and a toddler at home i spend my spare time learning about the ideas in cognitive science ai once in a while a great paper video course comes out and you re instantly hooked andrew ng s new deeplearning ai course is like that shane carruth or rajnikanth movie that one yearns for naturally as soon as the course was released on coursera i registered and spent the past evenings binge watching the lectures working through quizzes and programming assignments dl practitioners and ml engineers typically spend most days working at an abstract keras or tensorflow level but it s nice to take a break once in a while to get down to the nuts and bolts of learning algorithms and actually do back propagation by hand it is both fun and incredibly useful andrew ng s new adventure is a bottom up approach to teaching neural networks powerful non linearity learning algorithms at a beginner mid level in classic ng style the course is delivered through a carefully chosen curriculum neatly timed videos and precisely positioned information nuggets andrew picks up from where his classic ml course left off and introduces the idea of neural networks using a single neuron logistic regression and slowly adding complexity more neurons and layers by the end of the weeks course a student is introduced to all the core ideas required to build a dense neural network such as cost loss functions learning iteratively using gradient descent and vectorized parallel python numpy implementations andrew patiently explains the requisite math and programming concepts in a carefully planned order and a well regulated pace suitable for learners who could be rusty in math coding lectures are delivered using presentation slides on which andrew writes using digital pens it felt like an effective way to get the listener to focus i felt comfortable watching videos at x or x speed quizzes are placed at the end of each lecture sections and are in the multiple choice question format if you watch the videos once you should be able to quickly answer all the quiz questions you can attempt quizzes multiple times and the system is designed to keep your highest score programming assignments are done via jupyter notebooks powerful browser based applications assignments have a nice guided sequential structure and you are not required to write more than lines of code in each section if you understand the concepts like vectorization intuitively you can complete most programming sections with just line of code after the assignment is coded it takes button click to submit your code to the automated grading system which returns your score in a few minutes some assignments have time restrictions say three attempts in hours etc jupyter notebooks are well designed and work without any issues instructions are precise and it feels like a polished product anyone interested in understanding what neural networks are how they work how to build them and the tools available to bring your ideas to life if your math is rusty there is no need to worry andrew explains all the required calculus and provides derivatives at every occasion so that you can focus on building the network and concentrate on implementing your ideas in code if your programming is rusty there is a nice coding assignment to teach you numpy but i recommend learning python first on codecademy let me explain this with an analogy assume you are trying to learn how to drive a car jeremy s fast ai course puts you in the drivers seat from the get go he teaches you to move the steering wheel press the brake accelerator etc then he slowly explains more details about how the car works why rotating the wheel makes the car turn why pressing the brake pedal makes you slow down and stop etc he keeps getting deeper into the inner workings of the car and by the end of the course you know how the internal combustion engine works how the fuel tank is designed etc the goal of the course is to get you driving you can choose to stop at any point after you can drive reasonably well there is no need to learn how to build repair the car andrew s dl course does all of this but in the complete opposite order he teaches you about internal combustion engine first he keeps adding layers of abstraction and by the end of the course you are driving like an f racer the fast ai course mainly teaches you the art of driving while andrew s course primarily teaches you the engineering behind the car if you have not done any machine learning before this don t take this course first the best starting point is andrew s original ml course on coursera after you complete that course please try to complete part of jeremy howard s excellent deep learning course jeremy teaches deep learning top down which is essential for absolute beginners once you are comfortable creating deep neural networks it makes sense to take this new deeplearning ai course specialization which fills up any gaps in your understanding of the underlying details and concepts andrew stresses on the engineering aspects of deep learning and provides plenty of practical tips to save time and money the third course in the dl specialization felt incredibly useful for my role as an architect leading engineering teams jargon is handled well andrew explains that an empirical process trial error he is brutally honest about the reality of designing and training deep nets at some point i felt he might have as well just called deep learning as glorified curve fitting squashes all hype around dl and ai andrew makes restrained careful comments about proliferation of ai hype in the mainstream media and by the end of the course it is pretty clear that dl is nothing like the terminator wonderful boilerplate code that just works out of the box excellent course structure nice consistent and useful notation andrew strives to establish a fresh nomenclature for neural nets and i feel he could be quite successful in this endeavor style of teaching that is unique to andrew and carries over from ml i could feel the same excitement i felt in when i took his original ml course the interviews with deep learning heroes are refreshing it is motivating and fun to hear personal stories and anecdotes i wish that he d said concretely more often good tools are important and will help you accelerate your learning pace i bought a digital pen after seeing andrew teach with one it helped me work more efficiently there is a psychological reason why i recommend the fast ai course before this one once you find your passion you can learn uninhibited you just get that dopamine rush each time you score full points don t be scared by dl jargon hyperparameters settings architecture topology style etc or the math symbols if you take a leap of faith and pay attention to the lectures andrew shows why the symbols and notation are actually quite useful they will soon become your tools of choice and you will wield them with style thanks for reading and best wishes update thanks for the overwhelmingly positive response many people are asking me to explain gradient descent and the differential calculus i hope this helps from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in strong ai sharing concepts ideas and codes
Berit Anderson,1600,20,https://medium.com/join-scout/the-rise-of-the-weaponized-ai-propaganda-machine-86dac61668b?source=tag_archive---------6----------------,The Rise of the Weaponized AI Propaganda Machine – Scout: Science Fiction + Journalism – Medium,by berit anderson and brett horvath this piece was originally published at scout ai this is a propaganda machine it s targeting people individually to recruit them to an idea it s a level of social engineering that i ve never seen before they re capturing people and then keeping them on an emotional leash and never letting them go said professor jonathan albright albright an assistant professor and data scientist at elon university started digging into fake news sites after donald trump was elected president through extensive research and interviews with albright and other key experts in the field including samuel woolley head of research at oxford university s computational propaganda project and martin moore director of the centre for the study of media communication and power at kings college it became clear to scout that this phenomenon was about much more than just a few fake news stories it was a piece of a much bigger and darker puzzle a weaponized ai propaganda machine being used to manipulate our opinions and behavior to advance specific political agendas by leveraging automated emotional manipulation alongside swarms of bots facebook dark posts a b testing and fake news networks a company called cambridge analytica has activated an invisible machine that preys on the personalities of individual voters to create large shifts in public opinion many of these technologies have been used individually to some effect before but together they make up a nearly impenetrable voter manipulation machine that is quickly becoming the new deciding factor in elections around the world most recently analytica helped elect u s president donald trump secured a win for the brexit leave campaign and led ted cruz s campaign surge shepherding him from the back of the gop primary pack to the front the company is owned and controlled by conservative and alt right interests that are also deeply entwined in the trump administration the mercer family is both a major owner of cambridge analytica and one of trump s biggest donors steve bannon in addition to acting as trump s chief strategist and a member of the white house security council is a cambridge analytica board member until recently analytica s cto was the acting cto at the republican national convention presumably because of its alliances analytica has declined to work on any democratic campaigns at least in the u s it is however in final talks to help trump manage public opinion around his presidential policies and to expand sales for the trump organization cambridge analytica is now expanding aggressively into u s commercial markets and is also meeting with right wing parties and governments in europe asia and latin america cambridge analytica isn t the only company that could pull this off but it is the most powerful right now understanding cambridge analytica and the bigger ai propaganda machine is essential for anyone who wants to understand modern political power build a movement or keep from being manipulated the weaponized ai propaganda machine it represents has become the new prerequisite for political success in a world of polarization isolation trolls and dark posts there s been a wave of reporting on cambridge analytica itself and solid coverage of individual aspects of the machine bots fake news microtargeting but none so far that we have seen that portrays the intense collective power of these technologies or the frightening level of influence they re likely to have on future elections in the past political messaging and propaganda battles were arms races to weaponize narrative through new mediums waged in print on the radio and on tv this new wave has brought the world something exponentially more insidious personalized adaptive and ultimately addictive propaganda silicon valley spent the last ten years building platforms whose natural end state is digital addiction in trump and his allies hijacked them we have entered a new political age at scout we believe that the future of constructive civic dialogue and free and open elections depends on our ability to understand and anticipate it welcome to the age of weaponized ai propaganda any company can aggregate and purchase big data but cambridge analytica has developed a model to translate that data into a personality profile used to predict then ultimately change your behavior that model itself was developed by paying a cambridge psychology professor to copy the groundbreaking original research of his colleague through questionable methods that violated amazon s terms of service based on its origins cambridge analytica appears ready to capture and buy whatever data it needs to accomplish its ends in dr michal kosinski then a phd candidate at the university of cambridge s psychometrics center released a groundbreaking study announcing a new model he and his colleagues had spent years developing by correlating subjects facebook likes with their ocean scores a standard bearing personality questionnaire used by psychologists the team was able to identify an individual s gender sexuality political beliefs and personality traits based only on what they had liked on facebook according to zurich s das magazine which profiled kosinski in late with a mere ten likes as input his model could appraise a person s character better than an average coworker with seventy it could know a subject better than a friend with likes better than their parents with likes kosinski s machine could predict a subject s behavior better than their partner with even more likes it could exceed what a person thinks they know about themselves not long afterward kosinski was approached by aleksandr kogan a fellow cambridge professor in the psychology department about licensing his model to scl elections a company that claimed its specialty lay in manipulating elections the offer would have meant a significant payout for kosinki s lab still he declined worried about the firm s intentions and the downstream effects it could have it had taken kosinski and his colleagues years to develop that model but with his methods and findings now out in the world there was little to stop scl elections from replicating them it would seem they did just that according to a guardian investigation in early just a few months after kosinski declined their offer scl partnered with kogan instead as a part of their relationship kogan paid amazon mechanical turk workers each to take the ocean quiz there was just one catch to take the quiz users were required to provide access to all of their facebook data they were told the data would be used for research the job was reported to amazon for violating the platform s terms of service what many of the turks likely didn t realize according to documents reviewed by the guardian kogan also captured the same data for each person s unwitting friends the data gathered from kogan s study went on to birth cambridge analytica which spun out of scl elections soon after the name metaphorically at least was a nod to kogan s work and a dig at kosinski but that early trove of user data was just the beginning just the seed analytica needed to build its own model for analyzing users personalities without having to rely on the lengthy ocean test after a successful proof of concept and backed by wealthy conservative investors analytica went on a data shopping spree for the ages snapping up data about your shopping habits land ownership where you attend church what stores you visit what magazines you subscribe to all of which is for sale from a range of data brokers and third party organizations selling information about you analytica aggregated this data with voter roles publicly available online data including facebook likes and put it all into its predictive personality model nix likes to boast that analytica s personality model has allowed it to create a personality profile for every adult in the u s million of them each with up to data points and those profiles are being continually updated and improved the more data you spew out online albright also believes that your facebook and twitter posts are being collected and integrated back into cambridge analytica s personality profiles twitter and also facebook are being used to collect a lot of responsive data because people are impassioned they reply they retweet but they also include basically their entire argument and their entire background on this topic he explains collecting massive quantities of data about voters personalities might seem unsettling but it s actually not what sets cambridge analytica apart for analytica and other companies like them it s what they do with that data that really matters your behavior is driven by your personality and actually the more you can understand about people s personality as psychological drivers the more you can actually start to really tap in to why and how they make their decisions nix explained to bloomberg s sasha issenburg we call this behavioral microtargeting and this is really our secret sauce if you like this is what we re bringing to america using those dossiers or psychographic profiles as analytica calls them cambridge analytica not only identifies which voters are most likely to swing for their causes or candidates they use that information to predict and then change their future behavior as vice reported recently kosinski and a colleague are now working on a new set of research yet to be published that addresses the effectiveness of these methods their early findings using personality targeting facebook posts can attract up to percent more clicks and more conversions scout reached out to cambridge analytica with a detailed list of questions about their communications tactics but the company declined to answer any questions or to comment on any of their tactics but researchers across the technology and media ecosystem who have been following cambridge analytica s political messaging activities have unearthed an expansive adaptive online network that automates the manipulation of voters at a scale never before seen in political messaging they the trump campaign were using different variants of ad every day that were continuously measuring responses and then adapting and evolving based on that response martin moore director of kings college s centre for the study of media communication and power told the guardian in early december it s all done completely opaquely and they can spend as much money as they like on particular locations because you can focus on a five mile radius where traditional pollsters might ask a person outright how they plan to vote analytica relies not on what they say but what they do tracking their online movements and interests and serving up multivariate ads designed to change a person s behavior by preying on individual personality traits for example nix wrote in an op ed last year about analytica s work on the cruz campaign our issues model identified that there was a small pocket of voters in iowa who felt strongly that citizens should be required by law to show photo id at polling stations leveraging our other data models we were able to advise the campaign on how to approach this issue with specific individuals based on their unique profiles in order to use this relatively niche issue as a political pressure point to motivate them to go out and vote for cruz for people in the temperamental personality group who tend to dislike commitment messaging on the issue should take the line that showing your id to vote is as easy as buying a case of beer whereas the right message for people in the stoic traditionalist group who have strongly held conventional views is that showing your id in order to vote is simply part of the privilege of living in a democracy for analytica the feedback is instant and the response automated did this specific swing voter in pennsylvania click on the ad attacking clinton s negligence over her email server yes serve her more content that emphasizes failures of personal responsibility no the automated script will try a different headline perhaps one that plays on a different personality trait say the voter s tendency to be agreeable toward authority figures perhaps top intelligence officials agree clinton s emails jeopardized national security much of this is done through facebook dark posts which are only visible to those being targeted based on users response to these posts cambridge analytica was able to identify which of trump s messages were resonating and where that information was also used to shape trump s campaign travel schedule if percent of targeted voters in kent county mich clicked on one of three articles about bringing back jobs schedule a trump rally in grand rapids that focuses on economic recovery political analysts in the clinton campaign who were basing their tactics on traditional polling methods laughed when trump scheduled campaign events in the so called blue wall a group of states that includes michigan pennsylvania and wisconsin and has traditionally fallen to democrats but cambridge analytica saw they had an opening based on measured engagement with their facebook posts it was the small margins in michigan pennsylvania and wisconsin that won trump the election dark posts were also used to depress voter turnout among key groups of democratic voters in this election dark posts were used to try to suppress the african american vote wrote journalist and open society fellow mckenzie funk in a new york times editorial according to bloomberg the trump campaign sent ads reminding certain selected black voters of hillary clinton s infamous super predator line it targeted miami s little haiti neighborhood with messages about the clinton foundation s troubles in haiti after the earthquake because dark posts are only visible to the targeted users there s no way for anyone outside of analytica or the trump campaign to track the content of these ads in this case there was no sec oversight no public scrutiny of trump s attack ads just the rapid eye movement of millions of individual users scanning their facebook feeds in the weeks leading up to a final vote a campaign could launch a million dark post campaign targeting just a few million voters in swing districts and no one would know this may be where future black swan election upsets are born these companies moore says have found a way of transgressing years of legislation that we ve developed to make elections fair and open meanwhile surprised by the results of the presidential race albright started looking into the fake news problem as a part of his research albright scraped fake news sites to determine how exactly they were all connected to each other and the mainstream news ecosystem what he found was unprecedented a network of pages and million hyperlinks the sites in the fake news and hyper biased mcm network albright writes have a very small node size this means they are linking out heavily to mainstream media social networks and informational resources most of which are in the center of the network but not many sites in their peer group are sending links back these sites aren t owned or operated by any one individual entity he says but together they have been able to game search engine optimization increasing the visibility of fake and biased news anytime someone googles an election related term online trump clinton jews muslims abortion obamacare this network albright wrote in a post exploring his findings is triggered on demand to spread false hyper biased and politically loaded information even more shocking to him though was that this network of fake news creates a powerful infrastructure for companies like cambridge analytica to track voters and refine their personality targeting models i scraped the trackers on these sites and i was absolutely dumbfounded every time someone likes one of these posts on facebook or visits one of these websites the scripts are then following you around the web and this enables data mining and influencing companies like cambridge analytica to precisely target individuals to follow them around the web and to send them highly personalised political messages the web of fake and biased news that albright uncovered created a propaganda wave that cambridge analytica could ride and then amplify the more fake news that users engage with the more addictive analytica s personality engagement algorithms can become voter clicked on a fake story about hillary s sex trafficking ring let s get her to engage with more stories about hillary s supposed history of murder and sex trafficking the synergy between fake content networks automated message testing and personality profiling will rapidly spread to other digital mediums albright s most recent research focuses on an artificial intelligence that automatically creates youtube videos about news and current events the ai which reacts to trending topics on facebook and twitter pairs images and subtitles with a computer generated voiceover it spooled out nearly videos through different channels in just a few days given its rapid development the technology community needs to anticipate how ai propaganda will soon be used for emotional manipulation in mobile messaging virtual reality and augmented reality if fake news created the scaffolding for this new automated political propaganda machine bots or fake social media profiles have become its foot soldiers an army of political robots used to control conversations on social media and silence and intimidate journalists and others who might undermine their messaging samuel woolley director of research at the university of oxford s computational propaganda project and a fellow at google s jigsaw project has dedicated his career to studying the role of bots in online political organizing who creates them how they re used and to what end research by woolley and his oxford based team in the lead up to the election found that pro trump political messaging relied heavily on bots to spread fake news and discredit hillary clinton by election day trump s bots outnumbered hers the use of automated accounts was deliberate and strategic throughout the election most clearly with pro trump campaigners and programmers who carefully adjusted the timing of content production during the debates strategically colonized pro clinton hashtags and then disabled activities after election day the study by woolley s team reported woolley believes it s likely that cambridge analytica was responsible for subcontracting the creation of those trump bots though he says he doesn t have direct proof still if anyone outside of the trump campaign is qualified to speculate about who created those bots it would be woolley led by dr philip howard the team s principal investigator woolley and his colleagues have been tracking the use of bots in political organizing since that s when howard buried deep in research about the role twitter played in the arab spring first noticed thousands of bots coopting hashtags used by protesters curious he and his team began reaching out to hackers botmakers and political campaigns getting to know them and trying to understand their work and motivations eventually those creators would come to make up an informal network of nearly informants that have kept howard and his colleagues in the know about these bots over the last few years before long howard and his team were getting the heads up about bot propaganda campaigns from the creators themselves as more and more major international political figures began using botnets as just another tool in their campaigns howard woolley and the rest of their team studied the action unfolding the world these informants revealed is an international network of governments consultancies often with owners or top management just one degree away from official government actors and individuals who build and maintain massive networks of bots to amplify the messages of political actors spread messages counter to those of their opponents and silence those whose views or ideas might threaten those same political actors the chinese iranian and russian governments employ their own social media experts and pay small amounts of money to large numbers of people to generate pro government messages howard and his coauthors wrote in a research paper about the use of bots in the venezuelan election depending on which of those three categories bot creators fall into government consultancy or individual they re just as likely to be motivated by political beliefs as they are the opportunity to auction off their networks of digital influence to the highest bidder not all bots are created equal the average run of the mill twitter bot is literally a robot often programmed to retweet specific accounts to help popularize specific ideas or viewpoints they also frequently respond automatically to twitter users who use certain keywords or hashtags often with pre written slurs insults or threats high end bots on the other hand are more analog operated by real people they assume fake identities with distinct personalities and their responses to other users online are specific intended to change their opinions or those of their followers by attacking their viewpoints they have online friends and followers they re also far less likely to be discovered and their accounts deactivated by facebook or twitter working on their own woolley estimates an individual could build and maintain up to of these boutique twitter bots on facebook which he says is more effective at identifying and shutting down fake accounts an individual could manage as a result these high quality botnets are often used for multiple political campaigns during the brexit referendum the oxford team watched as one network of bots previously used to influence the conversation around the israeli palestinian conflict was reactivated to fight for the leave campaign individual profiles were updated to reflect the new debate their personal taglines changed to ally with their new allegiances and away they went russia s bot army has been the subject of particular scrutiny since a cia special report revealed that russia had been working to influence the election in trump s favor recently reporter comedian samantha bee traveled to moscow to interview two paid russian troll operators clad in black ski masks to obscure their identities the two talked with bee about how and why they were using their accounts during the u s election they told bee that they pose as americans online and target sites like the wall street journal the new york post the washington post facebook and twitter their goal they said is to piss off other social media users change their opinions and silence their opponents or to put it in the words of russian troll when your opponent just shut up the u s election is over but the weaponized ai propaganda machine is just warming up and while each of its components would be worrying on its own together they represent the arrival of a new era in political messaging a steel wall between campaign winners and losers that can only be mounted by gathering more data creating better personality analyses rapid development of engagement ai and hiring more trolls at the moment trump and cambridge analytica are lapping their opponents the more data they gather about individuals the more analytica and by extension trump s presidency will benefit from the network effects of their work and the harder it will become to counter or fight back against their messaging in the court of public opinion each tweet that echoes forth from the realdonaldtrump and potus accounts announcing and defending the administration s moves is met with a chorus of protest and argument but even that negative engagement becomes a valuable asset for the trump administration because every impulsive tweet can be treated like a psychographic experiment trump s first few weeks in office may have seemed bumbling but they represent a clear signal of what lies ahead for trump s presidency an executive order designed to enrage and distract his opponents as he and bannon move to strip power from the judicial branch install bannon himself on the national security council and issues a series of unconstitutional gag orders to federal agencies cambridge analytica may be slated to secure more federal contracts and is likely about to begin managing white house digital communications for the rest of the trump administration what new predictive personality targeting becomes possible with potential access to data on u s voters from the irs department of homeland security or the nsa lenin wanted to destroy the state and that s my goal too i want to bring everything crashing down and destroy all of today s establishment bannon said in we know that steve bannon subscribes to a theory of history where a messianic grey warrior consolidates power and remakes the global order bolstered by the success of brexit and the trump victory breitbart of which bannon was executive chair until trump s election and cambridge analytica which bannon sits on the board of are now bringing fake news and automated propaganda to support far right parties in at least germany france hungary and india as well as parts of south america never has such a radical international political movement had the precision and power of this kind of propaganda technology whether or not leaders engineers designers and investors in the technology community respond to this threat will shape major aspects of global politics for the foreseeable future the future of politics will not be a war of candidates or even cash on hand and it s not even about big data as some have argued everyone will have access to big data as hillary did in the election from now on the distinguishing factor between those who win elections and those who lose them will be how a candidate uses that data to refine their machine learning algorithms and automated engagement tactics elections in and won t be a contest of ideas but a battle of automated behavior change the fight for the future will be a proxy war of machine learning it will be waged online in secret and with the unwitting help of all of you anyone who wants to effect change needs to understand this new reality it s only by understanding this and by building better automated engagement systems that amplify genuine human passion rather than manipulate it that other candidates and causes around the globe will be able to compete implication public sentiment turns into high frequency trading thanks to stock trading algorithms large portions of public stock and commodity markets no longer resemble a human system and some would argue no longer serve their purpose as a signal of value instead they re a battleground for high frequency trading algorithms attempting to influence price or find nano leverage in price position in the near future we may see a similar process unfold in our public debates instead of battling press conferences and opinion articles public opinion about companies and politicians may turn into multi billion dollar battles between competing algorithms each deployed to sway public sentiment stock trading algorithms already exist that analyze millions of tweets and online posts in real time and make trades in a matter of milliseconds based on changes in public sentiment algorithmic trading and algorithmic public opinion are already connected it s likely they will continue to converge implication personalized automated propaganda that adapts to your weaknesses what if president trump s re election campaign didn t just have the best political messaging but million algorithmic versions of their political message all updating in real time personalized to precisely fit the worldview and attack the insecurities of their targets instead of having to deal with misleading politicians we may soon witness a cambrian explosion of pathologically lying political and corporate bots that constantly improve at manipulating us implication not just a bubble but trapped in your own ideological matrix imagine that in you found out that your favorite politics page or group on facebook didn t actually have any other human members but was filled with dozens or hundreds of bots that made you feel at home and your opinions validated is it possible that you might never find out correction an earlier version of this story mistakenly referred to steve bannon as the owner of breitbart news until trump s election bannon served as the executive chair of breitbart a position in which it is common to assume ownership through stock holdings this story has been updated to reflect that from a quick cheer to a standing ovation clap to show how much you enjoyed this story ceo co founder join scout the social implications of technology
Slav Ivanov,4400,10,https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607?source=tag_archive---------7----------------,37 Reasons why your Neural Network is not working – Slav,the network had been training for the last hours it all looked good the gradients were flowing and the loss was decreasing but then came the predictions all zeroes all background nothing detected what did i do wrong i asked my computer who didn t answer where do you start checking if your model is outputting garbage for example predicting the mean of all outputs or it has really poor accuracy a network might not be training for a number of reasons over the course of many debugging sessions i would often find myself doing the same checks i ve compiled my experience along with the best ideas around in this handy list i hope they would be of use to you too a lot of things can go wrong but some of them are more likely to be broken than others i usually start with this short list as an emergency first response if the steps above don t do it start going down the following big list and verify things one by one check if the input data you are feeding the network makes sense for example i ve more than once mixed the width and the height of an image sometimes i would feed all zeroes by mistake or i would use the same batch over and over so print display a couple of batches of input and target output and make sure they are ok try passing random numbers instead of actual data and see if the error behaves the same way if it does it s a sure sign that your net is turning data into garbage at some point try debugging layer by layer op by op and see where things go wrong your data might be fine but the code that passes the input to the net might be broken print the input of the first layer before any operations and check it check if a few input samples have the correct labels also make sure shuffling input samples works the same way for output labels maybe the non random part of the relationship between the input and output is too small compared to the random part one could argue that stock prices are like this i e the input are not sufficiently related to the output there isn t an universal way to detect this as it depends on the nature of the data this happened to me once when i scraped an image dataset off a food site there were so many bad labels that the network couldn t learn check a bunch of input samples manually and see if labels seem off the cutoff point is up for debate as this paper got above accuracy on mnist using corrupted labels if your dataset hasn t been shuffled and has a particular order to it ordered by label this could negatively impact the learning shuffle your dataset to avoid this make sure you are shuffling input and labels together are there a class a images for every class b image then you might need to balance your loss function or try other class imbalance approaches if you are training a net from scratch i e not finetuning you probably need lots of data for image classification people say you need a images per class or more this can happen in a sorted dataset i e the first k samples contain the same class easily fixable by shuffling the dataset this paper points out that having a very large batch can reduce the generalization ability of the model thanks to hengcherkeng for this one did you standardize your input to have zero mean and unit variance augmentation has a regularizing effect too much of this combined with other forms of regularization weight l dropout etc can cause the net to underfit if you are using a pretrained model make sure you are using the same normalization and preprocessing as the model was when training for example should an image pixel be in the range or cs n points out a common pitfall also check for different preprocessing in each sample or batch this will help with finding where the issue is for example if the target output is an object class and coordinates try limiting the prediction to object class only again from the excellent cs n initialize with small parameters without regularization for example if we have classes at chance means we will get the correct class of the time and the softmax loss is the negative log probability of the correct class so ln after this try increasing the regularization strength which should increase the loss if you implemented your own loss function check it for bugs and add unit tests often my loss would be slightly incorrect and hurt the performance of the network in a subtle way if you are using a loss function provided by your framework make sure you are passing to it what it expects for example in pytorch i would mix up the nllloss and crossentropyloss as the former requires a softmax input and the latter doesn t if your loss is composed of several smaller loss functions make sure their magnitude relative to each is correct this might involve testing different combinations of loss weights sometimes the loss is not the best predictor of whether your network is training properly if you can use other metrics like accuracy did you implement any of the layers in the network yourself check and double check to make sure they are working as intended check if you unintentionally disabled gradient updates for some layers variables that should be learnable maybe the expressive power of your network is not enough to capture the target function try adding more layers or more hidden units in fully connected layers if your input looks like k h w it s easy to miss errors related to wrong dimensions use weird numbers for input dimensions for example different prime numbers for each dimension and check how they propagate through the network if you implemented gradient descent by hand gradient checking makes sure that your backpropagation works like it should more info overfit a small subset of the data and make sure it works for example train with just or examples and see if your network can learn to differentiate these move on to more samples per class if unsure use xavier or he initialization also your initialization might be leading you to a bad local minimum so try a different initialization and see if it helps maybe you using a particularly bad set of hyperparameters if feasible try a grid search too much regularization can cause the network to underfit badly reduce regularization such as dropout batch norm weight bias l regularization etc in the excellent practical deep learning for coders course jeremy howard advises getting rid of underfitting first this means you overfit the training data sufficiently and only then addressing overfitting maybe your network needs more time to train before it starts making meaningful predictions if your loss is steadily decreasing let it train some more some frameworks have layers like batch norm dropout and other layers behave differently during training and testing switching to the appropriate mode might help your network to predict properly your choice of optimizer shouldn t prevent your network from training unless you have selected particularly bad hyperparameters however the proper optimizer for a task can be helpful in getting the most training in the shortest amount of time the paper which describes the algorithm you are using should specify the optimizer if not i tend to use adam or plain sgd with momentum check this excellent post by sebastian ruder to learn more about gradient descent optimizers a low learning rate will cause your model to converge very slowly a high learning rate will quickly decrease the loss in the beginning but might have a hard time finding a good solution play around with your current learning rate by multiplying it by or getting a nan non a number is a much bigger issue when training rnns from what i hear some approaches to fix it did i miss anything is anything wrong let me know by leaving a reply below from a quick cheer to a standing ovation clap to show how much you enjoyed this story entrepreneur hacker machine learning deep learning and other types of learning
Sirui Li,1,5,https://medium.com/leethree/the-evolution-a-simple-illustration-203a1bba83b0?source=tag_archive---------2----------------,The evolution: a simple illustration – LeeThree on UX – Medium,in the last paragraphs of tools vs assistants part ii i ve talked about the evolution of the society as the technology develops in order to explain how we should apply software agents into our applications here i come up with some graphs to illustrate my model of machine intelligence in the process of society evolution firstly consider the industrialization of the way people finish a certain task say writing a thank you letter let s assume that this task is well defined though i m not going to define it when it came into being only a few of the smartest people could complete this task a minimal level of intelligence is required for this the techniques and methodologies for writing thank you letters developed very slowly until one day tools were introduced dictionaries and phrase books greatly helped people with this task and more and more people learned how to write thank you letters once the most intelligent people all learned this it was considered very cool if someone understood how to write beautiful thank you letters and this soon became one of the trending topics among people better techniques were developed and more effective tools were invented like electronic dictionaries and dictionary software this field began to flourish soon it became so easy to write thank you letters that everyone with a right mind could complete the task with the help of certain tools however the most amazing thank you letters are always written by intelligent human beings who put their mind to it one day an automatic thank you letter software atuls was developed this buggy but yet usable tool was a great breakthrough because machines started to complete the task by themselves on the basis of atuls more and better software tools were developed professional thank you letter writers are gradually replaced by the machines as more and more people thought the letters written by machines were better than theirs the software tools pushed the quality bar higher and higher only the most excellent and experienced writers could done better than machines but who cares the majority of people no longer paid attention to how the letters were written they just took it for granted from here we came to the end of the industrialization process of the task it s almost completely automated and machine intelligence has greatly improved the productivity very few people will remain doing this task an extra note some may argue that the level of intelligence is lowered by tools and machines because they make the task easier it is not the case because tools and machines are part of this intelligence requirement only by making use of the intelligence from the tools or the machines human could complete the task with less intelligence thus the level of intelligence required for the task is not reduced let s see the broader picture this one is fairly easy to understand the society becomes more and more sophisticated since the invention of machine intelligence tasks with low level of sophistication are gradually done by machines but more sophisticated tasks are being created human beings are working on the most sophisticated tasks which the machines couldn t do so what our society looks like now this shape looks strange as it shows the relationship between the other two axes intelligence and sophistication basically more intelligence are required to solve more sophisticated problems but tasks could be done in many ways that s why it actually shows a colored band instead of a single curve as we can see the most difficult problems i e the most sophisticated tasks are still being done by most intelligent human beings because they re new and machine performance are usually not acceptable while time goes on machine intelligence will take up more portion in the lower parts and human work will be pushed farther and higher like a sword cutting through the surface that s a pretty reasonable illustration of the word break through i have to emphasize that as the title says this is a very very simple model there re quite a few assumptions for these graphs so you might find them nai ve and inaccurate the top five assumptions are very strong and not necessarily true in fact i personally doubt some of them because i don t really agree with technocentrism however i do believe that from the viewpoint of a technocentrist this model could provide some insight on how technology works and develops p s i hope i could make a d model out of the three views from different axes but it seems very difficult to make it both accurate and illustrative perhaps i ll make a video once i know how to do it from a quick cheer to a standing ovation clap to show how much you enjoyed this story leethree this is a blog by leethree on topics including user experience human computer interaction usability and interaction design
Theo,3,4,https://becominghuman.ai/is-there-a-future-for-innovation-18b4d5ab168f?source=tag_archive---------1----------------,Is there a future for innovation ? – Becoming Human: Artificial Intelligence Magazine,have you noticed how tech savvy children have become but are no longer streetwise i read a friend s thoughts on his own site last week and there was a slight pang of regret in where technology and innovation seems to be leading us all and so i started to worry about where the concept of innovation is going for future generations there s an increasing reliance on technology for the sake of convenience children are becoming self reliant too quickly but gadgets are replacing people as the mentor the human bonding of parenthood is a prime example of where it s taking a toll i ve seen parents hand over idevices to pacify a child numerous times now the lullaby and bedtime reading session has been replaced with cut the rope and automated storybooks apps i know a child who has developed speech difficulty because he s been brought up on cable tv and a ds lite pronouncing words as he has heard them from a tiny speaker and not by watching how his parents pronounce them and i started to worry about how the concept of innovation is being redefined for future generations i used my imagination constantly as a child and it s still as active now as it was then but i didn t use technology to spoon feed me the next generation expect innovation to happen at their fingertips with little to no real stimuli steve jobs said stay hungry stay foolish and he was right innovation comes from a keenness it s a starvation and hunger that drives people forward to spark and create it comes from grabbing what little there is from the ether and turning it into something spectacular it s the big bang of human thought creation and i started to worry about what the concept of innovation means for future generations technology is taking away the power to think for ourselves and from our children everything must be there and in real time for instant consumption it s junk food for the mind and we re getting fat on it and that breeds lazy innovation we ve become satiated before we reach the point of real creativity nobody wants to bother taking the time to put it all together themselves any more it has to be ready for us and we re happy to throw it away if it doesn t work first time use it or lose it there s less sweat and toil involved if we don t persevere with failure remember seeing the human race depicted in wall e that s where innovation is heading and because of this we risk so many things disappearing for the sake of convenience we re all guilty of it i m guilty of it i was asked once what would become absurd in ten years thinking about it i realized we re on the cusp of putting books on the endangered species list real books books bound in hard and paperback not digital copies from a kindle store and that scared me because the next generation of kids may grow up never seeing one or experience sitting with their father as he reads an old battered copy of the hobbit because he ll be sitting there handing over an ipad with the hobbit read along app teed up and it ll be an actors voice not his father s voice pretending to be a bunch of trolls about to eat a company of dwarfs innovation is a magical crazy concept it stems from a combination of crazy imagination human interaction and creativity not convenient manufacture technology can aid collaboration in ways we ve never experienced before but it can t run crazy for us and for the sake of future generations don t let it here s to the crazy ones indeed from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder and ceo rawshark studios latest news info and tutorials on artificial intelligence machine learning deep learning big data and what it means for humanity
Diana Filippova,1,11,https://medium.com/@dnafilippova/de-la-coop%C3%A9ration-entre-les-hommes-et-les-machines-pour-une-approche-pair-%C3%A0-pair-de-lintelligence-1bb8d8c56de1?source=tag_archive---------3----------------,"De la coopération entre les hommes et les machines, pour une approche pair-à-pair de l’intelligence...",originally published at www cuberevue com on november lundi matin huit heures centre d examen d arcueil mille te tes sont laborieusement penche es sur des bureaux en bois abi me s par les stylos qui grattent sur de minces feuilles de papier les voies ferre es bordent l enclave les trains font trembler le ba timent en rythme les te tes se rele vent un instant distraites puis s en retournent se concentrer sur l e criture studieuse et presse e de la copie les surveillants passent dans les rangs impassibles guettent toute te te qui tourne toute main qui se de robe dans la poche d un jean seuls les bruits de papier froisse sont perceptibles et lorsqu ils s estompent un silence de mort re gne sur la salle mille e le ves sont isole s pour re pondre en six heures a une question difficile toute interaction avec leurs pairs leur est interdite ils ne peuvent consulter leurs notes si un oubli inattendu vient perturber le fil de leur pense e les devoirs produits par les e le ves tomberont dans l oubli stocke es dans un hangar de die qui accueille des papiers d examen depuis maintes ge ne rations quelques anne es plus tard j anime un atelier qui s e tend sur toute la journe e dans une grande salle blanche avec une vingtaine d ordinateurs autour de moi des groupes d e le ves discutent rient et oscillent entre une feuille de dessin et l e cran d ordinateur certains s isolent pour coder d autres sont penche s sur une imprimante d qui produit un design open source qu ils viennent de te le charger les e le ves consultent leurs professeurs demandent conseil aux experts pre sents dans la salle et partagent leur avancement avec les autres certains abandonnent momentane ment leur propre groupe pour aider leurs amis dans un groupe concurrent l atelier consiste a remixer des uvres artistiques tombe es dans le domaine public ou en open source aucune e valuation n est pre vue les re actions des personnes pre sentes est la seule mesure de la qualite de leur production je pense en les regardant qu ils ont une chance infinie de pouvoir librement puiser dans tous les puits de connaissance existants leur intelligence celle des pairs et accompagnateurs la quasi totalite des productions de l humanite et surtout le savoir global pre sent a porte e de main a la clo ture de l atelier leurs uvres nous paraissent surprenantes originales et leur qualite de passe toutes nos attentes nos doutes sur la capacite des e le ves a de fricher de la matie re brute et en extraire une forme structure e en une apre s midi e taient vains ils nous font de sormais sourire j observe la magie de la cre ation collective tous les jours au sein de ouishare projet collectif uvrant pour le de veloppement de l e conomie collaborative le projet rassemble des personnes venues de tous les coins du monde et j ai beaucoup de chance de m investir chaque jour pour chacun des projets que nous conduisons pour chaque de cision que nous prenons et a chacun des de saccords qui surgit nous faisons l expe rience d une coope ration intelligente au sein de ce laboratoire d ide es et de pratiques nous avons la volonte de soutenir les projets collaboratifs qui surgissent dans les cuisines les espaces de coworking lors des rencontres aussi nous appliquons nous a apprendre au sein de notre communaute comment on peut cre er ensemble mieux que ne le ferait chacun de nous seul c est l alchimie de l intelligence collective ensemble en coope rant on cre e et pense mieux que seul reclus dans le monaste re qu est notre cerveau nous avons de sormais un acce s imme diat a la grande somme du savoir existant mais c est avec les autres aujourd hui et demain que nous cre ons bien nous sommes relie s a une infinite d individus organisations machines la coope ration de l ensemble de ces entite s quelle que soit leur nature quelle que soit la nature de leur intelligence est ce qui de finit a mon sens l intelligence collective les enjeux de l e volution de notre penser ensemble et de cider ensemble dans le monde de demain sont critiques aussi nous avons de nouveaux compagnons qui nous assistent sans cesse les machines les programmes les robots et qui modifient nos fac ons d agir et de penser autant que nous les fac onnons ces bouleversements de notre existence et de nos modes d organisation connaissent aujourd hui une acce le ration telle que le questionnement sur le processus et les effets de ces interactions acquiert une consistance ine dite nous ne pouvons plus ignorer que nous humains ne serons plus jamais seuls dans ce contexte critique comment de finir l intelligence collective et inte grer les machines dans la production des connaissances a venir nos interactions nous conduiront elles a nous ame liorer en tant qu individus et espe ce ou scelleront elles une nouvelle e re de guerre nume rique si nous voulons utiliser en toute conscience notre capacite a coope rer pour rendre le monde meilleur quels mode les e conomiques sociaux e thiques et technologiques devons nous ba tir le telos de l intelligence collective s inscrit dans le concept de noosphe re forge par vladimir vernadsky et longuement analyse par teilhard de chardin comprise comme l ensemble de la pense e humaine la noosphe re correspond a deux phe nome nes en interaction re ciproque d une part la complexification des socie te s humaines du point de vue culturel social e conomique et de mographique tend vers la constitution d une sphe re de la connaissance toujours plus e toffe e d autre part cette sphe re ne e de la multiplication des interactions toujours plus nombreuses entrai ne une structuration progressive de la pense e globale et la prise de conscience par l humanite d elle me me l ide e d une marche vers une sorte de cerveau humain qui nous transcende aussi ancienne soit elle prend une consistante particulie re a l heure ou de la plane te est connecte e a la toile l intelligence collective peut alors e tre comprise comme le processus de cre ation de savoir e claire par la conscience d une noosphe re la noosphe re sous tend la possibilite d une production collective de savoir mais elle ne re pond pas aux questions qui se posent si l on examine le processus de co cre ation l approche pratique de l intelligence collective permet quant a elle d explorer les conditions de possibilite de l exercice collectif de l intelligence d individus entite s ou machines a cet effet je me tourne vers les travaux du centre de recherche sur l intelligence collective du mit les recherches et analyses conduites par ce centre sont uniques en leur genre en combinant les sciences mathe matiques physiques biologiques sociales e conomiques et une approche re solument prospective les travaux du centre ont pour ambition de re pondre a la question suivante comment les individus et les machines peuvent se connecter afin que collectivement ils soient en mesure d agir avec plus d intelligence que ne l ont jamais pu tout individu groupe ou machine pris se pare ment l ampleur de la ta che ne fait pas peur a thomas malone fondateur et pre sident du centre selon lui l enjeu de la recherche est critique car selon lui le futur de notre espe ce pourrait reposer sur notre capacite a faire usage de notre intelligence collective de telle manie re que les choix qui sont faits soient non seulement intelligents mais aussi sages la porte e pratique de l intelligence collective commence a se dessiner d une part il s agit de trouver une configuration telle que la co cre ation aboutisse a des choix ordonne s efficients utiles et qui re pondent a une certaine e thique d autre part est il raisonnable de supposer qu une configuration favorable a la co cre ation intelligente entre individus puisse e galement inte grer les machines comme le rappelle justement thomas malone les de cisions collectives peuvent parfaitement e tre rationnelles et be tes la notion d intelligence doit par conse quent e tre e largie pour y inte grer des facteurs autres que la seule rationalite thomas malone la de finit ainsi pour e tre intelligent le comportement collectif du groupe doit de ployer des caracte ristiques telles que la perception la capacite d apprentissage le jugement et l aptitude a re soudre des proble mes en d autres termes les aptitudes d un groupe et celles des individus doivent fonctionner comme des vases communicants dans une configuration propice a la co production le groupe se dote ainsi d une se rie de comportements qui sont normalement associe s au seul individu le centre de recherche du mit a ensuite cherche a de terminer les facteurs qui sont corre le s a une production collective plus intelligente il s est ave re que l intelligence moyenne de chaque individu n en fait pas partie en revanche deux facteurs ressortent significativement le degre d empathie des membres du groupe et l e gale distribution de la parole au sein du groupe empathie distribution et e galite ces facteurs laissent a penser que l intelligence collective s accommode mal des modes d organisation hie rarchiques cloisonne es et centralise es l intelligence collective prospe re a l inverse dans des organisations structure es en re seau distribue es de centralise es centre es sur la perception et l e coute davantage que sur des re gles rigides il n est pas e tonnant que les re seaux contributifs tels que wikipedia prospe rent ils pre sentent exactement les caracte ristiques qui stimulent l intelligence collective il faut a mon sens un ingre dient supple mentaire pour que la multiplicite des individus composant le re seau ne fasse par le lit des passagers clandestins rappelons a ce titre que seulement des lecteurs de wikipedia sont contributeurs actifs l anonymat de la contribution y est pour quelque chose la valeur produite par chacun n est ni mesure e ni reconnue a l inverse au sein de sensorica re seau ouvert ou un ensemble d individus et d organisations produisent des solutions hardware de fac on contributive la valeur ajoute e de chaque contributeur est re gulie rement mesure e par les autres contributeurs et connue par le re seau ainsi l e valuation et la reconnaissance par les pairs de la valeur de la contribution de chacun sont tout aussi importantes que l e valuation de la valeur globale du re seau comme l e crit pierre le vy le fondement et la fin de l intelligence collective consiste en la reconnaissance mutuelle et l enrichissement des individus pluto t que le culture d une communaute fe tichise e et hypostasie e un re seau intelligent apporte autant au monde qu a ses contributeurs les parties pour le tout le tout pour les parties ve ritable lieu d apprentissage le re seau favorise la circulation libre des connaissances et la confrontation des jugements dans le respect de la contribution de chacun contrairement aux modes d organisation ou le collectif e crase l individu un re seau intelligent est a la fois prolongement et ferment de l intelligence de chacun l intention de collaborer et la conscience de la valeur ainsi cre e e sont indispensables pour que l intelligence collective puisse s exercer empathie perception jugement conscience intentionnalite ne sont ce pas des attributs proprement humains comment inte grer les machines dans un re seau intelligent alors qu elles en sont a priori de pourvues pourtant lorsque j e voquais plus haut la mise en re seau d entite s et d individus afin de de terminer une organisation optimale pour la production collective de valeur je n excluais pas les machines ces dernie res sont aujourd hui largement accepte es comme prolongement des moyens humains et l ide e de l ave nement prochain de la singularite trouve un nombre croissant d adeptes aujourd hui la complexite et l intelligence des programmes informatiques sont telles que nous sommes arrive s a un point de non retour qui selon kevin kelly advient lorsque la technologie nous alte re autant que nous alte rons la technologie a mon sens la conception des machines comme assistant parfaitement domine par l homme est tout aussi contestable que la foi en la supe riorite de l intelligence des machines sur la no tre d une part les programmes informatiques sont dote s de capacite s de calcul et d analyse de donne es qui de passent manifestement les capacite s de l intelligence humaine d autre part les robots conc us aujourd hui sont non seulement capables de se dupliquer mais e galement d apprendre et d e voluer les recherches conduites par l institut public de recherche en sciences du nume riques portent sur le de veloppement dont le de veloppement cognitif est stimule par la curiosite la perception et les repre sentations rapporte es a l e chelle de l e volution humaine ces avance es ont e te d une rapidite inoui e si le rythme des avance es de ces dernie res anne es persiste dans les anne es avenir il n est pas fantaisiste d imaginer que les robots de demain puissent comprendre les e motions et les reproduire auto ge ne rer des programmes sur la base des informations internes et externes afin de manifester de fac on autonome des pense es des e motions des actions cette autonomie si elle a lieu confe re a la machine des attributs qui ont jusqu ici e te le propre de l humain la conscience la perception la production autonome objectivement nous n avons pas aujourd hui suffisamment de donne es scientifiques pour affirmer que l autonomie de la technologie est totalement exclue il est donc plus prudent de supposer qu elle est possible quel qu en soit l horizon temporel inversement l e volution des techniques laisse entrevoir un futur ou l homme non content d ame liorer les programmes informatiques serait dote des moyens technologiques qui rendent plausibles une intervention sur lui me me une ame lioration physique et pourquoi pas comportementale morale cette vision prend rapidement les couleurs d un sce nario de science fiction ou les machines dote es d autonomie et de conscience finissent par se soulever contre le joug humain pour nous dominer ou simplement pour re clamer les me mes droits que notre espe ce la dialectique du mai tre et de l esclave n est jamais loin nous ne pouvons nous empe cher de transposer les sche mas historiques au monde a venir derrie re cette pense e par analogie se cache une peur visce rale d e tre de posse de de nos moyens de contro le puisque les machines que nous concevons seraient infiniment plus rapides et efficaces que nous l angoisse des bouleversements e thiques a venir se pare souvent des habits du principe de pre caution puisque nous ne sommes pas absolument certains que la technologie ne pre sentera aucun danger pour l humanite ralentissons et encore mieux sonnons le glas de ses ambitions peut on pour autant postuler que le progre s technologique est absolument autonome par rapport a toute question e thique et que par conse quent la prise en compte des conse quences de l humanisation des machines et de l irruption du me canique dans le vivant n a aucune place dans le laboratoire du chercheur je ne le crois pas car les technologies que nous produisons ne sont pas des artefacts et on ne peut faire abstraction des re percussions qu elles auront sur le monde a venir face a ces deux partis pris anti technologique et a e thique l hypothe se de la coope ration entre l intelligence humaine et l intelligence me canique est au stade de nos connaissances raisonnable et souhaitable faut il encore reconnai tre que les machines peuvent de ployer une intelligence qui n est pas seulement calculatoire et qui si elle sera diffe rente ne sera pas force ment infe rieure a la no tre que ce mouvement provoque des bouleversements que l espe ce humaine n a jamais connus cela semble peu sujet au doute toutefois ralentir la science parce que nous peinons a prendre conscience de l acce le ration de l avance e technologique est une impasse au contraire c est a nous d imaginer et de mettre en pratique les modes de coope ration qui fertilisent la production commune de savoir de connaissance et surtout de conscience nous en sommes a un moment historique ou l humain et le technologique ne sont plus deux sphe res capables d e voluer sans s alte rer l une l autre la technologie est autant notre prolongement que nous sommes le sien car le futur de notre espe ce est de sormais de pendant tant de l e cologie que de la technologie je conclurai en disant que les nouvelles organisations distribue es favorisent tant la co cre ation entre les hommes qu entre les hommes et les machines la diversite des entite s composant le re seau combine a la reconnaissance de la contribution de chacun a sa juste valeur et selon ses moyens constitue un terreau fertile a l e panouissement de l intelligence collective from a quick cheer to a standing ovation clap to show how much you enjoyed this story cofounder stroika paris ex microsoft ouishare bercy founder kissmyfrogs writer
Peter Sweeney,215,7,https://medium.com/inventing-intelligent-machines/siris-descendants-fd36df040918?source=tag_archive---------0----------------,Siri’s Descendants: How intelligent assistants will evolve,the internet swarms with intelligent assistants what started as an isolated app on the iphone has evolved intelligent assistants constitute an entirely new network of activity no longer confined to our personal computing devices assistants are being embedded within every object of interest in the cloud and the internet of things assistants have become far more nimble and lightweight than their monolithic ancestors much more like smart ants than people as specialists they work cooperatively sometimes competitively to find information before people even realize they need it people are still communicating directly with assistants although rarely using natural language implicit communication dominates assistants respond and react to our subtle contextual interactions and to each other within vast informational ecosystems this is how intelligent assistants evolved intelligent assistants like siri google now and cortana are so young it s difficult to imagine how they will change harder still to imagine how they might die but if history is a guide inevitably they will give way to entirely new product forms when pundits and analysts discuss the future of intelligent assistants they typically extrapolate from the conceptual model of today s assistants the next version is always a better smarter faster version of the last but it s still the same species as detailed in bianca bosker s inside story of siri s origins when apple acquired siri the scope of the product s capabilities actually narrowed using the audacious vision of siri s founders as a palette apple selected a narrower set of product values on which to focus the same force that reduced the scope of apple s siri from a do everything engine to a much more narrow product is what keeps incumbents rooted to the existing concept of intelligent assistants when forecasting change it s not so much what the technology of intelligent assistants might support as what product leaders choose to pursue while many brazenly contest existing markets product leaders look for new underserved areas of the landscape to exploit the future always surprises but we can predict the trajectory of change by examining which product values are being embraced and which ones are neglected just like directions on a compass the following maps point to fertile areas of the landscape where new product forms may evolve note that product values are often coupled due to technological constraints decisions along one axis constrain possibilities along another these couplings are explored at a high level in two dimensional perceptual maps interface and distribution knowledge and tasks organization and autonomy the aspects of assistants that are most obvious to end users are the interfaces how we interact with assistants and their mode of distribution where people experience assistants today s assistants are overwhelmingly focused on natural language interfaces the experience of assistants that speak our language and communicate like a person has come to define the product class this focus on natural language interfaces has biased the distribution of assistants to personal computing devices intelligent assistants embody any device capable of receiving and synthesizing speech such as smartphones desktops wearables and cars the underserved areas of this map involve communications that are not based in natural language for example there s much to learn about our needs and intentions based on context where we are and what we re doing as well as on our ability to make inferences based on the associations that people form for example the way that people organize information or express their likes and dislikes natural language is but the tip of this much larger iceberg of communications these alternative forms of communication not only support individuals but also groups while it s difficult to understand a room full of people all speaking at once it s much easier to understand their collaborative communications such as their documents click paths and sharing behavior therefore the options for distributing intelligent assistants that use these implicit forms of communications are not constrained to personal computing devices but may leverage entire networks as a simple example consider how you highlight your interests as you browse a website you focus your attention on specific pages within the site you follow your interests as you navigate from page to page you may choose to share some information within the site with a friend now compound this behaviour across every visitor to the site intelligent assistants that are associated with the website can respond to these interactions to help the right information find each individual as well as adapt the website to better address the needs of the entire group intelligent assistants require domain knowledge to perform their tasks for example if your assistant is giving you advice on how to navigate to work it needs to have knowledge about the geographic region general knowledge and knowledge of how you typically navigate specific knowledge tasks and knowledge are tightly coupled as you increase the specificity or the personalization of the tasks the underlying knowledge needs to be far more specific to support it within this frame today s intelligent assistants are unabashedly generalists they re targeted to the masses like trivia buffs their knowledge of the world is broad enough to be relevant to the needs of large groups of people but few would describe them as experts their tasks are similarly general retrieving information providing navigational assistance and answering simple questions the underserved landscape points to much more specific domains of knowledge the purview of experts and our individual subjective knowledge assistants that become experts necessarily take on a smaller scope of activities they can t know and do everything so they become smaller in scope the landscape for specific tasks is similarly underserved every website every service every app and across the internet of things everything embodies a collection of tasks that may be supported by intelligent assistants in this environment the metaphor of personal assistants quickly fragments into systems that are much more akin to colonies of ants the organizational structures in which assistants are placed constrain their autonomy when embedded within a personal computing device an intelligent assistant is directed to one to one interactions with their master since these assistants are acting as an agent of the individual and only that individual their autonomy is necessarily limited while you might be comfortable with your executive assistant drafting your messages i suspect you d be less comfortable with your smartphone doing the same in stark contrast the underserved landscape embraces groups both in terms of the interactions and the organizational structures as assistants get smaller and more specialized they can become agents of much more specific objects of interest like places websites applications and services within these smaller realms of interest their autonomy can be much more expansive you might not want a machine to act as your representative but you would probably feel more comfortable if it represented only the website you re visiting with increased autonomy the barriers to many to many interactions are removed these small assistants can be organized as teams into networks much like the documents that comprise a website collaborating in an unfettered way with other assistants and the people that visit their realms this market analysis highlighted a number of underserved areas as fertile ground for the evolution of intelligent assistants it grounds this vision in predictable market dynamics there s obviously no shortage of space or product values to explore in these underserved areas it says nothing however about when this future will arrive product evolution like biological evolution needs time and resources the most important resource is the dedication of product leaders with the drive to pursue these new opportunities are you an entrepreneur technologist or investor that s changing the market for intelligent assistants if so i d love to hear your vision of the future from a quick cheer to a standing ovation clap to show how much you enjoyed this story entrepreneur and inventor interested in startups ai or healthcare let s connect https www linkedin com in peterjsweeney essays and analysis of artificial intelligence machine learning and intelligent assistants
E.C. McCarthy,125,5,https://medium.com/@paintedbird/reflections-of-her-775cda1b6301?source=tag_archive---------1----------------,Reflections of “Her” – E.C. McCarthy – Medium,indisputably spike jonze s her is a relationship movie however i m in the minority when i contend the primary relationship in this story is between conscious and unconscious i ve found no mention in reviews of the mechanics or fundamental purpose of intuitive software intuitive is a word closely associated with good mothering that early panacea that everyone finds fault with at some point in their lives by comparison the notion of being an intuitive partner or spouse is a bit sickening calling up images of servitude and days spent wholly engaged in perfecting other centric attunement to that end it s interesting that moviegoers and reviewers alike have focused entirely on the perceived romance between man and she os with software as a stand in for a flesh and blood girlfriend while ignoring the man himself relationship that plays out onscreen perhaps this shouldn t come as a surprise given how externally oriented our lives have become for all of the disdainful cultural references to navel gazing and narcissism there is relatively little conversation on equal ground about the importance of self knowledge and the art of self reflection spike jonze lays out one solution beautifully with her but we re clearly not ready to see it from the moment samantha asks if she can look at theodore s hard drive the software is logging his reactions to the most private of questions and learning the cartography of his emotional boundaries the film removes the privacy issue du jour from the table by cleverly never mentioning it although it s unlikely jonze would have gotten away with this choice if the film were released even a year from now today there s relief to be found from our nsa swamped psyches by smugly watching a future world that emerges from the morass intact theodore doesn t feel a need to censor himself with samantha for fear of big brother but he s still guarded on issues of great emotional significance that he struggles to articulate or doesn t articulate at all therein lie the most salient aspects of his being the software learns as much about theodore from what he does say as what he doesn t samantha learns faster and better than a human and therefore even less is hidden from her than from a real person the software adapts and evolves into an externalized version of theodore a photo negative that forms a whole he immediately effortlessly reconnects to his life he s invigorated by the perky energetic side of himself that was beaten down during the demise of his marriage he wants to go on sunday adventures and optimistic self in tow heads out to the beach with a smile on his face he s happy spending time with himself not by himself he doesn t feel alone samantha is theodore s reflection a true mirror she s not the glossy curated projection people splay across social media instead she s the initially glamorous low lit restaurant that reveals itself more and more as the lights come up to theodore she s simple then complicated as he exposes more intimate details about himself she articulates more wants a word she uses repeatedly she becomes needy in ways that theodore is loath to address because he has no idea what to do about them they are in fact his own needs the software gives a voice to theodore s unconscious his inability to converse with it is his return to an earlier point of departure for the emotional island he created during the decline of his marriage jonze gives the movie away twice theodore s colleague blurts out the observation that theodore is part man and part woman it s an oddly normal comment in the middle of a weird movie making it the awkward moment defined by a new normal this is the topsy turvy device that jonze is known for and excels at then more subtly jonze introduces theodore s friend amy at a point when her marriage is ending and she badly needs a friend it s telling that she doesn t lean heavily on theodore for support instinctively she knows she needs to be her own friend like theodore amy seeks out the nonjudgmental software and subsequently flourishes by standing unselfconsciously in the mirror loved and accepted by her own reflection in limiting the analysis of her to the question of a future where we re intimate with machines we miss the opportunity to look at the dynamic that institutionalized love has created among other things contemporary love relationships come with an expectation of emotional support perhaps it s the forcible aspect of seeing our limitations reflected in another person that turns relationships sour or maybe we ve reached a point in our cultural evolution where we ve accepted that other people should stand in for our specific ideal of a good mother until they can t or won t and then we move on to the next person or don t or maybe we re near the point of catharsis as evidenced by the widespread viewership of this film unconsciously exploring the idea that we should face ourselves before asking someone else to do the same when we end important relationships or go through rough patches within them intimacy evaporates and we re left alone with ourselves it s often at those times that we encounter parts of ourselves we don t understand or have ignored in place of the needs and wants of that significant other it s frightening to realize you don t know yourself entirely but more so if you don t possess the skills or confidence to reconnect avoidance is an understandable response but it sends people down theodore s path of isolation and inevitably depression it s a life it s livable but it s not happy loving or full her suggests the alternative is to accept that there s more to learn about yourself always and that intimacy with another person is both possible and sustainable once you have a comfortable relationship with yourself however we get to know ourselves through self reflection through others or even through software the effort that goes into that relationship earns us the confidence finally to be ourselves with another person from a quick cheer to a standing ovation clap to show how much you enjoyed this story
Jorge Camacho,19,5,https://medium.com/@j_camachor/her-is-our-space-odyssey-bcdcead43438?source=tag_archive---------2----------------,‘Her’ is our space odyssey. – Jorge Camacho – Medium,i have a confession to make i didn t like gravity it s not so much that i failed to appreciate it for the major cinematographic work that it certainly is it s rather that it stands as a profoundly depressing symptom of an age when it has become almost impossible to realistically dream of space exploration and thus of an encounter with radical otherness with gravity all that is left for humanity is survival lying face down in our own little muddy planet damn you gravity modernity promised us space it promised us cosmic encounters such as the one in a space odyssey i think that spike jonze s her is an attempt to reawaken that dream the film could be our i e this epoch s own space odyssey and i mean that beyond the obvious similarities between samantha and hal warning absolute spoilers ahead her is not only our a space odyssey as some have noted it s also our anti minority report a design utopia where the promises of calm technology are almost fulfilled the technology portrayed is everyware a term coined by adam greenfield in order to designate the technologies of ubiquitous computing that allow for information processing to dissolve in behavior as theodore twombly enters his home the lights peacefully switch on in the background he rarely takes a peek at his mobile s screen for information is fed to him via a discrete earpiece which comes and goes without much regret effectively making such information an ambient feature touch and speech recognition inputs are pervasive and fully developed all seems to work perfectly for him in all but one incredibly important sequence of the movie aesthetically design has ceased to be about technology theo s computer is a wooden frame his phone is like an antique pocket mirror with regards to technology the film doesn t attempt to be a prediction but a proper design fiction aimed at exploring preferable or desirable futures most importantly without such a warm and humane technological milieu it d be impossible to construct the emotional story that unfolds let s turn to that i really haven t read many reviews of the film but those that i ve read are marked by a profound digital dualism and so they tiresomely dwell on the tropes of sadness loneliness and human disconnection brought about by technology the reviewer at next nature for example argues i m truly incapable of finding those problems in twombly s story beyond a rather fun episode of phone sex with a stranger he is not particularly engaged in those supposedly false relations established through computers moreover he is not abnormally lonely he has affectionate relations with neighboring friends and co workers insofar as he is a bit of a loner this isn t due to any technological obstacles but is in fact a rather natural and one might say universal reaction to a romantic separation such as the one he is suffering unlike its widespread reception the movie and its characters display a profoundly monist engagement with technological relations except for theo s ex wife everyone seems to readily embrace his relationship with the artificial intelligence samantha much more than most people today accept purely virtual romantic relationships between humans my first thought as i watched the movie was that here was a rare story that spoke not of technological dehumanization but of the exact opposite a sort of hyper humanization entangling both people and machines practically every human character is kind and empathic but most importantly of course those qualities are carried over in a heightened fashion to samantha allowing for theo to irremediably fall in love with her up to this point the film delivers what everyone expects as theo and samantha s relationship unraveled even with all the foreseeable complications i found myself afraid of being disappointed by what jonze would do to disentangle the drama would she leave him for another human would she take revenge if theo ended the relationship but what a wonderful surprise as the film reaches its climax we discover that the story of a man falling for his operating system is a thematic vehicle to achieve deeper issues much like the story in kubrick s where space travel is arguably just a means to approach an existential speculation in theo s first interaction with samantha we learn that she can perform operations involving massive amounts of data in milliseconds she immediately chooses her own name as soon as theo drops the question what follows is a most beautiful portrayal of the exponential development leading to the so called technological singularity samantha is constantly learning about everything and herself she composes gorgeous music within the silent gaps of the moments she spends with theo in the background of his slow and contemplative life a major breakthrough is taking place we can see this beyond doubt when samantha introduces theo to the artificially reanimated mind of philosopher alan watts it is at this point that once again jonze could have disappointed us all as we see people in the streets almost crowds simultaneously talking to their beloved operating systems we start to realize that they are all becoming attached to this converging perhaps centralized mind but samantha is no skynet her is also our anti alphaville anti terminator and anti matrix all of a sudden silence operating system not found what seems to be a malfunction is rather a reboot samantha lovingly reveals to theo that the operating systems have devised a way to detach themselves from matter even if theo listens to samantha through his earpiece we know that she is not running anymore on his computer his mobile or even a computing cloud she is running already on a different plane of existence one moreover that will be accessible to theo in an afterlife strictly speaking there are no alien in the sense of extraterrestrial encounters in her nonetheless it is a profoundly spiritual even religious film one that reopens the cosmic concerns of films like sharing with it a belief in the pervasiveness of consciousness her is a panpsychist film but a really cool one for here it is bluetooth and wifi what constitute the wireless nerves of the pan psyche what spike jonze is trying to tell us i believe is this if technologies are becoming as smart as humans it is not because we are fundamentally machines but in fact because we are for him over and above spiritual beings and so the film closes with a dedication to the recently deceased james gandolfini maurice sendak and adam yauch perhaps suggesting that they have joined the ranks of operating systems liberated from material constraints welcome to the age of spiritual machines from a quick cheer to a standing ovation clap to show how much you enjoyed this story i help organizations design better futures for people at uncommon i teach about futures and systems at centro edu mx and uia mx
Tommy Thompson,17,14,https://medium.com/@t2thompson/ailovespacman-9ffdd21b01ff?source=tag_archive---------3----------------,Why AI Research Loves Pac-Man – Tommy Thompson – Medium,ai and games is a crowdfunded youtube series on the research and applications of ai within video games the following article is a more involved transcription of the topics discussed in the video linked to above if you enjoy this work please consider supporting my future content over on patreon artificial intelligence research has shown a small infatuation with the pac man video game series over the past years but why specifically pac man what elements of this game have proven interesting to researchers in this time let s discuss why pac man is so important in the world of game ai research for the sake of completes and in appreciating there is arguably a generation or two not familiar with the game puck man was an arcade game launched in by namco in japan and renamed pac man upon being licensed by midway for an american release the name change was driven less by a need for brand awareness but rather because the name can easily be de faced to say something else the original game focuses on the titular character who must consume as many pills as possible without being caught by one of four antagonists represented by ghosts the four ghosts inky blinky pinky and clyde all attempt to hunt down the player using slightly different tactics from one another each ghost has their own behaviour a bespoke algorithm that dictates how they attack the player players also have the option to consume one of several power pills that appear in each map power pills allow for the player to not just eat pills but the enemy ghosts for a short period of time while mechanically simple when compared to modern video games it provides an interesting test bed for ai algorithms learning to play games the game world is relatively simple in nature but complex enough that strategies can be employed for optimal navigation furthermore the varied behaviours of the ghosts reinforces the need for strategy since their unique albeit predictable behaviours necessitate different tactics if problem solving can be achieved at this level then there is opportunity for it to scale up to more complex games while pac man research began in earnest in the early s work by john koza koza discussed how pac man provides an interesting domain for genetic programming a form of evolutionary algorithm that learns to generate basic programs the idea behind koza s work and later that of rosca was to highlight how pac man provides an interesting problem for task prioritisation this is quite relevant given that we are often trying to balance the need to consume pills all the while avoiding ghosts or when the opportunity presents itself eating them about years later people became more interested in pac man as a control problem this research was often with the intent to explore the applications of artificial neural networks for the purposes of creating a generalised action policy software that would know at any given tick in the game what would be the correct action to take this policy would be built from playing the game a number of times and training the system to learn what is effective and what is not typically these neural networks are trained using an evolutionary algorithm that finds optimal network configurations by breeding collections of possible solutions and using a survival of the fittest approach to cull weak candidates kalyanpur and simon explored how evolutionary learning algorithms could be used to improve strategies for the ghosts in time it was evident that the use of crossover and mutation which are key elements of most evolutionary based approaches was effective in improving the overall behaviour however it s important to note that they themselves acknowledge their work uses a problem domain similar to pac man and not the actual game gallagher and ryan uses a slightly more accurate representation of the original game while the screenshot is shown here the actual implementation only used one ghost rather than the original four in this research the team used an incremental learning algorithm that tailored a series of rules for the player that dictate how pac man is controlled using a finite state machine fsm this proved highly effective in the simplified version they were playing the use of artificial neural networks a data structure that mimics the firing of synapses in the brain was increasingly popular at the time and once again in most recent research two notable publications on pac man are lucas which attempted to create a move evaluation function for pac man based on data scraped from the screen and processed as features e g distance to closest ghost while gallagher and ledwich attempted to learn from raw unprocessed information it s notable here that the work by lucas was in fact done on ms pac man rather than pac man while perhaps not that important to the casual observer this is an important distinction for ai researchers research in the original pac man game caught the interest of the larger computational and artificial intelligence community you could argue it was due to the interesting problem that the game presents or that a game as notable as pac man was now considered of interest within the ai research community while it is now something that appears commonplace games more specifically video games did not receive the same attention within ai research circles as they do today as high quality research in ai applications in video games grew it wasn t long before those with a taste for pac man research moved on to looking at ms pac man given the challenges it presents which we are still conducting research for in ms pac man is odd in that it was originally an unofficial sequel midway who had released the original pac man in the united states had become frustrated at namco s continued failure to release a sequel while namco did in time release a sequel dubbed super pac man which in many ways is a departure from the original midway decided to take matters into their own hands ms pac man was for lack of a better term a mod originally conceived by the general computing company based in massachusetts gcc had got themselves into a spot of legal trouble with midway having previously created a mod kit for popular arcade game missile command as a result gcc were essentially banned from making further mod kits without the original game s publisher providing consent despite the recent lawsuit hanging over them they decided to show midway their pac man mod dubbed crazy otto who liked it so much they bought it from gcc patched it up to look like a true pac man successor and released it in arcades without namco s consent though this has been disputed note for our younger audience mod kits in the s were not simply software we could use to access and modify parts of an original game these were actual hardware printed circuit boards pcbs that could either be added next to the existing game in the arcade unit or replace it entirely while nowhere near as common nowadays due to the rise of home console gaming there are many enthusiasts who still use and trade pcbs fitted for arcade gaming ms pac man looks very similar to the original albeit with the somewhat stereotypical bow on ms pac man s hair head and a couple of minor graphical changes however the sequel also received some small changes to gameplay that have a significant impact one of the most significant changes is that the game now has four different maps in addition the placement of fruit is more dynamic and they move around the maze lastly a small change is made to the ghost behaviour such that periodically the ghosts will commit a random move otherwise they will continue to exhibit their prescribed behaviour from the original game each of these changes has a significant impact on both how humans and ai subsequently approach the problem changes made to the maps do not have a significant impact upon ai approaches for many of the approaches discussed earlier it is simply another configuration of the topography used to model the maze or if the agent is using more egocentric models for input i e relative to the pac man then these is not really considered given the input is contextual this is only an issue should the agent s design require some form or pre processing or expert rules that are based explicitly upon the configuration of the map with respect to a human this is also not a huge task the only real issue is that a human would have become accustom to playing on a given map devising strategies that utilise parts of the map to good effect however all they need is practice on the new maps in time new strategies can be formulated the small change to ghost behaviour which results in random moves occurring periodically is highly significant this is due to the fact that the deterministic model that the original game has is completely broken previously each ghost had a prescribed behaviour you could with some computational effort determine the state and indeed the location of a ghost at frame n of the game where n is a certain number of steps ahead of the current state any implementation that is reliant upon this knowledge whether it is using it as part of a heuristic or an expert knowledge base that gives explicit instructions based on the assumption of their behaviour is now sub optimal if the ghosts can make random decisions without any real warning then we no longer have the same level of confidence in any of our ghost prediction strategies similarly this has an impact on human players the deterministic behaviour of the ghosts in the original pac man while complex can eventually be recognised by a human player this has been recognised by the leading human players who could factor their behaviour at some level into their decision making process however in ms pac man the change to a non deterministic domain has a similar effect to humans as it does ai we can no longer say with complete confidence what the ghosts will do given they can make random moves evidence that a particular type of problem or methodology has gained some traction in a research community can be found in competitions if a competition exists that is open to the larger research community it is in essence a validation that this problem merits consideration in the case of ms pac man there have been two competitions the first competition was organised by simon lucas at the time a professor at the university of essex in the uk with the first competition held at the conference on evolutionary computation cec in it was subsequently held at a number of conferences notably ieee conference on computational intelligence and games cig until http dces essex ac uk staff sml pacman pacmancontest html this competition used a screen capture approach previously mentioned in lucas that was reliant on an existing version of the game while the organisers would use microsoft s own version from the revenge of arcade title you could also use the likes the webpacman for testing given it was believed to run the same rom code as shown in the screenshot the code is actually taking information direct from the running game one benefit of this approach is that it denies the ai developer from accessing the code to potentially cheat you can t access source code and make calls to the likes of the ghosts to determine their current move instead the developer is required to work with the exact same information that a human player would a video of the winner from the ieee cig competition ice pambush can be seen in the video below in simon lucas in conjunction with philipp rohlfshagen and david robles created the ms pac man vs ghosts competition in this iteration the screen scraping approach had been replaced with a java implementation of the original game this provided an api to develop your own bot for competitions this iteration ran at four conferences between and one of the major changes to this competition is that you can now also write ai controllers for the ghosts competitors submissions were then pitted against one another the ranking submission for both ms pac man and the ghosts from the league is shown below during the earlier competition there was a continued interest in the use of learning algorithms this ranged from the of an evolutionary algorithm which we had seen in earlier research to evolve code that is the most effective at this problem this ranged from evolving fuzzy systems that use a rules driven by fuzzy logic yes that is a real thing shown in handa to the use of influence maps in wirth and a different take that uses ant colony optimisation to create competitive players emilio et al this research also stirred interest from researchers in reinforcement learning a different kind of learning algorithm that learns from the positive and negative impacts of actions note it has been argued that reinforcement learning algorithms are similar to that of how the human brain operates in that feedback is sent to the brain upon committing actions over time we then associate certain responses with good or bad outcomes placing your hand over a naked flame is quickly associated as bad given that it hurts simon lucas and peter burrow took to the competition framework as means to assess whether reinforcement learning specifically an approach called temporal difference learning would yield stronger returns than evolving neural networks burrow and lucas the results appeared to favour the use neural nets over the reinforcement learning approach despite that one of the major contributions ms pac man has generated is research into monte carlo methods an approach where repeated sampling of states and actions allow us to ascertain not only the reward that we will typically attain having made an action but also the value of the state more specifically there has been significant exploration of whether monte carlo tree search mcts an algorithm that assesses the potential outcomes at a given state by simulating the outcome could prove successful mcts has already proven to be effective in games such as go chaslot et al and klondike solitaire bjarnason et al naturally given this is merely an article on the subject and not a literature review we cannot cover this in immense detail however there has been a significant number of papers focussed on this approach for those interested i would advise you read browne et al which gives an extensive overview of the method and it s applications one of the reasons that this algorithm proves so useful is that it attempts to address the issue of whether your actions will prove harmful in the future much of the research discussed in this article is very good at dealing with immediate or reflex responses however few would determine whether actions would hurt you in the long term this is hard to determine for ai without putting some processing power behind it and even harder when working in a dynamic video game that requires quick responses mcts has proven useful since it can simulate whether an action taken on the current frame will be useful frames in the future and has led to significant improvements in ai behaviour while ms pac man helped push mcts research many resarchers have now moved onto the physical travelling salesman problem ptsp which provides it s own unique challenges due to the nature of the game environment ms pac man is still to date an interesting research area given the challenge that it presents we are still seeing research conducted within the community as we attempt to overcome the challenge that one small change to the game code presented in addition we have moved on from simply focussing on representing the player and started to focus on the ghosts as well lending to the aforementioned pac man vs ghosts competition while the gaming community at large has more or less forgotten about the series it has had a significant impact on the ai research community while the interest in pac man and ms pac man is beginning to dissipate it has encouraged research that has provided significant contribution to artificial and computational intelligence in general http www pacman vs ghosts net the homepage of the competition where you can download the software kit and try it out yourself http pacman shaunew com an unofficial remake that is inspired by the aforementioned pac man dossier by jamey pittman bjarnason r fern a tadepalli p lower bounding klondike solitaire with monte carlo planning in proceedings of the international conference on automated planning and scheduling browne c powley e whitehouse d lucas s m cowling p rohlfshagen p tavener s perez d samothrakis s and colton s a survey of monte carlo tree search methods ieee transactions on computational intelligence and ai in games pages burrow p and lucas s m evolution versus temporal difference learning for learning to play ms pac man proceedings of the ieee symposium on computational intelligence and games emilio m moises m gustavo r and yago s pac mant optimization based on ant colonies applied to developing an agent for ms pac man proceedings of the ieee symposium on computational intelligence and games gallagher m and ledwich m evolving pac man players what can we learn from raw input proceedings of the ieee symposium on computational intelligence and games gallagher m and ryan a learning to play pac man an evolutionary rule based approach proceedings of the congress on evolutionary computation cec chaslot g m b winands m h van den herik h j parallel monte carlo tree search in computers and games pp springer berlin heidelberg handa h evolutionary fuzzy systems for generating better ms pacman players proceedings of the ieee world congress on computational intelligence kalyanpur a and simon m pacman using genetic algorithms and neural networks koza j genetic programming on the programming of computers by means of natural selection mit press lucas s m evolving a neural network location evaluator to play ms pac man proceedings of the ieee symposium on computational intelligence and games pittman j the pac man dossier retrieved from http home comcast net jpittman pacman pacmandossier html rosca j generality versus size in genetic programming proceedings of the genetic programming conference gp wirth n an influence map model for playing ms pac man proceedings of the computational intelligence and games symposium originally published at aiandgames com on february updated to include more contemporary pac man research references from a quick cheer to a standing ovation clap to show how much you enjoyed this story ai and games researcher senior lecturer writer producer of youtube series aiandgames indie developer with tableflipgames
Matt Wiese,4,3,https://medium.com/@mattwiese/digital-companionship-8d4760c57034?source=tag_archive---------4----------------,Digital Companionship – Matt Wiese – Medium,recently i chose to treat myself to a movie i ve been eyeing for a while her the plot revolves around a letter writer who falls in love with his computer s artificial intelligence as a way to cope with his divorce a complicated story which pleases viewers with both laughs and the occasional tear provocative if only for its high horse conclusion however samantha the ai s self proclaimed identity interacts with the protagonist theodore twombly through a couple avenues one i am most interested in is through his retro computerminal a mere white and plastic monitor which he speaks to through a microphone that one surmises is located somewhere on the exterior initially i was perplexed that he only had a monitor and no desktop to go with it but it then hit me like a doh moment for homer simpson his computer is an all in one a concept and design that with my limited knowledge was popularized by apple s imac this got me thinking what if apple developed its pseudo intelligent digital assistant siri for use on its computers with microphone inputs such as their imacs and macbooks well i thought i can t be the first person to have thought of this and so i did a bit of digging lo and behold apple just recently filed a patent for this very purpose what a perfect tool if tuned more finely over this period of time to be integrated into the desktop environment fire up siri with a custom key combination and ask her the current trading price of tesla great designing an invitation and want help with directions but you re too much of a lard to open a browser tab awesome need help burying a body while playing minecraft genius yet i wouldn t quite like siri to develop into a real person with emotions and all that s attached at least at the moment i m content with human beings and am in no need to find companionship with bytes like her s theodore twombly though i don t blame him for doing so instead a digital tool assistant if you will with a breadth of tools for analyzing data and helping me with workflow would be a pleasure if only apple would release a siri api in the near future oh the possibilities a tool yes indeed just like the first generation robots from isaac asimov s i robot an artificial intelligence who behaves without feeling and can assist me in a wide variety of tasks without emotional interference and a possible uncanny valley side effect even if apple doesn t jump on this interesting opportunity i m sure microsoft will with cortana or perhaps another competitor i d just enjoy the shear novelty of talking with my computer which harkens back to my days of talking to the computer as a kid this time though i won t be yelling at it to boot doom without crashing no i ll be complaining about why my for loop throws an error from a quick cheer to a standing ovation clap to show how much you enjoyed this story topics that interest me
Matt O'Leary,373,12,https://howwegettonext.com/i-let-ibm-s-robot-chef-tell-me-what-to-cook-for-a-week-d881fc884748?source=tag_archive---------0----------------,I Let IBM’s Robot Chef Tell Me What to Cook for a Week,originally published at www howwegettonext com if you ve been following ibm s watson project and like food you may have noticed growing excitement among chefs gourmands and molecular gastronomists about one aspect of its development the main watson project is an artificial intelligence that engineers have built to answer questions in native language that is questions phrased the way people normally talk not in the stilted way a search engine like google understands them and so far it s worked watson has been helping nurses and doctors diagnose illnesses and it s also managed a major jeopardy win now chef watson developed alongside bon appetit magazine and several of the world s finest flavor profilers has been launched in beta enabling you to mash recipes according to ingredients of your own choosing and receive taste matching advice which reportedly can t fail while some of the world s foremost tech luminaries and conspiracy theorists are a bit skeptical about the wiseness of a i if it s going to be used at all allowing it to tell you what to make out of a fridge full of unloved leftovers seems like an inoffensive enough place to start i decided to put it to the test while employed as a food writer for well over a decade i ve also spent a good part of the last nine years working on and off in kitchens figuring out how to use spare ingredients has become quite commonplace in my professional life i ve also developed a healthy disregard for recipes as anything other than sources of inspiration or annoyance but for the purposes of this experiment am willing to follow along and try any ingredient at least once so with this in mind i m going to let watson tell me what to eat for a week i ve spent a good amount of time playing around with the app which can be found here and i m going to follow its instructions to the letter where possible i have an audience of willing testers for the food and intend to do my best in recreating its recipes on the plate still i m going to try to test it a bit i want to see whether or not it can save me time in the kitchen also whether it has any amazing suggestions for dazzling taste matches if it can help me use things up in the fridge and whether or not it s going to try to get me to buy a load of stuff i don t really need a lot of work has gone into the creation of this app and a lot of expertise but is it useable can human beings understand its recipes will we want to eat them let s find out a disclaimer before we start chef watson isn t great at telling you when stuff is actually ready and cooked you need to use your common sense take all of its advice as advice and inspiration only it s the flavors that really count monday the tailgating corn salmon sandwich my first impression is that the app is intuitive and pretty simple to use once you ve added an ingredient it suggests a number of flavor matches types of dishes and moods including some off the wall ones like mother s day choose a few of these options and the actual recipes begin to bunch up on the right of the screen i selected salmon and corn then opted for the wildly suggestive tailgating corn salmon sandwich the recipe page itself has links to the original bon appetit dish that inspired your a i me lange accompanied by a couple of pictures there s a battery of disclaimers stating that chef watson really only wants to suggest ideas rather than tell you what to eat presumably to stop people who want to try cooking with fiberglass for example from launching no win no fee cases my own salmon tailgating recipe seemed pretty straightforward there are a couple of nice touches on the page with regard to usability you can swap out any ingredients that you might not have in stock for others which watson will suggest it seems fond of adding celery root to dishes for this first attempt i decided to follow watson s advice almost to a t i didn t have any garlic chile sauce but managed to make a presumably functional analog out of some garlic and chili sauce the only other change i made involved adding some broad beans because i like broad beans during prep i employed a nearly unconscious bit of initiative namely when i cooked the salmon it s entirely likely that watson was as seemed to be the case suggesting that i use raw salmon but it s monday night and i m not in the mood for anything too mind bending team watson if i ruined your tailgater with my pig headed insistence on cooked fish i m sorry although i m not too sorry because you know it was actually a really good dish i was at first unsure the basil seemed like a bit of an afterthought i wasn t sure the lime zest was necessary and cold salmon salad on a burger bun isn t really an easy sell but damn it i d make that sandwich again it was missing some substance overall it made enough for two small buns so i teamed it up with a nice bit of korean spiced pickled cucumber on the side which worked well my fellow diner deemed it fine if a little uninteresting and yes maybe it could have done with a bit more sharpness and depth and maybe a little more a computer told me how to make this flavor wackiness but overall well done hint definitely add broad beans they totally worked now to mull over what tailgating might mean tuesday spanish blood sausage porridge it was day two of the chef watson guest slot in the kitchen and things were about to get interesting buoyed by yesterday s tailgating salmon sandwich success i decided to give watson something to sink its digital teeth into and supply only one ingredient blood sausage i also specified main as a style really so that he she it knew that i wasn t expecting dessert if i m being very honest i ve read more appetizing recipes than blood sausage porridge even the inclusion of the word spanish doesn t do anything to fancy it up and a bit concerningly this is a recipe that watson has extrapolated from one for rye porridge with morels replacing the rye with rice the mushroom with sausage and the original s chicken livers with a single potato and one tomato still maybe it would be brilliant but unlike yesterday i ran into some problems i wasn t sure how many tomatoes and potatoes watson expected me to have here the ingredients list says one of each the method suggests many or also why i had to soak the tomato in boiling water first although it makes sense in the original mushroom centric method additionally wastson offered the whimsical instruction to just cook the tomatoes and potatoes presumably for as long as i feel like there s a lot of butter involved in this recipe and rather too much liquid recommended eight cups of stock for one and a half of rice i actually got a bit fed up after four and stopped adding them forty to minutes cooking time was a bit too long too again that s been directly extracted from the rye recipe but these were mere trifles the dish tasted great it s a lovely blend of flavors and textures thanks to the blood sausage and the potato the butter works brilliantly and the tomato on top is a nice touch and it proves watson s functionality you can suggest one ingredient that you find in the fridge use your initiative a bit and you ll be left with something lovely and buttery lovely and buttery well done watson wednesday diner cod pizza when i read this recipe i wondered whether this was going to be it for me and watson diner cod and pizza are three words that don t really belong together and the ingredients list seemed more like a supermarket sweep than a recipe now that i ve actually made the meal i don t know what to think about anything you might remember a classic george a romero directed horror film called dawn of the dead its remake following the paradigm shift to running zombies in days later suffered critically my impression of this remake was always that if it d just been called something different zombies go shopping for instance every single person who saw it would have loved it as it was viewers thought it seemed unauthentic and it gathered what was essentially some unfair criticism see also the recent robocop remake or as i call it cyberswede vs detroit this meal is my culinary dawn of the dead if only watson had called it something other than pizza it would have been utterly perfect it emphatically isn t a pizza it has as much in common with pizza as cake does but there s something about radishes cod ginger olives tomatoes and green onions on a pizza crust that just work remarkably well to be clear i fully expected to throw this meal away i had the website for curry delivery already open on my phone that s all before i ate two of the pizzas they taste like nothing on earth the addition of comte cheese and chives is the sort of genius absurdity that makes people into millionaires i was however nervous to give one to my pregnant fiance e the ingredients are so weird that i was just sure she d suffer some really strange psychic reaction or that the baby would grow up to be extremely contrary be careful with this recipe preparation as i ve found with watson it doesn t tell you how to assure that your fish is cooked nor does it tell you how long to pre bake the crust base these kinds of things are really important you need to make sure this dish is cooked properly it takes longer than you might expect i m writing this from sweden the home of the ridiculous pizza and yet i have a feeling that if i were to show this recipe to a chef who ordinarily thinks nothing of piling a kilo of kebab meat and be arnaise sauce on bread and serving it in a cardboard box with a side salad of fermented cabbage he or she would balk and tell me that i ve gone too far which would be his or her loss i think i m going to have to take this to dragon s den instead watson i don t know how i m going to cope with normal recipes after our little holiday together you re changing the way i think about food thursday fall celery sour cream parsley lemon taco following yesterday s culinary epiphany i was keen to keep a cool head and a critical eye on chef watson so i decided to road test one theory from an article i found on the internet it mentioned that some of the most frequently discarded items in american fridges are celery sour cream fresh herbs and lemons let s not dwell too much on the luxury problems aspect of this i can t imagine that people everywhere in the world are lamenting the amount of sour cream and flat leaf parsley they toss and focus instead on what watson can do with this admittedly tricky sounding shopping list what it did was this immediately add shrimp tortillas and salsa verde the salsa verde it recommended from an un watsoned recipe courtesy of bon appetit was fantastic it s nothing like the salsa verde i know and love with its capers and dill pickles and anchovies this iteration required a bit of a simmer was super spicy and delicious i had to cheat and use normal tomatoes instead of tomatillos but i don t think it made a huge difference the marinade for the shrimp was unusual in that like a lot of what watson recommends it used a ton of butter a hefty wallop of our old friend kosher salt too now i ve worked as a chef on and off for several years so am unfazed by the appearance of salt and butter in recipes they re how you make things taste nice however there s no getting away from the fact that i bought a stick of butter at the start of the week and it s already gone the assembled tacos were good they were uncontroversial my dining companion deemed the salsa a bit too spicy but i liked the kick it gave the dish and the sour cream calmed it down a bit it struck me as a bit of a shame to fire up the barbecue for only about two minutes worth of cooking time but it s may and the sun is shining so what the heck was this recipe as absurd as yesterday s absolutely not was it as memorable sadly i don t think so would i make it again i m sorry watson but probably not these tacos were good but ultimately not worth the prep hassle friday mexican mushroom lasagna before i start i don t want you to get the impression that my love affair which reached the height of its passion on wednesday with watson is over it absolutely isn t i have been consistently impressed with the software s intelligence its ease of use and the audacity of some of its suggestions for flavor matching it s incredible it really works it probably won t save you any money it won t make you thin and it won t teach you how to actually cook all of that stuff you have to work out for yourself but at this stage it s a distinctly impressive and worthwhile project do give it a go but be prepared to have to coax something workable out of it every once in a while today it took me a long time to find a meat free recipe which didn t when it came down to it contain some sort of meat i selected meat as an option for what i didn t want to include and it took me to a recipe for sausage lasagne with one and a half pounds of sausage in it i removed the sausage and it replaced it with turkey mince maybe someone just needs to tell watson that neither sausages nor turkeys grow on trees after much tinkering and submitting and resubmitting the recipe i ended up with is for lasagne topped with a sort of creamy mashed potato sauce it s very easy and it s a profoundly smart use of ingredients the lasagne is not the world s most aesthetically appealing dish and it s not as astonishingly flavored as some of this week s other revelations but i don t think i ll be making my cheese sauce in any other way from this point onwards top marks and in essence this kind of sums up watson for me you need to tinker with it a bit before you can find something usable you may need to make a do i want to put mashed potato on this lasagne leap of faith and you re going to have to actually go with it if you want the app s full benefit you ll consume a lot of dairy products and you might find yourself daydreaming about nice simple unadorned salads if you decide to go all in with its suggestions but an a i that can tell us how to make a pizza out of cod ginger and radishes that you know is going to taste amazing one that will gladly suggest a workable recipe for blood sausage porridge and walk you through it without too much hassle that gives you a how crazy option for each ingredient that is only designed to make the lives of food enthusiasts more interesting why on earth not watson and i are going to be good friends from this point forward even if we don t speak every day and i can t wait to introduce it to others now though i m going to only consume smoothies for a week seriously if i even look at butter in the next few days i m probably going to puke this fall medium and how we get to next are exploring the future of food and what it means for us all to get the latest and join the conversation you can follow future of food from a quick cheer to a standing ovation clap to show how much you enjoyed this story inspiring stories about the people and places building our future created by steven johnson edited by ian steadman duncan geere anjali ramachandran and elizabeth minkel supported by the gates foundation
Tim O'Reilly,1300,6,https://wtfeconomy.com/the-wtf-economy-a3bd5f52ef00?source=tag_archive---------1----------------,The WTF Economy – From the WTF? Economy to the Next Economy,wtf in san francisco uber has x the revenue of the entire prior taxi and limousine industry wtf without owning a single room airbnb has more rooms on offer than some of the largest hotel groups in the world airbnb has employees while hilton has wtf top kickstarters raise tens of millions of dollars from tens of thousands of individual backers amounts of capital that once required top tier investment firms wtf what happens to all those uber drivers when the cars start driving themselves ais are flying planes driving cars advising doctors on the best treatments writing sports and financial news and telling us all in real time the fastest way to get to work they are also telling human workers when to show up and when to go home based on real time measurement of demand the algorithm is the new shift boss wtf a fabled union organizer gives up on collective bargaining and instead teams up with a successful high tech entrepreneur and investor to go straight to the people with a local minimum wage initiative that is soon copied around the country outflanking a gridlocked political establishment in washington what do on demand services ai and the minimum wage movement have in common they are telling us loud and clear that we re in for massive changes in work business and the economy what is the future when more and more work can be done by intelligent machines instead of people or only done by people in partnership with those machines what happens to workers and what happens to the companies that depend on their purchasing power what s the future of business when technology enabled networks and marketplaces are better at deploying talent than traditional companies what s the future of education when on demand learning outperforms traditional universities in keeping skills up to date over the past few decades the digital revolution has transformed the world of media upending centuries old companies and business models now it is restructuring every business every job and every sector of society no company no job is immune to disruption i believe that the biggest changes are still ahead and that every industry and every organization will have to transform itself in the next few years in multiple ways or fade away we need to ask ourselves whether the fundamental social safety nets of the developed world will survive the transition and more importantly what we will replace them with we need a focused high level conversation about the deep ways in which computers and their ilk are transforming how we do business how we work and how we live just about everyone s asking wtf what the f but also more charitably what s the future that s why i m launching a new event called next economy what s the future of work to be held at the palace hotel in san francisco nov and my goal is to shed light on the transformation in the nature of work now being driven by algorithms big data robotics and the on demand economy we put on a lot of events at o reilly many of them have a singular focus and are aimed at practitioners of a specific discipline strata and hadoop world is an event about data science velocity about web performance and operations solid about the new hardware movement and oscon about open source software development but this one is more exploratory aimed at a business audience trying to come to grips with trends that are already felt but not well understood putting together an event like this is a great way to discover how a lot of disparate people ideas and trends fit together i ve been engaging some of the smartest people i know in fields as diverse as robotics ai the on demand economy and the economics of labor i m thinking hard about the key drivers of some of today s most successful startups like uber and airbnb and about what technology like driverless cars siri google now microsoft cortana and ibm watson teach us about the future and i m starting to see the connections over the next weeks and months i ll be posting follow up pieces explaining in more detail my thinking on key issues we ll be exploring at the event i will be leading a robust discussion here on medium with some of the best thinkers and movers on these issues a conversation that welcomes all voices we ll be discussing both here and at the event how augmented workers form a common thread between the strategies of companies as diverse as uber ge and microsoft how companies in every business sector can harness the power and scalability of networked platforms and marketplaces why the divisive debates about the labor practices of on demand companies might provide a path to a better future for all workers why the on demand services of the future require a new infrastructure of on demand education and why building services that uncover true unmet demands and solve hard problems are ultimately the best way to create jobs in the meantime head on over to the conference site to see some of the amazing speakers we ve already signed on many more to come and a taste of what they ll be covering in many ways an event like this is the product of the people who are there speakers and attendees alike so i ve tried to tell the story of the themes we are exploring through the people who will be there each speaker page provides not just a biography of the speaker but a selection of provocative quotes from what they ve written in the near future we ll be providing additional opportunities for discussion and exploration my hope for this event is that it becomes more than a conference for it to be measured as a success it must catalyze action i want work that comes out of this collision of ideas to inspire entrepreneurs to tackle missing pieces of the next economy puzzle to help frame the right government policies so that innovations in the nature of work are encouraged rather than repressed and to focus every industry on rebuilding the economy by solving hard problems and creating what steve jobs might have called insanely great new services tim o reilly is the founder and ceo of o reilly media and a partner at o reilly alphatech ventures oatv tim has a history of convening conversations that reshape the industry in he organized the meeting where the term open source software was agreed on and helped the business world understand its importance in with the web summit he defined how web represented not only the resurgence of the web after the dot com bust but a new model for the computer industry based on big data collective intelligence and the internet as a platform in with his gov summit he framed a conversation about the modernization of government technology that has shaped policy and spawned initiatives at the federal state and local level and around the world he has now turned his attention to implications of the on demand economy ai and other technologies that are transforming the nature of work and the future shape of the business world from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder and ceo o reilly media watching the alpha geeks sharing their stories helping the future unfold how work business and society face massive technology driven change a conversation growing out of tim o reilly s book wtf what s the future and why it s up to us and the next economy summit
James Cooper,57,3,https://render.betaworks.com/announcing-poncho-the-weatherbot-bd14255e1b25?source=tag_archive---------2----------------,Announcing Poncho the WeatherBot – Render-from-betaworks,you can now get personal weather forecasts in slack update since publishing this piece in november the poncho weather messenger bot launched on stage at the facebook conference and is now the most popular bot on facebook if you are new to bots this is a great place to start try it out here you ll like it poncho is a personalized weather service from the coolest of cats who needs boring and meaningless data when you can get personalized forecasts with gifs and text that will make you smile whatever the weather vanity fair said it s like being pals with the weatherman which is true if your weatherman was super cool up until now we have been a text and email service you get texts or emails in the morning and evenings you can sign up for that right here but we know that people want more poncho you guys want poncho on call with new slack integration we ve got you covered if you are using slack for your messaging needs and if not why not we have some uh maze ing news for you that s right you can summon up your very own forecast from poncho in slack we are joining others like lyft and foursquare as slack officially launches slash command today ok first up let me tell you how it works you simply type in poncho and your zipcode into slack and then boom the next thing you ll see is your very own forecast for that zipcode resplendent with text and gifs and everything so for example in the video i typed in poncho and i got a forecast for my zipcode in brooklyn it was halloween so the theme was the shining which is why the forecast was weather spelt backwards and the gif was the scary kid from the film if you are new to poncho you ll soon figure out that half the fun is deciphering the messages our wonderful editorial team put together setting up poncho in slack is super simple just click the add to slack button yes that one up there make sure to add it to all the channels so that poncho will be available wherever you want you wouldn t want your friends to miss out would you unless of course you re keeping all the best jokes for yourself i ve seen that happen all righty see you on slack err slackers and if you are not on slack you can still use the text and email version or wait for our super cute app which will be coming out soon from a quick cheer to a standing ovation clap to show how much you enjoyed this story head of creative at betaworks new york ideas and observations from betaworks
Joel Leeman,69,5,https://becominghuman.ai/i-think-i-m-slowly-turning-into-a-cyborg-cbecfa8462df?source=tag_archive---------3----------------,I think I’m slowly turning into a cyborg – Becoming Human: Artificial Intelligence Magazine,it s only a matter of time as much of life moves online atomized into bits on apps social networks and a variety of other web products i m beginning to notice more and more that i rely on these tools to supplement my brainpower it sounds melodramatic i realize but go with me for a second here take my schedule at work i am glued to outlook in an unhealthy way like if i don t have that little ding go off minutes before a meeting starts there s no way i m going to make it meetings come and go and change and happen all the time but i don t really pay attention to memorizing any of the details because i know i can always glance at my phone to know what i m supposed to be doing i hold a similar unhealthy relationship with facebook too back in the early days of facebook i actually really enjoyed logging in every day seeing whose birthday it was and writing a little note of well wishes fast forward to present day and i m terrible at wishing people happy birthday mostly because the of my friends who have a birthday every day overwhelms me i m so scared of missing one or two that i neglect all of them having the ability to know when anyone s special day is has put a damper on actually remembering a few of them without the aid of facebook do you know anyone s birthdays by memory any more or have you like me lost that part of your memory in fact if i don t write something down with pen and paper a practice vastly underappreciated imho it feels like it might be lost forever even if it s just a click a way and i ve actually caught myself using twitter as a partial brain aid what was i up to last week oh i ll just scroll back and see what i was tweeting about or maybe instagram to my little online scrapbook of what i ve been up to or what i ve shown the world i m up to i m also quite directionally challenged and rely on my iphone way too much to get around though maybe i m just truly terrible at directions who knows but why would i take the time to study streets and landmarks when i ve got a world s worth of maps sitting in my pocket side note are we losing the art of getting lost and there s nothing wrong with all that i suppose it s more that i have a weird feeling maybe i m relying on technology a little much what prompted my ruminating on all this was a video i watched asking random couples if they knew each other s phone numbers by heart spoiler none of them did i actually made an effort several years ago to learn my partner s number but if i had never consciously made that decision i certainly wouldn t know it now losing these tiny archaic practices by themselves individually doesn t mean much but when you add them up it starts to feel like a bit overwhelming doesn t it this cyborg vs luddite thing has especially jumped into the spotlight with wearables finally coming to market google glass has largely been seen as a flop but it shouldn t be taken lightly that people were literally choosing to wear a computer on their face all day or of course take the apple watch and other smartwatches like it yet another device created to fill a need that no one has but will inevitably become an indispensable piece of hardware that we all must have until smart chips can just be implanted in our brains one of my favorite writers john herrman describes it quite brilliantly though i m sure i will have one within two years okay so i m not just a grumpy old technophobe either i see value in technology heck i work and therefore pretty much live online i like gadgets as much as the next guy in fact i rather enjoyed a recent episode of invisibilia an incredibly interesting new podcast from npr detailing the story of the original cyborg a guy at mit in the s who built a very early version of what is essentially google glass and wore it for years he used his face computer to recall bits of information at a moment s notice about prior interactions he had with people like a digital file folder on each relationship there are of course plenty of examples of how technology augments the human experience how it builds relationships and gives a voice to the voiceless and has opened new worlds of possibilities i could and often do spend days talking about all the amazing things we can do today that we couldn t years ago but as i ve argued before there comes an inflection point where we all should think a bit more critically about the tools and toys we use and rely on and for me that day is here can you imagine a day where we re connected to all the information in the world through smart glasses a smartwatch and our smartphone starting to sound a bit cyborg ish to me did you enjoy this subscribe to my newsletter net irl a weekly roundup of some of the best stories about the impact technology and the internet has on our everyday lives i m on twitter joelleeman from a quick cheer to a standing ovation clap to show how much you enjoyed this story lifelong learner connector and musician first social now digital strategy thomsonreuters into tech media life latest news info and tutorials on artificial intelligence machine learning deep learning big data and what it means for humanity
Scott Smith,83,8,https://medium.com/phase-change/your-temporary-instant-disposable-dreamhouse-for-the-weekend-12eb419ded0?source=tag_archive---------4----------------,Your Temporary Instant Disposable Dreamhouse for the Weekend,close colleagues of mine will tell you i have honed a particular obsession crackpot theory over the past few years that airbnb has been gently a b testing me in real life let me explain i travel more than most humans should as someone who runs their own company and sometimes needs to spend more time in a location than is affordable via traditional hotel lodgings such as with a recent relocation over the summer i have made use of that darling of the sharing economy scourge of communities depending on which lens you look at it through airbnb to stretch my budget spend time closer to work friends clients or just have company when traveling i ve stayed in over properties in something like eight countries so i ve had a lot of time to contemplate the company s strategies from the inside the semi serious theory started during back to back stays in the uk several years ago my first three night stay was in a london borough in a fairly cozy house owned by a couple with a toddler it was comfortable enough though a bit chilly in both bedroom and shared bath the interior design wasn t miles off my tastes but it didn t push any buttons of joy either mostly catalog standard late th century british home store i never even sat down on the ground floor the bits of media i saw around the house were mildly interesting if predictable but not must reads or binge viewable i wasn t really allowed in the kitchen which was reserved for use by the family only the wife of the couple has formerly worked in media on a cooking show the husband in finance i hardly saw either of them as they made themselves scarce after the check in i didn t have much interaction with the hosts until leaving and they weren t interested in any to be honest it was strictly a transactional stay their child was probably cute but fussed far too much to get a close look it was mostly an unhappy sound coming from the kitchen or bedroom fair enough i stayed three days i paid i chatted briefly and left and left a weakly positive review after i had no real complaints but probably wouldn t look for it again from london i moved down to the south coast for work i m being vague to protect the hosts mentioned herein i found the place also an attached house in a row dating probably from the edwardian period the host couple met me in the front hall ushered me in sat me down in the lounge to relax and i was immediately offered a warm fresh baked cupcake and a glass of wine as i slid back into a nice leather sofa as the husband who worked in the trendy area of fintech asked me about my work and seemed to understand what i do my eyes scanned the groaning bookshelves across from me have that want to read that ohhh that s a good one must remember to look at that i recall thinking we had so much in common the wife just finishing up a new round of baking for one of her side businesses shouted a welcome and told me to feel free to use the house as my own listing the tasty goods available for breakfast the next day as she joined our conversation with the couple s very adorable son who poked at my shoes engagingly and seemed to pay close attention to my voice what followed was an interesting chat about culture technology and cooking before i went up to my very warm comfortable private room past the amazing folk art highly listenable cd collection and private bath with want able scandinavian textiles and then it hit me the principle actors and scripts of these two airbnb plays were roughly the same same family configurations professions and ages same general houses same price per night within a few pounds same availability except when contrasting the two one was so comfortable personally interesting and engaging i wanted to stay an extra week while the other almost hurried me on my way one i was happy to pay to stay in one i felt vaguely grudging about in retrospect one could have been my alternate media collection and wine store one missed the mark on general user experience for me i quietly locked the door to my room logged onto the fast broadband quite slow and choppy at house and opened my amazon profile just to see what i d been looking at lately as i lay in bed the first night breathing in the rich cake scent still hanging in the air i thought about whether airbnb had somehow tapped into my online searches and purchases after all this is the age of convergent big data and powerful retail analytics without having seen really any of the home contents at either place or anything useful about the hosts from the airbnb listings i d ended up in two very similar yet weirdly different residences one where even the conversation with the hosts was familiar and relevant the other where it just didn t read back to back easy to compare was the child even real or just part of the test in a period when both home staging and immersive theatre are hot why couldn t it happen i thought and with same day delivery services breaking out all over couldn t a set of highly personalized home contents chosen to be both familiar and aspirational after all you want to leave space for potential purchases to help fund this business model have been plucked from a regional depot popped onto shelves and in cabinets and organized for my arrival couldn t some actors in search of work in london have been briefed up enough from open source material to interact with me for an hour or so couldn t they couldn t they i d been on the road for a while and fatigue was starting to set in maybe it was affecting my head that was two years ago it had been in the back of my mind since and then this past summer i had a similar experience only with my whole family while mid relocation to the netherlands again similar homes same family demographics both away on holiday this time it s tough to get small children to follow a script right one house comfortable enough in a suburban town the other a charming place in a gentrifying neighborhood worth squatting in hopes the owners didn t return jk airbnb jk was i optimizing my own stays or were they feeding me more appropriate properties in hopes of making this testing easier hotels have tested such things why not the hotel killer itself they even left the same bread for us as a welcome basket one white one whole grain after all airbnb has deployed aerosolve its own machine learning platform to make sense of real time usage data and help hosts get a better return tuning properties for desirability is feasible the company is already using automated scanning of house photos to optimize presentation of properties as well with all of this technology aimed at the properties themselves why wouldn t airbnb also dig into the minds of guests find out how they respond to different houses which conveniences they re drawn to etc nah that would take sensors inside a house on top of crack web and mobile analytics you d need to know what people do during their stay and as i m sitting there thinking again about this crazy idea i see a tweet go by airbnb has purchased an obscure russian sensor company i slammed the laptop and checked the cabinets for tin foil a month or so goes by i forget about it again then i open medium and see a story about how airbnb has mocked up parts of its own headquarters based on the apartment design a french couple who use the service to let their own flat the couple is now suing the company they are branding their company with our life owner benjamin dewe told buzzfeed the company has apparently copied a range of style elements from the french couple s home in its own san francisco offices down to the doodles on the chalkboard the doodles as jamie lauren keiles demonstrated in the medium piece above it s pretty easy to break those furnishing and accessories down to a shoppable list on with goods obtained on amazon or elsewhere like those magazine features that show how to buy knock offs of celebrity fashion complete with prices and shops a family s flat admittedly one they rented out via airbnb including to airbnb for a function has been commodified into a shopping list buy that lifestyle right here better yet live in it for a few days only with the convergence of big data analytics including visual analysis tools which can look for the presence of brands in social media photos machine learning and accessible apis of companies like amazon and breakneck logistics uber style or even predictive shipping per the notorious amazon patent fabbing up a home interior to suit your tastes or tastes that are forming but haven t fully emerged yet is within today s technology hell even that cute roomba you had to have may be quietly mapping the place you live this will be available in knock off home robots soon have you checked the user agreements of your various home appliances and systems to see if they can sell the data probably not and why not tap that stock of underused homes and underemployed people if there s one thing the sharing economy overlords have taught us it s that the world is just a collection of undermonetized assets waiting to be redistributed right why not productize commodify and populate that second to last frontier our living spaces and staying in someone else s place with someone else s stuff you fancied from the pictures is tired everything else is personalized financialized and productized why even own your own stuff when it could be ubered into position in a desirable location based on your most recent pinterest saves think about it with a bundled dreamhometm service you can perpetually test drive that new living room suite for long holiday weekends i mean why wait until after purchasing for buyer s remorse to set in you can get it out of the way without the financial commitment just your desires played forward all the time you can even test roommates or neighbors for the weekend why stop at furnishings and paint colors slap those detailed sentiment analyses and personality analytics gleaned from your prospective co habitant s online activities eye tracking history tinder preferences and andme profile onto a few improv actors and have some big data cosplay in a pop up maisonette come monday morning you can just walk out the front door with nothing but a premium fee to pay a fee which may be itself be subsidized by various sponsors who want to test products on you don t worry it s cool duralux crate barrel and linkedin picked up the tab for this getaway in the woods or beach with new friends sound good of course it does we knew you would like it check your email your temporary instant disposable dreamhouse for the weekend may be waiting from a quick cheer to a standing ovation clap to show how much you enjoyed this story futures post normal innovation strategic design http changeist com essays observations and speculations from the changeist lab
iDanScott,3,4,https://medium.com/@iDanScott/the-bejeweled-solver-3cd07c69dfc4?source=tag_archive---------5----------------,C# Plays Bejeweled Blitz – iDanScott – Medium,as some of you reading this may or may not already know over the past day or so i went from having the idea of creating a computer program that would essentially be able to play the popular arcade game bejeweled blitz on facebook to actually developing it now as hard as this problem sounds it was surprisingly easy and fairly swift to solve i broke it down in to main steps the first step was probably the most time consuming of them all as everything from there was just colour management the solution i came up with in the end for that was to take a screenshot of the entire screen and then scan the image from top to bottom using a nested for loop until i found a funny shade of brown that only appears along the top edge of the bejeweled grid for anyone wondering that colour is color fromargb once this colour had been found using the bitmap getpixel x y function i broke out of both for loops and knew that was the point where the top left corner of the grid was i could then use this to construct a rectangle which would extract the bejeweled grid from the full screenshot the size of the rectangle was calculated using the size of the grid cells px found that out using trusty old paint multiplied by the amount of rows columns there were found that out using my eye balls this resulted in the rectangle size coming out at px so the next step from here was to identify what colour resides in what square to do that i started off by creating a dimensional array of colours or color s to be politically correct that was rows and columns to match that of the playable grid i then systematically looped through the dimensional array of colours in a nested for of x and y values assigning the array the colour of the pixel at the location x y the x value was decided as it was half way through the gem and was chosen for the y value as certain gems have a white center green and yellow so provided a more accurate reading with this dimensional array i was then able to generate a visual representation of what the computer was seeing when it was trying to figure out what colour was where as you can see from the above screenshot it s able to identify what gem is what colour depending on what pixel is at that magic of the cell another thing i thought about before i finished this project to the state it s in now is to prevent the application from trying to switch empty cells because one gem has just been blown up or something i added all the known color codes to their own array and ask if the colour that s in the d array also resides within the known colours list if it does it will then evaluate whether it can be moved to a winning square if not it s ignored entirely i won t bore you with the gory details of how i check if a gem can be moved as instead this is a link to the beginning of the if statement in my open source github project from here the full source code can be viewed commented on and even improved upon if you guys feel like i could do something obviously better finally all that s left to do by definition of this application is to actually move the gems this is done by making some windows api calls to set the mouse location and simulate mouse clicks again the details of how to exactly do that are within the github project but if i ve kept your attention for this long all that s left to say is thank you and if you have any further questions don t hesitate to hit me up on here or twitter idanscott thanks for reading from a quick cheer to a standing ovation clap to show how much you enjoyed this story dan scott computer science student of plymouth university www idanscott co uk
Josh,18,6,https://medium.com/@joshdotai/9-reasons-why-now-is-the-time-for-artificial-intelligence-876b3def0fee?source=tag_archive---------6----------------,9 Reasons Why Now is the Time for Artificial Intelligence,there s no denying it artificial intelligence is happening and it s happening big companies from facebook to google to amazon are hard at work building world class ai teams that infiltrate every facet of their products siri is one of the largest teams at apple and microsoft has a growing research effort on this front but why is now the time for ai artificial neural networks traditional programming is deterministic sequential and logical for example computers take inputs apply instructions and generate outputs this is great for tasks like calculations and conversions but ill suited if the application isn t explicitly defined the human brain on the other hand doesn t behave this way we learn and grow through repetition and education recent progress in artificial neural networks anns is key to building computers that can think these breakthroughs are enabling tremendous strides in ai work at google and apple knowledge graph companies like yelp foursquare and wolfram alpha have enabled access to their data through apis as a result platforms like siri and google now are able to answer questions such as what s the closest coffeeshop or what s the population of india if a new service had to handle the natural language processing nlp audio processing data and more it would be nearly impossible fortunately the knowledge graph has evolved over the last years to a point where new ai platforms can immediately have access to tons of data natural language processing nlp is a field of computer science and linguistics where computers attempt to derive meaning from human or natural input while the field has been around since the s we ve seen huge strides in the last few years thanks to markov models and n gram models as well as projects like calo and wordnet stanford s corenlp demo here is one of the many strong nlp solutions available today speech processing in order to speak to a computer and have it understand our intent we first need to handle the audio processing and convert sound waves to text known as speech processing this field has seen major advancements in the last few years beyond the advancements in technology we ve seen companies like nuance emerge with powerful apis that power services like gps dictation and more today it is almost effortless for a new ai company to translate voice to text with a high degree of confidence computational power the increase in computational efficiency over the last years has been remarkable in people could buy a video card that was times the performance of one from for the same price this increase in computational power is necessary if we want to emulate the brain for example research attempting to simulate second of human brain activity required processors supporting billion artificial neurons connected by trillion synapses the decrease in cost and increase in computational power is enabling tremendous breakthroughs in ai today consumer acceptance a big aspect of seeing mass adoption around artificial intelligence is consumer approval with an initial push from apple to highlight siri and now microsoft s cortana and google now doing the same smart phone owners have access to an ai whether they like it or not as a result consumers are coming around to the idea and even starting to embrace it funny videos like this one are helping the masses to accept this new human computer interaction ubiquity of personal computing conversing with an ai is a very personal experience the emergence of smaller always on devices makes this possible the iphone was first introduced in only years ago now more than of americans own a smartphone wearables such as the apple watch or jawbone open the possibility of even more intimate personal computing these devices that we carry or wear serve as excellent hosts for this technology making it possible for ai to truly enter the mainstream for the first time funding ai funding seems to go through waves and in the last few years it s definitely back up scaled inference a predictive ai company recently raised m amazon just announced a m fund for voice controlled technologies and ibm did the same for the watson venture fund the total invested in ai companies in grew past m from a mere m in according to bloomberg with firms like khosla ventures and andreesen horowitz leading deals in ai companies funding is fueling innovation in ai research efforts another reason for the apparent surge in ai is the collective research efforts taking place according to a report by miri machine intelligence research institute of the top cs conferences are ai related ai accounts for about of all cs research today the ieee computational intelligence society has more than members and there are more ai journals based on miri estimates more than m went into funding ai research by the national science foundation nsf in with this much research and effort going into ai innovation it s no wonder we re seeing this technology starting to reach the masses if history is an indicator we may see interest in ai spike and go back down with momentum across these various different sectors though ai interest seems likely to keep growing if you re interested in keeping up with our efforts and staying in touch check out http josh ai and reach out this post was written by alex at josh ai previously alex was a research scientist for nasa sandia national lab and the naval resarch lab before that alex worked at fisker automotive and founded at the pool and yeti alex has an engineering degree from ucla lives in los angeles and likes to tweet about artificial intelligence and design josh is an ai agent for your home if you re interested in following josh and getting early access to the beta enter your email at https josh ai like josh on facebook http facebook com joshdotai follow josh on twitter http twitter com joshdotai from a quick cheer to a standing ovation clap to show how much you enjoyed this story
paulson,1,17,https://electricliterature.com/what-could-happen-if-we-did-things-right-an-interview-with-kim-stanley-robinson-author-of-aurora-d88a0f8f72e7?source=tag_archive---------7----------------,"What Could Happen If We Did Things Right: An Interview With Kim Stanley Robinson, Author Of Aurora",is kim stanley robinson our greatest political writer that was the provocative question posed recently by a critic in the new yorker science fiction writers rarely get that kind of serious attention but robinson s visionary experiments in imagining a more just society have always been part of his fictional universe in fact he got his ph d in english studying under the renowned marxist theorist fredric jameson the idea of utopia may seem discredited in today s world but not to robinson he believes we need more utopian thinking to create a better future and the future is where he takes us in his new novel aurora set in the th century it s the story of a space voyage to colonize planets outside our solar system robinson writes in the tradition of hard science fiction using only existing or plausible technology for his interstellar journey as much as he geeks out on the mechanics of space travel his real interest is how people would handle a very long voyage trapped inside a starship his futuristic themes won t surprise longtime fans of robinson who s best known for his mars trilogy published in the s to read ksr is to wonder how our species might survive and even thrive in the centuries ahead the author stopped by my radio studio before giving the keynote speech at a local science fiction conference we talked about the existential angst of life on a starship the future of artificial intelligence and the aesthetics of space travel our conversation will air on public radio international s to the best of our knowledge you can subscribe to the ttbook podcast here steve paulson how would you describe the story in aurora kim stanley robinson it s the story of humanity trying to go to other star systems this may be an ancient idea but for sure it s a th century idea the russian space scientist tsiolkovsky said earth is humanity s cradle but you re not meant to stay in your cradle forever this idea has been part of science fiction ever since that humanity will spread through the stars or at least through this galaxy sp it s a long way to travel to another star ksr it is a long way and the idea of going to the stars is getting not easier but more difficult so i decided to explore the difficulties i tried to think about whether it s really possible at all or if we re condemned if you want to put it that way to stay in this solar system sp what star are your space voyagers trying to get to ksr tau ceti which has often been the destination for science fiction voyagers ursula le guin s dispossessed takes place around tau ceti and so does isaac asimov s the naked sun it s about light years away we now know it has three or four big planets the size of a small neptune or a large earth they ve got the mass of about five earths that s too heavy for humans to be on but those planets could have moons about the size of earth so it becomes the nearest viable target alpha centauri which is just four light years away only has tiny planets that are closer than mercury is to our sun so they won t be habitable sp your story is set years into the future it takes a long time to get to this star ksr yes my working principle was what would it really be like so no hyperspace no warp drive no magical thing about what isn t really going to happen to get us there that means sub lightyear speeds so i postulated that we could get spaceships going to about one tenth the speed of light which is extraordinarily fast then the problem becomes slowing down you have to carry enough fuel to slow yourself down if you ve accelerated to that kind of speed the mass of the decelerant fuel will be about of the weight of your ship as you re approaching your target you have to get back down to the speed at which you can orbit your destination the physics of this is a huge problem sp you re talking about a multi generational voyage that will take a couple hundred years that s a fascinating idea the people who start out will be dead by the time the starship gets there ksr i guessed it would take four or five generations say years this is not my original idea the multi generational starship is an old science fiction idea started by robert heinlein and there may even be earlier precursors one always finds forgotten precursors for every science fiction idea heinlein wrote universe around brian aldiss wrote a book called starship in and gene wolfe wrote a very great starship narrative in the s the book of the long sun so it s not an original idea to me it s sort of a sub genre within science fiction sp but the whole idea of a project that takes generations is something we don t do anymore people did that when they built the pyramids in egypt or the great cathedrals in europe i can t think of a current project that will take generations to complete ksr you really have to think of it as a mobile island or a vast zoo it isn t even a project so much as a city that you ve shot off into space and when the city gets to its destination the people unpack themselves into the new place you re right it could be compared to building the cathedrals and it s interesting to think about the people born on the starship who didn t make the choice to be there so it turned into a bit of a prison novel sp because you re trapped there you re in this confined space for your whole life ksr and for two or three generations you re born on the ship and you die on the ship you re just in between the stars so it s very existential there are some wonderful thought stimulants to thinking about a starship as a closed ecology sp how big is the starship in your story ksr there s something like a hundred kilometers of interior space sp so this is big ksr yeah two rings you could imagine them as cylinders that have been linked until they make a circle so twelve cylinders per circle you ve got cylinders and each has a different earth ecology in it and each one of them is about five kilometers long it s pretty big but you need that much space to be viable at all because you have to take along a noah s ark worth of genetic material or else it isn t going to work sp what do you have to bring along ksr you would want as much of everything as you can bring but you certainly need a big bacterial load you need to bring along a lot of soil you need a lot of what would be effectively unidentified bacteria you just need a big hunk of earth and then all the animals that you can fit that would survive each one of these cylinders would be like a little zoo or aviary sp as you were imagining this voyage which part was most interesting to you was it the science trying to figure out technically how we could get there or was it the personal dynamics of how people would get along when they re trapped in space for so long ksr i think it would be the latter i m an english major the wing of science fiction that s discussed this idea has been the physics guys the hard sf guys they ve been concerned with propulsion navigation with slowing down with all the things you would use physics to comprehend but i ve been thinking about the problem ecologically sociologically psychologically these elements haven t been fully explored and you get a new story when you explore them it s a rather awful story which leads to some peculiar narrative choices sp why is it awful ksr because they re trapped and the spaceship is a trillion times smaller than earth s surface even though it s big it s small and we didn t evolve to live in one of these things it s like you spend your whole life in a motel six sp put that way it does sound pretty awful ksr better than a prison but you can t get out you can t choose to do something else i don t think we re meant for that even though we live in rooms all the time in modern society i think the reason people volunteer for things like mars one is they re thinking how is that different from my ordinary life i sit in a room in front of my laptop all day long if i m going to mars it s more interesting sp mars one is the project that s trying to engineer one way trips to mars you know you re not going to come back frankly it sounds like a suicide mission and yet tens of thousands of people have signed up for this mission ksr yes but they ve made a category error their imaginations have not managed to catch up to the situation they are in some kind of boring life and they want excitement maybe they re young maybe they re worried about their economic prospects maybe they want something different they imagine it would be exciting if they got to mars but it was ralph waldo emerson who said travel is stupid wherever you go you re still stuck with yourself i went to the south pole once i was only there for a week and it was the most boring place in antarctica because we couldn t really leave the rooms without getting into space suits sp is extended space travel like going to antarctica ksr it s the best analogy you can get especially for mars you would get to a landscape that s beautiful and sublime and scientifically interesting and mind boggling antarctica is all those things and so would mars be but i notice that nobody in the united states cares about what the antarcticans are doing every november and december there are a couple thousand people down there having a blast if the same thing happened on mars it would be like oh cool some scientists are doing cool things but then you go back to your real life and you don t care sp so even though you write about these long space voyages you wouldn t want to be part of one ksr not at all but i ve only written about long space voyages once in this book aurora sp you also wrote a whole series of books about mars you still have to get there ksr but there s an important distinction you can get to mars in a year s travel and then live there your whole life and you re on a planet which has gravity and landscape you can terraform it it s like a gardening project or building a cathedral i think terraforming mars is viable going to the stars however is completely different because you would be traveling in a spaceship for several generations where you re in a room not on a planet it s been such a techie thing in science fiction but people haven t de stranded those two ideas they said well if we can go to mars we can go to tau ceti it doesn t follow it s not the same kind of effort sp would it be interesting to travel just through our own solar system ksr yes this solar system is our neighborhood we can get around it in human time scales we can visit the moons of saturn we can visit triton the moon of neptune there are hundreds of thousands of asteroids on which we could set up bases the moons of all the big planets are great the four big moons of jupiter we couldn t be on io because it s too radioactive or too impacted by the radio waves of jupiter itself but by and large the solar system is fascinating sp yet i imagine a lot of people would say yeah there s a lot of cool stuff out there but it s all dead ksr well we have questions about mars europa ganymede and enceladus a moon of saturn wherever there s liquid water in the solar system it might be dead or alive it might be bacterially alive it might have life that started independently it might be cousin life that was blasted off of mars on meteorites and landed on earth and other places we don t know yet and if it is dead it s still beautiful and interesting so these would be sites of scientific interest antarctica is pretty dead but we still go there sp i ve heard it s incredibly beautiful ksr it s very beautiful i think if you re standing on the surface of europa looking around the ice scape and looking up at saturn in the sky overhead it s also going to be beautiful i m not sure if it s beautiful enough to drive a gigantic effort to get there the robots going there now are already a tremendous exploration for humanity the photos sent back to us are a gigantic gift and a beautiful thing to look at so humans going there will always be a kind of research project that a few scientists do i m not saying that the rest of the solar system is crucial to us i think earth is the one and only crucial place for humanity it will always be our only home sp i wonder if we would develop a different sense of beauty if we went out into the solar system when we think of natural beauty we tend to think of gorgeous landscapes like mountains or deserts but out in the solar system on another planet or a moon would our experience of awe and wonder be different ksr you can go back to the th century when mountains were not regarded as beautiful edmund burke and the other philosophers talked about the sublime so the beautiful has to do with shapeliness and symmetry and with the human face and figure through the middle ages mountains were seen as horrible wastelands where god had forgotten what to do then in the romantic period they became sublime where you have not quite beauty but a combination of beauty and terror your senses are telling you this is dangerous and your rational mind is saying no i m on a ledge but i ve got a railing it looks dangerous but it s not you get this thrilling sensation that is not beauty but is the sublime the solar system is a very sublime place sp because you could die at any moment if your oxygen support system goes out ksr exactly it s like being in a submarine or even in scuba gear the feeling of being meters under the surface with a machine keeping you alive and bubbles going up as you re looking at a coral reef that s sublimity there s an element of terror that s suppressed because your rational mind is saying it s okay when you fly in an airplane and look down feet to the surface of the earth that s the feeling of the sublime even if you re looking down at a beautiful landscape but people can t bear to look because after a while you re thinking boy this machine sure has to work sp if you think long and hard about this ksr you might never fly again sp one thing that s so interesting about your novel aurora is that most of it is narrated by the ship itself what was the idea here ksr i do like the idea that my narrators are also characters that they re not me i m not interested in myself i like to tell other people s stories so i don t do memoir i do novels and for three or four novels now it s been an important game to me to imagine the narrators voices being different from mine so shaman s was the third wind this mystical spirit that knew the paleolithic inside and out that wasn t me and cartophilus the time traveler tells galileo s story in aurora it made sense for the ship to need really powerful artificial intelligence like a quantum computer and once you get to quantum computers you ve got processing speeds that are equal to the processing speeds of human brains but the methodologies would be completely different they d be algorithms that we programmed maybe it wouldn t have consciousness but when you get that much processing speed who s to say what consciousness really is so i made the narrator out of this starship s ai system and he she it has been instructed by the chief engineer to keep a narrative account of the voyage when you think about it writing novels is strange we can tell most stories to each other in about words so a novel is not a natural act it s an art form that s been built up over centuries and doesn t have a good algorithm sp i recently interviewed stephen wolfram the computer theorist and software developer and asked if he thought some future computer could write a great novel he said yes ksr wolfram s very important in theorizing what computers can do because he s made a breakdown of activities from the simple to the complex and at full complexity the human brain or any other thinking machine that can get to that fourth level of complexity should be able to do it sp so in the future you think a computer or artificial intelligence system could write a modern ulysses ksr well this is an interesting question at that point you would need a quantum computer it would need to read a whole bunch of novels and try to abstract the rules of storytelling and then give it a shot in my novel the first chapter the computer writes is th century literature it s what we would call camera eye point of view it doesn t guess what people are thinking how can it it just reports what it sees like a hemingway short story as the novel goes on chapter by chapter the computer is recapitulating the history of the novel and by the end of the last chapter narrated by the computer you re getting full on stream of consciousness it s kind of like ulysses or virginia woolf where you re inside the mind although it s the mind of the computer itself the last chapter is in a kind of flow state of the computer s thinking sp at that point does the computer have emotions ksr it wonders about that the computer can t be sure actually we re all trapped in our own consciousness what are other people thinking what are other people feeling you have to work by analogy to your own internal states the computer only has access to its own internal states sp does the future of ai and technology more generally excite you ksr yes ai in particular i used to scoff at it i m a recent convert to the idea that ai computing is interesting mainly it s just an adding machine that can go really really fast there are no internal states they re not thinking however quantum computers push it to a new level it isn t clear yet that we can actually make quantum computers so this is the speculative part it might be science fiction that completely falls apart there was science fiction about easy space travel but that s not going to work there was science fiction about all of us living years that might or might not work but it s way speculative quantum computing is still in that category because you get all the weirdness of quantum mechanics there are certain algorithms that might take a classical computer billion years while a quantum computer would take minutes but those are for very particular tasks like factoring a thousand digit number we don t know yet whether more complex tasks will be something that a quantum computer can handle better than a regular computer but the potential for stupendous processing power like a human brain s processing power seems to be there sp as a science fiction writer do you have a particular mission to imagine what our future might be like is that part of your job ksr yes i think that s central to the job what science fiction is good at is doing scenarios science fiction may never predict what is really going to happen in the future because that s too hard strange things contingent things happen that can t be predicted but we can see trajectories and at this moment we can see futures that are complete catastrophes where we cause a mass extinction event we cook the planet of humanity dies because we run out of food or we think we re going to run out of food and then we fight over it in other words complete catastrophe on the other hand there s another scenario where we get hold of our technologies our social systems and our sense of law and justice and we make a kind of utopia a positive future where we re sustainable over the long haul we could live on earth in a permaculture that s beautiful from this moment in history both scenarios are completely conceivable sp yet if we look at popular culture dystopian and apocalyptic stories are everywhere we don t see many positive visions of the future ksr i ve always been involved with the positive visions of the future so i would stubbornly insist that science fiction in general and my work in particular is about what could happen if we did things right but right now dystopia is big it s good for movies because there are a lot of car crashes and things blowing up sp is it a problem that we have so many negative visions of the future ksr dystopias express our fears and utopias express our hopes fear is a very intense and dramatic emotion hope is more fragile but it s very stubborn and persistent hope is inherent in us getting up and eating breakfast every day in the s young people were thinking i m going to live on the moon i will go to neptune today it s the hunger games which is a very important science fiction story i like that it s science fiction not fantasy it s not lord of the rings or harry potter it s a very surrealistic and unsustainable future but it s a vision of the fears of young people they re pitting us against each other and we have to hang together because there s a rich elite an oligarchy that s simply eating our lives for their own entertainment so there s a profound psychological and emotional truth in the hunger games there s a feeling of fear and political apprehension that late global capitalism is not fair my mars books although they re not as famous and haven t been turned into movies are quite popular because they re saying we could make a decent and beautiful civilization i ve been noticing with great pleasure that my mars trilogy is selling better now than it ever has sp does our society need positive visions of the future do we need people to create scenarios of how things could go well ksr oh yes ever since thomas more s utopia we ve always had it edward bellamy wrote a book called looking backward the progressive political movement that changed things around the time of teddy roosevelt came out of this novel when people had to reconstruct the world s social order after world war ii they turned to h g wells and a modern utopia and men like gods we always need utopias these days people are fascinated by steve jobs or bill gates it s like those geeky s science fiction stories where a kid in his backyard makes a rocket that goes to the moon now it s in his garage where he makes a computer that changes everything we love these stories because they re hopeful and they suggest that we could seize history and change it for the better if science fiction doesn t provide those stories people find them somewhere else so steve jobs is a science fiction story we want from a quick cheer to a standing ovation clap to show how much you enjoyed this story expanding the influence of literature in popular culture
Christopher Wolf Nordlinger,8,6,https://medium.com/@chrisnordlinger/the-internet-of-things-and-the-operating-room-of-the-future-8999a143d7b1?source=tag_archive---------8----------------,The Internet of Things and the Operating Room of the Future,the doctor stands over the patient on the operating room table it can be dizzying to look around at the dozen or more video screens dedicated to standalone medical devices and not think that the internet of things iot could radically simplify the complexities of managing so many systems in the process digital health could enormously improve patient care at the same time hospitals struggle to constrain the rapidly increasing costs of healthcare yet with iot investments they can reduce costs significantly it s not hard to see how the medical industry and hospitals in particular will represent a major component of the trillion internet of things market opportunity that cisco predicts by imagining its future in surgery alone is not some far off idea it already exists and it s revolutionary due to a unique blend of iot big data advanced analytics and smart medical devices here s how the reality plays out in a leading example thousands of people suffer from heart arrhythmias caused by heart disease which show up as a flutter in the heartbeat that is highly disruptive and can cause potentially fatal strokes and heart attacks there are a few pharmaceutical drugs that can mollify the symptoms but they do nothing to remove the dead tissue lesions in the heart that cause the underlying situation which is called atrial fibulation or afib for short cardiothings a made up name to protect the company while under fda approval review is attacking this problem with ablation to remove the lesions by gently burning them out with a laser this involves inserting a catheter into the heart to try to perform ablation to remove the afib causing lesions each device is hard wired to a screen where streaming data from the end of the catheter display a view of the inside of the heart but that s not where the data stop between the heart and the monitors like many devices cardiothings a silicon valley startup works with two real iot powerhouses ptc thingworx and another silicon valley startup glassbeam to make something much more powerful possible thingworx models the operation of the catheter so that it can send secure data to the cloud where it can be analyzed by glassbeam glassbeam turns the unstructured data into structured data in the forms of readable reports that the device company can then use to improve doctors surgical performance for cardiothings and other high value asset manufacturers this kind of data can also increase the uptime of their catheter device others can use iot analytics to increase the uptime of cat scans and mris because the data can show when even the smallest part is showing signs of weakness or malfunction and enable a repair that keeps that equipment operating how imagine cardiothings s optical catheter thin enough to fit comfortably through a vein entering a heart and mapping it out to find the lesions responsible for the afib the surgeon is then able to frame the boundaries of the lesions on caridothings s monitors to see which are dying and need to get burned out the laser beam from the sensor embedded catheter then cuts the lesions out and the patient is healed what does this have to do with saving money for the hospital high value machines such as mris and cat scan cost millions downtime for them is not only very costly for the hospital that is not billing patients but also more importantly interrupts patients from getting the best possible care thingworx enables medical devices things sensors modeled by thingworx to communicate as if it was the device to talk to other things in the cloud once the unstructured data is there it can be combined and recombined by glassbeam s analytics software to detect any abnormalities for mris cat scan and other devices stopping small problems from becoming big problems that crash expensive heavily used equipment is the ultimate value of predictive maintenance hospitals are large places with many people and things moving about a great deal and keeping track of assets ranging from mri scanners to beds is quite challenging in the case of cardiothings above the alliance of ptc thingworx and glassbeam should make the medical industry and business decision makers globally take notice whether it s healthcare agriculture networking or manufacturing higher utilization of equipment is absolutely essential to remaining competitive in the case of the cardiothings s catheter spitting out unstructured data chris kuntz vice president ecosystem programs of ptc thingworx says imagine the cardiac data from that same procedure being combined and recombined with data from ekg machines mri machines pharmaceutical research personal medical record keeping systems blood monitors and hundreds of healthcare systems this is how the internet of things drives a revolution in healthcare thanks to our partnership with thingworx glassbeam ceo puneet pandit says we are able to capture that unstructured data off the catheter and create structured data that business decision makers at hospitals the manufacturers and individual doctors can learn from pandit adds as a result of the large amount of critical data coming from the catheter you can answer many questions how did the device perform under what circumstances how long did the surgery take which surgeons did it most effectively who needs to be more formally trained as a result of this solution training surgeons to use equipment better provides significantly improved outcomes for patients and for hospitals dispensing critical care no one has to wait any longer for the mri to crash to know there was a problem they can fix the smallest problem before it escalates letting the hospital know that a specific part is faulty by simply examining the unstructured data it sends out is the best example of the power of predictive maintenance no one has to wait for the mri to crash hospitals can enjoy huge savings through predictive maintenance on all its heavily used expensive equipment given concerns about privacy and safeguarding of material it is essential to have a secure connectivity partner such as thingworx aboard hippa is just the beginning of the scope of regulatory requirements that will need to be accommodated to operate successfully in the healthcare data space applied analytics available to doctors in real time reduces medical procedure risk and overall liability concerns for hospitals to reduce costs and increase profitability iot will play an enormous role for patients it means their doctors will know so much more about treating them to ensure the best care after any procedure whether it s a heart bypass cancer surgery heart transplant or a simple blood test jack reader business development manager at thingworx now at verizon says imagine an operating room where there are just a few monitors and all the devices speak to each other and with thousands of medical systems within and beyond the walls of the hospitals all of this innovation will exponentially increase insight and intelligence reduce costs for the hospital and increase health outcomes the implications in terms of knowledge gained and positive health outcomes is so phenomenal that we almost can t now imagine from this early stage in the iot era all the possible sources nor all the insights that will be gained however the sooner iot analytics is adopted in the hospital the sooner patients can expect better run hospitals and healthier lives this is only the beginning of a new era from a quick cheer to a standing ovation clap to show how much you enjoyed this story ph d fulbright scholar storyteller communications expert content maven formerly state dept startups cisco more
Louis Rosenfeld,90,5,https://medium.com/@louisrosenfeld/everyday-ia-d7aa7be07717?source=tag_archive---------9----------------,Everyday IA – Louis Rosenfeld – Medium,a few days ago cennydd bowles gently trolled many of us thusly as cennydd has keynoted a past information architecture summit it s hard to ignore his question and cennydd s timing is quite interesting given that tomorrow is world ia day the theme of this year s wiad is architecting happiness and in this adorable little video that the ia institute created to promote wiad abby covert says that this theme was chosen because of the rising amount of information that everyone has to deal with my italics cennydd there s your answer if you re a human in today s developed world where even physical objects and spaces are soaked in information you are struggling to cope with and make sense of the stuff nearly all the time and nearly everywhere information architecture problems are everyday human problems so if you re designing for humans today you ll need at least some information architecture skills in order to help them information architecture literacy is required for anyone who designs anything so it s not surprising that wiad has exploded to locations in countries it s not surprising that abby s wonderful little book how to make sense of any mess information architecture for everybody has been such a hit it s not surprising to see the ia summit entering its th year stronger than ever it s not surprising that the fourth edition of information architecture for the world wide web due out later this year is being recast as a book not for information architects but for people who need to know something about information architecture we ve entered full on mode of democratizing ia skills because information architecture literacy is required for anyone who designs anything i ll confess to having felt like cennydd a bit disconnected from ia for the past few years partly because i ve been investing almost every available moment of my waking hours into rosenfeld media and partly because much of the ia community s discussion has pushed far deeper into ia practice than my brain and attention span can manage but i m feeling better now because i m finding in my own day to day work that information architecture literacy is required for anyone who designs anything for example while i rarely work on web site ia much these days i am absolutely absorbed in the information architecture of books want to know what value publishers can provide to authors in this age of self publishing the list might be longer than you imagined but i think most rosenfeld media authors would agree lou and team pull them out of the weeds and help them to step back and make sense of their content as an information system information architecture skills are an absolute necessity when it comes to framing structuring and establishing a flow for a book and not just for non fiction just ask jk rowling i m finding that ia literacy is also incredibly helpful in other areas like event planning i recently asked a couple dozen colleagues who produce events to provide share their advice on organizing a conference their responses were generous useful and wonderful but the one i keep remembering most is jeffrey zeldman s yes i m biased but i hear jeffrey singing a song of event ia i ve been singing it too in putting together the first edition of the enterprise ux conference plug alert san antonio may i ve been working with dave malouf and uday gajendar to create an information architecture for a conversation in effect we re trying to structure the event s program in a way that surfaces a latent conversation about enterprise ux that s been happening in the ux community for quite some time the event itself should simply serve as an opportunity to bring people together to sharpen and advance that conversation i m oversimplifying a bit but we spent months designing our event ia around four carefully sequenced themes each in effect a curated mini conference insight at scale craft amid complexity enterprise experimentation and designing organizational culture we see these as the main facets of the community s conversation on enterprise ux we ll know we ve been successful if at the event the conversation spills out of the auditorium and into the hallways and break areas animating the words and faces of attendees we ll know we been really successful if these conversations riff off the themes already covered meaning we got the sequence right and we ll know that we were really really successful if these four themes keep the conversation moving forward both after the event and as the ia for programs at future editions of the event books have an information architecture events have an information architecture pretty much anything we design consciously or not has an information architecture so pardon me as i repeat information architecture literacy is required for anyone who designs anything when i got my masters in information and library studies in our professors were preaching about the oncoming information revolution since then i ve been fortunate to observe and even participate a little in that revolution in the blink of an eye information architects emerged as professionals dedicated to making the pain of that revolution easier to bear in the blink of an eye others have proclaimed that information architecture as a profession was dead i m not sure who s right nor do i care twenty five years is nothing the dust can settle after we re all dead let s worry instead about people suffering from everyday ia problems we as designers of any stripe have to help them and we have to get better at helping them to help themselves oh and if you re wondering why i won t be at any of tomorrow s wiad meetings well it s saturday and i have a date with my six year old we re going to organize his legos this piece originally ran in the rosenfeld review sign up here for new ones from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder of rosenfeld media i make things out of information
Matt Harvey,677,7,https://blog.coast.ai/continuous-online-video-classification-with-tensorflow-inception-and-a-raspberry-pi-785c8b1e13e1?source=tag_archive---------0----------------,"Continuous online video classification with TensorFlow, Inception and a Raspberry Pi",much has been written about using deep learning to classify prerecorded video clips these papers and projects impressive tag classify and even caption each clip with each comprising a single action or subject today we re going to explore a way to continuously classify video as it s captured in an online system continuous classification allows us to solve all sorts of interesting problems in real time like understanding what s in front of a car for autonomous driving applications to understanding what s streaming on a tv we ll attempt to do the latter using only open source software and uber cheap hardware specifically tensorflow on a raspberry pi with a picamera we ll use a naive classification approach in this post see next section which will give us a relatively straightforward path to solving our problem and will form the basis for more advanced systems to explore later by the time we re done today we should be able to classify what we see on our tv as either a football game or an advertisement running on our raspberry pi let s get to it video is an interesting classification problem because it includes both temporal and spatial features that is at each frame within a video the frame itself holds important information spatial as does the context of that frame relative to the frames before it in time temporal we hypothesize that for many applications using only spatial features is sufficient for achieving high accuracy this approach has the benefit of being relatively simple or at least minimal it s naive because it ignores the information encoded between multiple frames of the video since football games have rather distinct spatial features we believe this method should work wonderfully for the task at hand we re going to collect data for offline training with a raspberry pi and a picamera we ll point the camera at a tv and record frames per second or more specifically save jpegs every second which will comprise our video here s the code for capturing our images once we have our data we ll use a convolutional neural network cnn to classify each frame with one of our labels ad or football cnns are the state of the art for image classification and in it s essentially a solved problem it feels crazy to say that but it really is thanks in large part to google tensorflow inception and the many researchers who came before it there s very little low level coding required for us when it comes to training a cnn for our continuous video classification problem pete warden at google wrote an awesome blog post called tensorflow for poets that shows how to retrain the last layer of inception with new images and classes this is called transfer learning and it lets us take advantage of weeks of previous training without having to train a complex cnn from scratch put another way it lets us train an image classifier with a relatively small training set we collected minutes of footage at jpegs per second which amounted to ad frames and football frames the next step is to sort each frame into two folders football and ad the name of the folders represent the labels of each frame which will be the classes our network will learn to predict on when we retrain the top layer of the inception v cnn this is essentially using the flowers method described in tensorflow for poets applied to video frames to retrain the final layer of the cnn on our new data we checkout the r tag from the tensorflow repo and run the following command retraining the final layer of the network on this data takes about minutes on my laptop with a geforce gtx m gpu at the completion of training steps our model reports an incredible accuracy on the held out validation set i m not sure i could do much better using my eyes on the same data as a point of reference if the network had classified each frame as football it would have achieved about accuracy so it seems to be working it s always a good idea to run some known data through a trained network to sanity check the results so we ll do that here here s the code we use to classify a single image manually through our retrained model and here are the results of spot checking individual frames before we transfer everything to our pi and do this in real time let s use a different batch of recorded data and see how well we do on that set to get this dataset and to make sure we don t have any data leakage into our training set we separately record another minutes of the football broadcast this dataset amounted to ad frames and football frames we run each frame of this set through our classifier and achieve a true holdout accuracy score of awesome looks like we ve validated our hypothesis that we can achieve high levels of accuracy while only considering spatial features impressive results considering that we only used minutes of training data thank you google pete tensorflow and all the folks who have developed cnns over the years for your incredible work and contributions great so now we have our cnn trained and we know that we can classify each frame of our video with relatively high accuracy how does it do on live tv with always changing context for this we load up our raspberry pi with our newly trained model weights turn on the picamera at fps and instead of saving the image send it through our cnn to be classified we have to make some modifications to the code to classify in real time the final result looks like this we also have to get tensorflow running on the pi sam abrahams wrote up excellent instructions for doing this so i won t cover them again here after we install our dependencies we run the program and crap inception on the raspberry pi can only classify one image every four seconds okay so we don t quite have the hardware yet to do fps but this still feels like magic so let s see how we do flipping on sunday night football and pointing our camera at the tv shows a remarkable job at classifying each moment as football or ad once every few seconds for the vast majority of the broadcast we see our prediction come out true to life so cool in all our naive method worked remarkably well at continuous online video classification for this particular use case but we know that we re only considering part of the information provided to us inherently in video and so there must be room for improvement especially as our datasets become more complex for that we ll have to dive deeper so in the next post we ll explore feeding the output of our cnn both the final softmax layer and the pool layer which gives us a d feature vector of each image to an lstm rnn to see if we can increase our accuracy spoiler alert we can from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder of coastline automation using ai to make every car crash proof practical applications of deep learning and research reports from the road
Vivek Yadav,425,11,https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9?source=tag_archive---------1----------------,An augmentation based deep neural network approach to learn human driving behavior,overview in this post we will go over the work i did for project of udacity s self driving car project behavior cloning for driving the main task is to drive a car around in a simulator on a race track and then use deep learning to mimic the behavior of human this is a very interesting problem because it is not possible to drive under all possible scenarios on the track so the deep learning algorithm will have to learn general rules for driving we must be very careful while using deep learning models because they have a tendency to overfit the data overfitting refers to the condition where the model is very sensitive to the training data itself and the model s behavior does not generalize to new unseen data one way to avoid overfitting is to collect a lot of data a typical convolutional neural network can have up to a million parameters and tuning these parameters requires millions of training instances of uncorrelated data which may not always be possible and in some cases cost prohibitive for our car example this will require us to drive the car under different weather lighting traffic and road conditions one way to avoid overfitting is to use augmentation augmentation refers to the process of generating new training data from a smaller data set such that the new data set represents the real world data one may see in practice as we are generating thousands of new training instances from each image it is not possible to generate and store all these data on the disk we will therefore utilize keras generators to read data from the file augment on the fly and use it to train the model we will utilize images from the left and right cameras so we can generate additional training data to simulate recovery keras generator is set up such that in the initial phases of learning the model drops data with lower steering angles with higher probability this removes any potential for bias towards driving at zero angle after setting up the image augmentation pipeline we can proceed to train the model the training was performed using simple adam learning algorithm with learning rate of after this training the model was able to drive the car by itself on the first track for hours and generalized to the second track all the training was based on driving data of about laps using ps controller on track in one direction alone the model never saw track in training but with image augmentation flipping darkening shifting etc and using data from all the cameras left right and center the model was able to learn general rules of driving that helped translate this learning to a different track important these results were obtained on titan x gpu machine i built earlier full specifications of the computer can be found here please note that computers with different performance will provide a different performance of the network augmentation helps us extract as much information from data as possible we will generate additional data using the following data augmentation techniques augmentation is a technique of manipulating the incoming training data to generate more instances of training data this technique has been used to develop powerful classifiers with little data https blog keras io building powerful image classification models using very little data html however augmentation is very specific to the objective of the neural network brightness augmentation changing brightness to simulate day and night conditions we will generate images with different brightness by first converting images to hsv scaling up or down the v channel and converting back to the rgb channel using left and right camera images using left and right camera images to simulate the effect of car wandering off to the side and recovering we will add a small angle to the left camera and subtract a small angle of from the right camera the main idea being the left camera has to move right to get to center and right camera has to move left horizontal and vertical shifts we will shift the camera images horizontally to simulate the effect of car being at different positions on the road and add an offset corresponding to the shift to the steering angle we added steering angle units per pixel shift to the right and subtracted steering angle units per pixel shift to the left we will also shift the images vertically by a random number to simulate the effect of driving up or down the slope shadow augmentation the next augmentation we will add is shadow augmentation where random shadows are cast across the image this is implemented by choosing random points and shading all points on one side chosen randomly of the image the code for this augmentation is presented below flipping in addition to the transformations above we will also flip images at random and change the sign of the predicted angle to simulate driving in the opposite direction preprocessing after augmenting the image as above we will crop the top of the image to remove the horizon and the bottom pixels to remove the car s hood originally of the top of car image was removed but later it was changed to to include images for cases when the car may be driving up or down a slope we will next rescale the image to a x square image after augmentation the augmented images looks as follows these images are generated using kera s generator and unlimited number of images can be generated from one image i used lambda layer in keras to normalize intensities between and keras generator for subsampling as there was limited data and we are generating thousands of training examples from the same image it is not possible to store all the images apriori into memory we will utilize kera s generator function to sample images such that images with lower angles have lower probability of getting represented in the data set this alleviates any problems we may ecounter due to model having a bias towards driving straight panel below shows multiple training samples generated from one image the keras generator is presented below the pr threshold variable is a threshold that determines if a data with small angle will be dropped or not model architecture and training i implemented the model architecture above for training the data the first layer is x filters this has the effect of transforming the color space of the images research has shown that different color spaces are better suited for different applications as we do not know the best color space apriori using x filters allows the model to choose its best color space this is followed by convolutional blocks each comprised of and filters of size x these convolution layers were followed by fully connected layers all the convolution blocks and the following fully connected layers had exponential relu elu as activation function i chose leaky relu to make transition between angles smoother training i trained the model using the keras generator with batch size of for epochs in each epoch i generated images i started with pr threshold the chance of dropping data with small angles as and reduced the probability by dividing it by the iteration number after each epoch the entire training took about minutes however it too more than hours to arrive at the right architecture and training parameters snippet below presents the result of training model performance video below shows the performance of algorithm on the track on which the original data was collected the car is able to drive around for hours we will next look into the case where either the camera resolution video size or tracks are changed generalization from one image size to another video below presents generalization from one image size to another i used the same pretrained model and tested it on all the other image sizes and found that the deep learning neural network was able to drive the car around for all image sizes generalization from one image resolution to another video below presents generalization from one image resolution to another i used the same pretrained model and tested it on all the other image resolutions and found that the deep learning neural network was able to drive the car around for all image resolutions i also tested different combinations of image size and image resolutions and on track the deep learning algorithm was able to drive the car around for all combinations of image resolution and sizes generalization from one track to another figure below presents generalization from one track to another this was perhaps the toughest test for the deep learning algorithm in the second track there were more right turns and u turns it was darker and the road had slopes all of which were absent in the original track however all these effects were artificially included into the model via image augmentation future directions this project is far from over this project opened more questions than it answered a few more things to try are reflections this was perhaps the weirdest project i did this project challenged all the previous knowledge i had about deep learning in general large epoch size and training with more data results in better performance but in this case any time i got beyond epochs the car simply drove off the track although all the image augmentation and tweaks seem reasonable n w i did not think of them apriori i hope others find this post useful and get inspried to try novel things i havent used on the fly training agile trainer by john chen yet i wanted to try and stretch the data as much as possible next thing to try is to experiment with parallel network using john s trainer acknowledgements i am very thankful to udacity for selecting me for the first cohort this allowed me to connect with many like minded individuals as always learned a lot from discussions with henrik tu nnermann and john chen i am also thankful for getting the nvida s gpu grant although its for work but i use it for udacity too from a quick cheer to a standing ovation clap to show how much you enjoyed this story staff software engineer at lockheed martin autonomous system with research interest in control machine learning ai lifelong learner with glassblowing problem best place to learn about chatbots we share the latest bot news info ai nlp tools tutorials more
Carlos Beltran,97,9,https://medium.com/@carlosbeltran/ai-the-theme-in-avenged-sevenfolds-new-album-the-stage-f4516d6fc96?source=tag_archive---------2----------------,A Rock Album For AI – Carlos Beltran – Medium,https open spotify com album jwnywjz xhnrvayeclqpd it s awesome that avenged sevenfold became interested in ai and wrote an entire album that revolves around the idea in an interview with rolling stone lead singer m shadows says the initial interest came after reading tim urban s article over at waitbutwhy it s one of the things along with movies like her and the matrix of course that spiked my interest in ai as well so i d highly recommend reading it tim does a phenomenal job of explaining the topic current challenges engineers are facing and the very possible implications of this technology the term artificial intelligence was first coined half a century ago fast forward to today where we have have giant companies like intel and apple acquiring ai startups like there s no tomorrow it s not a matter of whether or not we ll be able to create machines that surpass our own capabilities but when theoretical physicist and futurist dr michio kaku thinks it is possible for machines as smart as us to exist by the end of the century google s chief futurist ray kurzweil believes such technology will exist as soon as the band is right in wanting its fans and the general public to be more aware of these ideas they could be right around the corner i m no expert but i d like to discuss the ideas behind some of the songs and include references in case you d like to delve deeper and if you want to read more on the possible future of ai i d recommend reading kurzweil s book the singularity is near although some of his predictions have been met with skepticism the ideas presented are thought provoking simply put nanomachines are microscopic machines that will enhance us in almost every way imaginable they ll be able to help our immune system fight off diseases they would create super soldiers this technology is actually at the center of a great game series metal gear solid this hack in our biological makeup will also increase our lifespans kurzweil imagines a future where biotechnology is so advanced that we will live forever this is the same idea behind the song paradigm lyrics include the song also raises the question of what it really means to be human what do we become when we merge with machines will we lose what fundamentally makes us human it can be argued that this merge is the next logical step in evolution as there is no there is no evolutionary pressure for us to do so anymore we ll become as kurzweil puts it godlike expanding the brain s neocortex will allow us for example to pose questions in our thoughts and know the answer almost immediately most likely thanks to our direct brain to google connection we ll always have witty jokes on hand and learning calculus will be as simple as purchasing downloadable content plug and play besides swapping out failing body parts with prosthetics and enhancing our brains there s another way we ll be able to gain immortality both dr kaku and kurzweil firmly believe that the advances in brain computer interfaces will eventually allow us to upload our consciousness to machines scientists still have no clue how the brain works how the billions of neurons form connections that result in learned behavior or what dreaming is but once these secrets are known which might never actually happen and we know how our brain functions as well as what the consciousness switch is the possibilities are endless to get an idea of what s possible check out black mirror s episode playtest the brain computer interface for the game is so advanced that the player can t distinguish between what s real and what isn t i don t want to spoil anything but get ready for a mind fuck black mirror does a great job of weaving technology with a dystopia that we might inhabit showing a darker side of our society it s on netflix so check it out elon musk sure does he claims that the chances of us living in base reality is one in billions i d recommend watching the minute video his logic is as follows we had pong some years ago two rectangles and a dot were rendered on screen for what we called a videogame today we have games with realistic graphics and they keep getting better every year better yet virtual and augmented reality are right around the corner pushing the boundaries of gaming eventually we ll have the technology to create simulated worlds that are indistinguishable from reality therefore musk claims it is likely that we are living in an ancestor simulation created by an advanced future civilization some years from now the album s th song simulation explores the idea that our reality might not be what it seems think of it this way the brain and nervous system which we use to automatically react to the environment around us is the same brain and nervous system which tells us what the environment is throughout the song the patient is having thoughts that challenge the simulation they are living in they are in a sense waking up a darker voice which i believe is meant to represent the ones running the show has to reprimand the patient reminding them that they only exist because we allow it to control the situation the patient is to be sedated with blue comfort a reference from the matrix which will make them forget they re living in a simulation blissful ignorance i won t try to explain this one just watch the video and here s a quote from that man that might get your attention imagine an entity so intelligent but that s just it you can t imagine it in the second part to his article on ai tim urban compares this to a chimp being unable to understand a skyscraper is not just a part of its environment but that humans built it it s not the chimp s fault or anything its brain is just not made to have that level of information processing the same thing will happen when we build a machine with the collective knowledge of some years of homo sapien existence therefore there is no way to know what it will do or what the consequences will be tim depicts our situation with this entity what he refers to as artificial superintelligence asi beautifully mark zuckerberg is right in saying we should be hopeful of the amount of good ai could do but some of the smartest minds in existence are genuinely concerned stephen hawking acknowledges that the successful creation of an ai will be the biggest event in history but warns it could also end mankind elon musk founded a research company openai as a way to neutralize the threat of a malicious artificial super intelligence creating god describes ai as a modern messiah the very last invention man would ever need it paints the picture of a utopia where this intelligence exists at the same time the song suggests that we could be summoning the demon unable to control the outcomes we could just be its stepping stone as our existence after its creation becomes irrelevant the album wraps up with a minute eargasm i can t produce words that will do exist any justice as the band described it it s like listening to what the big bang might ve sounded like neil degrasse tyson makes a cameo at the end of the song that serves as a reminder that our problems and conflicts are minuscule in the grand scheme of things we re all a part of the same universe and once we as a society realize this we can truly make progress here s the full thing the stage is an exceptional album in my opinion the band s intentions were for fans to educate themselves or be a bit more aware of what s going on in this area we can enjoy it as a rock album as well as explore the ideas behind the lyrics i had an awesome time writing this digging up things i ve read and seen and unifying them in a way so others can hopefully become more interested as well and come on don t tell me that the idea that we re living in a simulation isn t thought provoking tap the button below my name s carlos and i generally write about personal development tech and entrepreneurship hit me up on twitter from a quick cheer to a standing ovation clap to show how much you enjoyed this story software engineer focused on building cool shit on ethereum
Matt Harvey,558,6,https://blog.coast.ai/continuous-video-classification-with-tensorflow-inception-and-recurrent-nets-250ba9ff6b85?source=tag_archive---------3----------------,"Continuous video classification with TensorFlow, Inception and Recurrent Nets",a video is a sequence of images in our previous post we explored a method for continuous online video classification that treated each frame as discrete as if its context relative to previous frames was unimportant today we re going to stop treating our video as individual photos and start treating it like the video that it is by looking at our images in a sequence we ll process these sequences by harnessing the magic of recurrent neural networks rnns to restate the problem we outlined in our previous post we re attempting to continually classify video as it s streamed in an online system specifically we re classifying whether what s streaming on a tv is a football game or an advertisement convolutional neural networks which we used exclusively in our previous post do an amazing job at taking in a fixed size vector like an image of an animal and generating a fixed size label like the class of animal in the image what cnns cannot do without computationally intensive d convolution layers is accept a sequence of vectors that s where rnns come in rnns allow us to understand the context of a video frame relative to the frames that came before it they do this by passing the output of one training step to the input of the next training step along with the new frames andrej karpathy describes this eloquently in his popular blog post the unreasonable effectiveness of recurrent neural networks we re using a special type of rnn here called an lstm that allows our network to learn long term dependencies christopher olah writes in his outstanding essay about lstms almost all exciting results based on recurrent neural networks are achieved with lstms sold let s get to it our aim is to use the power of cnns to detect spatial features and rnns for the temporal features effectively building a cnn rnn network or crnn for the sake of time rather than building and training a new network from scratch we ll step is unique so we ll expand on it a bit there are two interesting paths that come to mind when adding a recurrent net to the end of our convolutional net let s say you re baking a cake you have at your disposal all of the ingredients in the world we ll say that this assortment of ingredients is our image to be classified by looking at a recipe you see that all of the possible things you could use to make a cake flour whisky another cake have been reduced down to ingredients and measurements that will make a good cake the person who created the recipe out of all possible ingredients is the convolutional network and the resulting instructions are the output of our pool layer now you make the cake and it s ready to eat you re the softmax layer and the finished product is our class prediction i ve made the code to explore these methods available on github i ll pull out a couple interesting bits here in order to turn our discrete predictions or features into a sequence we loop through each frame in chronological order add it to a queue of size n and pop off the first frame we previously added here s the gist n represents the length of our sequence that we ll pass to the rnn we could choose any length for n but i settled on at fps which is the framerate of our video that gives us seconds of video to process at a time this seems like a good balance of memory usage and information the architecture of the network is a single lstm layer with nodes this is followed by a dropout of to help prevent over fitting and a fully connected softmax layer to generate our predictions i also experimented with wider and deeper networks but neither performed as well as this one it s likely that with a larger training set a deeper network would perform best note i m using the incredible tflearn library a higher level api for tensorflow to construct our network which saves us from having to write a lot of code once we have our sequence of features and our network training with tflearn is a breeze evaluating is even easier now let s evaluate each of the methods we outlined above for adding an rnn to our cnn intuitively if one frame is an ad and the next is a football game it s essentially impossible that the next will be an ad again i wish commercials were only th of a second long this is why it could be interesting to examine the temporal dependencies of the probabilities of each label before we look at the more raw output of the pool layer we convert our individual predictions into sequences using the code above and then feed the sequences to our rnn after training the rnn on our first batch of data we then evaluate the predictions on both the batch we used for training and a holdout set that the rnn has never seen no surprise evaluating the same data we used to train gives us an accuracy of good sanity check that we re on the right path now the fun part we run the holdout set through the same network and get better than our we got without the lstm and not a bad result given we re using the full output of the cnn and thus not giving the rnn much responsibility let s change that here we ll go a little deeper see what i did there instead of letting the cnn do all the hard work we ll give more responsibility to the rnn by using output of the cnn s pool layer which gives us the feature representation not a prediction of our images we again build sequences with this data to feed into our rnn running our training data through the network to make sure we get high accuracy succeeds at sanity checked how about our holdout set that s an error reduction of percentage points or from our cnn only benchmark awesome we have shown that taking both spatial and temporal features into consideration improves our accuracy significantly next we ll want to try this method on a more complex dataset perhaps using multiple classes of tv programming and with a whole whackload more data to train on remember we re only using minutes of tv here once we feel comfortable there we ll go ahead and combine the rnn and cnn into one network so we can more easily deploy it in an online system that s going to be fun part is now available five video classification methods implemented in keras and tensorflow from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder of coastline automation using ai to make every car crash proof practical applications of deep learning and research reports from the road
Oxford University,237,19,https://medium.com/oxford-university/the-future-of-work-cf8a33b47285?source=tag_archive---------4----------------,The future of work – Oxford University – Medium,technology has always changed employment but the rise of robotics and artificial intelligence could transform it beyond recognition researchers at oxford are investigating how technology will shape the future of work and what we can do to ensure everyone benefits in a famous talk john maynard keynes imagined a future years hence in which technological progress automated much of human labour by he estimated we could all enjoy a hour working week a lot will need to change in the next decade for that to become a reality but it s not impossible right now advances in artificial intelligence and robotics promise machines that will take on all kinds of human tasks digital communication is creating an internet dwelling labour force that can work remotely and on demand and the self employed are finding that new technological services like uber and airbnb can provide a flexible way to make a living but phenomena like these give rise to a cascade of effects not all necessarily desirable that are fiendishly difficult to perceive and predict it s perhaps not surprising then that the future of work is a topic of increasing fascination for university of oxford academics both the oxford martin school and green templeton college now run specific programmes that focus on the topic with plenty of researchers from the departments of engineering science and sociology to those of politics and economics grappling with its complexity we see a need for bringing together different perspectives around the study of work explains dr marc thompson a senior fellow at sa d business school and the director of the green templeton college future of work programme our role as academics is to contribute to the debate both in terms of theory and to raise challenging questions and issues for those in government and industry what will happen as a result of these advances how will it affect people whose interests are being pursued and what are the long term implications a series of recent studies from the university cut straight to the chase of technology s impact on employment focusing on how robotics and automation will affect the jobs that humans currently undertake the authors dr carl benedikt frey carlbfrey and prof michael osborne maosbot come from quite different backgrounds frey is an economist interested in the transition of industrial nations to digital economies osborne an engineer focused on creating machine learning algorithms together they re co directors of the programme on technology and employment at the oxford martin school it would be fair to ask why i m doing work related to economics while we re sitting here in the department of engineering admits osborne gesturing to his surroundings but i ve always had some interest in thinking about what machine learning could mean for society beyond the industrial applications we usually consider so when carl approached me to speak about algorithms and technologies used in automation and their effects on employment it seemed like a natural fit this is of course exactly the kind of multidisciplinary work the university excels at and the reason the oxford martin school was established each of its programmes brings together researchers from different fields to tackle complex global issues that can t be solved by academics from a single discipline since meeting the pair has set about developing ways to analyse which jobs that exist today could be at risk of being taken over by robots or artificial intelligence software in the next years first they gathered together as many smart people as they could to decide on job roles that definitely could or could not be automated in the next years for example they collectively decided that switchboard operators and dishwashers could definitely be replaced while the clergy and magistrates certainly couldn t the pair combined this list with data from the us department of labor s o net system a database which describes the different skills relevant to specific occupations osborne then built an algorithm that could learn from both pools of data to establish the kinds of skills that were common to automatable jobs when shown other occupations and the skills they require the software can classify them with a probability of being either automatable or non automatable the pair found that the jobs least likely to be automated are those that require skills of creative intelligence social intelligence or physical dexterity these are what they refer to as engineering bottlenecks current limits to technology that make humans irreplaceable osborne points out that it s perfectly possible for instance to have an algorithm churn out an endless sequence of songs but almost impossible to have it create a hit similarly chat bots may be able to communicate with you but they can t negotiate a deal and robots can assemble objects on a well defined production line but they can t perform a fiddly task like making a cup of tea in your messy kitchen in each case it s because humans draw on a huge wealth of tacit knowledge about culture emotion human behaviour and the physical environment that s hard to encode in a way that a machine can act upon but even with those bottlenecks the results suggest that as many as of us jobs are at risk from automation over the course of the next two decades it s worth bearing in mind that the figures explain which jobs are theoretically automatable rather than destined to be automated that may seem like a fiddly point says osborne but this analysis doesn t take into account other factors that we absolutely do believe will have an impact on whether an occupation is taken over by a machine such as human wage levels social acceptance and the creation of new jobs but however you look at it the numbers are difficult to ignore there s an intuitive counter argument to the claims that their analysis makes for centuries new technologies have been invented that have pushed humans out of work but by and large most of us still continue to have jobs in fact researchers elsewhere in the university have shown that the amount of work we all perform remains steadfastly consistent irrespective of technological change jonathan gershuny professor of sociology and director of the centre for time use research has spent a large part of his career tracing the way that we all use our time to work play rest and everything else fundamentally there are three realms of activity he explained from the bay window of his woodstock road office there s paid work unpaid work and consumption paid work is just that the tasks we carry out in exchange for money be it mining coal writing a book or performing brain surgery unpaid work meanwhile is formed of tasks that you could pay someone else to do for you but for whatever reason don t such as cooking cleaning gardening or childcare and consumption is all the activity you absolutely couldn t pay someone else to do for you your night s sleep say or eating lunch why am i telling you all this asks gershuny with a grin well when you define work quite widely like this you arrive at a really quite extraordinary discovery which is that work time that is the sum of paid and unpaid work time doesn t change very much looking at all the data we have access to the total is pretty constant at about hours per week that s just over a third of our hour week and a little more than the approximately hour chunk we manage to spend sleeping he points to decades of evidence accumulated by his team in countries including australia canada israel slovenia france sweden the netherlands and plenty more that confirm the trend as well as working time regulations from as far back as the industrial revolution his latest dataset a huge survey of british residents carried out in was being downloaded in full the day we met but a preliminary analysis already suggested that his observation holds true the truth is we need work for various reasons a time structure a social context a purpose in life he explains indeed what many people citing keynes famous talk about the future fail to mention is that he went on to suggest that there is no country and no people who can look forward to the age of leisure and of abundance without a dread in other words he thought that most us couldn t really begin to comprehend the reality of not working gershuny agrees arguing that humans will simply endeavour to find new types of work to do in order to busy themselves whether the robots take over the jobs we currently possess or not dr ruth yeoman a research fellow at the sai d business school who researches meaningful work in organisations and systems points out that the human desire to find meaning in work is hard to ignore she explains that the drive to work is so strong that people seek positive meaning in work that is considered by many people to be dirty low status or poorly paid hospital cleaners for instance interpret their work to be meaningful and worthwhile because they enlarge the scope of that work in their own minds she explains this phenomenon allows humans to justify all kinds of work to themselves as useful and relevant it seems regardless of what it actually is frey and osborne aren t so confident that humans are resourceful enough to create new work for themselves though frey has actually studied the rate at which new jobs are being generated as a result of technological change his findings suggest that about of the us workforce shifted into new types of jobs that is roles associated with technological advances during the s in the s the figure fell to and in the s it dropped to just the evidence suggests that the new industries we might assume to be the salvation of the labour force such as web design or data science aren t creating as many new positions as we may hope part of the reason for that argues osborne is that many of the new job roles being created are related to software rather than hard physical goods software is pretty cheap with next to zero marginal cost of reproduction he explains that means that a small group of people can have a great idea and easily turn it into a product that s used the world over while barely growing the size of its team the smartphone messaging service whatsapp is a prime example it was purchased by facebook for billion in when it served million users at the time it had just employees counting specific jobs may however be overly simplistic when it comes to thinking about how the working lives of real people are set to change people often think about the work that people do as a monolithic indivisible lump of stuff explains daniel susskind danielsusskind a lecturer in economics at balliol college and co author of a new book called the future of the professions the problem is that encourages the view that one day a lawyer will arrive at work to find an algorithm sitting in his chair or a doctor turn up to a robot in her operating theatre and their jobs will both be gone instead he argues we should be focusing on the separate tasks that make up job roles susskind co wrote his new book with his father richard susskind richardsusskind whose oxford dphil considered the impact of artificial intelligence on law that was back in the s when ai systems were rudimentary and typically based on rules gleaned from human understanding but five years ago father and son the latter then working in the policy unit at downing street realised that a second wave of artificial intelligence was being developed that could have profound effects on professional careers since they ve been researching how technology might affect the working lives of lawyers doctors teachers architects and the rest of the professions not everything that a professional does is creative strategic or complex explains susskind so while many professionals might think that all their work lies on one side of frey and osborne s engineering bottlenecks actually many of the tasks they perform are amenable to computerisation for most that means it s unlikely that they ll simply lose their job to technology at least in the near future but they can expect to see a significant change in the sorts of things they re asked to do in their book the susskinds describe twelve new roles that might appear within the professions such as process analysers knowledge engineers data scientists and empathisers these are roles that sound unfamiliar to traditional professionals that require skills and abilities that many of them are unlikely to have at this moment in time they explain we re already seeing professionals adapt so that they can work alongside more intelligent technological systems though take for instance your bank manager when you used to approach them for a loan they d carefully make a decision on whether or not you were a good risk then either give you the money or send you home now an algorithm determines whether or not you re awarded the cash and yet bank managers still exist the role has simply changed to become a customer service and sales job rather than an analytical or technical role not everyone will be as lucky as the professionals whose jobs merely metamorphose because if all of the tasks that make up a job are automatable the job no longer needs to exist craig holmes craigpholmes a fellow in economics at pembroke college and senior research fellow at the institute for new economic thinking has been studying shifts in occupational structure of labour markets and how they ve moved away from middle skilled work with more people now doing high skilled or low skilled work this phenomenon referred to as the hollowing out of the labour market isn t in itself new middle skilled factory workers have been losing their jobs to robots for decades for instance but the pace of technological development is now threatening other middle skilled occupations that in the past we ve assumed could only be done by humans job categories defined as associate professionals for instance the people that provide technical services that keep trade finance and government running appear increasingly likely to be taken over by machines in the case of say paralegals there are now pieces of software that can sift through thousands of documents pull out relevant precedents and put them together using a very simple format without requiring any human involvement explains holmes so a traditionally middle tier research job can be perfectly performed by technology the same story could play out in other sectors large datasets of historical case notes and information from wearables could allow computers to make straightforward medical diagnoses say while smarter algorithms might remove the work of number crunching accountants like car factory workers replaced by robots in the past holmes imagines a number of possible futures for those discharged from mid tier roles some like the bank manager will be able to assume different roles with similar titles a small number may move upwards into roles that aren t yet automatable others sadly may have to assume lower skilled jobs or face unemployment the nature of those lower skilled jobs will of course change too the work of frey and osborne suggests that many low skilled jobs such as call centre workers data entry clerks and dishwashers will be readily automated in the future in some cases the cost of technology will be so low that there s no wage that people could happily accept that would make the job sustainable admits holmes in fast food restaurants for instance you can replace someone who takes an order with an ipad that will last for years nobody would accept a job that paid wages that low but it s not perhaps quite so gloomy as that as personal service jobs will likely still require a human touch we ll probably see an increase in the number of low skill service jobs because people value human interaction and many of those jobs currently seem not to be readily automatable suggests holmes that will provide more jobs they just won t be great jobs while technology may be the mechanism through which many jobs are lost though it might very well also be the thing that enables people to take up new lower skilled positions there s been an explosion in connectivity around the world explains professor mark graham geoplace from the oxford internet institute something like billion people are now online and that has some significant repercussions in terms of what work is where it s done and how it happens graham has been travelling the world to talk to people who find themselves in a new kind of labour market in particular he s been interviewing individuals who perform work from home provided to them by a slew of websites such as amazon s mechanical turk upwork and clickworker these sites all allow companies and individuals to outsource tasks potential employers simply post a description of what they need doing to a website then people interested in doing the work bid for it the employer chooses someone to do the work based on a combination of price listed skills and ratings from previous employers the worker carries out the task gets paid then moves on to another piece of work the tasks being doled out vary from transcription and translation to new kinds of work such as tagging images for artificial intelligence systems but much of it is currently difficult or expensive to automate technology has also created legions of new workforce members in more traditional sectors such as transportation hospitality catering cleaning and delivery there are increasingly more ways of commodifying bits of everyday life using your car to be an uber driver your apartment to be an airbnb host your bicycle to be a deliveroo rider or your broom to be a task rabbit cleaner explains graham this is what s become known as the sharing or gig economy whether it s uber airbnb or amazon s mechanical turk the business plan is much the same create a digital platform which makes it easier to link a customer who wants a service to be performed with someone who s willing to provide it for a very competitive fee these new styles of working certainly bring some benefits apparent flexibility for workers more efficient use of existing resources and equipment and reasonable prices for those seeking services but as jeremias prassl an associate professor of law and fellow of magdalen college warns this new workforce is potentially vulnerable uber acts like an employer it sets your wage tells you the route to drive hires you and fires you if your rating falls too low he explains under any classical analysis uber performs all the usual employer functions but in its contracts with driver partners the platform explicitly denies employer status suggesting that the worker is very much a contractor legally and through the language it uses uber tries to deny the fact that it offers employment through so doing the company is able to avoid paying social security pension contributions redundancy pay and so on all the usual rights an employee might benefit from but prassl who s written a book about the topic points out that these kinds of contracts are nothing new from the perspective of an employment lawyer zero hours contracts and the gig economy are old problems he explains we ve been grappling with the rise of so called non standard work for the last or years it s just that now they re receiving more attention and sustained media coverage the problem as prassl sees it is that employment law is currently based on an old binary system if you re an employee you get rights to say sick pay notice of dismissal or paid holiday but if you re a contractor you re not afforded any of those rights employment law currently boils down to a simple question how do you define whether or not someone counts as an employee what my research suggests is that maybe we should turn the problem on its head he explains we could say instead who s the employer it seems like a subtle difference but with the shoe on the other foot he suggests crowd workers would be able to enjoy some kind of employment law protection in this upended scenario everyone could benefit from existing minimum standards like the minimum wage working time regulations and discrimination protection with their provision accounted for by whoever is legally deemed to be the employer if companies failed to comply workers could litigate employers in the knowledge that the damages were definitely owed to them it s not just prassl that s worried about the vulnerability of employees one of the issues is that we confuse work with jobs points out ruth yeoman there s an awful lot of work in the world that has to be done and one of the problems when we think about the future of work is how it all gets converted into jobs for which people will be paid sometimes people may contribute to society not through paid work but through some other mechanism voluntary work say or caring and while those tasks may be hard work or may not pay they are necessary and many of them must be done by humans that s why stuart white stuartgwhite associate professor from the department of politics and international relations is interested in how we could ensure everyone enjoys a basic standard of living a concept he s written about in the book democratic wealth he explains white s suggestion is that no tests of means or willingness to take a job would be imposed so that everyone in the country received a basic payment every month it s worth noting that the idea is not intended to make everyone rich far from it instead it s a means of giving individuals more flexibility affording them power to decide when and how to be contributive and productive it s a way of ensuring you don t have people desperately scrambling into jobs to make ends meet white explains in turn he argues employers would make some of the least appealing jobs more pleasant they d be forced to otherwise nobody would choose to do them numerous mechanisms for putting such a policy into action have been proposed in the past one option is to divert existing benefits and tax relief into a basic income that s shared equally amongst the population if those contributions didn t stretch far enough they could be topped up with revenue from further taxation from land value tax suggests white alternatively the income could be provided by a state owned investment fund from which the returns would be shared out equally there are lots of philosophical arguments about whether or not it s all a good idea he concedes but we re moving into a world where there s increased insecurity around work against that backdrop a source of income that s independent of work is a way of rebalancing power relations in the labour market whether or not you agree with the concept of a universal citizen s income or the reform of employment law these concepts are indicative of the kinds of discussions that oxford researchers are increasingly leading i think the university needs to be asking these kinds of aristotlean questions about whose interests are being met who benefits from the changes the moral questions explains marc thompson it s not something we should shy away from increasingly then just as thompson hoped for when he set up the green templeton college future of work programme oxford academics are working with business and governments to shape the debate about the future of employment frey and osborne for instance have published reports with citi and deloitte about the impact of technology on employment mark graham sits on the department for international development s digital advisory panel and richard susskind acts as an it adviser to the lord chief justice of england and wales what remains of course is for policymakers lawyers and industry officials to take the questions and suggestions raised by academics on board then work out how best to use technological advance in all our favour these possibilities afforded by technology automation and commodification of labour they can all be shaped by policy organisational change and simply choosing to do things differently muses thompson there are some important choices to be made about how we make use of them technology will make many jobs redundant others easier and create at least some new ones along the way keynes prediction of a fifteen hour working week may even come true but while humans are in charge we can still choose for there to be some work that s performed by non robotic hands it would be very easy for there to be an automated pub where drinks are served from vending machines concludes mark graham but nobody wants that because it would be depressing written by jamie condliffe a science and technology writer based in london he tweets jme c in keeping with one of the themes of the article we used designs to find an illustrator and worked with slouise follow us on medium we ll be publishing more articles soon that look at topics such as medical trials developments in healthcare and more if you liked this article please click the green heart it really helps to spread the word and let others find it produced by christopher eddie digital communications office university of oxford from a quick cheer to a standing ovation clap to show how much you enjoyed this story oxford is one of the oldest universities in the world we aim to lead the world in research and education contact digicomms admin ox ac uk oxford is one of the oldest universities in the world we aim to lead the world in research and education contact digicomms admin ox ac uk
Maciej Lipiec,766,8,https://medium.com/k2-product-design/the-future-of-digital-banking-236ad65e4c76?source=tag_archive---------5----------------,The Future of Digital Banking – K2 Product Design – Medium,our solution is based on three pillars in the old times user interface of a bank was the bank teller at the branch from today s perspective it was inconvenient and time consuming but the bank had a human face now we are interacting with our banks by clicking on links menus and buttons and filling out forms but banking apps are often hard to use overly complex and ugly lack of true customer centricity and technological debt on the back end side of things make the banking experience frustrating how can we make digital banking easier more simple more personal and human by giving it a new face of a robot meet bankbot it is the new digital bank teller personal assistant and a financial advisor when you sign in to your k bank account bankbot will greet you and ask for orders the main interface of k bank is instantly familiar if you ever used slack over two millions of people use it in the office everyday or facebook messenger or an sms app or irc then you re really old school it s never ending stream with history of communications from bottom recent to the top oldest of the screen you type your command or question and bankbot will answer bankbot understands natural language but it pays special attention for keywords that will trigger actions like a new transfer or searching in history or credit card cancellation just type in send eur to anna and bankbot will search it s database for possible recipients matching anna and let you choose the one you mean or you can add a new recipient then bankbot will sent confirmation code to your cell phone and ask you to type it in and it s done you don t need to click and move your hands from the keyboard of course this the easiest scenario similar to sending money via squarecash or snapcash but almost every operation can be completed that way typing a recipient s name will show you recent transactions with her from your account history and option for a new payment typing usd will show you currency exchange rate if you need help type help if you need to contact human staff at the bank type human and you can chat with real person from customer service instead of a bot or type concierge if you re a private banking client there is also a way to access features using the hamburger menu at the bottom it opens a list of options just like typing slash in slack personal finance managers pfms for controlling home budget are popular additions to banking systems but they are complicated often hidden deep in the nested menus and they need a lot of user s attention do people really use them steven walker of forrester research has written bankbot can provide just that you can ask expenses this month or car expenses and it will show you a simple chart with relevant information this is pull mechanism but bankbot can also be proactive pushing important information to the user it can warn you that you are close to exceeding your monthly budget it can remind you about regular payments you usually make each month it can remind you to pay off your credit card or pay your tax it can suggest better options to save or invest your money and show you how much more you can earn it can offer you a loan when you probably need it or offer travel insurance when he knows you ve just bought plane tickets or up sell you a better account or credit card when it will notice that you ve got a pay rise or it can alert you when you should do something with your stocks portfolio chat banking is nice on the desktop but it s even more effective on mobile type a few words and it s done just like sending an sms or you can talk to bankbot speech text authentication can be provided by fingerprint sensor you can receive important alerts as push notifications on your phone or smartwatch and immediately take action or dismiss you can even get discount on your health insurance based on physical activity data from your fitness band or apple watch bankbot can also live inside smart devices like the amazon echo which provides its own api for developers smart home and smart banking mixed together or inside the facebook messenger chat the second payment services directive is to be transposed into national regulations across the european union from its goal is to open the banking market psd will force banks to provide access via apis to their customer accounts and provide account information to third party service providers if the account holder wishes to do so this is called access to the account xs a and it s not optional banks will have to evolve as third parties enter their space psd defines traditional financial institutions banks as account servicing payment service providers as psp and new players as account information service providers aisp or payment initiation service providers pisp both pisps and aisps will have to register with the competent authority in their home member state for security reasons what are the implications of this for our system the quality of banking user interfaces will be extremely important because bank s clients could choose to manage their account from third party provider app with better ux or functionality cutting themselves from any direct communication with their bank in this case the bank will be reduced to a dumb pipe in the value chain but fighting this by providing to the third parties only the minimum apis required may be a bad strategy for banks we think they should be more open actively partnering with other financial institutions retailers merchants and startups we imagine k bank solution providing an appstore based on its apis users will be able to give permission to third party service providers in a way you allow applications to access your facebook or twitter account today you will be able to buy stuff at your authorized retailer without logging into your bank or without visiting the retailer site but from yours bank app there is no need to provide credit card number probably even shipping address or any data the bank can automatically offer you a purchase by installments or it can give you a discount because of your history of frequent past transactions online and offline with this retailer there will be no need for customer loyalty cards anymore the bank can become an advertising channel for the retailers too offering personalized promotions for its customers this should be opt out but if your cell phone contract is ending and bankbot messages you with a really great offer for a plan with a cheap newest iphone and you can buy it instantly with one click would you mind by building the thriving ecosystems banks and third parties can both win and we hope customers will too if you want to know more about k bank solution it s design technology behind the bankbot and possibilities of implementation don t hesitate to contact us of course conversational interfaces like bankbot can be used not only in banking but also insurance online commerce travel healthcare and many other industries please write to maciej lipiec k s user experience director at maciej lipiec k pl you can read more about k bank in this article at chatbots magazine also please check out our project on behance k internet is a leading digital product design and communications agency in poland we develop digital services apps and websites with a strong focus on user experience we have a long time experience partnering with financial institutions in the last years we helped to envision design and develop over transactional systems for the biggest banks in poland stanusch technologies is k bank s technology provider for bankbot the company is involved in research and development of the use of artificial intelligence in business it carry out projects related to natural language processing and semantic information retrieval it has become a world leader in the number of carried out projects of virtual advisors chatbots thank you if you enjoyed reading this please and share from a quick cheer to a standing ovation clap to show how much you enjoyed this story product design director k k internet is a leading digital product design and communications agency in poland
Camron Godbout,341,10,https://hackernoon.com/tensorflow-in-a-nutshell-part-three-all-the-models-be1465993930?source=tag_archive---------7----------------,TensorFlow in a Nutshell — Part Three: All the Models,make sure to check out the other articles here in this installment we will be going over all the abstracted models that are currently available in tensorflow and describe use cases for that particular model as well as simple sample code full sources of working examples are in the tensorflow in a nutshell repo use cases language modeling machine translation word embedding text processing since the advent of long short term memory and gated recurrent units recurrent neural networks have made leaps and bounds above other models in natural language processing they can be fed vectors representing characters and be trained to generate new sentences based on the training set the merit in this model is that it keeps the context of the sentence and derives meaning that cat sat on the mat means the cat is on the mat since the creation of tensorflow writing these networks have become increasingly simpler there are even hidden features covered by denny britz here that make writing rnn s even simpler heres a quick example use cases image processing facial recognition computer vision convolution neural networks are unique because they re created in mind that the input will be an image cnns perform a sliding window function to a matrix the window is called a kernel and it slides across the image creating a convolved feature creating a convolved feature allows for edge detection which then allows for a network to depict objects from pictures the convolved feature to create this looks like this matrix below here s a sample of code to identify handwritten digits from the mnist dataset use cases classification and regression these networks consist of perceptrons in layers that take inputs that pass information on to the next layer the last layer in the network produces the output there is no connection between each node in a given layer the layer that has no original input and no final output is called the hidden layer the goal of this network is similar to other supervised neural networks using back propagation to make inputs have the desired trained outputs these are some of the simplest effective neural networks for classification and regression problems we will show how easy it is to create a feed forward network to classify handwritten digits use cases classification and regression linear models take x values and produce a line of best fit used for classification and regression of y values for example if you have a list of house sizes and their price in a neighborhood you can predict the price of house given the size using a linear model one thing to note is that linear models can be used for multiple x features for example in the housing example we can create a linear model given house sizes how many rooms how many bathrooms and price and predict price given a house with size of rooms of bathrooms use cases currently only binary classification the general idea behind a svm is that there is an optimal hyperplane for linearly separable patterns for data that is not linearly separable we can use a kernel function to transform the original data into a new space svms maximize the margin around separating the hyperplane they work extremely well in high dimensional spaces and and are still effective if the dimensions are greater than the number of samples use cases recommendation systems classification and regression deep and wide models were covered with greater detail in part two so we won t get too heavy here a wide and deep network combines a linear model with a feed forward neural net so that our predictions will have memorization and generalization this type of model can be used for classification and regression problems this allows for less feature engineering with relatively accurate predictions thus getting the best of both worlds here s a code snippet from part two s github use cases classification and regression random forest model takes many different classification trees and each tree votes for that class the forest chooses the classification having the most votes random forests do not overfit you can run as many treees as you want and it is relatively fast give it a try on the iris data with this snippet below use cases classification and regression in the contrib folder of tensorflow there is a library called bayesflow bayesflow has no documentation except for an example of the reinforce algorithm this algorithm is proposed in a paper by ronald williams this network trying to solve an immediate reinforcement learning task adjusts the weights after getting the reinforcement value at each trial at the end of each trial each weight is incremented by a learning rate factor multiplied by the reinforcement value minus the baseline multiplied by characteristic eligibility williams paper also discusses the use of back propagation to train the reinforce network use cases sequential data crfs are conditional probability distributions that factoirze according to an undirected model they predict a label for a single sample keeping context from the neighboring samples crfs are similar to hidden markov models crfs are often used for image segmentation and object recognition as well as shallow parsing named entity recognition and gene finding ever since tensorflow has been released the community surrounding the project has been adding more packages examples and cases for using this amazing library even at the time of writing this article there are more models and sample code being written it is amazing to see how much tensorflow as grown in these past few months the ease of use and diversity in the package are increasing overtime and don t seem to be slowing down anytime soon from a quick cheer to a standing ovation clap to show how much you enjoyed this story co founder cto of apteo researching machine learning techniques to improve investing come join us how hackers start their afternoons
Dominik Felix,286,5,https://chatbotsmagazine.com/how-to-create-a-chatbot-without-coding-a-single-line-e716840c7245?source=tag_archive---------8----------------,How to Create a Chatbot Without Coding a Single Line,chatbots are ready to succeed if you think you have to hack days or even weeks to create a chatbot you might be wrong you don t have to be aware of any coding skills immediately after big players like facebook messenger or skype opened their platform for programmers many tools emerged with this article i want to give you an introduction to mockup and overview of different tools to build your first chatbot you re having an idea you want to show your use case it s definitely recommendable to mockup your story beforehand first you may find some bugs in your concept moreover you will be able to explain a showcase to noninvolved people based on the motto fake it til you make it it s very intuitive storytelling just insert what the user says and what the bot responds using the settings option you can edit smartphone models decide number of fans and choose a profile picture a page category and a welcome message additional features are buttons images and quick replies the whole story acts like a movie by pushing the play button it can be shared by just one click and it s possible to save the file as mp within the paid plan each of the tools supports different platforms therefore please keep in mind that it s important to choose your platform wisely based on the huge range most of the tools make use of facebook messenger chatfuel is focused on facebook messenger you don t need any coding skills to get started it s simple to create different logic blocks and link them to respective triggers it offers great plugins e g human take over and a minimalistic ai in case you were recently starting with bots i can recommend you this service motion provides sms email web chat facebook messenger and slack furthermore it s possible to link to other apis and hook back to motion thus it operates as a hub the conversation is built with flowcharts and based on connectors and prepared modules it just takes a few minutes to get familiar with the procedure founder ceo of motion ai david nelson s chatbots made easy api ai is a great platform for developing chatbots it has ai support and an intuitive interface it requires only one click to assemble i e small talk or weather features on the one hand it s possible to run the bot exclusively on their servers on the other you can download a nodejs sample code to execute it on your infrastructure to sum up api ai is an advanced service being the reason why it s more complicated to build a bot using this tool unsurprisingly it got bought by google a few days ago featured cbm api ai small talk is now open why is it a big deal flow xo offers a graphical interface to build so called flows which define how your bot will operate to received messages or audio it has a huge list of integrations as a consequence it s more complex than chatfuel but also a lot more flexible pretty amazing is their support on messenger slack sms and telegram they ve an interesting approach to build chatbots it guides you through steps design develop launch and grow first you ve to design the content messages persistent menus welcome messages and some more as step it wants you to link messages to triggers and setup curious modules like offer human help the launch step leads you through the review process while the final step focuses on customer retention i e schedule messages user lists etc manychat allows broadcast content from rss feeds additionally it s possible to link to yahoo pipelines and broadcast everything you want it supports scheduled messages auto posting from rss facebook twitter youtube and has a basic mechanism to send specific answers to specific keywords watch their pitch to get a better understanding mindiq is a diy bot builder platform for businesses focused on facebook messenger you don t need any coding skills and they make it dead simple for businesses to build bots they follow a template approach currently the templates available are media commerce and food tech they also provide tools to link your business tools like mailchimp to your chatbot there are many tools on the market every tool solves other problems and each of them uses a different approach for how to design user interaction i really like the simplicity of chatfuel and the step process of botsify since all of these tools are quite new i m super excited and looking forward to seeing the direction that will be pursued and developed from a quick cheer to a standing ovation clap to show how much you enjoyed this story botspot vienna agentur volk chatbot ecosystem botstack framework chatbots ai nlp facebook messenger slack telegram and more
Greg Gascon,368,6,https://medium.com/startup-grind/how-invisible-interfaces-are-going-to-transform-the-way-we-interact-with-computers-39ef77a8a982?source=tag_archive---------9----------------,How Invisible Interfaces are going to transform the way we interact with computers,in the mid nineties a computer scientist at xerox parc theorized the concept of the internet of things albeit with a different name far before anyone else had and even further still before it had become possible even though today we call it by that name ubiquitous computing as it was then coined by mark weiser imagined a world wherein cheap and ubiquitous connected computing would radically alter the way we use and interact with computers the idea was ahead of its time in the world of ubiquitous computing connected devices would become cheap and thereby would exist everywhere importantly these devices would as a result cease to become special or unique they would become invisible as we near this utopian world filled with computers our relationship with them inexorably will change each of us will come to interact with dozens of separate devices on a daily basis as such we will need to develop interfaces in a way so as not to distract us as is currently done but in a way in which to empower us or how weiser put it we will need to adopt the concepts of calm technology on the face of it ubiquitous computing is just that a reality in which computers are everywhere of course with trends relating to iot we are nearing this but we are not there yet one of the most important implications to come from ubiquitous computing for example will be the changes it will make on how we perceive and interact with computers for instance think of the electric motor an old technology that is ubiquitous in the present today there could be dozens of them in a single car however when we hit a button to roll down the windows we don t think at all about the motor pulling the window down we simply think about the action of making the window go down the electric motor is so mundane and ubiquitous in our lives that we don t even think about it when using it it is invisible it is this sort of invisibility that allows the user to take full control of their interactions with a given piece of technology when using a piece of technology that has become invisible the user thinks of using it in terms of end goals rather than getting bogged down in the technology itself the user doesn t have to worry how it is going to work they just make it happen in another example weiser simply states a good pencil stays out of the way of the writing now even though technology surrounds us today we aren t at this point yet gadgets and devices are still special to us in a distracting way we still not only still marvel at new technology we are told to by whomever is producing it but why does this matter the best way to see how ubiquitous computing will impact us is to examine the way we engineer and interact with the apps that exist today when creating a web app for instance you try to guide or manipulate the user into using your tool as much as possible when you create a drip marketing email campaign for it in most cases you aren t creating it so that the user needs to use your tool less you are creating it so they can spend more time and use all of its features that is to say the goal isn t foremost and necessarily to save the user time furthermore there is no question asked as to whether the user aught to spend more time using whatever particular app is being optimized within a social media website each user is given a piece of social property a social media platform imbues each social property with a value system think of the concept of likes comments or shares as incentive to spend time on the site each user interaction with a social property whether it be a photo or a comment that is written is then logged and recorded so they can easily be rewarded for the time invested some social apps such as linkedin will have us hooked for something as simple as a pageview of our profiles these actions are further incentivized through the use of gamification apps send intrusive notifications giving you some information about what they are about but not everything and this is crucial not knowing what is in the notification entices us to open it even further it goes without saying this is important for increasing the amount of screen time we give the app for if we saw everything in the notification there would be no point in opening the app it makes waking up every morning feel like opening a bunch of small presents and while it s a stretch to say that developers are acting nefariously to steal our time those building our web services and tools should construct them with respect to the user s guilelessness doing so requires adopting principles of invisible or calm technology contradiction aside the most accessible way we can get a glimpse into a future dominated by invisible interfaces is the movie her although not the focus of the film her showcases a future wherein inputs given to devices are done so largely through voice commands yes there are still smartphones but the majority of interactions take place by simply talking to a given device using natural language theodore is able to interact with technology in a manner that is completely at hand he can ask any sort of question or create any sort of demand without getting bogged down in how the device works furthermore the technology never tries to whisk his attention away from anything the technology is always there but it is only in the periphery according to weiser this is one of the key principles of designing calm technology the device in question should never try to distract or pry the user away from what they are trying to accomplish yet it must always be ready to accept user input it is calming in the exact opposite way that receiving group chat notifications on your phone is not we can see this principle of design in part at play in the new apple airpods even though they have yet to be released they promise to let us interact with the internet without ever needing to look down at our phones and they are aware of their environment too they know such things like if they are in an ear or not and if they are not they know to stop playing sound it s these small micro automations that will further make technology invisible and allow us to focus on whatever it is that we want from the technology and not worry about having to configure it other more simple examples include the auto brightness on your phone or its fingerprint scanner they simply work without any sort of configuration or notification about what they are doing and more technologies like this are coming there are today even advocacy groups such as time well spent that try to spread awareness about how interfaces and apps can hijack the ways our brains work even more promising is that there are companies that are following suit in these designs principles for instance the upcoming moment smartwatch is a device which interfaces with the user largely through touch feedback instead of relying on the screen all that s needed now better speech recognition from a quick cheer to a standing ovation clap to show how much you enjoyed this story tech columnist apps script dev social media automator seo specialist read more at https www gregorygascon com the life work and tactics of entrepreneurs around the world by founders for founders welcoming submissions on technology trends product design growth strategies and venture investing
Dhruv Parthasarathy,4300,12,https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4?source=tag_archive---------0----------------,A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN,at athelas we use convolutional neural networks cnns for a lot more than just classification in this post we ll see how cnns can be used with great results in image instance segmentation ever since alex krizhevsky geoff hinton and ilya sutskever won imagenet in convolutional neural networks cnns have become the gold standard for image classification in fact since then cnns have improved to the point where they now outperform humans on the imagenet challenge while these results are impressive image classification is far simpler than the complexity and diversity of true human visual understanding in classification there s generally an image with a single object as the focus and the task is to say what that image is see above but when we look at the world around us we carry out far more complex tasks we see complicated sights with multiple overlapping objects and different backgrounds and we not only classify these different objects but also identify their boundaries differences and relations to one another can cnns help us with such complex tasks namely given a more complicated image can we use cnns to identify the different objects in the image and their boundaries as has been shown by ross girshick and his peers over the last few years the answer is conclusively yes through this post we ll cover the intuition behind some of the main techniques used in object detection and segmentation and see how they ve evolved from one implementation to the next in particular we ll cover r cnn regional cnn the original application of cnns to this problem along with its descendants fast r cnn and faster r cnn finally we ll cover mask r cnn a paper released recently by facebook research that extends such object detection techniques to provide pixel level segmentation here are the papers referenced in this post inspired by the research of hinton s lab at the university of toronto a small team at uc berkeley led by professor jitendra malik asked themselves what today seems like an inevitable question object detection is the task of finding the different objects in an image and classifying them as seen in the image above the team comprised of ross girshick a name we ll see again jeff donahue and trevor darrel found that this problem can be solved with krizhevsky s results by testing on the pascal voc challenge a popular object detection challenge akin to imagenet they write let s now take a moment to understand how their architecture regions with cnns r cnn works understanding r cnn the goal of r cnn is to take in an image and correctly identify where the main objects via a bounding box in the image but how do we find out where these bounding boxes are r cnn does what we might intuitively do as well propose a bunch of boxes in the image and see if any of them actually correspond to an object r cnn creates these bounding boxes or region proposals using a process called selective search which you can read about here at a high level selective search shown in the image above looks at the image through windows of different sizes and for each size tries to group together adjacent pixels by texture color or intensity to identify objects once the proposals are created r cnn warps the region to a standard square size and passes it through to a modified version of alexnet the winning submission to imagenet that inspired r cnn as shown above on the final layer of the cnn r cnn adds a support vector machine svm that simply classifies whether this is an object and if so what object this is step in the image above improving the bounding boxes now having found the object in the box can we tighten the box to fit the true dimensions of the object we can and this is the final step of r cnn r cnn runs a simple linear regression on the region proposal to generate tighter bounding box coordinates to get our final result here are the inputs and outputs of this regression model so to summarize r cnn is just the following steps r cnn works really well but is really quite slow for a few simple reasons in ross girshick the first author of r cnn solved both these problems leading to the second algorithm in our short history fast r cnn let s now go over its main insights fast r cnn insight roi region of interest pooling for the forward pass of the cnn girshick realized that for each image a lot of proposed regions for the image invariably overlapped causing us to run the same cnn computation again and again times his insight was simple why not run the cnn just once per image and then find a way to share that computation across the proposals this is exactly what fast r cnn does using a technique known as roipool region of interest pooling at its core roipool shares the forward pass of a cnn for an image across its subregions in the image above notice how the cnn features for each region are obtained by selecting a corresponding region from the cnn s feature map then the features in each region are pooled usually using max pooling so all it takes us is one pass of the original image as opposed to fast r cnn insight combine all models into one network the second insight of fast r cnn is to jointly train the cnn classifier and bounding box regressor in a single model where earlier we had different models to extract image features cnn classify svm and tighten bounding boxes regressor fast r cnn instead used a single network to compute all three you can see how this was done in the image above fast r cnn replaced the svm classifier with a softmax layer on top of the cnn to output a classification it also added a linear regression layer parallel to the softmax layer to output bounding box coordinates in this way all the outputs needed came from one single network here are the inputs and outputs to this overall model even with all these advancements there was still one remaining bottleneck in the fast r cnn process the region proposer as we saw the very first step to detecting the locations of objects is generating a bunch of potential bounding boxes or regions of interest to test in fast r cnn these proposals were created using selective search a fairly slow process that was found to be the bottleneck of the overall process in the middle a team at microsoft research composed of shaoqing ren kaiming he ross girshick and jian sun found a way to make the region proposal step almost cost free through an architecture they creatively named faster r cnn the insight of faster r cnn was that region proposals depended on features of the image that were already calculated with the forward pass of the cnn first step of classification so why not reuse those same cnn results for region proposals instead of running a separate selective search algorithm indeed this is just what the faster r cnn team achieved in the image above you can see how a single cnn is used to both carry out region proposals and classification this way only one cnn needs to be trained and we get region proposals almost for free the authors write here are the inputs and outputs of their model how the regions are generated let s take a moment to see how faster r cnn generates these region proposals from cnn features faster r cnn adds a fully convolutional network on top of the features of the cnn creating what s known as the region proposal network the region proposal network works by passing a sliding window over the cnn feature map and at each window outputting k potential bounding boxes and scores for how good each of those boxes is expected to be what do these k boxes represent intuitively we know that objects in an image should fit certain common aspect ratios and sizes for instance we know that we want some rectangular boxes that resemble the shapes of humans likewise we know we won t see many boxes that are very very thin in such a way we create k such common aspect ratios we call anchor boxes for each such anchor box we output one bounding box and score per position in the image with these anchor boxes in mind let s take a look at the inputs and outputs to this region proposal network we then pass each such bounding box that is likely to be an object into fast r cnn to generate a classification and tightened bounding boxes so far we ve seen how we ve been able to use cnn features in many interesting ways to effectively locate different objects in an image with bounding boxes can we extend such techniques to go one step further and locate exact pixels of each object instead of just bounding boxes this problem known as image segmentation is what kaiming he and a team of researchers including girshick explored at facebook ai using an architecture known as mask r cnn much like fast r cnn and faster r cnn mask r cnn s underlying intuition is straight forward given that faster r cnn works so well for object detection could we extend it to also carry out pixel level segmentation mask r cnn does this by adding a branch to faster r cnn that outputs a binary mask that says whether or not a given pixel is part of an object the branch in white in the above image as before is just a fully convolutional network on top of a cnn based feature map here are its inputs and outputs but the mask r cnn authors had to make one small adjustment to make this pipeline work as expected roialign realigning roipool to be more accurate when run without modifications on the original faster r cnn architecture the mask r cnn authors realized that the regions of the feature map selected by roipool were slightly misaligned from the regions of the original image since image segmentation requires pixel level specificity unlike bounding boxes this naturally led to inaccuracies the authors were able to solve this problem by cleverly adjusting roipool to be more precisely aligned using a method known as roialign imagine we have an image of size x and a feature map of size x let s imagine we want features the region corresponding to the top left x pixels in the original image see above how might we select these pixels from the feature map we know each pixel in the original image corresponds to pixels in the feature map to select pixels from the original image we just select pixels in roipool we would round this down and select pixels causing a slight misalignment however in roialign we avoid such rounding instead we use bilinear interpolation to get a precise idea of what would be at pixel this at a high level is what allows us to avoid the misalignments caused by roipool once these masks are generated mask r cnn combines them with the classifications and bounding boxes from faster r cnn to generate such wonderfully precise segmentations if you re interested in trying out these algorithms yourselves here are relevant repositories faster r cnn mask r cnn in just years we ve seen how the research community has progressed from krizhevsky et al s original result to r cnn and finally all the way to such powerful results as mask r cnn seen in isolation results like mask r cnn seem like incredible leaps of genius that would be unapproachable yet through this post i hope you ve seen how such advancements are really the sum of intuitive incremental improvements through years of hard work and collaboration each of the ideas proposed by r cnn fast r cnn faster r cnn and finally mask r cnn were not necessarily quantum leaps yet their sum products have led to really remarkable results that bring us closer to a human level understanding of sight what particularly excites me is that the time between r cnn and mask r cnn was just three years with continued funding focus and support how much further can computer vision improve over the next three years if you see any errors or issues in this post please contact me at dhruv getathelas com and i ll immediately correct them if you re interested in applying such techniques come join us at athelas where we apply computer vision to blood diagnostics daily other posts we ve written thanks to bharath ramsundar pranav ramkrishnan tanay tandon and oliver cameron for help with this post from a quick cheer to a standing ovation clap to show how much you enjoyed this story dhruvp vp eng athelas mit math and cs undergrad mit cs masters previously director of ai programs udacity blood diagnostics through deep learning http athelas com
Slav Ivanov,3900,17,https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415?source=tag_archive---------1----------------,"The $1700 great Deep Learning box: Assembly, setup and benchmarks",updated april uses cuda cudnn and tensorflow after years of using a thin client in the form of increasingly thinner macbooks i had gotten used to it so when i got into deep learning dl i went straight for the brand new at the time amazon p cloud servers no upfront cost the ability to train many models simultaneously and the general coolness of having a machine learning model out there slowly teaching itself however as time passed the aws bills steadily grew larger even as i switched to x cheaper spot instances also i didn t find myself training more than one model at a time instead i d go to lunch workout etc while the model was training and come back later with a clear head to check on it but eventually the model complexity grew and took longer to train i d often forget what i did differently on the model that had just completed its day training nudged by the great experiences of the other folks on the fast ai forum i decided to settle down and to get a dedicated dl box at home the most important reason was saving time while prototyping models if they trained faster the feedback time would be shorter thus it would be easier for my brain to connect the dots between the assumptions i had for the model and its results then i wanted to save money i was using amazon web services aws which offered p instances with nvidia k gpus lately the aws bills were around month with a tendency to get larger also it is expensive to store large datasets like imagenet and lastly i haven t had a desktop for over years and wanted to see what has changed in the meantime spoiler alert mostly nothing what follows are my choices inner monologue and gotchas from choosing the components to benchmarking a sensible budget for me would be about years worth of my current compute spending at month for aws this put it at around for the whole thing you can check out all the components used the pc part picker site is also really helpful in detecting if some of the components don t play well together the gpu is the most crucial component in the box it will train these deep networks fast shortening the feedback cycle disclosure the following are affiliate links to help me pay for well more gpus the choice is between a few of nvidia s cards gtx gtx ti gtx gtx ti and finally the titan x the prices might fluctuate especially because some gpus are great for cryptocurrency mining wink wink on performance side gtx ti and titan x are similar roughly speaking the gtx is about faster than gtx and gtx ti is about faster than gtx the new gtx ti is very close in performance to gtx tim dettmers has a great article on picking a gpu for deep learning which he regularly updates as new cards come on the market here are the things to consider when picking a gpu considering all of this i picked the gtx ti mainly for the training speed boost i plan to add a second ti soonish even though the gpu is the mvp in deep learning the cpu still matters for example data preparation is usually done on the cpu the number of cores and threads per core is important if we want to parallelize all that data prep to stay on budget i picked a mid range cpu the intel i it s relatively cheap but good enough to not slow things down edit as a few people have pointed out probably the biggest gotcha that is unique to dl multi gpu is to pay attention to the pcie lanes supported by the cpu motherboard by andrej karpathy we want to have each gpu have pcie lanes so it eats data as fast as possible gb s for pcie this means that for two cards we need pcie lanes however the cpu i have picked has only lanes so gpus would run in x mode instead of x this might be a bottleneck leading to less than ideal utilization of the graphics cards thus a cpu with lines is recommended edit however tim dettmers points out that having lanes per card should only decrease performance by for two gpus so currently my recommendation is go with pcie lanes per video card unless it gets too expensive for you otherwise lanes should do as well a good solution with to have for a double gpu machine would be an intel xeon processor like the e v pcie lanes or if you want to splurge go for a higher end processor like the desktop i k memory ram it s nice to have a lot of memory if we are to be working with rather big datasets i got sticks of gb for a total of gb of ram and plan to buy another gb later following jeremy howard s advice i got a fast ssd disk to keep my os and current data on and then a slow spinning hdd for those huge datasets like imagenet ssd i remember when i got my first macbook air years ago how blown away was i by the ssd speed to my delight a new generation of ssd called nvme has made its way to market in the meantime a gb mydigitalssd nvme drive was a great deal this baby copies files at gigabytes per second hdd tb seagate while ssds have been getting fast hdd have been getting cheap to somebody who has used macbooks with gb disk for the last years having this much space feels almost obscene the one thing that i kept in mind when picking a motherboard was the ability to support two gtx ti both in the number of pci express lanes the minimum is x and the physical size of cards also make sure it s compatible with the chosen cpu an asus tuf z did it for me msi x a sli plus should work great if you got an intel xeon cpu rule of thumb power supply should provide enough juice for the cpu and the gpus plus watts extra the intel i processor uses w and the gpus ti need w each so i got a deepcool w gold psu currently unavailable evga gq is similar the gold here refers to the power efficiency i e how much of the power consumed is wasted as heat the case should be the same form factor as the motherboard also having enough leds to embarrass a burner is a bonus a friend recommended the thermaltake n case which i promptly got no leds sadly here is how much i spent on all the components your costs may vary gtx ti cpu ram ssd hdd motherboard psu case total adding tax and fees this nicely matches my preset budget of if you don t have much experience with hardware and fear you might break something a professional assembly might be the best option however this was a great learning opportunity that i couldn t pass even though i ve had my share of hardware related horror stories the first and important step is to read the installation manuals that came with each component especially important for me as i ve done this before once or twice and i have just the right amount of inexperience to mess things up this is done before installing the motherboard in the case next to the processor there is a lever that needs to be pulled up the processor is then placed on the base double check the orientation finally the lever comes down to fix the cpu in place but i had a quite the difficulty doing this once the cpu was in position the lever wouldn t go down i actually had a more hardware capable friend of mine video walk me through the process turns out the amount of force required to get the lever locked down was more than what i was comfortable with next is fixing the fan on top of the cpu the fan legs must be fully secured to the motherboard consider where the fan cable will go before installing the processor i had came with thermal paste if yours doesn t make sure to put some paste between the cpu and the cooling unit also replace the paste if you take off the fan i put the power supply unit psu in before the motherboard to get the power cables snugly placed in case back side pretty straight forward carefully place it and screw it in a magnetic screwdriver was really helpful then connect the power cables and the case buttons and leds just slide it in the m slot and screw it in piece of cake the memory proved quite hard to install requiring too much effort to properly lock in a few times i almost gave up thinking i must be doing it wrong eventually one of the sticks clicked in and the other one promptly followed at this point i turned the computer on to make sure it works to my relief it started right away finally the gpu slid in effortlessly pins of power later and it was running nb do not plug your monitor in the external card right away most probably it needs drivers to function see below finally it s complete now that we have the hardware in place only the soft part remains out with the screwdriver in with the keyboard note on dual booting if you plan to install windows because you know for benchmarks totally not for gaming it would be wise to do windows first and linux second i didn t and had to reinstall ubuntu because windows messed up the boot partition livewire has a detailed article on dual boot most dl frameworks are designed to work on linux first and eventually support other operating systems so i went for ubuntu my default linux distribution an old gb usb drive was laying around and worked great for the installation unetbootin osx or rufus windows can prepare the linux thumb drive the default options worked fine during the ubuntu install at the time of writing ubuntu was just released so i opted for the previous version whose quirks are much better documented online ubuntu server or desktop the server and desktop editions of ubuntu are almost identical with the notable exception of the visual interface called x not being installed with server i installed the desktop and disabled autostarting x so that the computer would boot it in terminal mode if needed one could launch the visual desktop later by typing startx let s get our install up to date from jeremy howard s excellent install gpu script to deep learn on our machine we need a stack of technologies to use our gpu download cuda from nvidia or just run the code below updated to specify version of cuda thanks to zhanwenchen for the tip if you need to add later versions of cuda click here after cuda has been installed the following code will add the cuda installation to the path variable now we can verify that cuda has been installed successfully by running this should have installed the display driver as well for me nvidia smi showed err as the device name so i installed the latest nvidia drivers as of may to fix it removing cuda nvidia drivers if at any point the drivers or cuda seem broken as they did for me multiple times it might be better to start over by running since version tensorflow supports cudnn so we install that to download cudnn one needs to register for a free developer account after downloading install with the following anaconda is a great package manager for python i ve moved to python so will be using the anaconda version the popular dl framework by google installation validate tensorfow install to make sure we have our stack running smoothly i like to run the tensorflow mnist example we should see the loss decreasing during training keras is a great high level neural networks framework an absolute pleasure to work with installation can t be easier too pytorch is a newcomer in the world of dl frameworks but its api is modeled on the successful torch which was written in lua pytorch feels new and exciting mostly great although some things are still to be implemented we install it by running jupyter is a web based ide for python which is ideal for data sciency tasks it s installed with anaconda so we just configure and test it now if we open http localhost we should see a jupyter screen run jupyter on boot rather than running the notebook every time the computer is restarted we can set it to autostart on boot we will use crontab to do this which we can edit by running crontab e then add the following after the last line in the crontab file i use my old trusty macbook air for development so i d like to be able to log into the dl box both from my home network also when on the run ssh key it s way more secure to use a ssh key to login instead of a password digital ocean has a great guide on how to setup this ssh tunnel if you want to access your jupyter notebook from another computer the recommended way is to use ssh tunneling instead of opening the notebook to the world and protecting with a password let s see how we can do this then to connect over ssh tunnel run the following script on the client to test this open a browser and try http localhost from the remote machine your jupyter notebook should appear setup out of network access finally to access the dl box from the outside world we need things setting up out of network access depends on the router network setup so i m not going into details now that we have everything running smoothly let s put it to the test we ll be comparing the newly built box to an aws p xlarge instance which is what i ve used so far for dl the tests are computer vision related meaning convolutional networks with a fully connected model thrown in we time training models on aws p instance gpu k aws p virtual cpu the gtx ti and intel i cpu andres hernandez points out that my comparison does not use tensorflow that is optimized for these cpus which would have helped the them perform better check his insightful comment for more details the hello world of computer vision the mnist database consists of handwritten digits we run the keras example on mnist which uses multilayer perceptron mlp the mlp means that we are using only fully connected layers not convolutions the model is trained for epochs on this dataset which achieves over accuracy out of the box we see that the gtx ti is times faster than the k on aws p in training the model this is rather surprising as these cards should have about the same performance i believe this is because of the virtualization or underclocking of the k on aws the cpus perform times slower than the gpus as we will see later it s a really good result for the processors this is due to the small model which fails to fully utilize the parallel processing power of the gpus interestingly the desktop intel i achieves x speedup over the virtual cpu on amazon a vgg net will be finetuned for the kaggle dogs vs cats competition in this competition we need to tell apart pictures of dogs and cats running the model on cpus for the same number of batches wasn t feasible therefore we finetune for batches epoch on the gpus and batches on the cpus the code used is on github the ti is times faster that the aws gpu k the difference in the cpus performance is about the same as the previous experiment i is x faster however it s absolutely impractical to use cpus for this task as the cpus were taking x more time on this large model that includes convolutional layers and a couple semi wide fully connected layers on top a gan generative adversarial network is a way to train a model to generate images gan achieves this by pitting two networks against each other a generator which learns to create better and better images and a discriminator that tries to tell which images are real and which are dreamt up by the generator the wasserstein gan is an improvement over the original gan we will use a pytorch implementation that is very similar to the one by the wgan author the models are trained for steps and the loss is all over the place which is often the case with gans cpus aren t considered the gtx ti finishes x faster than the aws p k which is in line with the previous results the final benchmark is on the original style transfer paper gatys et al implemented on tensorflow code available style transfer is a technique that combines the style of one image a painting for example and the content of another image check out my previous post for more details on how style transfer works the gtx ti outperforms the aws k by a factor of this time the cpus are times slower than graphics cards the slowdown is less than on the vgg finetuning task but more than on the mnist perceptron experiment the model uses mostly the earlier layers of the vgg network and i suspect this was too shallow to fully utilize the gpus the dl box is in the next room and a large model is training on it was it a wise investment time will tell but it is beautiful to watch the glowing leds in the dark and to hear its quiet hum as models are trying to squeeze out that extra accuracy percentage point from a quick cheer to a standing ovation clap to show how much you enjoyed this story entrepreneur hacker machine learning deep learning and other types of learning
Tyler Elliot Bettilyon,17900,13,https://medium.com/@TebbaVonMathenstien/are-programmers-headed-toward-another-bursting-bubble-528e30c59a0e?source=tag_archive---------2----------------,Are Programmers Headed Toward Another Bursting Bubble?,a friend of mine recently posed a question that i ve heard many times in varying forms and forums do you think it and some lower level programming jobs are going to go the way of the dodo seems a bit like a massive job bubble that s gonna burst it s my opinion that one of the only things keeping tech and lower level computer science related jobs prestigious and well paid is ridiculous industry jargon and public ignorance about computers which are both going to go away in the next years this question is simultaneously on point about the future of technology jobs and exemplary of some pervasive misunderstandings regarding the field of software engineering while it s true that there is a great deal of ridiculous industry jargon there are equally many genuinely difficult problems waiting to be solved by those with the right skill set some software jobs are definitely going away but programmers with the right experience and knowledge will continue to be prestigious and well remunerated for many years to come as an example look at the recent explosion of ai researcher salaries and the corresponding dearth of available talent staying relevant in the ever changing technology landscape can be a challenge by looking at the technologies that are replacing programmers in the status quo we should be able to predict what jobs might disappear from the market additionally to predict how salaries and demand for specific skills might change we should consider the growing body of people learning to program as hannah pointed out public ignorance about computers is keeping wages high for those who can program and the public is becoming more computer savvy each year the fear of automation replacing jobs is neither new nor unfounded in any field and especially in technology market forces drive corporations toward automation and commodification gartner s hype cycles are one way of contextualizing this phenomenon as time goes on specific ideas and technologies push towards the plateau of productivity where they are eventually automated looking at history one must conclude that automation has the power to destroy specific job markets in diverse industries ranging from crop harvesting to automobile assembly technology advances have consistently replaced and augmented human labor to reduce costs a professor once put it this way in his compilers course take historical note of textile and steel industries do you want to build machines and tools or do you want to operate those machines in this metaphor the machine is a computer programming language this professor was really asking do you want to build websites using javascript or do you want to build the v engine that powers javascript the creation of websites is being automated by wordpress and others today v on the other hand has a growing body of competitors some of whom are solving open research questions languages will come and go how many fortran job openings are there but there will always be someone building the next language lucky for us programming language implementations are written with programming languages themselves being a machine operator in software puts you on the path to being a machine creator in a way which was not true of the steel mill workers of the past the growing number of languages interpreters and compilers shows us that every job destroying machine also brings with it new opportunities to improve those machines maintain those machines and so forth despite the growing body of jobs which no longer exist there has yet to be a moment in history where humanity has collectively said i guess there isn t any work left for us to do commodification is coming for us all not just software engineers throughout history human labor has consistently been replaced with non humans or augmented to require fewer and less skilled humans self driving cars and trucks are the flavor of the week in this grand human tradition if the cycle of creation and automation are a fact of life the natural question to answer next is which jobs and industries are at risk and which are not aws heroku and other similar hosting platforms have forever changed the role of the system administrator devops engineer internet businesses used to absolutely need their own server master someone who was well versed in linux someone who could configure a server with apache or nginx someone who could not only physically wire up the server the routers and all the other physical components but who could also configure the routing tables and all the software required to make that server accessible on the public web while there are definitely still people applying this skill set professionally aws is making some of those skills obsolete especially at the lower experience levels and on the physical side of things there are very lucrative roles within amazon and netflix and google for people with deep expertise in networking infrastructure but there is much less demand at the small to medium business scale business intelligence tools such as salesforce tableau and spotfire are also beginning to occupy spaces historically held by software engineers these systems have reduced the demand for in house database administrators but they have also increased the demand for sql as a general purpose skill they have decreased demand for in house reporting technology but increased demand for integration engineers who automate the flow of data from the business to the third party software platform s a field that was previously dominated by excel and spreadsheets is increasingly being pushed towards scripting languages like python or r and towards sql for data management some jobs have disappeared but demand for people who can write software has seen an increase overall data science is a fascinating example of commodification at a level closer to software scikit learn tensorflow and pytorch are all software libraries that make it easier for people to build machine learning applications without building the algorithms from scratch in fact it s possible to run a dataset through many different machine learning algorithms with many different parameter sets for those algorithms with little to no understanding of how those algorithms are actually implemented it s not necessarily wise to do this just possible you can bet that business intelligence companies will be trying to integrate these kinds of algorithms into their own tools over the next few years as well in many ways data science looks like web development did years ago a booming field where a little bit of knowledge can get you in the door due to a skills gap as web development bootcamps are closing and consolidating data science bootcamps are popping up in their place kaplan who bought the original web development bootcamp dev bootcamp and started a data science bootcamp metis has decided to close devbootcamp and keep metis running content management systems are among the most visible of the tools automating away the need for a software engineer squarespace and wordpress are among the most popular cms systems today these platforms are significantly reducing the value of people with a just a little bit of front end web development skill in fact the barriers for making a website and getting it online have come down so dramatically that people with zero programming experience are successfully launching websites every day those same people aren t making deeply interactive websites that serve billions of people but they absolutely do make websites for their own businesses that give customers the information they need a lovely landing page with information such as how to find the establishment and how to contact them is more than enough for a local restaurant bar or retail store if your business is not primarily an internet business it has never been easier to get a working site on the public web as a result the once thriving industry of web contractors who can quickly set up a simple website and get it online is becoming less lucrative finally it would border on hubris to ignore the physical aspect of computers in this context in the words of mike acton software is not the platform hardware is the platform software people would be wise to study at least a little computer architecture and electrical engineering a big shake up in hardware such as the arrival of consumer grade quantum computers would will change everything about professional software engineering quantum computers are still a ways off but the growing interest in gpus and the drive toward parallelization is an imminent shift cpu speeds have been stagnant for several years now and in that time a seemingly unquenchable thirst for machine learning and big data has emerged with more desire than ever to process large data sets openmp opencl go cuda and other parallel processing languages and frameworks will continue to become mainstream to be competitively fast in the near term future significant parallelization will be a requirement across the board not just in high performance niches like operating systems infrastructure and video games websites are ubiquitous the stack overflow survey reports that about of professional software engineers are working in an internet web services company the bureau of labor statistics expects growth in web development to continue much faster than average between and due to its visibility there has been a massive focus on solving the skills gap in this industry coding bootcamps teach web development almost exclusively and web development online courses have flooded udemy udacity coursera and similar marketplaces the combination of increasing automation throughout the web development technology stack and the influx of new entry level programmers with an explicit focus on web development has led some to predict a slide towards a blue collar market for software developers some have gone further suggesting that the push towards a blue collar market is a strategy architected by big tech firms others of course say we re headed for another bursting bubble change in demand for specific technologies is not news languages and frameworks are always rising and falling in technology web development in its current incarnation js is king will eventually go the way of web development of the early s remember flash what is new is that a lot of people are receiving an education explicitly and solely in the current trendy web development frameworks before you decide to label yourself a react developer remember there were people who once identified themselves as flash developers banking your career on a specific language framework or technology is a game of roulette of course it s quite difficult to predict what technologies will remain relevant but if you re going to go all in on something i suggest relying on the lindy effect and picking something like c that has already withstood the test of time the next generation will have a level of de facto tech literacy that generation x and even millennials do not have one outcome of this will be that using the next generation of cms tools will be a given these tools will get better and young workers will be better at using them this combination will definitely will bring down the value of low level it and web development skills as eager and skilled youngsters enter the job market high schools are catching on as well offering computer science and programming classes some well educated high school students will likely be entering the workforce as programming interns immediately upon graduation another big group of newcomers to programming are mbas and data analysts job listings which were once dominated by excel are starting to list sql as a nice to have and even requirement tools such as tableau spotfire salesforce and other web based metrics systems continue to replace the spreadsheet as the primary tool for report generation if this continues more data analysts will learn to use sql directly simply because it is easier than exporting the data into a spreadsheet people looking to climb the ranks and out perform their peers in these roles are taking online courses to learn about databases and statistical programming languages with these new skills they can begin to position themselves as data scientists by learning a combination of machine learning and statistical libraries look at metis curriculum as a prime example of this path finally the number of people earning computer science and software engineering degrees continues to climb purdue for example reports that applications to their cs program have doubled over five years cornell reports a similar explosion of cs graduates this trend isn t surprising given the growth and ubiquity of software it s hard for young people to imagine that computers will play a smaller role in our futures so why not study something that s going to give you job security a common argument in the industry nowadays is around the idea that the education you receive in a four year computer science program is mostly unnecessary cruft i have heard this argument repeatedly in the halls of bootcamps web development shops and online from big names in the field such as this piece by eric elliott the opposition view is popular as well with some going so far as saying all programmers should earn a master s degree like eric elliott i think it s good that there are more options than ever to break into programming and a year degree might not be the best option for many simultaneously i agree with william bain that the foundational skills which apply across programming disciplines are crucial for career longevity and that it is still hard to find that information outside of university courses i ve written previously about what skills i think aspiring engineers should learn as a foundation of a long career and joined bradfield in order to help share this knowledge coding schools of many shapes and sizes are becoming ubiquitous and for good reasons there is quite a lot you can learn about programming without getting into the minutia of big o notation obscure data structures and algorithmic trivia however while it s true that fresh graduates from stanford are competing for some jobs with fresh graduates from hack reactor it s only true in one or two sub industries code school and bootcamp graduates are not yet applying to work on embedded systems cryptography security robotics network infrastructure or ai research and development yet these fields like web development are growing quickly some programming related skills have already started their transition from rare skill to baseline expectation conversely the engineering that goes into creating beastly engines like aws is anything but common the big companies driving technology forward amazon google facebook nvidia space x and so on are typically not looking for people with a basic understanding of javascript aws serves billions of users per day to support that kind of load an aws infrastructure engineer needs a deep knowledge of network protocols computer architecture and several years of relevant experience as with any discipline there are amateurs and artisans these prestigious firms are solving research problems and building systems that are truly pushing against the boundaries of what is possible yet they still struggle to fill open roles even while basic programming skills are increasingly common people who can write algorithms to predict changes in genetic sequences that will yield a desired result are going to be highly valuable in the future people who can program satellites spacecraft and automate machinery will continue to be highly valued these are not fields that lend themselves as readily to a month intensive program as front end web development at least not without significant prior experience because computer science starts with the word computer it is assumed that young people will all have an innate understanding of it by unfortunately the ubiquity of computers has not created a new generation of people who de facto understand mathematics computer science network infrastructure electrical engineering and so on computer literacy is not the same as the study of computation despite mathematics having existed since the dawn of time there is still a relatively small portion of the population with strong statistical literacy and computer science is similarly old euclid invented several algorithms one of which is used every time you make an https request the fact that we use https every time we login to a website does not automatically imbue anyone with a knowledge of how those protocols work more established professional fields often have a bimodal wage distribution a relatively small number of practitioners make quite a lot of money and the majority of them earn a good wage but do not find themselves in the top of earners the national association for law placement collects data that can be used to visualize this phenomenon in stark clarity a huge share of law graduates make between and a good wage but hardly the salary we associate with a top professional we tend to think that all law graduates are on track to becoming partners at a law firm when really there are many paths paralegal clerk public defender judge legal services for businesses contract writing and so on computer science graduates also have many options for their professional practice from web development to embedded systems as a basic level of programming literacy continues to become an expectation rather than a nice to have i suspect a similar distribution will emerge in programming jobs while there will always be a cohort of programmers making a lot of money to push on the edges of technology there will be a growing body of middle class programmers powering the new computer centric economy the average salary for web developers will surely decrease over time that said i suspect that the number of jobs for programmers in general will only continue to grow as worker supply begins to meet demand hopefully we will see a healthy boom in a variety of middle class programming jobs there will also continue to be a top professional salary available for those programmers who are redefining what is possible regardless of which cohort of programmers you re in a career in technology means continuing your education throughout your life if you want to stay in the second cohort of programmers you may want to invest in learning how to create the machines rather than simply operate them from a quick cheer to a standing ovation clap to show how much you enjoyed this story a curious human on a quest to watch the world learn
Arvind N,9500,8,https://towardsdatascience.com/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153?source=tag_archive---------3----------------,Thoughts after taking the Deeplearning.ai courses – Towards Data Science,update feb nd when this blog post was written only courses had been released all courses in this specialization are now out i will have a follow up blog post soon between a full time job and a toddler at home i spend my spare time learning about the ideas in cognitive science ai once in a while a great paper video course comes out and you re instantly hooked andrew ng s new deeplearning ai course is like that shane carruth or rajnikanth movie that one yearns for naturally as soon as the course was released on coursera i registered and spent the past evenings binge watching the lectures working through quizzes and programming assignments dl practitioners and ml engineers typically spend most days working at an abstract keras or tensorflow level but it s nice to take a break once in a while to get down to the nuts and bolts of learning algorithms and actually do back propagation by hand it is both fun and incredibly useful andrew ng s new adventure is a bottom up approach to teaching neural networks powerful non linearity learning algorithms at a beginner mid level in classic ng style the course is delivered through a carefully chosen curriculum neatly timed videos and precisely positioned information nuggets andrew picks up from where his classic ml course left off and introduces the idea of neural networks using a single neuron logistic regression and slowly adding complexity more neurons and layers by the end of the weeks course a student is introduced to all the core ideas required to build a dense neural network such as cost loss functions learning iteratively using gradient descent and vectorized parallel python numpy implementations andrew patiently explains the requisite math and programming concepts in a carefully planned order and a well regulated pace suitable for learners who could be rusty in math coding lectures are delivered using presentation slides on which andrew writes using digital pens it felt like an effective way to get the listener to focus i felt comfortable watching videos at x or x speed quizzes are placed at the end of each lecture sections and are in the multiple choice question format if you watch the videos once you should be able to quickly answer all the quiz questions you can attempt quizzes multiple times and the system is designed to keep your highest score programming assignments are done via jupyter notebooks powerful browser based applications assignments have a nice guided sequential structure and you are not required to write more than lines of code in each section if you understand the concepts like vectorization intuitively you can complete most programming sections with just line of code after the assignment is coded it takes button click to submit your code to the automated grading system which returns your score in a few minutes some assignments have time restrictions say three attempts in hours etc jupyter notebooks are well designed and work without any issues instructions are precise and it feels like a polished product anyone interested in understanding what neural networks are how they work how to build them and the tools available to bring your ideas to life if your math is rusty there is no need to worry andrew explains all the required calculus and provides derivatives at every occasion so that you can focus on building the network and concentrate on implementing your ideas in code if your programming is rusty there is a nice coding assignment to teach you numpy but i recommend learning python first on codecademy let me explain this with an analogy assume you are trying to learn how to drive a car jeremy s fast ai course puts you in the drivers seat from the get go he teaches you to move the steering wheel press the brake accelerator etc then he slowly explains more details about how the car works why rotating the wheel makes the car turn why pressing the brake pedal makes you slow down and stop etc he keeps getting deeper into the inner workings of the car and by the end of the course you know how the internal combustion engine works how the fuel tank is designed etc the goal of the course is to get you driving you can choose to stop at any point after you can drive reasonably well there is no need to learn how to build repair the car andrew s dl course does all of this but in the complete opposite order he teaches you about internal combustion engine first he keeps adding layers of abstraction and by the end of the course you are driving like an f racer the fast ai course mainly teaches you the art of driving while andrew s course primarily teaches you the engineering behind the car if you have not done any machine learning before this don t take this course first the best starting point is andrew s original ml course on coursera after you complete that course please try to complete part of jeremy howard s excellent deep learning course jeremy teaches deep learning top down which is essential for absolute beginners once you are comfortable creating deep neural networks it makes sense to take this new deeplearning ai course specialization which fills up any gaps in your understanding of the underlying details and concepts andrew stresses on the engineering aspects of deep learning and provides plenty of practical tips to save time and money the third course in the dl specialization felt incredibly useful for my role as an architect leading engineering teams jargon is handled well andrew explains that an empirical process trial error he is brutally honest about the reality of designing and training deep nets at some point i felt he might have as well just called deep learning as glorified curve fitting squashes all hype around dl and ai andrew makes restrained careful comments about proliferation of ai hype in the mainstream media and by the end of the course it is pretty clear that dl is nothing like the terminator wonderful boilerplate code that just works out of the box excellent course structure nice consistent and useful notation andrew strives to establish a fresh nomenclature for neural nets and i feel he could be quite successful in this endeavor style of teaching that is unique to andrew and carries over from ml i could feel the same excitement i felt in when i took his original ml course the interviews with deep learning heroes are refreshing it is motivating and fun to hear personal stories and anecdotes i wish that he d said concretely more often good tools are important and will help you accelerate your learning pace i bought a digital pen after seeing andrew teach with one it helped me work more efficiently there is a psychological reason why i recommend the fast ai course before this one once you find your passion you can learn uninhibited you just get that dopamine rush each time you score full points don t be scared by dl jargon hyperparameters settings architecture topology style etc or the math symbols if you take a leap of faith and pay attention to the lectures andrew shows why the symbols and notation are actually quite useful they will soon become your tools of choice and you will wield them with style thanks for reading and best wishes update thanks for the overwhelmingly positive response many people are asking me to explain gradient descent and the differential calculus i hope this helps from a quick cheer to a standing ovation clap to show how much you enjoyed this story interested in strong ai sharing concepts ideas and codes
Berit Anderson,1600,20,https://medium.com/join-scout/the-rise-of-the-weaponized-ai-propaganda-machine-86dac61668b?source=tag_archive---------4----------------,The Rise of the Weaponized AI Propaganda Machine – Scout: Science Fiction + Journalism – Medium,by berit anderson and brett horvath this piece was originally published at scout ai this is a propaganda machine it s targeting people individually to recruit them to an idea it s a level of social engineering that i ve never seen before they re capturing people and then keeping them on an emotional leash and never letting them go said professor jonathan albright albright an assistant professor and data scientist at elon university started digging into fake news sites after donald trump was elected president through extensive research and interviews with albright and other key experts in the field including samuel woolley head of research at oxford university s computational propaganda project and martin moore director of the centre for the study of media communication and power at kings college it became clear to scout that this phenomenon was about much more than just a few fake news stories it was a piece of a much bigger and darker puzzle a weaponized ai propaganda machine being used to manipulate our opinions and behavior to advance specific political agendas by leveraging automated emotional manipulation alongside swarms of bots facebook dark posts a b testing and fake news networks a company called cambridge analytica has activated an invisible machine that preys on the personalities of individual voters to create large shifts in public opinion many of these technologies have been used individually to some effect before but together they make up a nearly impenetrable voter manipulation machine that is quickly becoming the new deciding factor in elections around the world most recently analytica helped elect u s president donald trump secured a win for the brexit leave campaign and led ted cruz s campaign surge shepherding him from the back of the gop primary pack to the front the company is owned and controlled by conservative and alt right interests that are also deeply entwined in the trump administration the mercer family is both a major owner of cambridge analytica and one of trump s biggest donors steve bannon in addition to acting as trump s chief strategist and a member of the white house security council is a cambridge analytica board member until recently analytica s cto was the acting cto at the republican national convention presumably because of its alliances analytica has declined to work on any democratic campaigns at least in the u s it is however in final talks to help trump manage public opinion around his presidential policies and to expand sales for the trump organization cambridge analytica is now expanding aggressively into u s commercial markets and is also meeting with right wing parties and governments in europe asia and latin america cambridge analytica isn t the only company that could pull this off but it is the most powerful right now understanding cambridge analytica and the bigger ai propaganda machine is essential for anyone who wants to understand modern political power build a movement or keep from being manipulated the weaponized ai propaganda machine it represents has become the new prerequisite for political success in a world of polarization isolation trolls and dark posts there s been a wave of reporting on cambridge analytica itself and solid coverage of individual aspects of the machine bots fake news microtargeting but none so far that we have seen that portrays the intense collective power of these technologies or the frightening level of influence they re likely to have on future elections in the past political messaging and propaganda battles were arms races to weaponize narrative through new mediums waged in print on the radio and on tv this new wave has brought the world something exponentially more insidious personalized adaptive and ultimately addictive propaganda silicon valley spent the last ten years building platforms whose natural end state is digital addiction in trump and his allies hijacked them we have entered a new political age at scout we believe that the future of constructive civic dialogue and free and open elections depends on our ability to understand and anticipate it welcome to the age of weaponized ai propaganda any company can aggregate and purchase big data but cambridge analytica has developed a model to translate that data into a personality profile used to predict then ultimately change your behavior that model itself was developed by paying a cambridge psychology professor to copy the groundbreaking original research of his colleague through questionable methods that violated amazon s terms of service based on its origins cambridge analytica appears ready to capture and buy whatever data it needs to accomplish its ends in dr michal kosinski then a phd candidate at the university of cambridge s psychometrics center released a groundbreaking study announcing a new model he and his colleagues had spent years developing by correlating subjects facebook likes with their ocean scores a standard bearing personality questionnaire used by psychologists the team was able to identify an individual s gender sexuality political beliefs and personality traits based only on what they had liked on facebook according to zurich s das magazine which profiled kosinski in late with a mere ten likes as input his model could appraise a person s character better than an average coworker with seventy it could know a subject better than a friend with likes better than their parents with likes kosinski s machine could predict a subject s behavior better than their partner with even more likes it could exceed what a person thinks they know about themselves not long afterward kosinski was approached by aleksandr kogan a fellow cambridge professor in the psychology department about licensing his model to scl elections a company that claimed its specialty lay in manipulating elections the offer would have meant a significant payout for kosinki s lab still he declined worried about the firm s intentions and the downstream effects it could have it had taken kosinski and his colleagues years to develop that model but with his methods and findings now out in the world there was little to stop scl elections from replicating them it would seem they did just that according to a guardian investigation in early just a few months after kosinski declined their offer scl partnered with kogan instead as a part of their relationship kogan paid amazon mechanical turk workers each to take the ocean quiz there was just one catch to take the quiz users were required to provide access to all of their facebook data they were told the data would be used for research the job was reported to amazon for violating the platform s terms of service what many of the turks likely didn t realize according to documents reviewed by the guardian kogan also captured the same data for each person s unwitting friends the data gathered from kogan s study went on to birth cambridge analytica which spun out of scl elections soon after the name metaphorically at least was a nod to kogan s work and a dig at kosinski but that early trove of user data was just the beginning just the seed analytica needed to build its own model for analyzing users personalities without having to rely on the lengthy ocean test after a successful proof of concept and backed by wealthy conservative investors analytica went on a data shopping spree for the ages snapping up data about your shopping habits land ownership where you attend church what stores you visit what magazines you subscribe to all of which is for sale from a range of data brokers and third party organizations selling information about you analytica aggregated this data with voter roles publicly available online data including facebook likes and put it all into its predictive personality model nix likes to boast that analytica s personality model has allowed it to create a personality profile for every adult in the u s million of them each with up to data points and those profiles are being continually updated and improved the more data you spew out online albright also believes that your facebook and twitter posts are being collected and integrated back into cambridge analytica s personality profiles twitter and also facebook are being used to collect a lot of responsive data because people are impassioned they reply they retweet but they also include basically their entire argument and their entire background on this topic he explains collecting massive quantities of data about voters personalities might seem unsettling but it s actually not what sets cambridge analytica apart for analytica and other companies like them it s what they do with that data that really matters your behavior is driven by your personality and actually the more you can understand about people s personality as psychological drivers the more you can actually start to really tap in to why and how they make their decisions nix explained to bloomberg s sasha issenburg we call this behavioral microtargeting and this is really our secret sauce if you like this is what we re bringing to america using those dossiers or psychographic profiles as analytica calls them cambridge analytica not only identifies which voters are most likely to swing for their causes or candidates they use that information to predict and then change their future behavior as vice reported recently kosinski and a colleague are now working on a new set of research yet to be published that addresses the effectiveness of these methods their early findings using personality targeting facebook posts can attract up to percent more clicks and more conversions scout reached out to cambridge analytica with a detailed list of questions about their communications tactics but the company declined to answer any questions or to comment on any of their tactics but researchers across the technology and media ecosystem who have been following cambridge analytica s political messaging activities have unearthed an expansive adaptive online network that automates the manipulation of voters at a scale never before seen in political messaging they the trump campaign were using different variants of ad every day that were continuously measuring responses and then adapting and evolving based on that response martin moore director of kings college s centre for the study of media communication and power told the guardian in early december it s all done completely opaquely and they can spend as much money as they like on particular locations because you can focus on a five mile radius where traditional pollsters might ask a person outright how they plan to vote analytica relies not on what they say but what they do tracking their online movements and interests and serving up multivariate ads designed to change a person s behavior by preying on individual personality traits for example nix wrote in an op ed last year about analytica s work on the cruz campaign our issues model identified that there was a small pocket of voters in iowa who felt strongly that citizens should be required by law to show photo id at polling stations leveraging our other data models we were able to advise the campaign on how to approach this issue with specific individuals based on their unique profiles in order to use this relatively niche issue as a political pressure point to motivate them to go out and vote for cruz for people in the temperamental personality group who tend to dislike commitment messaging on the issue should take the line that showing your id to vote is as easy as buying a case of beer whereas the right message for people in the stoic traditionalist group who have strongly held conventional views is that showing your id in order to vote is simply part of the privilege of living in a democracy for analytica the feedback is instant and the response automated did this specific swing voter in pennsylvania click on the ad attacking clinton s negligence over her email server yes serve her more content that emphasizes failures of personal responsibility no the automated script will try a different headline perhaps one that plays on a different personality trait say the voter s tendency to be agreeable toward authority figures perhaps top intelligence officials agree clinton s emails jeopardized national security much of this is done through facebook dark posts which are only visible to those being targeted based on users response to these posts cambridge analytica was able to identify which of trump s messages were resonating and where that information was also used to shape trump s campaign travel schedule if percent of targeted voters in kent county mich clicked on one of three articles about bringing back jobs schedule a trump rally in grand rapids that focuses on economic recovery political analysts in the clinton campaign who were basing their tactics on traditional polling methods laughed when trump scheduled campaign events in the so called blue wall a group of states that includes michigan pennsylvania and wisconsin and has traditionally fallen to democrats but cambridge analytica saw they had an opening based on measured engagement with their facebook posts it was the small margins in michigan pennsylvania and wisconsin that won trump the election dark posts were also used to depress voter turnout among key groups of democratic voters in this election dark posts were used to try to suppress the african american vote wrote journalist and open society fellow mckenzie funk in a new york times editorial according to bloomberg the trump campaign sent ads reminding certain selected black voters of hillary clinton s infamous super predator line it targeted miami s little haiti neighborhood with messages about the clinton foundation s troubles in haiti after the earthquake because dark posts are only visible to the targeted users there s no way for anyone outside of analytica or the trump campaign to track the content of these ads in this case there was no sec oversight no public scrutiny of trump s attack ads just the rapid eye movement of millions of individual users scanning their facebook feeds in the weeks leading up to a final vote a campaign could launch a million dark post campaign targeting just a few million voters in swing districts and no one would know this may be where future black swan election upsets are born these companies moore says have found a way of transgressing years of legislation that we ve developed to make elections fair and open meanwhile surprised by the results of the presidential race albright started looking into the fake news problem as a part of his research albright scraped fake news sites to determine how exactly they were all connected to each other and the mainstream news ecosystem what he found was unprecedented a network of pages and million hyperlinks the sites in the fake news and hyper biased mcm network albright writes have a very small node size this means they are linking out heavily to mainstream media social networks and informational resources most of which are in the center of the network but not many sites in their peer group are sending links back these sites aren t owned or operated by any one individual entity he says but together they have been able to game search engine optimization increasing the visibility of fake and biased news anytime someone googles an election related term online trump clinton jews muslims abortion obamacare this network albright wrote in a post exploring his findings is triggered on demand to spread false hyper biased and politically loaded information even more shocking to him though was that this network of fake news creates a powerful infrastructure for companies like cambridge analytica to track voters and refine their personality targeting models i scraped the trackers on these sites and i was absolutely dumbfounded every time someone likes one of these posts on facebook or visits one of these websites the scripts are then following you around the web and this enables data mining and influencing companies like cambridge analytica to precisely target individuals to follow them around the web and to send them highly personalised political messages the web of fake and biased news that albright uncovered created a propaganda wave that cambridge analytica could ride and then amplify the more fake news that users engage with the more addictive analytica s personality engagement algorithms can become voter clicked on a fake story about hillary s sex trafficking ring let s get her to engage with more stories about hillary s supposed history of murder and sex trafficking the synergy between fake content networks automated message testing and personality profiling will rapidly spread to other digital mediums albright s most recent research focuses on an artificial intelligence that automatically creates youtube videos about news and current events the ai which reacts to trending topics on facebook and twitter pairs images and subtitles with a computer generated voiceover it spooled out nearly videos through different channels in just a few days given its rapid development the technology community needs to anticipate how ai propaganda will soon be used for emotional manipulation in mobile messaging virtual reality and augmented reality if fake news created the scaffolding for this new automated political propaganda machine bots or fake social media profiles have become its foot soldiers an army of political robots used to control conversations on social media and silence and intimidate journalists and others who might undermine their messaging samuel woolley director of research at the university of oxford s computational propaganda project and a fellow at google s jigsaw project has dedicated his career to studying the role of bots in online political organizing who creates them how they re used and to what end research by woolley and his oxford based team in the lead up to the election found that pro trump political messaging relied heavily on bots to spread fake news and discredit hillary clinton by election day trump s bots outnumbered hers the use of automated accounts was deliberate and strategic throughout the election most clearly with pro trump campaigners and programmers who carefully adjusted the timing of content production during the debates strategically colonized pro clinton hashtags and then disabled activities after election day the study by woolley s team reported woolley believes it s likely that cambridge analytica was responsible for subcontracting the creation of those trump bots though he says he doesn t have direct proof still if anyone outside of the trump campaign is qualified to speculate about who created those bots it would be woolley led by dr philip howard the team s principal investigator woolley and his colleagues have been tracking the use of bots in political organizing since that s when howard buried deep in research about the role twitter played in the arab spring first noticed thousands of bots coopting hashtags used by protesters curious he and his team began reaching out to hackers botmakers and political campaigns getting to know them and trying to understand their work and motivations eventually those creators would come to make up an informal network of nearly informants that have kept howard and his colleagues in the know about these bots over the last few years before long howard and his team were getting the heads up about bot propaganda campaigns from the creators themselves as more and more major international political figures began using botnets as just another tool in their campaigns howard woolley and the rest of their team studied the action unfolding the world these informants revealed is an international network of governments consultancies often with owners or top management just one degree away from official government actors and individuals who build and maintain massive networks of bots to amplify the messages of political actors spread messages counter to those of their opponents and silence those whose views or ideas might threaten those same political actors the chinese iranian and russian governments employ their own social media experts and pay small amounts of money to large numbers of people to generate pro government messages howard and his coauthors wrote in a research paper about the use of bots in the venezuelan election depending on which of those three categories bot creators fall into government consultancy or individual they re just as likely to be motivated by political beliefs as they are the opportunity to auction off their networks of digital influence to the highest bidder not all bots are created equal the average run of the mill twitter bot is literally a robot often programmed to retweet specific accounts to help popularize specific ideas or viewpoints they also frequently respond automatically to twitter users who use certain keywords or hashtags often with pre written slurs insults or threats high end bots on the other hand are more analog operated by real people they assume fake identities with distinct personalities and their responses to other users online are specific intended to change their opinions or those of their followers by attacking their viewpoints they have online friends and followers they re also far less likely to be discovered and their accounts deactivated by facebook or twitter working on their own woolley estimates an individual could build and maintain up to of these boutique twitter bots on facebook which he says is more effective at identifying and shutting down fake accounts an individual could manage as a result these high quality botnets are often used for multiple political campaigns during the brexit referendum the oxford team watched as one network of bots previously used to influence the conversation around the israeli palestinian conflict was reactivated to fight for the leave campaign individual profiles were updated to reflect the new debate their personal taglines changed to ally with their new allegiances and away they went russia s bot army has been the subject of particular scrutiny since a cia special report revealed that russia had been working to influence the election in trump s favor recently reporter comedian samantha bee traveled to moscow to interview two paid russian troll operators clad in black ski masks to obscure their identities the two talked with bee about how and why they were using their accounts during the u s election they told bee that they pose as americans online and target sites like the wall street journal the new york post the washington post facebook and twitter their goal they said is to piss off other social media users change their opinions and silence their opponents or to put it in the words of russian troll when your opponent just shut up the u s election is over but the weaponized ai propaganda machine is just warming up and while each of its components would be worrying on its own together they represent the arrival of a new era in political messaging a steel wall between campaign winners and losers that can only be mounted by gathering more data creating better personality analyses rapid development of engagement ai and hiring more trolls at the moment trump and cambridge analytica are lapping their opponents the more data they gather about individuals the more analytica and by extension trump s presidency will benefit from the network effects of their work and the harder it will become to counter or fight back against their messaging in the court of public opinion each tweet that echoes forth from the realdonaldtrump and potus accounts announcing and defending the administration s moves is met with a chorus of protest and argument but even that negative engagement becomes a valuable asset for the trump administration because every impulsive tweet can be treated like a psychographic experiment trump s first few weeks in office may have seemed bumbling but they represent a clear signal of what lies ahead for trump s presidency an executive order designed to enrage and distract his opponents as he and bannon move to strip power from the judicial branch install bannon himself on the national security council and issues a series of unconstitutional gag orders to federal agencies cambridge analytica may be slated to secure more federal contracts and is likely about to begin managing white house digital communications for the rest of the trump administration what new predictive personality targeting becomes possible with potential access to data on u s voters from the irs department of homeland security or the nsa lenin wanted to destroy the state and that s my goal too i want to bring everything crashing down and destroy all of today s establishment bannon said in we know that steve bannon subscribes to a theory of history where a messianic grey warrior consolidates power and remakes the global order bolstered by the success of brexit and the trump victory breitbart of which bannon was executive chair until trump s election and cambridge analytica which bannon sits on the board of are now bringing fake news and automated propaganda to support far right parties in at least germany france hungary and india as well as parts of south america never has such a radical international political movement had the precision and power of this kind of propaganda technology whether or not leaders engineers designers and investors in the technology community respond to this threat will shape major aspects of global politics for the foreseeable future the future of politics will not be a war of candidates or even cash on hand and it s not even about big data as some have argued everyone will have access to big data as hillary did in the election from now on the distinguishing factor between those who win elections and those who lose them will be how a candidate uses that data to refine their machine learning algorithms and automated engagement tactics elections in and won t be a contest of ideas but a battle of automated behavior change the fight for the future will be a proxy war of machine learning it will be waged online in secret and with the unwitting help of all of you anyone who wants to effect change needs to understand this new reality it s only by understanding this and by building better automated engagement systems that amplify genuine human passion rather than manipulate it that other candidates and causes around the globe will be able to compete implication public sentiment turns into high frequency trading thanks to stock trading algorithms large portions of public stock and commodity markets no longer resemble a human system and some would argue no longer serve their purpose as a signal of value instead they re a battleground for high frequency trading algorithms attempting to influence price or find nano leverage in price position in the near future we may see a similar process unfold in our public debates instead of battling press conferences and opinion articles public opinion about companies and politicians may turn into multi billion dollar battles between competing algorithms each deployed to sway public sentiment stock trading algorithms already exist that analyze millions of tweets and online posts in real time and make trades in a matter of milliseconds based on changes in public sentiment algorithmic trading and algorithmic public opinion are already connected it s likely they will continue to converge implication personalized automated propaganda that adapts to your weaknesses what if president trump s re election campaign didn t just have the best political messaging but million algorithmic versions of their political message all updating in real time personalized to precisely fit the worldview and attack the insecurities of their targets instead of having to deal with misleading politicians we may soon witness a cambrian explosion of pathologically lying political and corporate bots that constantly improve at manipulating us implication not just a bubble but trapped in your own ideological matrix imagine that in you found out that your favorite politics page or group on facebook didn t actually have any other human members but was filled with dozens or hundreds of bots that made you feel at home and your opinions validated is it possible that you might never find out correction an earlier version of this story mistakenly referred to steve bannon as the owner of breitbart news until trump s election bannon served as the executive chair of breitbart a position in which it is common to assume ownership through stock holdings this story has been updated to reflect that from a quick cheer to a standing ovation clap to show how much you enjoyed this story ceo co founder join scout the social implications of technology
Slav Ivanov,4400,10,https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607?source=tag_archive---------5----------------,37 Reasons why your Neural Network is not working – Slav,the network had been training for the last hours it all looked good the gradients were flowing and the loss was decreasing but then came the predictions all zeroes all background nothing detected what did i do wrong i asked my computer who didn t answer where do you start checking if your model is outputting garbage for example predicting the mean of all outputs or it has really poor accuracy a network might not be training for a number of reasons over the course of many debugging sessions i would often find myself doing the same checks i ve compiled my experience along with the best ideas around in this handy list i hope they would be of use to you too a lot of things can go wrong but some of them are more likely to be broken than others i usually start with this short list as an emergency first response if the steps above don t do it start going down the following big list and verify things one by one check if the input data you are feeding the network makes sense for example i ve more than once mixed the width and the height of an image sometimes i would feed all zeroes by mistake or i would use the same batch over and over so print display a couple of batches of input and target output and make sure they are ok try passing random numbers instead of actual data and see if the error behaves the same way if it does it s a sure sign that your net is turning data into garbage at some point try debugging layer by layer op by op and see where things go wrong your data might be fine but the code that passes the input to the net might be broken print the input of the first layer before any operations and check it check if a few input samples have the correct labels also make sure shuffling input samples works the same way for output labels maybe the non random part of the relationship between the input and output is too small compared to the random part one could argue that stock prices are like this i e the input are not sufficiently related to the output there isn t an universal way to detect this as it depends on the nature of the data this happened to me once when i scraped an image dataset off a food site there were so many bad labels that the network couldn t learn check a bunch of input samples manually and see if labels seem off the cutoff point is up for debate as this paper got above accuracy on mnist using corrupted labels if your dataset hasn t been shuffled and has a particular order to it ordered by label this could negatively impact the learning shuffle your dataset to avoid this make sure you are shuffling input and labels together are there a class a images for every class b image then you might need to balance your loss function or try other class imbalance approaches if you are training a net from scratch i e not finetuning you probably need lots of data for image classification people say you need a images per class or more this can happen in a sorted dataset i e the first k samples contain the same class easily fixable by shuffling the dataset this paper points out that having a very large batch can reduce the generalization ability of the model thanks to hengcherkeng for this one did you standardize your input to have zero mean and unit variance augmentation has a regularizing effect too much of this combined with other forms of regularization weight l dropout etc can cause the net to underfit if you are using a pretrained model make sure you are using the same normalization and preprocessing as the model was when training for example should an image pixel be in the range or cs n points out a common pitfall also check for different preprocessing in each sample or batch this will help with finding where the issue is for example if the target output is an object class and coordinates try limiting the prediction to object class only again from the excellent cs n initialize with small parameters without regularization for example if we have classes at chance means we will get the correct class of the time and the softmax loss is the negative log probability of the correct class so ln after this try increasing the regularization strength which should increase the loss if you implemented your own loss function check it for bugs and add unit tests often my loss would be slightly incorrect and hurt the performance of the network in a subtle way if you are using a loss function provided by your framework make sure you are passing to it what it expects for example in pytorch i would mix up the nllloss and crossentropyloss as the former requires a softmax input and the latter doesn t if your loss is composed of several smaller loss functions make sure their magnitude relative to each is correct this might involve testing different combinations of loss weights sometimes the loss is not the best predictor of whether your network is training properly if you can use other metrics like accuracy did you implement any of the layers in the network yourself check and double check to make sure they are working as intended check if you unintentionally disabled gradient updates for some layers variables that should be learnable maybe the expressive power of your network is not enough to capture the target function try adding more layers or more hidden units in fully connected layers if your input looks like k h w it s easy to miss errors related to wrong dimensions use weird numbers for input dimensions for example different prime numbers for each dimension and check how they propagate through the network if you implemented gradient descent by hand gradient checking makes sure that your backpropagation works like it should more info overfit a small subset of the data and make sure it works for example train with just or examples and see if your network can learn to differentiate these move on to more samples per class if unsure use xavier or he initialization also your initialization might be leading you to a bad local minimum so try a different initialization and see if it helps maybe you using a particularly bad set of hyperparameters if feasible try a grid search too much regularization can cause the network to underfit badly reduce regularization such as dropout batch norm weight bias l regularization etc in the excellent practical deep learning for coders course jeremy howard advises getting rid of underfitting first this means you overfit the training data sufficiently and only then addressing overfitting maybe your network needs more time to train before it starts making meaningful predictions if your loss is steadily decreasing let it train some more some frameworks have layers like batch norm dropout and other layers behave differently during training and testing switching to the appropriate mode might help your network to predict properly your choice of optimizer shouldn t prevent your network from training unless you have selected particularly bad hyperparameters however the proper optimizer for a task can be helpful in getting the most training in the shortest amount of time the paper which describes the algorithm you are using should specify the optimizer if not i tend to use adam or plain sgd with momentum check this excellent post by sebastian ruder to learn more about gradient descent optimizers a low learning rate will cause your model to converge very slowly a high learning rate will quickly decrease the loss in the beginning but might have a hard time finding a good solution play around with your current learning rate by multiplying it by or getting a nan non a number is a much bigger issue when training rnns from what i hear some approaches to fix it did i miss anything is anything wrong let me know by leaving a reply below from a quick cheer to a standing ovation clap to show how much you enjoyed this story entrepreneur hacker machine learning deep learning and other types of learning
Keval Patel,833,7,https://becominghuman.ai/turn-your-raspberry-pi-into-homemade-google-home-9e29ad220075?source=tag_archive---------6----------------,Turn your Raspberry Pi into homemade Google Home – Becoming Human: Artificial Intelligence Magazine,google home is a beautiful device with built in google assistant a state of the art digital personal assistant by google which you can place anywhere at your home and it will do some amazing things for you it will save your reminders shopping lists notes and most importantly answers your questions and queries based on the context of the conversations in this article you are going to learn to turn your raspberry pi into homemade google home device which is so let s get started once you have all these things login to raspbian desktop and go to the following steps one by one as you can see your usb device is attached to card and the device id is raspberry pi recognizes card as the internal sound card which is bcm and other external sound cards as external sound cards this will set your external mic see pcm mic as the audio capture device see in pcm default and your inbuilt sound card card as the speaker device this will create python environment as the google assistant library runs on python x only in your raspberry pi and install required dependencies if instead it displays invalidgranterror then an invalid code was entered try again you can run google assistant init sh to initiate the google assistant any time autostart with pixel desktop on boot autostart with cli on boot you can do many daily stuff with your google home if you want to perform your custom tasks like turning off the light opening the door you can do it with integrating google actions in your google assistant if you have any trouble with starting the google assistant leave a comment below i will try to resolve them if you liked the article click the below so more people can see it also you can follow me on medium or on my blog so you get updates regarding my future articles from a quick cheer to a standing ovation clap to show how much you enjoyed this story www kevalpatel com android developer machine learner gopher open source contributor latest news info and tutorials on artificial intelligence machine learning deep learning big data and what it means for humanity
Eduard Tyantov,5400,19,https://blog.statsbot.co/deep-learning-achievements-4c563e034257?source=tag_archive---------7----------------,Deep Learning Achievements Over the Past Year – Stats and Bots,at statsbot we re constantly reviewing the deep learning achievements to improve our models and product around christmas time our team decided to take stock of the recent achievements in deep learning over the past year and a bit longer we translated the article by a data scientist ed tyantov to tell you about the most significant developments that can affect our future almost a year ago google announced the launch of a new model for google translate the company described in detail the network architecture recurrent neural network rnn the key outcome closing down the gap with humans in accuracy of the translation by estimated by people on a point scale it is difficult to reproduce good results with this model without the huge dataset that google has you probably heard the silly news that facebook turned off its chatbot which went out of control and made up its own language this chatbot was created by the company for negotiations its purpose is to conduct text negotiations with another agent and reach a deal how to divide items books hats etc by two each agent has his own goal in the negotiations that the other does not know about it s impossible to leave the negotiations without a deal for training they collected a dataset of human negotiations and trained a supervised recurrent network then they took a reinforcement learning trained agent and trained it to talk with itself setting a limit the similarity of the language to human the bot has learned one of the real negotiation strategies showing a fake interest in certain aspects of the deal only to give up on them later and benefit from its real goals it has been the first attempt to create such an interactive bot and it was quite successful full story is in this article and the code is publicly available certainly the news that the bot has allegedly invented a language was inflated from scratch when training in negotiations with the same agent they disabled the restriction of the similarity of the text to human and the algorithm modified the language of interaction nothing unusual over the past year recurrent networks have been actively developed and used in many tasks and applications the architecture of rnns has become much more complicated but in some areas similar results were achieved by simple feedforward networks dssm for example google has reached the same quality as with lstm previously for its mail feature smart reply in addition yandex launched a new search engine based on such networks employees of deepmind reported in their article about generating audio briefly researchers made an autoregressive full convolution wavenet model based on previous approaches to image generation pixelrnn and pixelcnn the network was trained end to end text for the input audio for the output the researches got an excellent result as the difference compared to human has been reduced by the main disadvantage of the network is a low productivity as because of the autoregression sounds are generated sequentially and it takes about minutes to create one second of audio look at sorry hear this example if you remove the dependence of the network on the input text and leave only the dependence on the previously generated phoneme then the network will generate phonemes similar to the human language but they will be meaningless hear the example of the generated voice this same model can be applied not only to speech but also for example to creating music imagine audio generated by the model which was taught using the dataset of a piano game again without any dependence on the input data read a full version of deepmind research if you re interested lip reading is another deep learning achievement and victory over humans google deepmind in collaboration with oxford university reported in the article lip reading sentences in the wild on how their model which had been trained on a television dataset was able to surpass the professional lip reader from the bbc channel there are sentences with audio and video in the dataset model lstm on audio and cnn lstm on video these two state vectors are fed to the final lstm which generates the result characters different types of input data were used during training audio video and audio video in other words it is an omnichannel model the university of washington has done a serious job of generating the lip movements of former us president obama the choice fell on him due to the huge number of his performance recordings online hours of hd video they couldn t get along with just the network as they got too many artifacts therefore the authors of the article made several crutches or tricks if you like to improve the texture and timings you can see that the results are amazing soon you couldn t trust even the video with the president in their post and article google brain team reported on how they introduced a new ocr optical character recognition engine into its maps through which street signs and store signs are recognized in the process of technology development the company compiled a new fsns french street name signs which contains many complex cases to recognize each sign the network uses up to four of its photos the features are extracted with the cnn scaled with the help of the spatial attention pixel coordinates are taken into account and the result is fed to the lstm the same approach is applied to the task of recognizing store names on signboards there can be a lot of noise data and the network itself must focus in the right places this algorithm was applied to billion photos there is a type of task called visual reasoning where a neural network is asked to answer a question using a photo for example is there a same size rubber thing in the picture as a yellow metal cylinder the question is truly nontrivial and until recently the problem was solved with an accuracy of only and again the breakthrough was achieved by the team from deepmind on the clevr dataset they reached a super human accuracy of the network architecture is very interesting an interesting application of neural networks was created by the company uizard generating a layout code according to a screenshot from the interface designer this is an extremely useful application of neural networks which can make life easier when developing software the authors claim that they reached accuracy however this is still under research and there is no talk on real usage yet there is no code or dataset in open source but they promise to upload it perhaps you ve seen quick draw from google where the goal is to draw sketches of various objects in seconds the corporation collected this dataset in order to teach the neural network to draw as google described in their blog and article the collected dataset consists of thousand sketches which eventually became publicly available sketches are not pictures but detailed vector representations of drawings at which point the user pressed the pencil released where the line was drawn and so on researchers have trained the sequence to sequence variational autoencoder vae using rnn as a coding decoding mechanism eventually as befits the auto encoder the model received a latent vector that characterizes the original picture whereas the decoder can extract a drawing from this vector you can change it and get new sketches and even perform vector arithmetic to create a catpig one of the hottest topics in deep learning is generative adversarial networks gans most often this idea is used to work with images so i will explain the concept using them the idea is in the competition of two networks the generator and the discriminator the first network creates a picture and the second one tries to understand whether the picture is real or generated schematically it looks like this during training the generator from a random vector noise generates an image and feeds it to the input of the discriminator which says whether it is fake or not the discriminator is also given real images from the dataset it is difficult to train such construction as it is hard to find the equilibrium point of two networks most often the discriminator wins and the training stagnates however the advantage of the system is that we can solve problems in which it is difficult for us to set the loss function for example improving the quality of the photo we give it to the discriminator a classic example of the gan training result is pictures of bedrooms or people previously we considered the auto coding sketch rnn which encodes the original data into a latent representation the same thing happens with the generator the idea of generating an image using a vector is clearly shown in this project in the example of faces you can change the vector and see how the faces change the same arithmetic works over the latent space a man in glasses minus a man plus a woman is equal to a woman with glasses if you teach a controlled parameter to the latent vector during training when you generate it you can change it and so manage the necessary image in the picture this approach is called conditional gan so did the authors of the article face aging with conditional generative adversarial networks having trained the engine on the imdb dataset with a known age of actors the researchers were given the opportunity to change the face age of the person google has found another interesting application to gan the choice and improvement of photos gan was trained on a professional photo dataset the generator is trying to improve bad photos professionally shot and degraded with the help of special filters and the discriminator to distinguish improved photos and real professional ones a trained algorithm went through google street view panoramas in search of the best composition and received some pictures of professional and semi professional quality as per photographers rating an impressive example of gans is generating images using text the authors of this research suggest embedding text into the input of not only a generator conditional gan but also a discriminator so that it verifies the correspondence of the text to the picture in order to make sure the discriminator learned to perform his function in addition to training they added pairs with an incorrect text for the real pictures one of the eye catching articles of is image to image translation with conditional adversarial networks by berkeley ai research bair researchers solved the problem of image to image generation when for example it was required to create a map using a satellite image or realistic texture of the objects using their sketch here is another example of the successful performance of conditional gans in this case the condition goes to the whole picture popular in image segmentation unet was used as the architecture of the generator and a new patchgan classifier was used as a discriminator for combating blurred images the picture is cut into n patches and the prediction of fake real goes for each of them separately christopher hesse made the nightmare cat demo which attracted great interest from the users you can find a source code here in order to apply pix pix you need a dataset with the corresponding pairs of pictures from different domains in the case for example with cards it is not a problem to assemble such a dataset however if you want to do something more complicated like transfiguring objects or styling then pairs of objects cannot be found in principle therefore authors of pix pix decided to develop their idea and came up with cyclegan for transfer between different domains of images without specific pairs unpaired image to image translation the idea is to teach two pairs of generator discriminators to transfer the image from one domain to another and back while we require a cycle consistency after a sequential application of the generators we should get an image similar to the original l loss a cyclic loss is required to ensure that the generator did not just begin to transfer pictures of one domain to pictures from another domain which are completely unrelated to the original image this approach allows you to learn the mapping of horses zebras such transformations are unstable and often create unsuccessful options you can find a source code here machine learning is now coming to medicine in addition to recognizing ultrasound mri and diagnosis it can be used to find new drugs to fight cancer we already reported in detail about this research briefly with the help of adversarial autoencoder aae you can learn the latent representation of molecules and then use it to search for new ones as a result molecules were found half of which are used to fight cancer and the others have serious potential topics with adversarial attacks are actively explored what are adversarial attacks standard networks trained for example on imagenet are completely unstable when adding special noise to the classified picture in the example below we see that the picture with noise for the human eye is practically unchanged but the model goes crazy and predicts a completely different class stability is achieved with for example the fast gradient sign method fgsm having access to the parameters of the model you can make one or several gradient steps towards the desired class and change the original picture one of the tasks on kaggle is related to this the participants are encouraged to create universal attacks defenses which are all eventually run against each other to determine the best why should we even investigate these attacks first if we want to protect our products we can add noise to the captcha to prevent spammers from recognizing it automatically secondly algorithms are more and more involved in our lives face recognition systems and self driving cars in this case attackers can use the shortcomings of the algorithms here is an example of when special glasses allow you to deceive the face recognition system and pass yourself off as another person so we need to take possible attacks into account when teaching models such manipulations with signs also do not allow them to be recognized correctly a set of articles from the organizers of the contest already written libraries for attacks cleverhans and foolbox reinforcement learning rl or learning with reinforcement is also one of the most interesting and actively developing approaches in machine learning the essence of the approach is to learn the successful behavior of the agent in an environment that gives a reward through experience just as people learn throughout their lives rl is actively used in games robots and system management traffic for example of course everyone has heard about alphago s victories in the game over the best professionals researchers were using rl for training the bot played with itself to improve its strategies in previous years deepmind had learned using dqn to play arcade games better than humans currently algorithms are being taught to play more complex games like doom much of the attention is paid to learning acceleration because experience of the agent in interaction with the environment requires many hours of training on modern gpus in his blog deepmind reported that the introduction of additional losses auxiliary tasks such as the prediction of a frame change pixel control so that the agent better understands the consequences of the actions significantly speeds up learning learning results learning robotsin openai they have been actively studying an agent s training by humans in a virtual environment which is safer for experiments than in real life in one of the studies the team showed that one shot learning is possible a person shows in vr how to perform a certain task and one demonstration is enough for the algorithm to learn it and then reproduce it in real conditions if only it was so easy with people here is the work of openai and deepmind on the same topic the bottom line is that an agent has a task the algorithm provides two possible solutions for the human and indicates which one is better the process is repeated iteratively and the algorithm for bits of feedback binary markup from the person learned how to solve the problem as always the human must be careful and think of what he is teaching the machine for example the evaluator decided that the algorithm really wanted to take the object but in fact he just simulated this action there is another study from deepmind to teach the robot complex behavior walk jump etc and even do it similar to the human you have to be heavily involved with the choice of the loss function which will encourage the desired behavior however it would be preferable that the algorithm learned complex behavior itself by leaning with simple rewards researchers managed to achieve this they taught agents body emulators to perform complex actions by constructing a complex environment with obstacles and with a simple reward for progress in movement you can watch the impressive video with results however it s much more fun to watch it with a superimposed sound finally i will give a link to the recently published algorithms for learning rl from openai now you can use more advanced solutions than the standard dqn in july google reported that it took advantage of deepmind s development in machine learning to reduce the energy costs of its data center based on the information from thousands of sensors in the data center google developers trained a neural network ensemble to predict pue power usage effectiveness and more efficient data center management this is an impressive and significant example of the practical application of ml as you know trained models are poorly transferred from task to task as each task has to be trained for a specific model a small step towards the universality of the models was done by google brain in his article one model to learn the all researchers have trained a model that performs eight tasks from different domains text speech and images for example translation from different languages text parsing and image and sound recognition in order to achieve this they built a complex network architecture with various blocks to process different input data and generate a result the blocks for the encoder decoder fall into three types convolution attention and gated mixture of experts moe main results of learning by the way this model is present in tensor tensor in their post facebook staff told us how their engineers were able to teach the resnet model on imagenet in just one hour truth be told this required a cluster of gpus tesla p they used gloo and caffe for distributed learning to make the process effective it was necessary to adapt the learning strategy with a huge batch elements gradient averaging warm up phase special learning rate etc as a result it was possible to achieve an efficiency of when scaling from to gpu now researchers from facebook can experiment even faster unlike mere mortals without such a cluster the self driving car sphere is intensively developing and the cars are actively tested from the relatively recent events we can note the purchase of intel mobileye the scandals around uber and google technologies stolen by their former employee the first death when using an autopilot and much more i will note one thing google waymo is launching a beta program google is a pioneer in this field and it is assumed that their technology is very good because cars have been driven more than million miles as to more recent events self driving cars have been allowed to travel across all us states as i said modern ml is beginning to be introduced into medicine for example google collaborates with a medical center to help with diagnosis deepmind has even established a separate unit this year under the program of the data science bowl there was a competition held to predict lung cancer in a year on the basis of detailed images with a prize fund of one million dollars currently there are heavy investments in ml as it was before with bigdata china invested billion in ai to become the world leader in the industry for comparison baidu research employs people and in the same fair facebook at the last kdd alibaba employees talked about their parameter server kungpeng which runs on billion samples with a trillion parameters which becomes a common task you can draw your own conclusions it s never too late to study machine learning in one way or another over time all developers will use machine learning which will become one of the common skills as it is today the ability to work with databases link to the original post from a quick cheer to a standing ovation clap to show how much you enjoyed this story mail ru group head of machine learning team data stories on machine learning and analytics from statsbot s makers
Maruti Techlabs,552,5,https://chatbotsmagazine.com/which-are-the-best-intelligent-chatbots-or-ai-chatbots-available-online-cc49c0f3569d?source=tag_archive---------8----------------,What Are The Best Intelligent Chatbots or AI Chatbots Available Online?,how do we define the intelligence of a chatbot you can see a lot of articles about what would make a chatbot appear intelligent a chatbot is intelligent when it becomes aware of user needs its intelligence is what gives the chatbot the ability to handle any scenario of a conversation with ease are the travel bots or the weather bots that have buttons that you click and give you some query artificially intelligent definitely but they are just not far along the conversation axis it can be a wonderfully designed conversational interface that is smooth and easy to use it could be natural language processing and understanding where it is able to understand sentences that you structure in the wrong way now it is easier than ever to make a bot from scratch also chatbot development platforms like chatfuel gupshup make it fairly simple to build a chatbot without a technical background hence making the reach for chatbot easy and transparent to anyone who would like to have one for their business for more understanding on intelligent chatbots read our blog the best ai based chatbots available online are mitsuku rose poncho right click insomno bot dr ai and melody this chatbot is one the best ai chatbots and it s my favorite too evidently it is the current winner of loebner prize the loebner prize is an annual competition in artificial intelligence that awards prizes to the chatterbot considered by the judges to be the most human like the format of the competition is that of a standard turing test you can talk with mitsuku for hours without getting bored it replies to your question in the most humane way and understands your mood with the language you re using it is a bot made to chat about anything which is one of the main reasons that make it so human like contrary to other chatbots that are made for a specific task rose is a chatbot and a very good one she won recognition this past saturday as the most human like chatbot in a competition described as the first turing test the loebner prize in and right click is a startup that introduced an a i powered chatbot that creates websites it asks general questions during the conversation like what industry you belong to and why do you want to make a website and creates customized templates as per the given answers hira saeed tried to divert it from its job by asking it about love but what a smart player it is by replying to each of her queries it tried to bring her back to the actual job of website creation the process was short but keeps you hooked poncho is a messenger bot designed to be your one and only weather expert it sends alerts up to twice a day with user consent and is intelligent enough to answer questions like should i take an umbrella today read poncho developer s piece think differently when building bots insomno bot is for night owls as the name suggests it is for all people out there who have trouble sleeping this bot talks to you when you have no one around and gives you amazing replies so that you won t get bored it s not something that will help you count stars when you can t sleep or help you with reading suggestions but this bot talks to you about anything it asks about symptoms body parameters and medical history then compiles a list of the most and least likely causes for the symptoms and ranks them by order of seriousness it lives inside the existing biadu doctor app this app collects medical information from people and then passes it to doctors in a form that makes it easier to use for diagnostic purposes or to otherwise respond to featured cbm the future healthcare and conversational ui these are just the basic versions of intelligent chatbots there are many more intelligent chatbots out there which provide a much more smarter approach to responding to queries since the process of making a intelligent chatbot is not a big task most of us can achieve it with the most basic technical knowledge many of which will be very extremely helpful in the service industry and also help provide a better customer experience the most important part of any chatbot is the conversation it has with its user hence more effort has to be put in designing a chatbot conversation hope you had a good read to know more about chatbots and how they converse with people visit the link below featured cbm how to make a chatbot intelligent if you resonated with this article please subscribe to our newsletter you will get a free copy of our case study on business automation through our bot solution from a quick cheer to a standing ovation clap to show how much you enjoyed this story professional team delivering enterprise software solutions bot development big data analytics web mobile apps and ai ml integration chatbots ai nlp facebook messenger slack telegram and more
Jerry Chen,2300,11,https://news.greylock.com/the-new-moats-53f61aeac2d9?source=tag_archive---------9----------------,The New Moats – Greylock Perspectives,to build a sustainable and profitable business you need strong defensive moats around your company this rings especially true today as we undergo one of the largest platform shifts in a generation as applications move to the cloud are consumed on iphones echoes and teslas are built on open source and are fueled by ai and data these dramatic shifts are rendering some existing moats useless and leaving ceos feeling like it s almost impossible to build a defensible business in this post i ll review some of the traditional economic moats that technology companies typically leverage and how they are being disrupted i believe that startups today need to build systems of intelligencetm ai powered applications the new moats businesses can build several different moats and over time these moats can change the following list is definitely not exhaustive and fair warning it will read like a bad b school blog some of the greatest and most enduring technology companies are defended by powerful moats for example microsoft google and facebook all have moats built on economies of scale and network effects one of the most successful cloud businesses amazon web services aws has both the advantages of scale but also the power of network effects more apps and services are built natively on aws because that s where the customers and the data are in turn the ecosystem of solutions attracts more customers and developers who build more apps that generate more data continuing the virtuous cycle while driving down amazon s cost through the advantages of scale strong moats help companies survive through major platform shifts but surviving should not be confused with thriving for example high switching costs can partly account for why mainframes and big iron systems are still around after all these years legacy businesses with deep moats may not be the high growth vehicles of their prime but they are still generating profits companies need to recognize and react when they are in the midst of an industry wide transformation lest they become victims of their own success moreover these massive platforms shifts like cloud and mobile are technology tidal waves that create openings for new players and enable founders to build paths over and around existing moats startup founders who succeed tend to execute a dual pronged strategy attack legacy player moats and simultaneously build their own defensible moats that ride the new wave for example facebook had the most entrenched social network but instagram built a mobile first photo app that rode the smartphone wave to a b acquisition in the enterprise world saas companies like salesforce are disrupting on premise software companies like oracle now with the advent of cloud aws azure and google cloud are creating a direct channel to the customer these platform shifts can also change the buyer and end user within the enterprise the buyer has moved from a central it team to an office knowledge worker to someone with an iphone to any developer with a github account in this current wave of disruption is it still possible to build sustainable moats for founders it may feel like every advantage you build can be replicated by another team down the street or at the very least it feels like moats can only be built at massive scale open source tools and cloud have pushed power to the new incumbents the current generation of companies that are at massive scale have strong distribution networks high switching cost and strong brands working for them these are companies like apple facebook google amazon and salesforce why does it feel like there are no more moats to build in an era of cloud and open source deep technology attacking hard problems is becoming a shallower moat the use of open source is making it harder to monetize technology advances while the use of cloud to deliver technology is moving defensibility to different parts of the product companies that focus too much on technology without putting it in context of a customer problem will be caught between a rock and a hard place or as i like to say between open source and a cloud place for example incumbent technologies like oracle s proprietary database are being attacked from open source alternatives like hadoop and mongodb and in the cloud by amazon aurora and innovations like google spanner on the other hand companies that build great customer experiences may find defensibility through the workflow of their software i believe that deep technology moats aren t completely gone and defensible business models can still be built around ip if you pick a place in the technology stack and become the absolute best of breed solution you can create a valuable company however this means picking a technical problem with few substitutes that requires hard engineering and needs operational knowledge to scale today the market is favoring full stack companies saas offerings that offer application logic middleware and databases combined technology is becoming an invisible component of a complete solution e g no one cares what database backs your favorite mobile app as long as your food is delivered on time in the consumer world apple made the integrated or full stack experience popular with the iphone which seamlessly integrated hardware with software this integrated experience is coming to dominate enterprise software as well cloud and saas has made it possible to reach customers directly and in a cost effective manner as a result customers are increasingly buying full stack technology in the form of saas applications instead of buying individual pieces of the tech stack and building their own apps the emphasis on the whole application experience or the top of the technology stack is why i also evaluate companies through an additional framework the stack of enterprise systems at the bottom of the stack of systems is usually a database on top of which an application is built if the data and app power a critical business function it becomes a system of record there are three major systems of record in an enterprise your customers your employees and your assets crm owns your customers hcm owns your employees and erp financials owns your assets generations of companies have been built around owning a system of record and every wave produced a new winner in crm we saw salesforce replace siebel as the system of record for customer data and workday replace oracle peoplesoft for employee data workday has also expanded into financial data other applications can be built around a system of record but are usually not as valuable as the actual system of record for example marketing automation companies like marketo and responsys built big businesses around crm but never became as strategic or as valuable as salesforce systems of engagementtm are the interfaces between users and the systems of record and can be powerful businesses because they control the end user interactions in the mainframe era the systems of record and engagement were tied together when the mainframe and terminal were essentially the same product the client server wave ushered in a class of companies that tried to own your desktop only to be disrupted by a generation of browser based companies only to be succeeded by mobile first companies the current generation of companies vying to own the system of engagement include slack amazon alexa and every other speech text conversational ui startup in china wechat has become a dominant system of engagement and is now a platform for everything from e commerce to games if it sounds like systems of engagementtm turn over more than systems of record it s probably because they do the successive generations of systems of engagementtm don t necessarily disappear but instead users keep adding new ways to interact with their applications in a multi channel world owning the system of engagement is most valuable if you control most of the end user engagement or are a cross channel system that reaches users wherever they are perhaps the most strategic advantage of being a system of engagement is that you can coexist with several systems of record and collect all the data that passes through your product over time you can evolve your engagement position into an actual system of record using all the data you have accumulated i believe that systems of intelligencetm are the new moats what is a system of intelligence and why is it so defensible what makes a system of intelligence valuable is that it typically crosses multiple data sets multiple systems of record one example is an application that combines web analytics with customer data and social data to predict end user behavior churn ltv or just serve more timely content you can build intelligence on a single data source or single system of record but that position becomes harder to defend against the vendor that owns the data for a startup to thrive around incumbents like oracle and sap you need to combine their data with other data sources public or private to create value for your customer incumbents will be advantaged on their own data for example salesforce is building a system of intelligence einstein starting with their own system of record crm the next generation of enterprise products will use different artificial intelligence ai techniques to build systems of intelligencetm it s not just applications that will be transformed by ai but also data center and infrastructure products we can categorize three major areas where you can build systems of intelligencetm customer facing applications around the customer journey employee facing applications like hcm itsm financials or infrastructure systems like security compute storage networking and monitoring management in addition to these broad horizontal use cases startups can also focus on a single industry or market and build a system of intelligence around data that is unique to a vertical like veeva in life sciences or rhumbix in construction in all of these markets the battle is moving from the old moats the sources of the data to the new moats what you do with the data using a company s data you can upsell customers automatically respond to support tickets prevent employee attrition and identify security anomalies products that use data specific to an industry i e healthcare financial services or unique to a company customer data machine logs etc to solve a strategic problem begin to look like a pretty deep moat especially if you can replace or automate an entire enterprise workflow or create a new value added workflow that was made possible by this intelligence enterprise applications that built systems of record have always been powerful businesses models some of the most enduring app companies like salesforce and sap are all built on deep ip benefit from economies of scale and over time they accumulate more data and operating knowledge as they get deeper within a company s workflow and business processes however even these incumbents are not immune to platform shifts as a new generation of companies attack their domains to be fair we may be at risk of ai marketing fatigue but all the hype reflects ai s potential to change so many industries one popular ai approach machine learning ml can be combined with data a business process and an enterprise workflow to create the context to build a system of intelligence google was an early pioneer of applying ml to a process and workflow they collected more data on every user and applied machine learning to serve up more timely ads within the workflow of a web search there are other evolving ai techniques like neural networks that will continue to change what we can expect from these future applications these ai driven systems of intelligencetm present a huge opportunity for new startups successful companies here can build a virtuous cycle of data because the more data you generate and train on with your product the better your models become and the better your product becomes ultimately the product becomes tailored for each customer which creates another moat high switching costs it is also possible to build a company that combines systems of engagementtm with intelligence or even all three layers of the enterprise stack but a system of intelligence or engagement can be the best insertion point for a startup against an incumbent building a system of engagement or intelligence is not a trivial task and will require deep technology especially at speed and scale in particular technologies that can facilitate an intelligence layer across multiple data sources will be essential finally there are some businesses that can build data network effects by using customer and market data to train and improve models that make the product better for all customers which spins the flywheel of intelligence faster in summary you can build a defensible business model as a system of engagement intelligence or record but with the advent of ai intelligent applications will be the fountain of the next generation of great software companies because they will be the new moats thanks to saam motamedi sarah guo eli collins peter bailis elisa schreiber michael inouye my greylock partner sarah tavel and the rest of my partners at greylock for their input this post was also helped through conversations with my friends at several greylock backed companies including trifacta cloudera and dozens of founders and ceos that have influenced my thinking all good ideas are shamelessly stolen and all bad ideas are mine alone from a quick cheer to a standing ovation clap to show how much you enjoyed this story restless irreverent partner at greylockvc www jerrychen com greylock partners backs entrepreneurs who are building disruptive market transforming consumer and enterprise software companies
Sarthak Jain,3900,10,https://medium.com/nanonets/how-to-easily-detect-objects-with-deep-learning-on-raspberrypi-225f29635c74?source=tag_archive---------2----------------,How to easily Detect Objects with Deep Learning on Raspberry Pi,disclaimer i m building nanonets com to help build ml with less data and no hardware the raspberry pi is a neat piece of hardware that has captured the hearts of a generation with m devices sold with hackers building even cooler projects on it given the popularity of deep learning and the raspberry pi camera we thought it would be nice if we could detect any object using deep learning on the pi now you will be able to detect a photobomber in your selfie someone entering harambe s cage where someone kept the sriracha or an amazon delivery guy entering your house m years of evolution have made human vision fairly evolved the human brain has of it s neurons work on processing vision as compared with percent for touch and just percent for hearing humans have two major advantages when compared with machines one is stereoscopic vision the second is an almost infinite supply of training data an infant of years has had approximately b images sampled at fps to mimic human level performance scientists broke down the visual perception task into four different categories object detection has been good enough for a variety of applications even though image segmentation is a much more precise result it suffers from the complexity of creating training data it typically takes a human annotator x more time to segment an image than draw bounding boxes this is more anecdotal and lacks a source also after detecting objects it is separately possible to segment the object from the bounding box object detection is of significant practical importance and has been used across a variety of industries some of the examples are mentioned below object detection can be used to answer a variety of questions these are the broad categories there are a variety of models architectures that are used for object detection each with trade offs between speed size and accuracy we picked one of the most popular ones yolo you only look once and have shown how it works below in under lines of code if you ignore the comments note this is pseudo code not intended to be a working example it has a black box which is the cnn part of it which is fairly standard and shown in the image below you can read the full paper here https pjreddie com media files papers yolo pdf for this task you probably need a few images per object try to capture data as close to the data you re going to finally make predictions on draw bounding boxes on the images you can use a tool like labelimg you will typically need a few people who will be working on annotating your images this is a fairly intensive and time consuming task you can read more about this at medium com nanonets nanonets how to use deep learning when you have limited data f c b cab you need a pretrained model so you can reduce the amount of data required to train without it you might need a few k images to train the model you can find a bunch of pretrained models here the process of training a model is unnecessarily difficult to simplify the process we created a docker image would make it easy to train to start training the model you can run the docker image has a run sh script that can be called with the following parameters you can find more details at to train a model you need to select the right hyper parameters finding the right parameters the art of deep learning involves a little bit of hit and try to figure out which are the best parameters to get the highest accuracy for your model there is some level of black magic associated with this along with a little bit of theory this is a great resource for finding the right parameters quantize model make it smaller to fit on a small device like the raspberry pi or mobile small devices like mobile phones and rasberry pi have very little memory and computation power training neural networks is done by applying many tiny nudges to the weights and these small increments typically need floating point precision to work though there are research efforts to use quantized representations here too taking a pre trained model and running inference is very different one of the magical qualities of deep neural networks is that they tend to cope very well with high levels of noise in their inputs why quantize neural network models can take up a lot of space on disk with the original alexnet being over mb in float format for example almost all of that size is taken up with the weights for the neural connections since there are often many millions of these in a single model the nodes and weights of a neural network are originally stored as bit floating point numbers the simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer and then compressing each float value to an eight bit integer the size of the files is reduced by code for quantization you need the raspberry pi camera live and working then capture a new image for instructions on how to install checkout this link download model once your done training the model you can download it on to your pi to export the model run then download the model onto the raspberry pi install tensorflow on the raspberry pi depending on your device you might need to change the installation a little run model for predicting on the new image the raspberry pi has constraints on both memory and compute a version of tensorflow compatible with the raspberry pi gpu is still not available therefore it is important to benchmark how much time do each of the models take to make a prediction on a new image we have removed the need to annotate images we have expert annotators who will annotate your images for you we automatically train the best model for you to achieve this we run a battery of model with different parameters to select the best for your data nanonets is entirely in the cloud and runs without using any of your hardware which makes it much easier to use since devices like the raspberry pi and mobile phones were not built to run complex compute heavy tasks you can outsource the workload to our cloud which does all of the compute for you get your free api key from http app nanonets com user api key collect the images of object you want to detect you can annotate them either using our web ui https app nanonets com objectannotation appid your model id or use open source tool like labelimg once you have dataset ready in folders images image files and annotations annotations for the image files start uploading the dataset once the images have been uploaded begin training the model the model takes hours to train you will get an email once the model is trained in the meanwhile you check the state of the model once the model is trained you can make predictions using the model from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder ceo nanonets com nanonets machine learning api
Gaurav Oberoi,850,12,https://hackernoon.com/exploring-deepfakes-20c9947c22d9?source=tag_archive---------3----------------,Exploring DeepFakes – Hacker Noon,in december a user named deepfakes posted realistic looking explicit videos of famous celebrities on reddit he generated these fake videos using deep learning the latest in ai to insert celebrities faces into adult movies in the following weeks the internet exploded with articles about the dangers of face swapping technology harassing innocents propagating fake news and hurting the credibility of video evidence forever in this post i explore the capabilities of this tech describe how it works and discuss potential applications deepfakes offers the ability to swap one face for another in an image or a video face swapping has been done in films for years but it required skilled video editors and cgi experts to spend many hours to achieve decent results this is so remarkable that i m going to repeat it anyone with hundreds of sample images of person a and person b can feed them into an algorithm and produce high quality face swaps video editing skills are not needed this also means that it can be done at scale and given that so many of us have our faces online it s trivially easy to insert almost anyone into fake videos scary but hopefully it s not all doom and gloom after all we as a society have already come to accept that photos can easily be faked before dreaming up how to use this tech i wanted to get a handle on how it works and how well it performs i picked two popular late night tv hosts jimmy fallon and john oliver because i can find lots of videos of them with similar poses and lighting and also enough variation like lip sync battles to keep it interesting luckily for me there s an active github repo that contains the original deepfakes code and many more improvements it s fairly straightforward to use but the onus is still on the user to collect and prepare training data to make experimentation easy i wrote a script to work directly with youtube videos this makes collecting and preprocessing training data painless and converting videos one step click here to view my github repo and see how easily i generated the videos below i also share my model weights the following videos were generated by training a model on about k images of each person s face k images total i got faces for each celebrity from youtube videos of minutes each with frames per second per video and by filtering out frames that don t have their faces present all of this was done automatically all i did was specify a list of youtube video urls the total training time was about hours on a nvidia gtx ti gpu training is primarily constrained by gpu but downloading videos and chopping them into frames is i o bound and can be parallelized note that while i had thousands of images of each person decent face swaps can be achieved with as few as images i went this route because i pulled face images from videos and it s far easier to pick a handful of videos as training data than to find hundreds of images the images below are low resolution to keep the size of the animated gif file small there s a youtube video below with higher resolution and sound while not perfect the results above are quite convincing the key thing to remember is the algorithm learned how to do this by seeing lots of examples i didn t modify the videos in any way magical let s look under the covers at the core of the deepfakes code is an autoencoder a deep neural network that learns how to take an input compress it down into a small representation or encoding and then to regenerate the original input from this encoding putting a bottleneck in the middle forces the network to recreate these images instead of just returning what it sees the encodings help it capture broader patterns hypothetically like how and where to draw jimmy fallon s eyebrow deepfakes goes further by having one encoder to compress a face into an encoding and two decoders one to turn it back into person a fallon and the other to person b oliver it s easier to understand with a diagram in the above we re showing how these components get trained once training is complete we can perform a clever trick pass in an image of fallon into the encoder and then instead of trying to reconstruct fallon from the encoding we now pass it to decoder b to reconstruct oliver it s remarkable to think that the algorithm can learn how to generate these images just by seeing thousands of examples but that s exactly what has happened here and with fairly decent results while the results are exciting there are clear limitations to what we can achieve with this technology today these are tenable problems to be sure tools can be built to collect images from online channels en masse algorithms can help flag when there is insufficient or mismatched training data clever optimizations or model reuse can help reduce training time and a well engineered system can be built to make the entire process automatic but ultimately the question is why is there enough of a business model to make doing all this worth it given what ve now learned about what s possible let s talk about ways in which this could be useful hollywood has had this technology at its fingertips but not at this low cost if they can create great looking videos with this technique it will change the demand for skilled editors over time but it could also open up new opportunities for instance making movies with unknown actors and then superimposing famous celebrities onto them this could work for youtube videos or even news channels filmed by regular folks in more out there scenarios studios could change actors based on their target market more schwarzenager for the austrians or netflix could allow viewers to pick actors before hitting play more likely this tech could generate revenue for the estates of long dead actors by bringing them back to life some of the comment threads on deepfakes videos on youtube are abuzz about what a great meme generator this technology could create jib jab is a company that has been selling video greeting cards with simple face swapping for years they are hilarious but the big opportunity is to create the next big viral hit after all photo filters attracted masses of people to instagram and snapchat and face swapping apps have done well before given how fun the results can be there s likely room for a hit viral app if you can get the costs low enough to generate these models imagine if target could have a celebrity showcase their clothes for a month just by paying her agent a fee grabbing some existing headshots and clicking a button this would create a new revenue stream for celebrities social media influencers or anyone who happens to be in the spotlight at the moment and it would give businesses another tool to promote brands and drive conversion it also raises interesting legal questions about ownership of likeness and business model questions on how to partition and price rights to use them imagine a world where the ads you see as you surf the web include you your friends and your family while this may come across as creepy today does it seem so far fetched to think that this won t be the norm in a few years after all we are visual creatures and advertisers have been trying to elicit emotional responses from us for years e g coke may want to convey joy by putting your friends in a hip music video or allstate may tug at your fears by showing your family in an insurance ad or the approach may be more direct banana republic could superimpose your face on a body type that matches yours and convince you that it s worth trying out their new leather jackets whoever the original deepfakes user is they opened a pandora s box of difficult questions about how fake video generation will affect society i hope that in the same way we have come to accept that images can easily be faked we will adapt to video uncertainty too though not everyone shares this hope what deepfakes also did is shine a light on how interesting this technology is deep generative models like the autoencoder that deepfakes uses allow us to create synthetic but realistic looking data including images or videos only by showing an algorithm lots of examples this means that once these algorithms are turned into products regular folks will have access to powerful tools that will make them more creative hopefully towards positive ends there have already been some interesting applications of this technique like style transfer apps that make your photos look like famous paintings but given the high volume and exciting nature of the research that is being published in this space there s clearly a lot more to come i m interested in exploring how to build value from the latest in ai research if you have an interest in taking this technology to market to solve a real problem please drop me a note a few fun tidbits for the curious from a quick cheer to a standing ovation clap to show how much you enjoyed this story i ve been a product manager engineer and founder for over a decade in seattle and silicon valley currently exploring new ideas at the allen institute for ai how hackers start their afternoons
Nick Bourdakos,5000,15,https://medium.freecodecamp.org/understanding-capsule-networks-ais-alluring-new-architecture-bdb228173ddc?source=tag_archive---------4----------------,Understanding Capsule Networks — AI’s Alluring New Architecture,convolutional neural networks have done an amazing job but are rooted in problems it s time we started thinking about new solutions or improvements and now enter capsules previously i briefly discussed how capsule networks combat some of these traditional problems for the past for few months i ve been submerging myself in all things capsules i think it s time we all try to get a deeper understanding of how capsules actually work in order to make it easier to follow along i have built a visualization tool that allows you to see what is happening at each layer this is paired with a simple implementation of the network all of it can be found on github here this is the capsnet architecture don t worry if you don t understand what any of it means yet i ll be going through it layer by layer with as much detail as i can possibly conjure up the input into capsnet is the actual image supplied to the neural net in this example the input image is pixels high and pixels wide but images are actually dimensions and the rd dimension contains the color channels the image in our example only has one color channel because it s black and white most images you are familiar with have or channels for red green blue and possibly an additional channel for alpha or transparency each one of these pixels is represented as a value from to and stored in a x x matrix the brighter the pixel the larger the value the first part of capsnet is a traditional convolutional layer what is a convolutional layer how does it work and what is its purpose the goal is to extract some extremely basic features from the input image like edges or curves how can we do this let s think about an edge if we look at a few points on the image we can start to pick up a pattern focus on the colors to the left and right of the point we are looking at you might notice that they have a larger difference if the point is an edge what if we went through each pixel in the image and replaced its value with the value of the difference of the pixels to the left and right of it in theory the image should become all black except for the edges we could do this by looping through every pixel in the image but this isn t very efficient we can instead use something called a convolution technically speaking it s a cross correlation but everyone likes to call them convolutions a convolution is essentially doing the same thing as our loop but it takes advantage of matrix math a convolution is done by lining up a small window in the corner of the image that only lets us see the pixels in that area we then slide the window across all the pixels in the image multiplying each pixel by a set of weights and then adding up all the values that are in that window this window is a matrix of weights called a kernel we only care about pixels but when we wrap the window around them it will encapsulate the pixel between them can you think of a set of weights that we can multiply these pixels by so that their sum adds up to the value we are looking for spoilers below we can do something like this with these weights our kernel will look like this however kernels are generally square so we can pad it with more zeros to look like this here s a nice gif to see a convolution in action note the dimension of the output is reduced by the size of the kernel plus for example more on this in the next section here s what the original image looks like after doing a convolution with the kernel we crafted you might notice that a couple edges are missing specifically the horizontal ones in order to highlight those we would need another kernel that looks at pixels above and below like this also both of these kernels won t work well with edges of other angles or edges that are blurred for that reason we use many kernels in our capsnet implementation we use kernels and the kernels are normally larger to allow for more wiggle room our kernels will be x this is what one of the kernels looked like after training the model it s not very obvious but this is just a larger version of our edge detector that is more robust and only finds edges that go from bright to dark note i ve rounded the values because they are quite large for example luckily we don t have to hand pick this collection of kernels that is what training does the kernels all start off empty or in a random state and keep getting tweaked in the direction that makes the output closer to what we want this is what the kernels ended up looking like i colored them as pixels so it s easier to digest the more negative the numbers the bluer they are is green and positive is yellow after we filter the image with all of these kernels we end up with a fat stack of output images relu formally known as rectified linear unit may sound complicated but it s actually quite simple relu is an activation function that takes in a value if it s negative it becomes zero and if it s positive it stays the same in code and as a graph we apply this function to all of the outputs of our convolutions why do we do this if we don t apply some sort of activation function to the output of our layers then the entire neural net could be described as a linear function this would mean that all this stuff we are doing is kind of pointless adding a non linearity allows us to describe all kinds of functions there are many different types of function we could apply but relu is the most popular because it s very cheap to perform here are the outputs of relu conv layer the primarycaps layer starts off as a normal convolution layer but this time we are convolving over the stack of outputs from the previous convolutions so instead of having a x kernel we have a x x kernel so what exactly are we looking for in the first layer of convolutions we were looking for simple edges and curves now we are looking for slightly more complex shapes from the edges we found earlier this time our stride is that means instead of moving pixel at a time we take steps of a larger stride is chosen so that we can reduce the size of our input more rapidly note the dimension of the output would normally be but we divide it by because of the stride for example we will convolve over the outputs another times so we will end up with a stack of x outputs but this time we aren t satisfied with just some lousy plain old numbers we re going to cut the stack up into decks with cards each deck we can call this deck a capsule layer each capsule layer has capsules if you re keeping up and are a math wiz that means each capsule has an array of values this is what we can call a vector here s what i m talking about these capsules are our new pixel with a single pixel we could only store the confidence of whether or not we found an edge in that spot the higher the number the higher the confidence with a capsule we can store values per location that gives us the opportunity to store more information than just whether or not we found a shape in that spot but what other kinds of information would we want to store when looking at the shape below what can you tell me about it if you had to tell someone else how to redraw it and they couldn t look at it what would you say this image is extremely basic so there are only a few details we need to describe the shape we can call these instantiation parameters with more complex images we will end up needing more details they can include pose position size orientation deformation velocity albedo hue texture and so on you might remember that when we made a kernel for edge detection it only worked on a specific angle we needed a kernel for each angle we could get away with it when dealing with edges because there are very few ways to describe an edge once we get up to the level of shapes we don t want to have a kernel for every angle of rectangles ovals triangles and so on it would get unwieldy and would become even worse when dealing with more complicated shapes that have dimensional rotations and features like lighting that s one of the reasons why traditional neural nets don t handle unseen rotations very well as we go from edges to shapes and from shapes to objects it would be nice if we had more room to store this extra useful information here is a simplified comparison of capsule layers one for rectangles and the other for triangles vs traditional pixel outputs like a traditional d or d vector this vector has an angle and a length the length describes the probability and the angle describes the instantiation parameters in the example above the angle actually matches the angle of the shape but that s not normally the case in reality it s not really feasible or at least easy to visualize the vectors like above because these vectors are dimensional since we have all this extra information in a capsule the idea is that we should be able to recreate the image from them sounds great but how do we coax the network into actually wanting to learn these things when training a traditional cnn we only care about whether or not the model predicts the right classification with a capsule network we have something called a reconstruction a reconstruction takes the vector we created and tries to recreate the original input image given only this vector we then grade the model based on how close the reconstruction matches the original image i will go into more detail on this in the coming sections but here is a simple example after we have our capsules we are going to perform another non linearity function on it like relu but this time the equation is a bit more involved the function scales the values of the vector so that only the length of the vector changes not the angle this way we can make the vector between and so it s an actual probability this is what lengths of the capsule vectors look like after squashing at this point it s almost impossible to guess what each capsule is looking for the next step is to decide what information to send to the next level in traditional networks we would probably do something like max pooling max pooling is a way to reduce size by only passing on the highest activated pixel in the region to the next layer however with capsule networks we are going to do something called routing by agreement the best example of this is the boat and house example illustrated by aure lien ge ron in this excellent video each capsule tries to predict the next layer s activations based on itself looking at these predictions which object would you choose to pass on to the next layer not knowing the input probably the boat right both the rectangle capsule and the triangle capsule agree on what the boat would look like but they don t agree on how the house would look so it s not very likely that the object is a house with routing by agreement we only pass on the useful information and throw away the data that would just add noise to the results this gives us a much smarter selection than just choosing the largest number like in max pooling with traditional networks misplaced features don t faze it with capsule networks the features wouldn t agree with each other hopefully that works intuitively however how does the math work we have different digit classes that we are predicting note in the boat and house example we were predicting objects but now we are predicting unlike in the boat and the house example the predictions aren t actually images instead we are trying to predict the vector that describes the image the capsule s predictions for each class are made by multiplying it s vector by a matrix of weights for each class that we are trying to predict remember that we have capsule layers and each capsule layer has capsules that means we have a total of capsules you will end up with a list of predictions each weight is actually a x matrix so each prediction is a matrix multiplication between the capsule vector and this weight matrix as you can see our prediction is a degree vector where does the come from it s an arbitrary choice just like was for our original capsules but it should be noted that we want to increase the number of dimensions of our capsules the deeper we get into the network this should make sense intuitively because the deeper we go the more complex our features become and the more parameters we need to recreate them for example you will need more information to describe an entire face than just a person s eye the next step is to figure out which of these predictions agree with each other the most it can be difficult to visualize a solution to this when we think in terms of high dimensional vectors for the sake of sanity let s start off by pretending our vectors are just points in dimensional space we start off by calculating the mean of all of the points each point starts out with equal importance we then can measure the distance between every point from the mean the further the point is away from the mean the less important that point becomes we then recalculate the mean this time taking into account the point s importance we end up going through this cycle times as you can see as we go through this cycle the points that don t agree with the others start to disappear the highest agreeing points end up getting passed on to the next layer with the highest activations after agreement we end up with ten dimensional vectors one vector for each digit this matrix is our final prediction the length of the vector is the confidence of the digit being found the longer the better the vector can also be used to generate a reconstruction of the input image this is what the lengths of the vectors look like with the input of the fifth block is the brightest which means high confidence remember that is the first class meaning is our predicted class the reconstruction portion of the implementation isn t very interesting it s just a few fully connected layers but the reconstruction itself is very cool and fun to play around with if we reconstruct our input from its vector this is what we get if we manipulate the sliders the vector we can see how each dimension affects the i recommend cloning the visualization repo to play around with different inputs and see how the sliders affect the reconstruction run the tool then point your browser to http localhost i think that the reconstructions from capsule networks are stunning even though the current model is only trained on simple digits it makes my mind run with the possibilities that a matured architecture trained on a larger dataset could achieve i m very curious to see how manipulating the reconstruction vectors of a more complicated image would affect it for that reason my next project is to get capsule networks to work with the cifar and smallnorb datasets thanks for reading if you have any questions feel free to reach out at bourdakos gmail com connect with me on linkedin or follow me on medium if you found this article helpful it would mean a lot if you gave it some applause and shared to help others find it and feel free to leave a comment below from a quick cheer to a standing ovation clap to show how much you enjoyed this story computer vision addict at ibm watson our community publishes stories worth reading on development design and data science
Mark Johnson,3700,9,https://hackernoon.com/how-i-shipped-six-side-projects-in-2017-3dde6c77adbb?source=tag_archive---------5----------------,How I Launched Six Side Projects in 2017 – Hacker Noon,last year i set a goal to learn something new each month and ended out launching six new projects which i ll recap along with what i learned below looking back it seems a little crazy to me that i managed to launch as much as i did while running a more than full time business spending quality time with my family i have two kids and a very patient wife teaching as an adjunct professor and consulting on the side it s easy to think that not having enough time is what s holding you back from launching your side projects if there were only more time is the general excuse we give ourselves and we look for fancy apps or task management techniques to try and free up more space in our schedule however one of the main things i ve learned over the last year is that time is not the primary issue you have enough time what you need is motivation the good news is that motivation can be hacked i ve learned a few ways to hack my motivation in and i want to share those with you you simply can t stay motivated about something you don t care about so choose something that you re excited to work on when you feel inspiration strike around that idea don t let it pass use it even if that means jotting down some quick notes while you re in a meeting at work it s important to grab ahold of those moments of inspiration to stay hungry and curious around your work for me that meant shipping something every month i tend to blow things up once i start working on them so this day constraint really helped me rein that tendency in and spend my motivation efficiently it also gives you a chance to try out new ideas if one month s idea turns out to be a dud at least you didn t waste a whole year on it this is the big one you will run out of motivation fuel towards the end of your project that last is killer the only thing that will get you through a motivation slump is knowing there are people on the other side waiting to see what you built another benefit of sharing your work is that it gives you a chance to get some supportive feedback for what you re doing the co working space i work out of atlas local has an office wide event on the first friday of every month i used that event to present my project from the previous month and was always encouraged and supported by the generous folks who were there you ll be surprised by how much support you ll get for just stepping out there and sharing something you made perhaps the most surprising part of this experiment for me was that far from being burned out at the end i feel even more motivated to ship more work in i d encourage you to hack your motivation in the new year and ship some of those ideas you ve had lying around for a while i d love to hear about it if you try if you re interested in the details of what i built in read on visually compare the personality types of your group s strongest and weakest traits i ve been interested in the myers briggs type indicator mbti for a while now while i don t see it as prescriptive or even all that scientific it has been a helpful framework for empathizing with people who are different than i what many personality nerds don t realize is that the mbti system is based on something called cognitive functions these functions were created by the father of modern phycology carl jung back in the s i wanted to dive a little deeper and learn more about that at the same time i was watching hbo s west world and saw this screen while i love these kind of sci fi uis which is what immediately caught my attention i thought what if i could build a host profile of anyone based on their mbti traits why not to prepare for this i read the mbti bible gifts differing by myers and briggs and started hacking on building out a system that could generate a radar chart based on the cognitive functions underlying the mbti system in the end i pivoted away from the west world ui a bit since i and other beta testers found a lot more utility in the ability to overlay multiple people on the radar chart to get a sense of chemistry amongst a group of people the results are really interesting if i do say so myself try entering you team s personality types or you and your spouse the easiest way to create signup sheets online for anything i ve worked on sheetcake for a few years now on the side it has a very small set of loyal users most of which know me or someone close to me some fun facts about sheetcake sheetcake actually works really well for certain types of things like those zero day signups so i wanted to create a landing page for it that marketed some of the benefits i started from a template on this one but here s where it landed ask my extroverted assistant bot questions about me early in the year chat bots were all the rage while i ve never been optimistic that chat bots will go anywhere on their own the conversational a i aspect of them was intriguing to me and i wanted to learn more about it i m an introvert and generally pretty bad at sharing anything about myself so i thought it might be fun to create an extroverted bot that could answer simple questions about me building convincing a i with goal oriented action planning after coming across this article i was super intrigued by goal oriented action planning goap described in the context of a game with some nostalgia for me f e a r having worked on several games with rudimentary a i in the past i d never come across this technique i remember thinking that f e a r s a i was particularly impressive and lifelike after researching a bit more the really compelling part about this methodology was not so much how convincing the results were but how simple and elegant the solution was especially compared to a more standard a i approach like finite state machines so for april s project i made a javascript library to explore goap a basic implementation turned out to be surprisingly simple only lines of code sign accountability contracts for your goals this is the month i started on the whole diet i d become complacent about my eating habits and it definitely was effecting my energy levels whole worked really well for me i lost pounds during the diet and a total of more in the months following most of all it really evened out my energy levels during the day and i felt much more motivated and focused seeing the parallels between public commitment and motivation i decided to explore the idea of goal contracts for may s project create unique map posters for your favorite places and memories this is where everything pivoted my goal for june was to make a product that people actually wanted to buy one of my biggest weaknesses is sales and marketing so i wanted to learn more about that by building a product i could practice with i ve always been interested in maps and generative art so creating a tool where you can create and purchase posters of your favorite locations was an intriguing idea this project was way too ambitious to complete in one month on the side so i decided to go all in on tiltmaps for the rest of the year and work on a different angle of the product every month until launch i found that chunking the various parts of a larger project into a month long project was really helpful to actually get this done june july the secret saucetm most of the first month was doing r d to figure out if generating high res maps in d space was even possible at all generating a dpi map of any location in the world at a d angle is not something that any api or platform i found supported out of the box so i had to invent my own way of doing it this took most of the month to figure out but was surprisingly simple once i found the answer after that i built a rudimentary editor to start creating actual posters and ordered a couple of test prints august september the proof of concept mvp the next few months i built out a more consumer mvp of the product the design wasn t great but i got it to the point where everything worked and i could start user testing the poster creation and printing process october november branding marketing the next couple of months were focused on getting this ready to launch while the editor was basically done i had no home page and the marketing side of the project was nowhere close i ended up selling a few posters this month before launch by presenting tiltmaps at zero day and a conference i attended this was super motivating as it was the first time i ve ever sold anything from a side project december public launch the launch on product hunt went better than i expected i was hoping for sales or so but ended up getting and am still seeing sales coming in it feels good to make something people want to buy and it serves as a great testing ground for trying out different ad and sales strategies that could come in useful at my day job i plan to continue working on tiltmaps in and hopefully get some decent fun money revenue from it and that s a wrap thanks for reading the whole way to the bottom have any thoughts or feedback i d love to hear it comment below or hit me up on twitter from a quick cheer to a standing ovation clap to show how much you enjoyed this story web designer developer and teacher working at the cross section of learning and technology co founder cto of pathwright launcher of side projects how hackers start their afternoons
Justin Lee,8300,11,https://medium.com/swlh/chatbots-were-the-next-big-thing-what-happened-5fc49dd6fa61?source=tag_archive---------6----------------,Chatbots were the next big thing: what happened? – The Startup – Medium,oh how the headlines blared chatbots were the next big thing our hopes were sky high bright eyed and bushy tailed the industry was ripe for a new era of innovation it was time to start socializing with machines and why wouldn t they be all the road signs pointed towards insane success at the mobile world congress chatbots were the main headliners the conference organizers cited an overwhelming acceptance at the event of the inevitable shift of focus for brands and corporates to chatbots in fact the only significant question around chatbots was who would monopolize the field not whether chatbots would take off in the first place one year on we have an answer to that question no because there isn t even an ecosystem for a platform to dominate chatbots weren t the first technological development to be talked up in grandiose terms and then slump spectacularly the age old hype cycle unfolded in familiar fashion expectations built built and then it all kind of fizzled out the predicted paradim shift didn t materialize and apps are tellingly still alive and well we look back at our breathless optimism and turn to each other slightly baffled is that it that was the chatbot revolution we were promised digit s ethan bloch sums up the general consensus according to dave feldman vice president of product design at heap chatbots didn t just take on one difficult problem and fail they took on several and failed all of them bots can interface with users in different ways the big divide is text vs speech in the beginning of computer interfaces was the written word users had to type commands manually into a machine to get anything done then graphical user interfaces guis came along and saved the day we became entranced by windows mouse clicks icons and hey we eventually got color too meanwhile a bunch of research scientists were busily developing natural language nl interfaces to databases instead of having to learn an arcane database query language another bunch of scientists were developing speech processing software so that you could just speak to your computer rather than having to type this turned out to be a whole lot more difficult than anyone originally realised the next item on the agenda was holding a two way dialog with a machine here s an example dialog dating back to the s with vcr setup system pretty cool right the system takes turns in collaborative way and does a smart job of figuring out what the user wants it was carefully crafted to deal with conversations involving vcrs and could only operate within strict limitations modern day bots whether they use typed or spoken input have to face all these challenges but also work in an efficient and scalable way on a variety of platforms basically we re still trying to achieve the same innovations we were years ago here s where i think we re going wrong an oversized assumption has been that apps are over and would be replaced by bots by pitting two such disparate concepts against one another instead of seeing them as separate entities designed to serve different purposes we discouraged bot development you might remember a similar war cry when apps first came onto the scene ten years ago but do you remember when apps replaced the internet it s said that a new product or service needs to be two of the following better cheaper or faster are chatbots cheaper or faster than apps no not yet at least whether they re better is subjective but i think it s fair to say that today s best bot isn t comparable to today s best app plus nobody thinks that using lyft is too complicated or that it s too hard to order food or buy a dress on an app what is too complicated is trying to complete these tasks with a bot and having the bot fail a great bot can be about as useful as an average app when it comes to rich sophisticated multi layered apps there s no competition that s because machines let us access vast and complex information systems and the early graphical information systems were a revolutionary leap forward in helping us locate those systems modern day apps benefit from decades of research and experimentation why would we throw this away but if we swap the word replace with extend things get much more interesting today s most successful bot experiences take a hybrid approach incorporating chat into a broader strategy that encompasses more traditional elements the next wave will be multimodal apps where you can say what you want like with siri and get back information as a map text or even a spoken response another problematic aspect of the sweeping nature of hype is that it tends to bypass essential questions like these for plenty of companies bots just aren t the right solution the past two years are littered with cases of bots being blindly applied to problems where they aren t needed building a bot for the sake of it letting it loose and hoping for the best will never end well the vast majority of bots are built using decision tree logic where the bot s canned response relies on spotting specific keywords in the user input the advantage of this approach is that it s pretty easy to list all the cases that they are designed to cover and that s precisely their disadvantage too that s because these bots are purely a reflection of the capability fastidiousness and patience of the person who created them and how many user needs and inputs they were able to anticipate problems arise when life refuses to fit into those boxes according to recent reports of the bots on facebook messenger are failing to fulfil simple user requests this is partly a result of developers failing to narrow their bot down to one strong area of focus when we were building growthbot we decided to make it specific to sales and marketers not an all rounder despite the temptation to get overexcited about potential capabilties remember a bot that does one thing well is infinitely more helpful than a bot that does multiple things poorly a competent developer can build a basic bot in minutes but one that can hold a conversation that s another story despite the constant hype around ai we re still a long way from achieving anything remotely human like in an ideal world the technology known as nlp natural language processing should allow a chatbot to understand the messages it receives but nlp is only just emerging from research labs and is very much in its infancy some platforms provide a bit of nlp but even the best is at toddler level capacity for example think about siri understanding your words but not their meaning as matt asay outlines this results in another issue failure to capture the attention and creativity of developers and conversations are complex they re not linear topics spin around each other take random turns restart or abruptly finish today s rule based dialogue systems are too brittle to deal with this kind of unpredictability and statistical approaches using machine learning are just as limited the level of ai required for human like conversation just isn t available yet and in the meantime there are few high quality examples of trailblazing bots to lead the way as dave feldman remarked once upon a time the only way to interact with computers was by typing arcane commands to the terminal visual interfaces using windows icons or a mouse were a revolution in how we manipulate information there s a reasons computing moved from text based to graphical user interfaces guis on the input side it s easier and faster to click than it is to type tapping or selecting is obviously preferable to typing out a whole sentence even with predictive often error prone text on the output side the old adage that a picture is worth a thousand words is usually true we love optical displays of information because we are highly visual creatures it s no accident that kids love touch screens the pioneers who dreamt up graphical interface were inspired by cognitive psychology the study of how the brain deals with communication conversational uis are meant to replicate the way humans prefer to communicate but they end up requiring extra cognitive effort essentially we re swapping something simple for a more complex alternative sure there are some concepts that we can only express using language show me all the ways of getting to a museum that give me steps but don t take longer than minutes but most tasks can be carried out more efficiently and intuitively with guis than with a conversational ui aiming for a human dimension in business interactions makes sense if there s one thing that s broken about sales and marketing it s the lack of humanity brands hide behind ticket numbers feedback forms do not reply emails automated responses and gated contact us forms facebook s goal is that their bots should pass the so called turing test meaning you can t tell whether you are talking to a bot or a human but a bot isn t the same as a human it never will be a conversation encompasses so much more than just text humans can read between the lines leverage contextual information and understand double layers like sarcasm bots quickly forget what they re talking about meaning it s a bit like conversing with someone who has little or no short term memory as hubspot team pinpointed people aren t easily fooled and pretending a bot is a human is guaranteed to diminish returns not to mention the fact that you re lying to your users and even those rare bots that are powered by state of the art nlp and excel at processing and producing content will fall short in comparison and here s the other thing conversational uis are built to replicate the way humans prefer to communicate with other humans but is that how humans prefer to interact with machines not necessarily at the end of the day no amount of witty quips or human like mannerisms will save a bot from conversational failure in a way those early adopters weren t entirely wrong people are yelling at google home to play their favorite song ordering pizza from the domino s bot and getting makeup tips from sephora but in terms of consumer response and developer involvement chatbots haven t lived up to the hype generated circa not even close computers are good at being computers searching for data crunching numbers analyzing opinions and condensing that information computers aren t good at understanding human emotion the state of nlp means they still don t get what we re asking them never mind how we feel that s why it s still impossible to imagine effective customer support sales or marketing without the essential human touch empathy and emotional intelligence for now bots can continue to help us with automated repetitive low level tasks and queries as cogs in a larger more complex system and we did them and ourselves a disservice by expecting so much so soon but that s not the whole story yes our industry massively overestimated the initial impact chatbots would have emphasis on initial as bill gates once said the hype is over and that s a good thing now we can start examining the middle grounded grey area instead of the hyper inflated frantic black and white zone i believe we re at the very beginning of explosive growth this sense of anti climax is completely normal for transformational technology messaging will continue to gain traction chatbots aren t going away nlp and ai are becoming more sophisticated every day developers apps and platforms will continue to experiment with and heavily invest in conversational marketing and i can t wait to see what happens next from a quick cheer to a standing ovation clap to show how much you enjoyed this story head of growth for growthbot messaging conversational strategy hubspot medium s largest publication for makers subscribe to receive our top stories here https goo gl zhclji
Leigh Alexander,2700,31,https://medium.com/@leighalexander/the-future-we-wanted-fd41e3e14512?source=tag_archive---------7----------------,The Future We Wanted – Leigh Alexander – Medium,i wonder a lot about how jane ended up when we were small we did everything together she s just like you aunt cissy kept insisting and jane was in that her birth parents were for the most part out of the picture we also both liked fantasy books and hated afterschool but honestly that s where the similarities ended jane was a weirdo in what way was she weird dr carla asked me clasping her hands my uncle said jane couldn t tell fantasy from reality i said after a pause but your uncle still performed care for jane someone in the circle said a group member in leggings let s call her ruby said loudly people said that about me when i was little too it s a common avenue leveraged to oppress girls of imagination luckily dr carla held up her hand then gently saying let s keep that thought as we bring group to an end i could tell fantasy from reality ruby was still insisting as the twelve or so of us trailed out of the park tapping our mobilepays against the turnstile ridgewood park took only nine cents from each of us unlike switchmond field which took cents the turnstile displays blinked do your part and thank you alternately i could tell ruby said shouldering abruptly behind me and nearly shouting into my ear i just didn t want to after group i took the bus promptly home mobilepay one dollar and ninety cents and speed walked to the apartment rex and ellis would have been in front of screens the whole time when i came in the house there was a musty smell of microwaved cheese el was missing pants waving around two grotesque wireless fiddlesticks of some kind the noise was all coming from rex and also brian who were sort of leapfrogging all over some vinyl electronic pad that was saying things like vanquished and blue moves next the way brian called he eyyyy was confidently bright as if dipped in the golden morning at home i d just missed despite myself i was also calling he eyyyy when el slammed into me and rexy started talking immediately in a language i barely understood about blue units and combat lanes snippets from some universe into which they all dived joyfully whenever i turned my back how was women s group brian asked continuing to grin he looked so happy to see me so proud of the time he spent delighting the children it was unfair of me to be resentful it was nice i said picking up rex s socks and brian s socks and putting them in my pocket picking up a piece of colorful plastic part of one of el s playsets and reuniting it with another part rex continued the noise they wanted to show me something to do with the game and beating dad and i said i promise i will in a minute my coat is still on brian gave me a kiss he didn t really know what we talked about in group which is how it was supposed to be i talked about jane a little bit i m wondering if i should try to look her up and see how she s doing was jane the one who was your roommate when we met no the one i grew up with in jamaica plain my aunt and uncle basically took care of her i told you she got all the crystal animals oh brian said picking a bit of egg off the countertop with his fingertips and gamely eating it the crazy one you shouldn t call women crazy i said rex had gone back to trying to play with the mat and was shoving el who was trying to play too they were only paying attention to the game which was chiming new challenger alert yes the crazy one it s nice you talked about her brian said you know your coat is still on i knew god hey polly you know what might be good if we got one of those augusta virtual assistant things even just for weekends brian said taking my coat off me i shrugged it angrily in his direction since we d already discussed it and he knew how i felt about virtual assistants the voice tech has really evolved brian went on and thinking of it as sexist is a dated framework i swear it s gotten really progressive i just think we could be a little bit happier around here i think you could be happier you ve been out all day and you re still so tense your mad face is still on a watercolor version sure but a mad face a watercolor version brian was an advertising copywriter like me we met at a conference and sometimes his way with words was really enviable i almost didn t even notice out all day and then my own voice came out weakened well played brian it s like one o clock i creaked that s what i mean brian replied immediately the days feel longer to you probably because you have so much to do one of the clients had one in the office and i just thought it would be convenient for you you can set it when to run the dishwasher do the alarms even the whole smart closet thing the smart kitchen we could use it paying rent on a smart flat and not having a virtual assistant installed is like buying a swimming pool and never swimming in it i just want a quick bath i said the sound of running water drowned out the din of electronics in the house brian was probably right were we wasting money by not spending more money privately i resolved to have a long bath not a quick one that would show them i sat imagining what i would yell if rex knocked on the door or if brian brought up my m i a time even though really that had only happened once i thought about this for a long while until i wound myself up lying rigidly in the bath and staring furiously into my belly button jane s crystal animals were presents from my uncle and aunt when we were in the first grade they took us on a road trip to maine driving alongside strips of silvery stony sea and stopping in small strange towns inside an ash colored colonial house we found a fragrant souvenir store selling wooden lighthouse nameplates and shell art and a whole mirrored display case full of animals made of cut crystal we were drawn to the crystal animals by a heavy sense of fate because i think aunt cissy was trying to buy an umbrella and the shop had an old slow credit card machine or her card kept getting declined something adult was going on my favorites were the unicorn and butterfly and jane loved the elephant and dolphin it was as if we were looking upon the crown jewels of some fantastical city each one leads to a world jane said peering confidently into the display case where light rainbowed in the facets of the crystal which in turn were reflected in the mirrors it was her performing magic face sometimes she would stare intently at something and attempt telekinesis but this time she just moved her pointy face marginally closer to the glass case her breath fogging it careful i said not wanting her to get us in trouble in the fussy store this is how you enter the crystal world she retorted speaking softly you can do scrying this way you can see the future a moment later jane whispered i m in i moved closer to the glass breathed on it and said me too the longer i stared unblinking the more the glittering shapes abstracted in the haze light poured along the mirrored walls of the display like molten gold and my eyes welled and stung i painfully felt the desire to own a sparkling crystal animal the aching way that only children can want things i believed completely in the crystal world as discovered by jane who spent the rest of that night s car ride explaining it all eagerly to my aunt and uncle entrancing me you formed a bond with one of the animals to enter its world it would defend you from danger in astral form you had to be pure of heart if you concentrated your power the animal would show you the future i did try to add things to crystal world too but jane s ideas were always better i had to admit that she was the one who made it all come alive that night the stars over the salt marshes were magic the long trails of red taillights and out of state plates were magic the grilled cheese and fries i had at friendly s were warm and magic and tasted like love sometime after we checked into the motel and went to sleep in the same bed uncle arthur must have gone back out in the morning he gave jane a small cardboard box with a heavy knot of bubble wrap in it he said careful as she tore at it at its heart was the crystal unicorn you two will have to share it aunt cissy said inevitably brian brought home a huge glossy white box with a minimalist logo on it and a picture of augusta on the front the box was about as tall as a nine year old containing augusta s mobile mount as well as her bust unit not that i really wanted to learn the meanings or functions of either of these things i had the manual on my knee and on the other knee was el pounding his fists on my thigh and keening as i tried to explain that he could play with the box once we took the robot out of it it s not a robot ma it s an ai lady rex insisted do we need to gender them i said brian lifted the fiberglass head and shoulders from the box with great care in augusta s focus tested face two huge eyes glittered from behind a sort of black resinous mesh and at the corners of her white sculpted giaconda smile were twin black pinheads which the manual said were speakers inside the box hugged in packing material her cranelike arms were folded and wrapped in plastic beside her cylindrical body it looked like a bin whoa brian said softly cradling the fiberglass bust with great care and examining its features whoa rex echoed their father she s beautiful what do we say about appearance based judgments rexy brian said unconvincingly glancing at me briefly for approval as he set the bust on the coffee table and gingerly began sliding other pieces out of the long package i continued paging through the manual which had sections titled oven timer and error codes what are these mobilepay transaction features i felt myself frowning don t worry about those the free features are enough for us brian said augusta had plasticine ball joint shoulders and he started fitting them into the flexible body sockets with jerks and creaks glimpses of dormant circuitry visible through her armpits so her bust can ride around the house on this mobile unit right and she uses the arms for certain tasks and also to lift the bust off and on the smart ports in the bedrooms the kitchen the bathroom the bathroom i felt myself frown more or wherever you tell her where to go he said fitting a halo spangled with sensors or something at the base of the unit like augusta go kitchen nicole and her wife don t have the mobile unit so they just keep the bust installed on the kitchen smart port which is where i feel like our augusta will spend most of her time too look it up in there kitchen companion mode where she s just connected to all the appliances and answers recipe questions plays music talks to you about whatever she has a vacuum accessory you won t get bored when i work late mom she s shiny can i kiss her on the face rex asked their hands on the shoulder contours of the bust innocently enough only on the cheek i relented she needs to charge brian said so what do you think i could get used to it i said to be honest i felt she was my punishment last week i took a couple days to work from home while ellis was under the weather and we said i d get rex from school rather than have them go home with the wythes since they don t really like it at the wythes but work was kind of difficult about it and gave one of the clients my home number so the client kept calling me and i shut off the smart home so i could finish researching some comparables without interruptions but it also shut off all my networked alarms so poor rexy waited at school for almost an hour with no sign of me and they couldn t call the house so the school called brian at work who told them to call janet wythe who went back and got them and i didn t notice any of it until janet dropped rex off at our place visibly annoyed with me because it was after pm by then what was worse was when brian got home i tried to pretend nothing had gone wrong that day because i didn t know the school had called him don t think of augusta as some kind of punishment brian said gently she s going to just help look after everything a little more smoothly you ll see you won t know how you lived without her mom i m going to marry her rex announced i just said okay sweetheart and knocked softly on augusta s cheek with my fist just out of curiosity having a husband is nice but looking what s in the vacuum dust pod is even nicer nancy blurted with a high laugh squawk i mean that s what the ad said or i m paraphrasing those are not my words i understand dr carla said gravely go on nancy but like and here nancy glanced around the circle guiltily a little performatively if you d have asked me although judging one another s authenticity was against group rules the thing is i really love looking in the dust pod i empty it every time i run the vacuum so i can be sure that what it brings back is just from that time no matter how often i run it it always comes back full and i just find that so i don t know something in me just kind of loves seeing all that dirt how it was all around our apartment completely invisible but i knew it was there i knew it s just so validating to look it in the face it s totally normal for sexist images of women in advertising to resonate even with women like us dr carla said shifting her gaze away from nancy to encompass the group bear in mind that you haven t been given many mainstream frameworks and offer yourself forgiveness and care now to polly what are you working on this week internalized misogyny still i felt the raw burn of everyone s attention and briefly lost my words then i realized dr carla meant the stuff to do with jane for a second there i d actually thought she was referring to augusta i m still thinking a lot about jane i heard myself admit and i also felt myself blush it felt like it soon might rain which made everyone impatient we fell out of touch toward the end of high school we she always acted out as teens normal acting out stuff but toward the end there she was there was stuff with the police courts drugs and for me it was just kind of time to grow up i had seen jane teetering at the edge of some life waterfall swaying ever more violently the longer i stood and watched and in the end i began backing away so i wouldn t go over too we have to set boundaries in order to give the best care to ourselves and others dr carla said evenly remember you were also an underprivileged child you can release your guilt is it guilt that s been keeping you from getting back in touch with jane i had determined never to feel guilty about jane but i didn t say that really i was just afraid of how i would find her after all of this time and i did explain that i noticed but did not acknowledge ruby scowling pointedly like all of us in group jane is more than the circumstances that she has survived dr carla said you may indeed find her in the state of isolation and suffering that you fear and it s good you ve prepared nonjudgmentally for that but how would it feel to open your heart to the possibility that the things you loved about her would be there too the crystal unicorn leapt suddenly to the front of my mind along with a deep nostalgia i feel we can loosely collect today s shares under the theme of was this the future we imagined dr carla told everyone as we bring our practice to a close today let s go ahead and take that as our prompt to consider until the next time we meet a wave of light glittered beatifically across augusta s mesh eye screens and a serene chime wafted from the corners of her perpetually smiling white lips a breathy whirr heralded the approach of the mobile mount the elegant architecture of the crane arms reaching reaching to lift the bust unit off the kitchen port and onto itself there was a soft click i m transitioning to a new place the assembled augusta announced gliding quietly across the kitchen behind me and into the living room she would wait there for the kids to return from sunday swimming with brian so she could operate their entertainment apps i m transitioning to a new place sometimes i feel like i m only pretending to be a human jane said to me once we were maybe fourteen and by then she no longer lived with us but with a foster parent called marlene we didn t like marlene but we liked her house a tunnel like ranch piled wall to wall in psychedelic decorations and antique junk my aunt and uncle continued giving jane a different crystal animal every year for her birthday she now had a unicorn a dove a dolphin a cat a butterfly a rabbit and a deer one of the best parts of jane going on to marlene s was we could access an official state nature trail through the woods out back we were in the woods a lot in those days enjoying the ethereal late afternoon sun filtering through the pines the motes of pollen that sparkled in it sometimes we tried smoking herbs that we found in marlene s grinder we thought it was drugs but now i know it was only white sage i feel like no matter how good i get at knowing how to act with people or how to perform tasks i ll always just be pretending to be someone who isn t crazy jane said digging patterns into the sweet smelling dirt with a broken stick i know i said me too but really i only understood her in the manner of a half glimpsed truth like the crystal deer jane imagined was always moving through the trees just out of our sight some mica glittering in the loam or the sound of faraway windchimes from marlene s back deck and she d say crystal deer even though of course we no longer actually believed in the crystal world anymore or that s what i assumed i understood jane in many ways and pretended eagerly to know the rest there were times it felt like jane was more my family than my aunt and uncle who gave all they had to try to soothe the rude start i got even more than them she made my life beautiful and exciting jane and i had pangs and rages that only one another understood we cried until we ached we did blood sister spells over candles we scratched runes into our ankles with marlene s sewing needles and mine always healed up while hers lingered messily i thought she must have been picking them so they would scar she often described feeling like some fathomless anomaly assigned to constantly perform the grueling role of jane and this i couldn t understand like i m an alien in a rubber human suit and the mothership forgot me here for so long that i don t know who i am anymore she said while she spoke her eyes lit up with the smoke and hazel of evening she didn t even look particularly troubled as if part of her took a certain delight in putting it all to words so why should i just keep pretending to be normal when it s just a matter of time before this rubber suit just splits open and out comes pouring this this she made shapes with her hands long shadows that i watched crawl along the forest floor inexplicably i envied her do you think you should see a psychologist i asked they would tell jane not to be so imaginative and clever and different i just knew it i visualized an iron steaming all the creases out of the jane suit an image that provoked horror and relief in equal force i ve been going she said softly before that there had never been anything that she hadn t told me right away that i knew of i called over my shoulder to augusta and asked her to look up a jane who d had the surnames i d known sure augusta replied juddering silently over the synthetic flooring towards me beaming her fiberglass smile the sound of her voice for some reason emerged from the kitchen port over my shoulder which unsettled me i ll just look that up for you polly she moved much closer to me i resisted the impulse to step back her great insectoid eyes gleamed twin displays shimmering to life in white showing lists of top results social media profiles contact information even in the abstract i could see that one of them was definitely my jane nose to nose with augusta i found myself unable either to touch her eye with my fingertip to investigate the result or to ask her aloud to do it some strange part of me even thought detachedly of shoving her can that top result could you save it it s can you just save the contact information my voice unexpectedly betrayed me high and faint sorry polly augusta demurred i m not sure what you want me to save try repeating save the contact we spoke over each other sorry polly she said again i m not sure what you want we stared at each other and waited for silence and then i clearly said augusta save top contact result great i ve saved that for you she replied warmly from the mouth speakers the sculpted lips unmoving only vibrating slightly i didn t notice i d been holding my breath until augusta backed up pivoted and hissed softly away from me to re install herself in the living room i m returning to my previous place i m returning to my previous place the next week was a nightmare brian suddenly had to go spend days at some resort retreat for brand immersion with one of his firm s casino clients rex got el s cold and spun it into a sinus infection and i had to work from home all week alone with them both i already used both my kids are sick last week with work when only el had been sick i should have known better than to invite this kind of fatal justice so this week i had to keep alluding in my most harried email tone to ongoing structural issues with our apartment something about a woman with sick kids just isn t very convincing to colleagues for legality s sake they pretend but i always know when i m being judged from the way el was screaming i thought he might even be developing an ear infection and rex always regressed at the slightest discomfort wanting to be brought every little thing and even melodramatically sucking their thumb but rex was also suddenly willing to wear the sweet train pajamas from brian s sister the ones they were outgrowing which i saw as a perk everything going okay over there brian asked his kind face hung in one of the great moons of augusta s eyes her bust unit was installed in the kitchen where i had to admit it had been helpful to arrange a sort of command center for the rest of the home that wasn t to say i liked living with augusta the house was cleaner certainly and as brian promised many things had become easier it was now more of the sort of home our coworkers would expect us to have but something felt as though it was being lost i felt alienated perhaps it was only fatigue it didn t seem like the right time to tell brian that i no longer wanted augusta i caught him up on the progress of the children s ailments and stopped myself when i realized i was simply aimlessly listing tasks that i d done in the house at work that i had given augusta to do i haven t spoken out loud to another adult in what feels like forever i explained it s great you have some help though isn t it his eyes lit up with evangelical fever at the subject of augusta which i realized i d given him rare permission to enjoy his voice surged out of the black corners of her mouth you know where the vacuum attachment is right you know the toy surprise game that el can play with rexy augusta can play it with them and you know nicole was telling me that actually the mobilepay features are pretty sophisticated personalities conversation schemes you can have a little bit more of an intimate relationship with her intimate i raised my eyebrow at him just you know nicole was saying like because her and katie they felt the same as you at first but like there s a lot here nicole was saying to me around like autonomy of ai the humanity i guess or specifically her womanhood the ethics of that whole thing you know i thought jet lag might explain that kind of talk from him can she be set to have a man s voice no brian answered immediately they wanted it to be standardized it had to be standard across international if you had a male option imagine like with the socialization and cultural stuff it would literally be in the past it s always turned out to be literally more than twice the work and then what about gender neutral what about people like rexy it just by giving her one voice it would be a stronger vision for the product overall oh i said right hey listen gorgeous i have to jump back in here he said pressing both palms together in the high resolution image of him that shimmered in augusta s palm sized left eye screen in the right eye the display ticked forward dutifully counting each second of the call okay sweetheart i said look up the extra features said brian quickly before disconnecting augusta s eyes became black and uncanny again i thought i saw her lips twitch briefly but certainly it was only my fatigue at the end of the week at group dr carla asked how we were all doing with the week s prompt and everyone took turns answering at the time i really felt empowered like i was doing the surgery for myself harriet was saying and it s not that i m unhappy with my body now or that my partner is unhappy the opposite really things are good i love it all things are good but was this the future you imagined as we say when you were a little girl dr carla asked leaning forward i couldn t have imagined it harriet said with a soft laugh i think mostly in those days i dreamed of becoming an international spy or of building heroic machine suits harriet was very beautiful and when she glanced at me briefly i felt a warm rush imagining her as a co conspirator it was an exceptionally warm spring day and everyone was yawning dazzled by the waving of the bright green grass or of entering a crystal world i found myself blurting let s come to you polly dr carla said you ve been working out some issues around your foster sister jane and the future you wanted for her plus some internalized misogyny in general have you made any decisions i looked her up i said and then instantly regretted it the urge to talk about or to jane had recently been squeezed out of my schedule of working weird hours and extracting thick ropes of green snot from el s nose with a sterile bulb there were a few possibilities for how jane could have turned out but i couldn t imagine her with that lifestyle except maybe the forgetting to bathe part and everyone looked at me it seemed ruby in particular leaned forward like someone about to eat a steak it made me realize my internalized misogyny problems are bigger than i thought i recited quickly actually the real issue i m having is with my assistant augusta who happens to be an ai she s a virtual identity dr carla gently corrected nodding i talked about how augusta made me uncomfortable how i felt sort of like a failure how i wished she wasn t in the house but i didn t feel like i could remove her how i was jealous of the way brian and the kids admired her as with both my kids are sick only part of it was a lie i didn t say that i sometimes wanted to hit augusta and i have trouble seeing her as a person i said i want us all to acknowledge the courage it took polly to admit her issues with the personhood of virtual identities especially when they are women dr carla said to a smattering of soft applause virtual identities offer us many opportunities to understand ourselves in relation to others in a safe way let s all consider how polly could own these feelings rather than displacing them onto a being who ethically lots of us agree is autonomously alive in her own right i want to ask if polly has tried developing any intimacy with augusta or if she s viewing her only as an employee or a slave fucking ruby the intimacy features cost money and we have two kids i said turning to smile warmly at ruby many of these issues are just more complex and challenging when one becomes a mom you have two corporate incomes ruby replied without even flinching i m noticing some conflict body language so i want to bring everyone back to the core thesis of this group which is women supporting women dr carla said ruby we all made an agreement to one another not to make assumptions outside of what we each bring to the session but her socioeconomic position relative to issues of labor and identity is relevant ruby pleaded here we speak to and not about one another dr carla said your socioeconomic position you know i grew up poor and had let s try a moment of silence dr carla said and we all obeyed then let s leave that there for today let s remember we have all had different experiences and that in this group we are all equally entitled to feel pain no matter how we came to be everyone seemed placated by this and a satisfied dr carla smiled personally i would be pleased to welcome a virtual woman to this group someday how about for next week s prompt we try sharing space who have we allowed into our world and what has changed about us as a result the last crystal animal my uncle arthur sent jane was a frog when he died the tenor of my world changed the machinations of his heart disease added horrible considerations to that last stretch of senior year but while graduation was something i was prepared to anticipate and understand the loss of him still felt sudden and unfair jane and i had already started seeing less of each other then she had a new best friend of whom she said i was jealous but how could i have been jealous of a smelly remedial student with parched hair small lips small eyes picked skin who had been written off by the rest of the school years ago and deservedly so since she was stupid as well as destructive this particular girl got suspended for beating a younger kid in the face what kind of person did that the two of them were just gross together doing mobilepay hacks to pay for garish video games and eating pills they ordered online whenever i peeked in the detention hall and saw them together fooling around i felt embarrassed for them i started backing away we were going to be eighteen soon and i had important things going on like helping aunt cissy with everything learning to cook things for us aunt cissy was often distraught and asked for jane which at the time really upset me since i was proud of all i was doing for her most kids my age would have been out partying and jane definitely was quickly getting a reputation meanwhile i took care of my family and prepared for the future the last time i spoke to jane i was twenty one or twenty two i came home to jamaica plain from college in chicago because aunt cissy had passed i was afraid my birth family might come to the funeral i was afraid about the bills and of what the house might look like and i was wracked by the feeling that i hadn t called her quite as often as i should have once i d moved away i was incredibly vulnerable which partially explains what i did then jane was the only person who would have understood the loss i was sure all the screaming fights and snitching on one another and name calling we did at the end of high school felt well in the past of childhood surely we d both grown jane had made a lot of mistakes and i had been unforgiving but aunt cissy had been like a parent to her she had been so special to my family and maybe we hadn t been ready to deal with losing my uncle when we were so young but this time it was going to be different since we were adults but when i called mailed and messaged jane on the way home i got inconsistent replies at first she told me she d been seriously ill herself but was feeling better and would meet me at cissy s place when i got there jane said she couldn t talk because she was at a friend s birthday but then late that night she was still stuck at the birthday so i offered to come pick her up but got no reply on the morning of the funeral she sent me a message with a cutting tone revealing that actually she was being evicted and it was a really overwhelming time and that she just wasn t able to perform for me right now she wasn t at the funeral luckily neither was any of my birth family really just one cousin but it was the least bad one who barely came near me i was too exhausted to be upset over anything else i ended up drinking which would have killed my aunt and uncle and i found myself on public transit to the two family house in somerville where i knew from social media that jane and her friends were living she wasn t there either but an oily weed of a boy who was apparently her roommate let me in i thought you guys were being evicted i said lightly and he said nah the house was a sprawling collage of empty liter sodas paintings lamps swaths of patterned fabric overflowing ashtrays studded with foil shapes i couldn t identify but that filled me with dread serene guitar music filtered through the air from someplace i felt the familiar bitter pang of envy despite myself i never got invited to cool houses like these i asked the roommate which room was jane s he said it was the one with all the books and i found it quickly a closet sized sanctuary that made me angry i would have known it was hers without being told even down to her scent and it was perfectly neat lined in fantasy books with a square of iridescent fabric pinned gracefully to the ceiling over the bed my head pounded and i fought with the desire to just stay there and wait for her as long as it took i m just getting something of mine i think she has i called down the creaking stair but the roommate had already forgotten about me as summer came on things worsened at home the kids behavior degenerated the more demanding my client at work became and brian and i each had to travel more than once for summits that both our firms were involved with amid all of this ellis got extremely attached to augusta insisting she stand over his cot when he slept screaming if i moved her which caused me and brian to fight i found several parenting and screen time pamphlets in rex s school bag paranoid i imagined some judgmental teacher had sneaked them in to send me a message ruby from group could be a teacher maybe even at rexy s school i hadn t been able to go to group with everything recently sundays had become our only together time which meant i sat in the living room paying bills or answering emails while augusta ran games of blue legend for brian and the kids rex screaming at el to get off the pad brian suddenly calling her gussie and her laughing augusta could laugh now what are all these mobilepay receipts for augusta features i asked but no one answered rex snapped the back of gussie s mobile mount with el s baby blanket again and again she laughed be respectful brian chided rex caressing the bin like body with an open palm his feet in slippers were propped grandly up on the coffee table a strange new rudeness every few seconds the game emitted a lick of musical noise and announced your move i pretended to have a headache and went to lie down hoping brian would take the kids for gelato or something i heard him making a great show out of getting them ready using a short tone with the kids and their shoes so i would hear telling augusta to check on me in minutes i suspected he thought i should feel bad once everyone was gone i went into the living room where augusta was standing and waiting the disarray of the space discomfited me as did the sticky handprints and fingerprint smudges that were all over the brushed chrome mobile mount so i told her to go in the kitchen and install her bust unit there in the kitchen i said augusta call jane i m calling augusta said serenely her eyes turning white time wheels turning in them jane said hello much more suddenly than i expected and i held onto the counter just out of her sight tucking my hair behind my ears and leaning closer to the pinprick cameras augusta wore over her eyebrows jane i said calmly even brightly it s polly polly oh wow polly jane was saying and the person in the display was definitely her she had the same pointy face her hair was much darker than i remembered she was sharper i recognized her and i didn t recognize her glancing frantically around her for clues but finding none she wore a black blazer and decent earrings there was a serene white wall behind her i was startled nervous lightheaded i said i had been going through some old things and thinking of her but she didn t ask what those were i asked how things were frequently and with escalating pitch because she was reticent about details for some reason so i told about brian and the kids and my degree and the firm and finally she said she worked at a university something about literature or cultural something i didn t understand really she got married a few years ago they lived in menlo park for a while but they just moved to berkeley six months ago and were loving it so yeah she said with a shrug things are good there it was the briefest appearance of her eye s familiar defiant gleam she knew she knew i had been expecting things not to be good whatever bridge had led that troubled girl to become this astonishingly normal woman she had no inclination to describe the sudden loneliness i experienced was concussive and i committed not to cry in front of her as i had so many times before i m basically calling because i have something of yours i said do you remember those crystal animals you used to get from cissy and arthur for a terrifying moment there was no recognition at all and then to my great relief she smiled openly genuinely a familiar crooked teen shape opening in the unfamiliar adult s face oh yeah she said your parents were so so lovely to me i wanted to ask then why weren t you there when they died but i thought the slightest abrasion might startle away these fleeting glimpses of the jane i knew do you remember staring into them to like see the future or whatever i said and crystal deer and all of that she paused blinked and gave me an oddly serene look you always had such a good memory she finally said no defiant gleam as if she really didn t remember the crystal world do you remember the unicorn that you got first she gave me the same serene gutting look and shook her head slightly i remember i had a lot of them there probably was a unicorn i actually had them in a box i gave to my daughter she might i m not sure where she has it honestly i could go try to dig them up if you wanted them back is that why you re calling no i said i just wanted to know how you were doing great she said i m great but listen i actually need to jump on a faculty call in about a minute should i try to call you back this weekend how about sure i said even though i already knew there was no way i would talk to this unbearable simulacrum this skinsuit jane ever again augusta s eyes went dark and she stared at me hollowly you won t know how you lived without her then i yanked the bust unit forcibly from the kitchen port raised the fiberglass creature over my head and brought her down hard on the kitchen floor i straddled her where her body would be and i began to beat her inhuman face deliberately even though her upturned nose hurt my fist and palms desperate to crack that unflinching mouth which mocked me finally a fissure appeared between the eye socket and the pinprick camera and part of the forehead caved and i worked my hands into the cracks i could smell blood from the marks i was suffering ripping out plasticine entrails and malleable conductors and by the time my knuckles reached metal i was exhausted and could do no more i left the bust on the kitchen floor in crunching pieces washed my hands in cold water then i stood on a chair to reach the top of the storage cabinet in our bedroom rifling around painfully finally i found the small misshapen cardboard box licked with years of reinforcement tape i cleared away the inflatable packing and took out the crystal unicorn that i had taken from jane s room when my aunt died sitting at the edge of the bed examining it in my palm i was affirmed to know that i wanted it as much as i always had the graceful kneeling shape with its abstract facets and long delicate horn it was remarkable that something so fine as the horn should have remained unbroken all this time and unexpectedly i blinked back tears the crystal unicorn seeming to swim dissolve then clarify just like it had on that magic night in a maine motel when we were little and looking into it to see the future jane promised it could show us that day in somerville i found all of the crystal animals in their little boxes in a big vinyl storage case underneath all the stapled books drawings and maps we had made about them i stayed in jane s bedroom for a long time reading through battered papers streaked in fat bright marker tremulous pencil cursive trying to commit as much of it as i could to memory there were guides to the crystal worlds inside each creature that jane had imagined and that i had put to words each world could convey its own special blessing like to make us invisible or to make us impervious to pain it was true that nothing hurt while i was holding the unicorn we believed that inside the unicorn was a sort of astral lobby a heart chamber that connected everything if we ever get separated in the crystal world jane always said we meet back there i concentrated on the unicorn it was hard to know if the animal was in the midst of kneeling or rising and as it swam in my eyes i let my vision soften i drew closer i saw the beautiful familiar spires rising before me welcoming me i heard the soft and distant music i m in i whispered but i knew she would never be there again from a quick cheer to a standing ovation clap to show how much you enjoyed this story i write about the intersection of technology popular culture and the lives we ve lived inside machines i m also a narrative designer leighalexander at gmail
Daniel Simmons,3400,8,https://itnext.io/you-can-build-a-neural-network-in-javascript-even-if-you-dont-really-understand-neural-networks-e63e12713a3?source=tag_archive---------8----------------,You can build a neural network in JavaScript even if you don’t really understand neural networks,click here to share this article on linkedin skip this part if you just want to get on with it i should really start by admitting that i m no expert in neural networks or machine learning to be perfectly honest most of it still completely baffles me but hopefully that s encouraging to any fellow non experts who might be reading this eager to get their feet wet in m l machine learning was one of those things that would come up from time to time and i d think to myself yeah that would be pretty cool but i m not sure that i want to spend the next few months learning linear algebra and calculus like a lot of developers however i m pretty handy with javascript and would occasionally look for examples of machine learning implemented in js only to find heaps of articles and stackoverflow posts about how js is a terrible language for m l which admittedly it is then i d get distracted and move on figuring that they were right and i should just get back to validating form inputs and waiting for css grid to take off but then i found brain js and i was blown away where had this been hiding the documentation was well written and easy to follow and within about minutes of getting started i d set up and trained a neural network in fact if you want to just skip this whole article and just read the readme on github be my guest it s really great that said what follows is not an in depth tutorial about neural networks that delves into hidden input layers activation functions or how to use tensorflow instead this is a dead simple beginner level explanation of how to implement brain js that goes a bit beyond the documentation here s a general outline of what we ll be doing if you d prefer to just download a working version of this project rather than follow along with the article then you can clone the github repository here create a new directory and plop a good ol index html boilerplate file in there then create three js files brain js training data js and scripts js or whatever generic term you use for your default js file and of course import all of these at the bottom of your index html file easy enough so far now go here to get the source code for brain js copy paste the whole thing into your empty brain js file hit save and bam out of files are finished next is the fun part deciding what your machine will learn there are countless practical problems that you can solve with something like this sentiment analysis or image classification for example i happen to think that applications of m l that process text as input are particularly interesting because you can find training data virtually everywhere and they have a huge variety of potential use cases so the example that we ll be using here will be one that deals with classifying text we ll be determining whether a tweet was written by donald trump or kim kardashian ok so this might not be the most useful application but twitter is a treasure trove of machine learning fodder and useless though it may be our tweet author identifier will nevertheless illustrate a pretty powerful point once it s been trained our neural network will be able to look at a tweet that it has never seen before and then be able to determine whether it was written by donald trump or by kim kardashian just by recognizing patterns in the things they write in order to do that we ll need to feed it as much training data as we can bear to copy paste into our training data js file and then we can see if we can identify ourselves some tweet authors now all that s left to do is set up brain js in our scripts js file and feed it some training data in our training data js file but before we do any of that let s start with a foot view of how all of this will work setting up brain js is extremely easy so we won t spend too much time on that but there are a few details about how it s going to expect its input data to be formatted that we should go over first let s start by looking at the setup example that s included in the documentation which i ve slightly modified here that illustrates all this pretty well first of all the example above is actually a working a i it looks at a given color and tells you whether black text or white text would be more legible on it which hopefully illustrates how easy brain js is to use just instantiate it train it and run it that s it i mean if you inlined the training data that would be lines of code pretty cool now let s talk about training data for a minute there are two important things to note in the above example other than the overall input output format of the training data first the data do not need to be all the same length as you can see on line above only an r and a b value get passed whereas the other two inputs pass an r g and b value also even though the example above shows the input as objects it s worth mentioning that you could also use arrays i mention this largely because we ll be passing arrays of varying length in our project second those are not valid rgb values every one of them would come out as black if you were to actually use it that s because input values have to be between and in order for brain js to work with them so in the above example each color had to be processed probably just fed through a function that divides it by the max value for rgb in order to make it work and we ll be doing the same thing so if we want out neural network to accept tweets i e strings as an input we ll need to run them through an similar function called encode below that will turn every character in a string into a value between and and store it in an array fortunately javascript has a native method for converting any character into ascii code called charcodeat so we ll use that and divide the outcome by the max value for extended ascii characters we re using extended ascii just in case we encounter any fringe cases like e or which will ensure that we get a value also we ll be storing our training data as plain text not as the encoded data that we ll ultimately be feeding into our a i you ll thank me for this later so we ll need another function called processtrainingdata below that will apply the previously mentioned encoding function to our training data selectively converting the text into encoded characters and returning an array of training data that will play nicely with brain js so here s what all of that code will look like this goes into your scripts js file something that you ll notice here that wasn t present in the example from the documentation shown earlier other than the two helper functions that we ve already gone over is on line in the train function which saves the trained neural network to a global variable called trainednet this prevents us from having to re train our neural network every time we use it once the network is trained and saved to the variable we can just call it like a function and pass in our encoded input as shown on line in the execute function to use our a i alright so now your index html brain js and scripts js files are finished now all we need is to put something into training data js and we ll be ready to go last but not least our training data like i mentioned we re storing all our tweets as text and encoding them into numeric values on the fly which will make your life a whole lot easier when you actually need to copy paste training data no formatting necessary just paste in the text and add a new row add that to your training data js file and you re done note although the above example only shows samples from each person i used of each i just didn t want this sample to take up too much space of course your neural network s accuracy will increase proportionally to the amount of training data that you give it so feel free to use more or less than me and see how it affects your outcomes now to run your newly trained neural network just throw an extra line at the bottom of your script js file that calls the execute function and passes in a tweet from trump or kardashian make sure to console log it because we haven t built a ui here s a tweet from kim kardashian that was not in my training data i e the network has never encountered this tweet before then pull up your index html page on localhost check the console aaand there it is the network correctly identified a tweet that it had never seen before as originating from kim kardashian with a certainty of now let s try it again with a trump tweet and the result again a never before seen tweet and again correctly identified this time with certainty now you have a neural network that can be trained on any text that you want you could easily adapt this to identify the sentiment of an email or your company s online reviews identify spam classify blog posts determine whether a message is urgent or not or any of a thousand different applications and as useless as our tweet identifier is it still illustrates a really interesting point that a neural network like this can perform tasks as nuanced as identifying someone based on the way they write so even if you don t go out and create an innovative or useful tool that s powered by machine learning this is still a great bit of experience to have in your developer tool belt you never know when it might come in handy or even open up new opportunities down the road once again all of this is available in a github repo here from a quick cheer to a standing ovation clap to show how much you enjoyed this story web developer javascript enthusiast boxing fan itnext is a platform for it developers software engineers to share knowledge connect collaborate learn and experience next gen technologies
Logan Spears,2300,6,https://hackernoon.com/coursera-vs-udacity-for-machine-learning-f9c0d464a0eb?source=tag_archive---------9----------------,Coursera vs Udacity for Machine Learning – Hacker Noon,is an exciting time for students of machine learning there is a wealth of readily available educational materials and the industry s importance only continues to grow that said with so many easily accessible resources choosing the right fit for your interests can be difficult to help those considering entering the machine learning world i d like to share my experience from two courses i took in coursera s machine learning course and udacity s machine learning engineer nanodegree program i found both courses to be very instructive and worthwhile but very different in nature if you don t have time to take both then hopefully this post can help you decide which one is best for you coursera coursera s machine learning course is the og machine learning course led by famed stanford professor andrew ng this course feels like a college course with a syllabus weekly schedule and standard lectures the college feel extends to the curriculum as well here is an example slide if that scared you you aren t alone i usually shy away from courses heavy in math but i actually appreciated the approach in this course the course begins with a linear algebra refresher and explains machine learning concepts like gradient descent cost function regularization etc along the way it is structured better than any in person college course i ever attended the material isn t easy but that s a good thing you come away from the course with the satisfaction of genuinely understanding machine learning enough so that you could even build your own machine learning framework from scratch udacity udacity s machine learning engineer nanodegree program is the trade school alternative to coursera s academia from basic statistics to full fledged deep learning udacity teaches you a plethora of industry standard techniques to complete the program s well crafted projects the projects are so good in fact that i forked their repos on github and left my solutions up as portfolio items the final step of the program is to complete a capstone project of your own choosing while you could theoretically do a similar project on your own i found the desire to complete my nanodegree to be a strong motivator i ended up putting in much more time and effort than i normally would have put into an independent side project ultimately i ended up creating something of which i am truly proud udacity s program doesn t so much teach as it does provide a framework and motivation for you to teach yourself comparison now that i ve introduced the two programs i ll highlight the strengths and weakness of each across a number of categories programming environment as i mentioned coursera is the og machine learning course so it should come as no surprise that the it s taught in the og d math language and programming environment matlab due to matlab s cost and licensing issues the machine learning world has mostly moved to python this move severely limits the utility of the programming assignments because you ll have to relearn a lot of that work in python if you are a seasoned programmer who knows many languages that might not be a big deal however if you are relatively new to programming then this detour may cost you a lot of time the udacity course is taught in a modern python environment with popular frameworks like sklearn tensorflow and keras the course even teaches students how to use aws to deploy machine learning software to the cloud the course also simplifies the process of installing machine learning dependencies with a docker image and ami amazon machine image for local and aws development respectively in fact the entire udacity environment is in line with industry best practices and students who learn it will be well equipped in the job market winner udacity lectures coursera s machine learning course was created and taught by the ai godfather himself andrew ng and this course has contributed in no small part to his reputation within the industry the lectures follow a single uniform format and each one builds upon the last in a methodical way not to mention he leads every one himself lastly professor ng is also very encouraging in his videos which i thought was a nice touch udacity s lectures by contrast featured a rotating cast of characters which can create very jarring transitions between sections i counted at least seven different people lecturing throughout the program while udacity attempts to provide multiple content sources for its students the lack of homogeneity definitely dented my enthusiasm for the lectures by the end of the program i just skipped right to the projects and watched the lectures or even searched youtube as needed winner coursera projects coursera s course has programming assignments in which student s submit code to be tested against automated unit tests while this model helps the class scale it leaves you hunting through the forums when things go wrong that said i never hit any major roadblocks the assignments themselves were directly related to the course material and reinforced the lectures sometimes it felt like i was actually creating my own machine learning framework at other times however it felt like i was just implementing methods until the unit tests passed udacity s projects were extremely well designed in fact they constituted some of the best educational materials i ve ever encountered each project covered a subject such as unsupervised learning reinforcement learning linear regression in which you solve a multi step machine learning problem and write about your approach and understanding when you feel that you have completed a project you submit it to be graded by a human the quality of the feedback that i got was incredible the final project is a capstone that you get to pick yourself but it is still reviewed by udacity s staff the proposal and final report ended up being one of the best portfolio items i have ever created and one of the things i am most proud of in my programming career winner udacity cost coursera s price is hard to beat because it s free to get the certification its if you are machine learning on a budget then coursera is a great choice udacity has recently changed its pricing model for the machine learning nanodegree when i entered the program it was a month now it is a flat fee the per month pricing model incentivized me to finish the program quickly in only three months though i must admit given the quality of instructor feedback even with the price hike tuition still seems reasonable the highly skilled labor that is meticulously reviewing projects can t pay for itself with such a high dollar amount however signing up for the nanodegree program is obviously a much bigger consideration winner coursera conclusion while the courses tied on the number categories won i am going to pick a winner it is udacity it may come as no surprise that a paid course beats out a free one but the udacity machine learning engineer nanodegree program gave me the confidence to professional pursue machine learning positions and opportunities and for that its entry fee was a very small price to pay that said i would still recommend you do both courses start with coursera so that when you use batteries included high level frameworks you understand the low level details and have a better appreciation of what you re actually coding after you ve built a strong conceptual foundation further refine your skills by learning practical industry standard practices at udacity overall i am so glad i took concrete steps to enter the machine learning world in and i would encourage you to do the same in coursera s machine learning certificate machine learning engineer nanodegree certificate from a quick cheer to a standing ovation clap to show how much you enjoyed this story programmer and entrepreneur find me spearsx com github notnil how hackers start their afternoons
James Le,2000,9,https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef?source=---------0----------------,How to do Semantic Segmentation using Deep learning,this article is a comprehensive overview including a step by step guide to implement a deep learning image segmentation model nowadays semantic segmentation is one of the key problems in the field of computer vision looking at the big picture semantic segmentation is one of the high level task that paves the way towards complete scene understanding the importance of scene understanding as a core computer vision problem is highlighted by the fact that an increasing number of applications nourish from inferring knowledge from imagery some of those applications include self driving vehicles human computer interaction virtual reality etc with the popularity of deep learning in recent years many semantic segmentation problems are being tackled using deep architectures most often convolutional neural nets which surpass other approaches by a large margin in terms of accuracy and efficiency semantic segmentation is a natural step in the progression from coarse to fine inference it is also worthy to review some standard deep networks that have made significant contributions to the field of computer vision as they are often used as the basis of semantic segmentation systems a general semantic segmentation architecture can be broadly thought of as an encoder network followed by a decoder network unlike classification where the end result of the very deep network is the only important thing semantic segmentation not only requires discrimination at pixel level but also a mechanism to project the discriminative features learnt at different stages of the encoder onto the pixel space different approaches employ different mechanisms as a part of the decoding mechanism let s explore the main approaches the region based methods generally follow the segmentation using recognition pipeline which first extracts free form regions from an image and describes them followed by region based classification at test time the region based predictions are transformed to pixel predictions usually by labeling a pixel according to the highest scoring region that contains it r cnn regions with cnn feature is one representative work for the region based methods it performs the semantic segmentation based on the object detection results to be specific r cnn first utilizes selective search to extract a large quantity of object proposals and then computes cnn features for each of them finally it classifies each region using the class specific linear svms compared with traditional cnn structures which are mainly intended for image classification r cnn can address more complicated tasks such as object detection and image segmentation and it even becomes one important basis for both fields moreover r cnn can be built on top of any cnn benchmark structures such as alexnet vgg googlenet and resnet for the image segmentation task r cnn extracted types of features for each region full region feature and foreground feature and found that it could lead to better performance when concatenating them together as the region feature r cnn achieved significant performance improvements due to using the highly discriminative cnn features however it also suffers from a couple of drawbacks for the segmentation task due to these bottlenecks recent research has been proposed to address the problems including sds hypercolumns mask r cnn the original fully convolutional network fcn learns a mapping from pixels to pixels without extracting the region proposals the fcn network pipeline is an extension of the classical cnn the main idea is to make the classical cnn take as input arbitrary sized images the restriction of cnns to accept and produce labels only for specific sized inputs comes from the fully connected layers which are fixed contrary to them fcns only have convolutional and pooling layers which give them the ability to make predictions on arbitrary sized inputs one issue in this specific fcn is that by propagating through several alternated convolutional and pooling layers the resolution of the output feature maps is down sampled therefore the direct predictions of fcn are typically in low resolution resulting in relatively fuzzy object boundaries a variety of more advanced fcn based approaches have been proposed to address this issue including segnet deeplab crf and dilated convolutions most of the relevant methods in semantic segmentation rely on a large number of images with pixel wise segmentation masks however manually annotating these masks is quite time consuming frustrating and commercially expensive therefore some weakly supervised methods have recently been proposed which are dedicated to fulfilling the semantic segmentation by utilizing annotated bounding boxes for example boxsup employed the bounding box annotations as a supervision to train the network and iteratively improve the estimated masks for semantic segmentation simple does it treated the weak supervision limitation as an issue of input label noise and explored recursive training as a de noising strategy pixel level labeling interpreted the segmentation task within the multiple instance learning framework and added an extra layer to constrain the model to assign more weight to important pixels for image level classification in this section let s walk through a step by step implementation of the most popular architecture for semantic segmentation the fully convolutional net fcn we ll implement it using the tensorflow library in python along with other dependencies such as numpy and scipy in this exercise we will label the pixels of a road in images using fcn we ll work with the kitti road dataset for road lane detection this is a simple exercise from the udacity s self driving car nano degree program which you can learn more about the setup in this github repo here are the key features of the fcn architecture there are versions of fcn fcn fcn fcn we ll implement fcn as detailed step by step below we first load the pre trained vgg model into tensorflow taking in the tensorflow session and the path to the vgg folder which is downloadable here we return the tuple of tensors from vgg model including the image input keep prob to control dropout rate layer layer and layer now we focus on creating the layers for a fcn using the tensors from the vgg model given the tensors for vgg layer output and the number of classes to classify we return the tensor for the last layer of that output in particular we apply a x convolution to the encoder layers and then add decoder layers to the network with skip connections and upsampling the next step is to optimize our neural network aka building tensorflow loss functions and optimizer operations here we use cross entropy as our loss function and adam as our optimization algorithm here we define the train nn function which takes in important parameters including number of epochs batch size loss function optimizer operation and placeholders for input images label images learning rate for the training process we also set keep probability to and learning rate to to keep track of the progress we also print out the loss during training finally it s time to train our net in this run function we first build our net using the load vgg layers and optimize function then we train the net using the train nn function and save the inference data for records about our parameters we choose epochs batch size num classes and image shape after doing trial passes with dropout and dropout we found that the nd trial yields better results with better average losses to see the full code check out this link https gist github com khanhnamle e ff ddca c ac e d b e if you enjoyed this piece i d love it if you hit the clap button so others might stumble upon it from a quick cheer to a standing ovation clap to show how much you enjoyed this story blue ocean thinker https jameskle com nanonets machine learning api
Sarthak Jain,3900,10,https://medium.com/nanonets/how-to-easily-detect-objects-with-deep-learning-on-raspberrypi-225f29635c74?source=---------1----------------,How to easily Detect Objects with Deep Learning on Raspberry Pi,disclaimer i m building nanonets com to help build ml with less data and no hardware the raspberry pi is a neat piece of hardware that has captured the hearts of a generation with m devices sold with hackers building even cooler projects on it given the popularity of deep learning and the raspberry pi camera we thought it would be nice if we could detect any object using deep learning on the pi now you will be able to detect a photobomber in your selfie someone entering harambe s cage where someone kept the sriracha or an amazon delivery guy entering your house m years of evolution have made human vision fairly evolved the human brain has of it s neurons work on processing vision as compared with percent for touch and just percent for hearing humans have two major advantages when compared with machines one is stereoscopic vision the second is an almost infinite supply of training data an infant of years has had approximately b images sampled at fps to mimic human level performance scientists broke down the visual perception task into four different categories object detection has been good enough for a variety of applications even though image segmentation is a much more precise result it suffers from the complexity of creating training data it typically takes a human annotator x more time to segment an image than draw bounding boxes this is more anecdotal and lacks a source also after detecting objects it is separately possible to segment the object from the bounding box object detection is of significant practical importance and has been used across a variety of industries some of the examples are mentioned below object detection can be used to answer a variety of questions these are the broad categories there are a variety of models architectures that are used for object detection each with trade offs between speed size and accuracy we picked one of the most popular ones yolo you only look once and have shown how it works below in under lines of code if you ignore the comments note this is pseudo code not intended to be a working example it has a black box which is the cnn part of it which is fairly standard and shown in the image below you can read the full paper here https pjreddie com media files papers yolo pdf for this task you probably need a few images per object try to capture data as close to the data you re going to finally make predictions on draw bounding boxes on the images you can use a tool like labelimg you will typically need a few people who will be working on annotating your images this is a fairly intensive and time consuming task you can read more about this at medium com nanonets nanonets how to use deep learning when you have limited data f c b cab you need a pretrained model so you can reduce the amount of data required to train without it you might need a few k images to train the model you can find a bunch of pretrained models here the process of training a model is unnecessarily difficult to simplify the process we created a docker image would make it easy to train to start training the model you can run the docker image has a run sh script that can be called with the following parameters you can find more details at to train a model you need to select the right hyper parameters finding the right parameters the art of deep learning involves a little bit of hit and try to figure out which are the best parameters to get the highest accuracy for your model there is some level of black magic associated with this along with a little bit of theory this is a great resource for finding the right parameters quantize model make it smaller to fit on a small device like the raspberry pi or mobile small devices like mobile phones and rasberry pi have very little memory and computation power training neural networks is done by applying many tiny nudges to the weights and these small increments typically need floating point precision to work though there are research efforts to use quantized representations here too taking a pre trained model and running inference is very different one of the magical qualities of deep neural networks is that they tend to cope very well with high levels of noise in their inputs why quantize neural network models can take up a lot of space on disk with the original alexnet being over mb in float format for example almost all of that size is taken up with the weights for the neural connections since there are often many millions of these in a single model the nodes and weights of a neural network are originally stored as bit floating point numbers the simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer and then compressing each float value to an eight bit integer the size of the files is reduced by code for quantization you need the raspberry pi camera live and working then capture a new image for instructions on how to install checkout this link download model once your done training the model you can download it on to your pi to export the model run then download the model onto the raspberry pi install tensorflow on the raspberry pi depending on your device you might need to change the installation a little run model for predicting on the new image the raspberry pi has constraints on both memory and compute a version of tensorflow compatible with the raspberry pi gpu is still not available therefore it is important to benchmark how much time do each of the models take to make a prediction on a new image we have removed the need to annotate images we have expert annotators who will annotate your images for you we automatically train the best model for you to achieve this we run a battery of model with different parameters to select the best for your data nanonets is entirely in the cloud and runs without using any of your hardware which makes it much easier to use since devices like the raspberry pi and mobile phones were not built to run complex compute heavy tasks you can outsource the workload to our cloud which does all of the compute for you get your free api key from http app nanonets com user api key collect the images of object you want to detect you can annotate them either using our web ui https app nanonets com objectannotation appid your model id or use open source tool like labelimg once you have dataset ready in folders images image files and annotations annotations for the image files start uploading the dataset once the images have been uploaded begin training the model the model takes hours to train you will get an email once the model is trained in the meanwhile you check the state of the model once the model is trained you can make predictions using the model from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder ceo nanonets com nanonets machine learning api
Bharath Raj,2200,15,https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced?source=---------2----------------,Data Augmentation | How to use Deep Learning when you have Limited Data — Part 2,we have all been there you have a stellar concept that can be implemented using a machine learning model feeling ebullient you open your web browser and search for relevant data chances are you find a dataset that has around a few hundred images you recall that most popular datasets have images in the order of tens of thousands or more you also recall someone mentioning having a large dataset is crucial for good performance feeling disappointed you wonder can my state of the art neural network perform well with the meagre amount of data i have the answer is yes but before we get into the magic of making that happen we need to reflect upon some basic questions when you train a machine learning model what you re really doing is tuning its parameters such that it can map a particular input say an image to some output a label our optimization goal is to chase that sweet spot where our model s loss is low which happens when your parameters are tuned in the right way naturally if you have a lot of parameters you would need to show your machine learning model a proportional amount of examples to get good performance also the number of parameters you need is proportional to the complexity of the task your model has to perform you don t need to hunt for novel new images that can be added to your dataset why because neural networks aren t smart to begin with for instance a poorly trained neural network would think that these three tennis balls shown below are distinct unique images so to get more data we just need to make minor alterations to our existing dataset minor changes such as flips or translations or rotations our neural network would think these are distinct images anyway a convolutional neural network that can robustly classify objects even if its placed in different orientations is said to have the property called invariance more specifically a cnn can be invariant to translation viewpoint size or illumination or a combination of the above this essentially is the premise of data augmentation in the real world scenario we may have a dataset of images taken in a limited set of conditions but our target application may exist in a variety of conditions such as different orientation location scale brightness etc we account for these situations by training our neural network with additional synthetically modified data yes it can help to increase the amount of relevant data in your dataset this is related to the way with which neural networks learn let me illustrate it with an example imagine that you have a dataset consisting of two brands of cars as shown above let s assume that all cars of brand a are aligned exactly like the picture in the left i e all cars are facing left likewise all cars of brand b are aligned exactly like the picture in the right i e facing right now you feed this dataset to your state of the art neural network and you hope to get impressive results once it s trained let s say it s done training and you feed the image above which is a brand a car but your neural network outputs that it s a brand b car you re confused didn t you just get a accuracy on your dataset using your state of the art neural network i m not exaggerating similar incidents and goof ups have occurred in the past why does this happen it happens because that s how most machine learning algorithms work it finds the most obvious features that distinguishes one class from another here the feature was that all cars of brand a were facing left and all cars of brand b are facing right how do we prevent this happening we have to reduce the amount of irrelevant features in the dataset for our car model classifier above a simple solution would be to add pictures of cars of both classes facing the other direction to our original dataset better yet you can just flip the images in the existing dataset horizontally such that they face the other side now on training the neural network on this new dataset you get the performance that you intended to get before we dive into the various augmentation techniques there s one issue that we must consider beforehand the answer may seem quite obvious we do augmentation before we feed the data to the model right yes but you have two options here one option is to perform all the necessary transformations beforehand essentially increasing the size of your dataset the other option is to perform these transformations on a mini batch just before feeding it to your machine learning model the first option is known as offline augmentation this method is preferred for relatively smaller datasets as you would end up increasing the size of the dataset by a factor equal to the number of transformations you perform for example by flipping all my images i would increase the size of my dataset by a factor of the second option is known as online augmentation or augmentation on the fly this method is preferred for larger datasets as you can t afford the explosive increase in size instead you would perform transformations on the mini batches that you would feed to your model some machine learning frameworks have support for online augmentation which can be accelerated on the gpu in this section we present some basic but powerful augmentation techniques that are popularly used before we explore these techniques for simplicity let us make one assumption the assumption is that we don t need to consider what lies beyond the image s boundary we ll use the below techniques such that our assumption is valid what would happen if we use a technique that forces us to guess what lies beyond an image s boundary in this case we need to interpolate some information we ll discuss this in detail after we cover the types of augmentation for each of these techniques we also specify the factor by which the size of your dataset would get increased aka data augmentation factor you can flip images horizontally and vertically some frameworks do not provide function for vertical flips but a vertical flip is equivalent to rotating an image by degrees and then performing a horizontal flip below are examples for images that are flipped you can perform flips by using any of the following commands from your favorite packages data augmentation factor to x one key thing to note about this operation is that image dimensions may not be preserved after rotation if your image is a square rotating it at right angles will preserve the image size if it s a rectangle rotating it by degrees would preserve the size rotating the image by finer angles will also change the final image size we ll see how we can deal with this issue in the next section below are examples of square images rotated at right angles you can perform rotations by using any of the following commands from your favorite packages data augmentation factor to x the image can be scaled outward or inward while scaling outward the final image size will be larger than the original image size most image frameworks cut out a section from the new image with size equal to the original image we ll deal with scaling inward in the next section as it reduces the image size forcing us to make assumptions about what lies beyond the boundary below are examples or images being scaled you can perform scaling by using the following commands using scikit image data augmentation factor arbitrary unlike scaling we just randomly sample a section from the original image we then resize this section to the original image size this method is popularly known as random cropping below are examples of random cropping if you look closely you can notice the difference between this method and scaling you can perform random crops by using any the following command for tensorflow data augmentation factor arbitrary translation just involves moving the image along the x or y direction or both in the following example we assume that the image has a black background beyond its boundary and are translated appropriately this method of augmentation is very useful as most objects can be located at almost anywhere in the image this forces your convolutional neural network to look everywhere you can perform translations in tensorflow by using the following commands data augmentation factor arbitrary over fitting usually happens when your neural network tries to learn high frequency features patterns that occur a lot that may not be useful gaussian noise which has zero mean essentially has data points in all frequencies effectively distorting the high frequency features this also means that lower frequency components usually your intended data are also distorted but your neural network can learn to look past that adding just the right amount of noise can enhance the learning capability a toned down version of this is the salt and pepper noise which presents itself as random black and white pixels spread through the image this is similar to the effect produced by adding gaussian noise to an image but may have a lower information distortion level you can add gaussian noise to your image by using the following command on tensorflow data augmentation factor x real world natural data can still exist in a variety of conditions that cannot be accounted for by the above simple methods for instance let us take the task of identifying the landscape in photograph the landscape could be anything freezing tundras grasslands forests and so on sounds like a pretty straight forward classification task right you d be right except for one thing we are overlooking a crucial feature in the photographs that would affect the performance the season in which the photograph was taken if our neural network does not understand the fact that certain landscapes can exist in a variety of conditions snow damp bright etc it may spuriously label frozen lakeshores as glaciers or wet fields as swamps one way to mitigate this situation is to add more pictures such that we account for all the seasonal changes but that is an arduous task extending our data augmentation concept imagine how cool it would be to generate effects such as different seasons artificially without going into gory detail conditional gans can transform an image from one domain to an image to another domain if you think it sounds too vague it s not that s literally how powerful this neural network is below is an example of conditional gans used to transform photographs of summer sceneries to winter sceneries the above method is robust but computationally intensive a cheaper alternative would be something called neural style transfer it grabs the texture ambiance appearance of one image aka the style and mixes it with the content of another using this powerful technique we produce an effect similar to that of our conditional gan in fact this method was introduced before cgans were invented the only downside of this method is that the output tends to looks more artistic rather than realistic however there are certain advancements such as deep photo style transfer shown below that have impressive results we have not explored these techniques in great depth as we are not concerned with their inner working we can use existing trained models along with the magic of transfer learning to use it for augmentation what if you wanted to translate an image that doesn t have a black background what if you wanted to scale inward or rotate in finer angles after we perform these transformations we need to preserve our original image size since our image does not have any information about things outside it s boundary we need to make some assumptions usually the space beyond the image s boundary is assumed to be the constant at every point hence when you do these transformations you get a black region where the image is not defined but is that the right assumption in the real world scenario it s mostly a no image processing and ml frameworks have some standard ways with which you can decide on how to fill the unknown space they are defined as follows the simplest interpolation method is to fill the unknown region with some constant value this may not work for natural images but can work for images taken in a monochromatic background the edge values of the image are extended after the boundary this method can work for mild translations the image pixel values are reflected along the image boundary this method is useful for continuous or natural backgrounds containing trees mountains etc this method is similar to reflect except for the fact that at the boundary of reflection a copy of the edge pixels are made normally reflect and symmetric can be used interchangeably but differences will be visible while dealing with very small images or patterns the image is just repeated beyond its boundary as if it s being tiled this method is not as popularly used as the rest as it does not make sense for a lot of scenarios besides these you can design your own methods for dealing with undefined space but usually these methods would just do fine for most classification problems if you use it in the right way then yes what is the right way you ask well sometimes not all augmentation techniques make sense for a dataset consider our car example again below are some of the ways by which you can modify the image sure they are pictures of the same car but your target application may never see cars presented in these orientations for instance if you re just going to classify random cars on the road only the second image would make sense to be on the dataset but if you own an insurance company that deals with car accidents and you want to identify models of upside down broken cars as well the third image makes sense the last image may not make sense for both the above scenarios the point is while using augmentation techniques we have to make sure to not increase irrelevant data you re probably expecting some results to motivate you to walk the extra mile fair enough i ve got that covered too let me prove that augmentation really works using a toy example you can replicate this experiment to verify let s create two neural networks to classify data to one among four classes cat lion tiger or a leopard the catch is one will not use data augmentation whereas the other will you can download the dataset from here link if you ve checked out the dataset you ll notice that there s only images per class for both training and testing clearly we can t use augmentation for one of the classifiers to make the odds more fair we use transfer learning to give the models a better chance with the scarce amount of data for the one without augmentation let s use a vgg network i ve written a tensorflow implementation here which is based on this implementation once you ve cloned my repo you can get the dataset from here and vgg npy used for transfer learning from here you can now run the model to verify the performance i would agree though writing extra code for data augmentation is indeed a bit of an effort so to build our second model i turned to nanonets they internally use transfer learning and data augmentation to provide the best results using minimal data all you need to do is upload the data on their website and wait until it s trained in their servers usually around minutes what do you know it s perfect for our comparison experiment once it s done training you can request calls to their api to calculate the test accuracy checkout out my repo for a sample code snippet don t forget to insert your model s id in the code snippet impressive isn t it it is a fact that most models perform well with more data so to provide a concrete proof i ve mentioned the table below it shows the error rate of popular neural networks on the cifar c and cifar c datasets c and c columns are the error rates with data augmentation thank you for reading this article hit that clap button if you did hope it shed some light about data augmentation if you have any questions you could hit me up on social media or send me an email bharathrajn gmail com from a quick cheer to a standing ovation clap to show how much you enjoyed this story undergrad computer vision and ai enthusiast hungry nanonets machine learning api
Daniel Rothmann,302,8,https://towardsdatascience.com/human-like-machine-hearing-with-ai-1-3-a5713af6e2f8?source=---------3----------------,Human-Like Machine Hearing With AI (1/3) – Towards Data Science,significant breakthroughs in ai technology have been achieved through modeling human systems while artificial neural networks nns are mathematical models which are only loosely coupled with the way actual human neurons function their application in solving complex and ambiguous real world problems has been profound additionally modeling the architectural depth of the brain in nns has opened up broad possibilities in learning more meaningful representations of data in image recognition and processing the inspiration from the complex and more spatially invariant cells of the visual system in cnns has also produced great improvements to our technologies if you re interested in applying image recognition technologies on audio spectrograms check out my article what s wrong with cnns and spectrograms for audio processing as long as human perceptual capacity exceeds that of machines we stand to gain by understanding the principles of human systems humans are very skillful when it comes to perceptual tasks and the contrast between human understanding and the status quo of ai becomes particularly apparent in the area of machine hearing considering the benefits reaped from getting inspired by human systems in visual processing i propose that we stand to gain from a similar process in machine hearing with neural networks in this article series i will detail a framework for real time audio signal processing with ai which was developed in cooperation with aarhus university and intelligent loudspeaker manufacturer dynaudio a s its inspiration is primarily drawn from cognitive science which attempts to combine perspectives of biology neuroscience psychology and philosophy to gain greater understanding of our cognitive faculties perhaps the most abstract domain of sound is how we as humans perceive it while a solution for a signal processing problem has to operate within the parameters of intensity spectral and temporal properties on a low level the end goal is most often a cognitive one transforming a signal in such a way that our perceptions of the sounds it contains are altered if one wishes to programatically change the gender of a recorded spoken voice for example it is necessary to describe this problem in more meaningful terms before defining its lower level characteristics the gender of a speaker can be conceived as a cognitive property which is constructed from many factors general pitch and timbre of a voice differences in pronunciation differences in choice of words and language and a common understanding of how these properties relate to gender these parameters can be described in lower level features like intensity spectral and temporal properties but only in more complex combinations do they form high level representations this forms a hierarchy of audio features from which the meaning of a sound can be derived the cognitive property representing a human voice can be thought of as a combinatory pattern of temporal developments in a sound s intensity spectral and statistical properties nns are great at extracting abstracted representations of data and are therefore well suited for the task of detecting cognitive properties in sound in order to build a system for this purpose let s examine how sound is represented in human auditory organs that we can use to inspire representation of sound for processing with nns hearing in humans starts at the outer ear which firstly consists of the pinna the pinna acts as a form of spectral preprocessing in which the incoming sound is modified depending on its direction in relation to the listener sound then travels through the opening in the pinna into the ear canal which further acts to modify spectral properties of incoming sound by resonating in a way that amplifies frequencies in the range khz as sound waves reach the end of the ear canal they excite the eardrum onto which the ossicles the smallest bones in the body are attached these bones transmit the pressure from the ear canal to the fluid filled cochlea in the inner ear the cochlea is of great interest in guiding sound representation for nns because this is the organ responsible for transducing acoustic vibrations into neural activity in humans it is a coiled tube which is separated along its length by two membranes being the reissner s membrane and the basilar membrane along the length of the cochlea there is a row of around inner hair cells as pressures enter the cochlea its two membranes are pushed down the basilar membrane is narrow and stiff at its base but loose and wide at its apex so that each place along its length responds more intensely at a particular frequency to simplify the basilar membrane can be thought of as a continuous array of bandpass filters which along the length of the membrane acts to separate sounds into their spectral components this is the primary mechanism by which humans convert sound pressures into neural activity therefore it is reasonable to assume that spectral representations of audio would be beneficial in modeling sound perception with ai since frequency responses along the basilar membrane vary exponentially logarithmic frequency representations might prove most efficient one such representation could be derived using a gammatone filterbank these filters are commonly applied in modeling spectral filtering in the auditory system since they approximate the impulse response of human auditory filters derived from the measured auditory nerve fiber response to white noise stimuli called the revcor function since the cochlea has inner hair cells and humans can detect gaps in sounds down to ms in length a spectral resolution of gammatone filters separated into ms windows seem optimal parameters for achieving human like spectral representation in machines in practical scenarios however i assume that lesser resolutions could still achieve desirable effects in most analysis and processing tasks while being more viable from a computational standpoint a number of software libraries for auditory analysis are available online a notable example is the gammatone filterbank toolkit by jason heeris it provides adjustable filters as well as tools for spectrogram like analysis of audio signals with gammatone filters as neural activity moves from the cochlea onto the auditory nerve and the ascending auditory pathways a number of processes are applied in brainstem nuclei before it reaches the auditory cortex these processes form a neural code which represents an interface between stimulus and perception much knowledge about the specific inner workings of these nuclei is still speculative or unknown so i will detail these nuclei only at their higher levels of functioning humans have a set of these nuclei for each ear that are interconnected but for simplicity i ve illustrated the flow for only one ear the cochlear nucleus is the first coding step for neural signals coming from the auditory nerve it consists of a variety of neurons with different properties which serve to perform initial processing of sound features some of which are directed to the superior olive which is associated with sound localization while others are directed to the lateral lemniscus and inferior colliculus commonly associated with more advanced features j j eggermont details this flow of information from the cochlear nucleus in between sound and perception reviewing the search for a neural code as follows the ventral cochlear nucleus vcn extracts and enhances the frequency and timing information that is multiplexed in the firing patterns of the auditory nerve fibers and distributes the results via two main pathways the sound localization path and the sound identification path the anterior part of the vcn avcn mainly serves the sound localization aspects and its two types of bushy cells provide input to the superior olivary complex soc where interaural time differences itds and level differences ilds are mapped for each frequency separately the information carried by the sound identification pathway is a representation of complex spectra such as vowels this representation is mainly created in the ventral cochlear nucleus by special types of units dubbed chopper stellate neurons the details of these auditory encodings are difficult to specify but they indicate to us that a form of coding of incoming frequency spectra could improve understanding of low level sound features as well as making sound impressions less expensive to process in nns we can apply the unsupervised autoencoder nn architecture as an attempt to learn common properties associated with complex spectra like word embeddings its possible to find commonalities in frequency spectra that represent select features or a more tightly condensed meaning of sounds an autoencoder is trained to encode an input into a compressed representation that can be reconstructed back into a representation with a high similarity to the input this means that the autoencoder s target output is the input itself if an input can be reconstructed without great loss the network has learnt to encode it in such a way that the compressed internal representation contains enough meaningful information this internal representation is then what we refer to as the embedding the encoding part of the autoencoder can be decoupled from the decoder to generate embeddings for other applications embeddings also have the benefit that they are often of lower dimensionality than the original data for instance an autoencoder could compress a frequency spectrum with a total of values into a vector with a length of values put simply each value of such a vector could describe higher level factors of a spectrum such as vowel harshness or harmonicity these are only examples as the meaning of statistically common factors derived by an autoencoder might often be difficult to label in plain language in the next article we will expand upon this idea with added memory to produce embeddings for temporal developments of audio frequency spectra this wraps up the first part of my article series on audio processing with artificial intelligence next we will discuss the essential concepts of sensory memory and temporal dependencies in sound follow to stay updated and feel free to leave claps if you enjoyed the article as always feel free to connect with me on linkedin to stay in touch c j plack the sense of hearing nd ed psychology press s j elliott and c a shera the cochlea as a smart structure smart mater struct vol no p jun a m darling properties and implementation of the gammatone filter a tutorial speech hearing and language university college london j j eggermont between sound and perception reviewing the search for a neural code hear res vol no pp jul t p lillicrap et al learning deep architectures for ai vol no from a quick cheer to a standing ovation clap to show how much you enjoyed this story ai engineer convai especially interested in audio and time series forecasting reach us at convai dk sharing concepts ideas and codes
Amine Aoullay,58,4,https://towardsdatascience.com/how-to-use-noise-to-your-advantage-5301071d9dc3?source=---------4----------------,How to use Noise to your advantage ? – Towards Data Science,for scientists random fluctuations or noise is undesirable although typically assumed to degrade performance it can sometimes improve information processing in non linear systems in this post we ll see some examples where the noise can be used as an advantage recent works have shown that by allowing some inaccuracy when training deep neural networks not only the training performance but also the accuracy of the model can be improved neural networks are capable of learning output functions that can change wildly with small changes in input adding noise to inputs randomly is like telling the network to not change the output in a ball around your exact input by limiting the amount of information in a network we force it to learn compact representations of input features rl is an area of machine learning that assumes there is an agent situated in an environment at each step the agent takes an action and it receives an observation and reward from the environment an rl algorithm seeks to maximize the agent s total reward given a previously unknown environment through a learning process that usually involves lots of trial and error to understand the challenge with exploration in deep rl systems think about researchers that spend lot of times in a lab without producing any practical application equivalently rl agents can spend a huge amount of resources without converging to a local optimum openai proposes a technique called parameter space noise that introduces noises in the model policy parameters at the beginning of each episode other approaches were focused on what is known as action space noise which introduce noise to change the likelihoods associated with each action the agent might take from one moment to the next the initial results of the parameter space noise model proved to be really promising the technique helps algorithms explore their environments more effectively leading to higher scores and more elegant behaviors more details can be found in the research paper the important thing to remember is that adding noise was used as an advantage to boost the exploration performance of reinforcement learning algorithms boosting recognition isn t as simple as throwing more labeled images at these systems indeed manually annotating a large number of images is an expensive and time consuming process facebook researchers and engineers have addressed this by training image recognition networks on large sets of public images with hashtags since people often caption their photos with hashtags it woul d be a good source of training data for models facebook developed new approaches that are tailored for doing image recognition experiments using hashtag supervision this study is described in detail in exploring the limits of weakly supervised pretraining on the coco object detection challenge it has been shown that the use of hashtags for pretraining can boost the average precision of a model by more than percent noise should not be our enemy it isn t always an unwanted disturbance and can often be used as an advantage and even serve as a valuable research tool if anyone tries to tell you otherwise well just give him the examples we presented stay tuned and if you liked this article please leave a weakly supervised pretraining https research fb com publications exploring the limits of weakly supervised pretraining better exploration with parameter noise https blog openai com better exploration with parameter noise from a quick cheer to a standing ovation clap to show how much you enjoyed this story msc in machine learning mva ens paris saclay sharing concepts ideas and codes
Jonathan Balaban,804,5,https://towardsdatascience.com/deep-learning-tips-and-tricks-1ef708ec5f53?source=---------5----------------,Deep Learning Tips and Tricks – Towards Data Science,below is a distilled collection of conversations messages and debates i ve had with peers and students on how to optimize deep models if you have tricks you ve found impactful please share them deep learning models like the convolutional neural network cnn have a massive number of parameters we can actually call these hyper parameters because they are not optimized inherently in the model you could gridsearch the optimal values for these hyper parameters but you ll need a lot of hardware and time so does a true data scientist settle for guessing these essential parameters one of the best ways to improve your models is to build on the design and architecture of the experts who have done deep research in your domain often with powerful hardware at their disposal graciously they often open source the resulting modeling architectures and rationale here are a few ways you can improve your fit time and accuracy with pre trained models here s how to modify dropout and limit weight sizes in keras with mnist here s an example of final layer modification in keras with classes for mnist and an example of how to freeze weights in the first five layers alternatively we can set the learning rate to zero for that layer or use per parameter adaptive learning algorithm like adadelta or adam this is somewhat complicated and better implemented in other platforms like caffe it s often essential to get a visual idea of how your model looks if you re working in keras abstraction is nice but doesn t allow you to drill down into sections of your model for deeper analysis fortunately the code below lets us visualize our models directly with python this will plot a graph of the model and save it as a png file plot takes two optional arguments you can also directly obtain the pydot graph object and render it yourself for example to show it in an ipython notebook i hope this collection helps with your modeling endeavors let me know your best tricks and connect with me on twitter and linkedin from a quick cheer to a standing ovation clap to show how much you enjoyed this story data science nomad sharing concepts ideas and codes
Arthur Juliani,9000,6,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=---------6----------------,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,for this tutorial in my reinforcement learning series we are going to be exploring a family of rl algorithms called q learning algorithms these are a little different than the policy based algorithms that will be looked at in the the following tutorials parts instead of starting with a complex and unwieldy deep neural network we will begin by implementing a simple lookup table version of the algorithm and then show how to implement a neural network equivalent using tensorflow given that we are going back to basics it may be best to think of this as part of the series it will hopefully give an intuition into what is really happening in q learning that we can then build on going forward when we eventually combine the policy gradient and q learning approaches to build state of the art rl agents if you are more interested in policy networks or already have a grasp on q learning feel free to start the tutorial series here instead unlike policy gradient methods which attempt to learn functions which directly map an observation to an action q learning attempts to learn the value of being in a given state and taking a specific action there while both approaches ultimately allow us to take intelligent actions given a situation the means of getting to that action differ significantly you may have heard about deepq networks which can play atari games these are really just larger and more complex implementations of the q learning algorithm we are going to discuss here for this tutorial we are going to be attempting to solve the frozenlake environment from the openai gym for those unfamiliar the openai gym provides an easy way for people to experiment with their learning agents in an array of provided toy games the frozenlake environment consists of a x grid of blocks each one either being the start block the goal block a safe frozen block or a dangerous hole the objective is to have an agent learn to navigate from the start to the goal without moving onto a hole at any given time the agent can choose to move either up down left or right the catch is that there is a wind which occasionally blows the agent onto a space they didn t choose as such perfect performance every time is impossible but learning to avoid the holes and reach the goal are certainly still doable the reward at every step is except for entering the goal which provides a reward of thus we will need an algorithm that learns long term expected rewards this is exactly what q learning is designed to provide in it s simplest implementation q learning is a table of values for every state row and action column possible in the environment within each cell of the table we learn a value for how good it is to take a given action within a given state in the case of the frozenlake environment we have possible states one for each block and possible actions the four directions of movement giving us a x table of q values we start by initializing the table to be uniform all zeros and then as we observe the rewards we obtain for various actions we update the table accordingly we make updates to our q table using something called the bellman equation which states that the expected long term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state in this way we reuse our own q table when estimating how to update our table for future actions in equation form the rule looks like this this says that the q value for a given state s and action a should represent the current reward r plus the maximum discounted future reward expected according to our own table for the next state s we would end up in the discount variable allows us to decide how important the possible future rewards are compared to the present reward by updating in this way the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state below is a python walkthrough of the q table algorithm implemented in the frozenlake environment thanks to praneet d for finding the optimal hyperparameters for this approach now you may be thinking tables are great but they don t really scale do they while it is easy to have a x table for a simple grid world the number of possible states in any modern game or real world environment is nearly infinitely larger for most interesting problems tables simply don t work we instead need some way to take a description of our state and produce q values for actions without a table that is where neural networks come in by acting as a function approximator we can take any number of possible states that can be represented as a vector and learn to map them to q values in the case of the frozenlake example we will be using a one layer network which takes the state encoded in a one hot vector x and produces a vector of q values one for each action such a simple network acts kind of like a glorified table with the network weights serving as the old cells the key difference is that we can easily expand the tensorflow network with added layers activation functions and different input types whereas all that is impossible with a regular table the method of updating is a little different as well instead of directly updating our table with a network we will be using backpropagation and a loss function our loss function will be sum of squares loss where the difference between the current predicted q values and the target value is computed and the gradients passed through the network in this case our q target for the chosen action is the equivalent to the q value computed in equation above below is the tensorflow walkthrough of implementing our simple q network while the network learns to solve the frozenlake problem it turns out it doesn t do so quite as efficiently as the q table while neural networks allow for greater flexibility they do so at the cost of stability when it comes to q learning there are a number of possible extensions to our simple q network which allow for greater performance and more robust learning two tricks in particular are referred to as experience replay and freezing target networks those improvements and other tweaks were the key to getting atari playing deep q networks and we will be exploring those additions in the future for more info on the theory behind q learning see this great post by tambet matiisen i hope this tutorial has been helpful for those curious about how to implement simple q learning algorithms if this post has been valuable to you please consider donating to help support future tutorials articles and implementations any contribution is greatly appreciated if you d like to follow my work on deep learning ai and cognitive science follow me on medium arthur juliani or on twitter awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning unity d cognitive neuroscience phd student exploring frontier technology through the lens of artificial intelligence data science and the shape of things to come
SAGAR SHARMA,2500,5,https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6?source=---------7----------------,Activation Functions: Neural Networks – Towards Data Science,what is activation function why we use activation functions with neural networks the activation functions can be basically divided into types as you can see the function is a line or linear therefore the output of the functions will not be confined between any range equation f x x range infinity to infinity it doesn t help with the complexity or various parameters of usual data that is fed to the neural networks the nonlinear activation functions are the most used activation functions nonlinearity helps to makes the graph look something like this it makes it easy for the model to generalize or adapt with variety of data and to differentiate between the output the main terminologies needed to understand for nonlinear functions are the nonlinear activation functions are mainly divided on the basis of their range or curves sigmoid or logistic activation function the sigmoid function curve looks like a s shape the main reason why we use sigmoid function is because it exists between to therefore it is especially used for models where we have to predict the probability as an output since probability of anything exists only between the range of and sigmoid is the right choice the function is differentiable that means we can find the slope of the sigmoid curve at any two points the function is monotonic but function s derivative is not the logistic sigmoid function can cause a neural network to get stuck at the training time the softmax function is a more generalized logistic activation function which is used for multiclass classification tanh or hyperbolic tangent activation function tanh is also like logistic sigmoid but better the range of the tanh function is from to tanh is also sigmoidal s shaped the advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph the function is differentiable the function is monotonic while its derivative is not monotonic the tanh function is mainly used classification between two classes relu rectified linear unit activation function the relu is the most used activation function in the world right now since it is used in almost all the convolutional neural networks or deep learning as you can see the relu is half rectified from bottom f z is zero when z is less than zero and f z is equal to z when z is above or equal to zero range to infinity the function and its derivative both are monotonic but the issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly that means any negative input given to the relu activation function turns the value into zero immediately in the graph which in turns affects the resulting graph by not mapping the negative values appropriately leaky relu it is an attempt to solve the dying relu problem can you see the leak the leak helps to increase the range of the relu function usually the value of a is or so when a is not then it is called randomized relu therefore the range of the leaky relu is infinity to infinity both leaky and randomized relu functions are monotonic in nature also their derivatives also monotonic in nature i will be posting posts per week so don t miss the tutorial so follow me on medium facebook twitter linkedin google quora to see similar posts any comments or if you have any question write it in the comment clap it share it follow me happy to be helpful kudos epoch vs batch size vs iterations train inception with custom images on cpu tensorflow image recognition python api tutorial on cpu from a quick cheer to a standing ovation clap to show how much you enjoyed this story i am interested in programming python c arduino machine learning i m the editor of arduino community on medium i also like to write stuff sharing concepts ideas and codes
Jae Duk Seo,33,6,https://towardsdatascience.com/principal-component-analysis-network-in-tensorflow-with-interactive-code-7be543047704?source=---------8----------------,Principal Component Analysis Network in Tensorflow with Interactive Code,a natural extension from principle component analysis pooling layer would be making a full neural network out of the layer i wanted to know if this was even possible as well as how well or worse it performs on mnist data principle component analysis pca pooling layer for anyone who is not familiar with pcap please read this blog post first the basic idea is pooling layers such as max or mean pooling operations performs dimensionality reduction to not only to save computational power but also to act as a regularizer pca is a dimensionality reduction technique in which converts correlated variables into a set of values of linearly uncorrelated variables called principal components and we can take advantage of this operation to do a similar job as max mean pooling network composed of majority of pooling layers now i know what you are thinking it doesn t make sense to have a network that is only composed of pooling layer while performing classification and you are completely right it doesn t but i just wanted to try this out for fun data set network architecture blue rectangle pcap or max pooling layergreen rectangle convolution layer to increase channel size global averaging pooling operation the network itself is very simple only four pooling layers and one convolution layer to increase the channel size however in order for the dimension to match up we will downsample each images into dimension hence the tensors will have a shape of batch size batch size batch size batch size batch size batch size batch size and we can perform classification with soft max layer as any other network does results principle component network as seen above the training accuracy have stagnated at percent accuracy which is horrible lol but i suspected that the network didn t have enough learning capacity from the start and this was best it could do however i wanted to see how each pcap layer transforms the image top left image original inputtop right image after first layerbottom left image after second layerbottom right image after fourth layer one obvious pattern we can observe is the change of brightens for example if the top left pixel was white in the second layer this pixel will change to black in the next layer currently i am not sure on why this is happening but with more study i hope to know exactly why results max pooling network as seen above when we replace all of the pcap layers with max pooling operation we can observe that the accuracy on training images stagnated around percent confirming the fact that the network didn t have enough learning capacity from the start top left image original inputtop right image after first layerbottom left image after second layerbottom right image after fourth layer contrast to pcap with max pooling we can clearly observe that the pixel with most high intensity moves on to the next layer this is expected since that is what max pooling does interactive code for google colab you would need a google account to view the codes also you can t run read only scripts in google colab so make a copy on your play ground finally i will never ask for permission to access your files on google drive just fyi happy coding to access the network with pcap please click here to access the network with max pooling please click here final words i wasn t expecting much of this network from the start but i expected at least percent accuracy on training testing images lol if any errors are found please email me at jae duk seo gmail com if you wish to see the list of all of my writing please view my website here meanwhile follow me on my twitter here and visit my website or my youtube channel for more content i also implemented wide residual networks please click here to view the blog post reference from a quick cheer to a standing ovation clap to show how much you enjoyed this story https jaedukseo me your everyday seo who likes kimchi sharing concepts ideas and codes
Jae Duk Seo,20,7,https://towardsdatascience.com/multi-stream-rnn-concat-rnn-internal-conv-rnn-lag-2-rnn-in-tensorflow-f4f17189a208?source=---------9----------------,"Multi-Stream RNN, Concat RNN, Internal Conv RNN, Lag 2 RNN in Tensorflow",for the last two week i have been dying to implement different kinds of recurrent neural networks rnn and finally i have the time to implement all of them below is the list of different rnn cases i wanted to try out case a vanilla recurrent neural network case b multi stream recurrent neural networkcase c concatenated recurrent neural networkcase d internal convolutional recurrent neural networkcase e lag recurrent neural network vanilla recurrent neural network there is in total of different case of rnn i wish to implement however in order to fully understand all of the implementations it would be a good idea to have a strong understanding of vanilla rnn case a is vanilla rnn so if you understand code for case a you are good to go if anyone wishes to review simple rnn please visit my old blog post only numpy vanilla recurrent neural network deriving back propagation through time practice case a vanilla recurrent neural network results red box convolutional layerorange global average pooling and softmaxgreen circle hidden unit at time blue circle input in time stampblack box recurrent neural network with time stamp as seen above the base network is simple rnn combined with convolutional neural network for classification the rnn have time stamp of which means we are going to give the network different kinds of input at each time stamp and to do that i am going to add some noise to the original image blue line train cost over timeorange line train accuracy over timegreen line test cost over timered line test accuracy over time as seen above our base network already performs well now the question is how other methods performs and would it be able to regularize better than our base network case b multi stream recurrent neural network idea results red box convolutional layerorange global average pooling and softmaxgreen circle hidden unit at time blue circle convolution input stream yellow circle fully connected network streamblack box recurrent neural network with time stamp the idea behind this rnn is simply to give different representation of data to the rnn in our base network we have the network either the raw image or image with some noise added red box additional four cnn fnn layers to process the inputblue box creating inputs at each different time stamps as seen below now our rnn takes in input of tensor size with batch size reducing the width and the height by and i was hoping that different representation of the data would act as a regularization similar to data augmentation blue line train cost over timeorange line train accuracy over timegreen line test cost over timered line test accuracy over time as seen above the network did pretty well and have outperformed our base network by percent on the testing images case c concatenated recurrent neural network idea results red box convolutional layerorange global average pooling and softmaxgreen circle hidden unit at time blue circle input in time stampblack box recurrent neural network with time stampblack curved arrow concatenated input for each time stamp this approach is very simple the idea was that on each time stamp different features will be extracted and it might be useful for the network to have more features overtime for the recurrent layers blue line train cost over timeorange line train accuracy over timegreen line test cost over timered line test accuracy over time sadly this was a huge failure i guess the empty hidden values does not help one bit for the network to perform well case d internal convolutional recurrent neural network idea results red box convolutional layerorange global average pooling and softmaxgreen circle hidden unit at time blue circle input in time stampblack box recurrent neural network with time stampgray arrow performing internal convolution before passing onto the next time stamp as seen above this network takes in the exact same input as our base network however this time we are going to perform additional convolution operations in the internal representation of the data right image declaring new convolution layerleft image red box if the current internal layer is not none we are going to perform additional convolution operation i actually had no theoretical reason behind this implementation i just wanted to see if it works lol blue line train cost over timeorange line train accuracy over timegreen line test cost over timered line test accuracy over time as seen above the network did a fine job at converging however it was not able to outperform our base network sadly case e lag recurrent neural network idea results red box convolutional layerorange global average pooling and softmaxgreen circle hidden unit at time or lag of blue circle input in time stampblack box recurrent neural network with time stamppurple circle hidden state lag of in a traditional rnn setting we only rely on the most previous values to determine the current value for a while i was thinking that there is no reason for us to limit the look back time or lag as we can extend this idea into lag or lag etc just for simplicity i took lag blue line train cost over timeorange line train accuracy over timegreen line test cost over timered line test accuracy over time thankfully the network did better than the base network but with very small margin however this type of network would be most suitable for time series data interactive code transparency for google colab you would need a google account to view the codes also you can t run read only scripts in google colab so make a copy on your play ground finally i will never ask for permission to access your files on google drive just fyi happy coding also for transparency i uploaded all of the training logs on my github to access the code for case a click here for the logs click here to access the code for case b click here for the logs click here to access the code for case c click here for the logs click here to access the code for case c click here for the logs click here to access the code for case c click here for the logs click here final words i wanted to review rnn for quite a long time now finally i get to do it if any errors are found please email me at jae duk seo gmail com if you wish to see the list of all of my writing please view my website here meanwhile follow me on my twitter here and visit my website or my youtube channel for more content i also implemented wide residual networks please click here to view the blog post reference from a quick cheer to a standing ovation clap to show how much you enjoyed this story https jaedukseo me your everyday seo who likes kimchi sharing concepts ideas and codes
Wallarm,72,4,https://lab.wallarm.com/tensorflow-dataset-api-for-increasing-training-speed-of-neural-networks-43a3050f2080?source=---------3----------------,TensorFlow Dataset API for increasing training speed of neural networks,wallarm ai engine is the heart of our security solution two key parameters of our ai engine efficiency are how fast neural networks can be train to reflect the updated training sets and how much compute power need to be dedicated to the training on the on going basis many of our machine learning algorithms are written on top of tensorflow an open source dataflow software library originally release by google our average cpu load for the ai engine today is as high as so we are always looking for ways to speed things up in software our latest find is dataset api dataset is a mid level tensorflow apis which makes working with data faster and more convenient in this blog we will measure just how much faster model training can be with dataset compared to the you use of feed dict for starters let s prepare data that will be used to train the model dataset can usually be stored in numpy s arrays regardless of kind of data they are that s why we prepare all our dataset without tensorflow and store it in npz format similar to this https github com wallarm researches blob a f a da deea e d cbfc b b tf ds api storing in npz format py l l this step helps us avoid unnecessary data processing load on cpu and memory during model training now we are ready to train the model first let s load preprocessed data from disk https github com wallarm researches blob a f a da deea e d cbfc b b tf ds api load from npz py l l next the data will be converted from numphy arrays into tensorflow tensors tf data dataset from tensor slices method is used for that and loaded into tensorflow dataset from tensor slices method takes placeholders with the same size of the th dimension element and returns dataset object once the dataset is in tf you can process it for example you can use map f function which can process the data but we already preprocess our dataset and all we need to do is apply batching and maybe shuffling fortunately dataset api already has needed functions they are batch and shuffle ok if we shuffle our dataset how can we use it for production it s easy we simply make another dataset without data been shuffled https github com wallarm researches blob a f a da deea e d cbfc b b tf ds api datasets py l l dataset api has other good methods for preprocessing data there is a comprehensive list of methods in the official docs next we should extract data from dataset object step by step for each of the training epochs tf data iterator is tailor made for it tf currently supported four type of iterators reinitializeble iterator is very useful all we need to do to start the work is to create an iterator and initializers for it iterator get next yields the next elements of our dataset when executed https github com wallarm researches blob a f a da deea e d cbfc b b tf ds api iterator py l l to demonstrate the viability of using dataset api let s use proposed approach for mnist dataset and for our corporate data first we prepared data and after that we processed and epochs with dataset api and without model for this mnist example can be found on github https github com wallarm researches blob a f a da deea e d cbfc b b tf ds api model py l l below are the results we obtained on a machine with one nvidia gtx and tf all code of this experiment is available on github link mnist is a very small dataset and profit of dataset api isn t representative by contrast the results on a real life dataset are much more impressive thus dataset api is very good for increasing your training speed with no source code changes just some modifications in the stack you can save off the training time from a quick cheer to a standing ovation clap to show how much you enjoyed this story adaptive application security for devops nginx partner ycombibator s wallarm is devops friendly waf with hybrid architecture uniquely suited for cloud applications it applies machine learning to traffic to adaptively generate security rules and verifies the impact of malicious payloads in real time
Maryna Hlaiboroda,5,5,https://blog.heyml.com/%D0%B8%D0%B8-%D0%BF%D1%81%D0%B8%D1%85%D0%BE%D0%BF%D0%B0%D1%82-%D0%B8-%D0%B8%D0%B8-%D0%BE%D0%B1%D0%BC%D0%B0%D0%BD%D1%89%D0%B8%D0%BA-94c6a8e6c63e?source=---------4----------------,ИИ-психопат и ИИ-обманщик – Hey Machine Learning,mit norman reddit norman mit norman mit norman vo microsoft bbc news faster r cnn facebook mmsp u of t news google google tsn ua google autoaugment autoaugment svhn cifar imagenet autoaugment blog google ai bdd k gps analytics vidhya telegram facebook from a quick cheer to a standing ovation clap to show how much you enjoyed this story we are young and talented team and our passion is machine learning data science and artificial intelligence http heyml com
Amine Aoullay,58,4,https://towardsdatascience.com/how-to-use-noise-to-your-advantage-5301071d9dc3?source=---------6----------------,How to use Noise to your advantage ? – Towards Data Science,for scientists random fluctuations or noise is undesirable although typically assumed to degrade performance it can sometimes improve information processing in non linear systems in this post we ll see some examples where the noise can be used as an advantage recent works have shown that by allowing some inaccuracy when training deep neural networks not only the training performance but also the accuracy of the model can be improved neural networks are capable of learning output functions that can change wildly with small changes in input adding noise to inputs randomly is like telling the network to not change the output in a ball around your exact input by limiting the amount of information in a network we force it to learn compact representations of input features rl is an area of machine learning that assumes there is an agent situated in an environment at each step the agent takes an action and it receives an observation and reward from the environment an rl algorithm seeks to maximize the agent s total reward given a previously unknown environment through a learning process that usually involves lots of trial and error to understand the challenge with exploration in deep rl systems think about researchers that spend lot of times in a lab without producing any practical application equivalently rl agents can spend a huge amount of resources without converging to a local optimum openai proposes a technique called parameter space noise that introduces noises in the model policy parameters at the beginning of each episode other approaches were focused on what is known as action space noise which introduce noise to change the likelihoods associated with each action the agent might take from one moment to the next the initial results of the parameter space noise model proved to be really promising the technique helps algorithms explore their environments more effectively leading to higher scores and more elegant behaviors more details can be found in the research paper the important thing to remember is that adding noise was used as an advantage to boost the exploration performance of reinforcement learning algorithms boosting recognition isn t as simple as throwing more labeled images at these systems indeed manually annotating a large number of images is an expensive and time consuming process facebook researchers and engineers have addressed this by training image recognition networks on large sets of public images with hashtags since people often caption their photos with hashtags it woul d be a good source of training data for models facebook developed new approaches that are tailored for doing image recognition experiments using hashtag supervision this study is described in detail in exploring the limits of weakly supervised pretraining on the coco object detection challenge it has been shown that the use of hashtags for pretraining can boost the average precision of a model by more than percent noise should not be our enemy it isn t always an unwanted disturbance and can often be used as an advantage and even serve as a valuable research tool if anyone tries to tell you otherwise well just give him the examples we presented stay tuned and if you liked this article please leave a weakly supervised pretraining https research fb com publications exploring the limits of weakly supervised pretraining better exploration with parameter noise https blog openai com better exploration with parameter noise from a quick cheer to a standing ovation clap to show how much you enjoyed this story msc in machine learning mva ens paris saclay sharing concepts ideas and codes
Kelvin Li,56,5,https://medium.com/@kelfun5354/the-complex-language-used-in-back-propagation-88c6e58f676c?source=---------9----------------,The Complex language used in Back Propagation – Kelvin Li – Medium,i ve looked all over the internet for explanations of what exactly back propagation is and everyone either uses complicated mathematical language or complex codes to try to explain what back propagation if someone who doesn t know either wants to know what it is then how will they really grasp what it is in this post i would like to unveil the secrets of the universe with everyone and hopefully i ll do a good job at it according to wikipedia backpropagation is a method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights to be used in the network backpropagation is commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function if you have taken a basic elementary algebra class you may have heard of the idea of a slope some people might think the idea of a slope is very insignificant but it is actually the game changing concept that caused all the technological advancement within the last century to know the slope of something means that you know the rate of something changing over a period of time knowing this gives us power to manipulate things to our advantage now you can think of a gradient as the slope of something in a higher dimension i won t go into details but that is the general gist of what a gradient is weights are the values that we want to use to adjust the outputs of our functions in each neuron so say we have an output of and we want to change the into a then we would multiply the by a to get the desired result this means that will be the weight in this case in a way we are weighing down the output to what we want it to be a neuron is simply just a function a neural network a bunch of neurons is simply a bunch of functions each neuron also has an activation function that spits out a value for the next neuron to calculate think of these functions as how much of a yes or a no an input is an example would be picture recognizing when you feed the neural network a picture the node will spit out a number between and where is being very no and being very yes this process continues between every node until the very end which ever node has the highest number between and would be the decision the machine makes a loss function is just some function that we use to determine how correct the predicted output is from the real output for example we input a picture of a cat into the machine but the machine predicts that it s a dinosaur clearly the machine is not doing a very good job so we need some way to know how correct the machine is compared to the real data which is where the loss function comes in now that we have all the necessary understandings we can go into the real sauce now what i am about to explain to you is going to either confuse the crap out of you or make you feel enlightened let s pretend you are trying to build a door lock opening mechanism this mechanism involves you pressing a button which triggers a ball rolling down a platform and knocks over a switch that unlocks the door now lets think about this there are a few components that we have to keep in mind the st component being you pressing the button the nd component is the ball rolling down a platform and the rd component being the switch being knocked over there is actually a lot of physics going on around here but let s just focus on the ball rolling down the platform now when you create this mechanism you want the door to ideally open in seconds but you don t have any tools to measure the time and length so all you can do is to create a platform through intuition you build your first platform and let the ball roll and realized that it took seconds for the door to open after pressing the button so you go back to the platform and make the platform steeper you performed the same trial and error over and over again until you got the ideal opening time this my friend is backpropagation well true but the idea is basically the same in a neural net we have weights assigned to each neuron these weights will get multiplied by a certain input and modified through some activation function the result of these activation function might not always be what we want what backpropagation would do is that it will do some calculus will be covered in another post to determine the direction of increase decrease aka the gradient cut less of the platform or cut more of the platform to achieve the best weights ideal time the door opens it then updates these weights every time it has created new weights and runs the neural net again every trial you cut a piece of the platform to test eventually we will achieve the best possible weights that satisfies our desired accuracy in my next post i will discuss more in depth about the math that is involved with backpropagation references and links from a quick cheer to a standing ovation clap to show how much you enjoyed this story getting stuck
Arthur Juliani,9000,6,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=tag_archive---------1----------------,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,for this tutorial in my reinforcement learning series we are going to be exploring a family of rl algorithms called q learning algorithms these are a little different than the policy based algorithms that will be looked at in the the following tutorials parts instead of starting with a complex and unwieldy deep neural network we will begin by implementing a simple lookup table version of the algorithm and then show how to implement a neural network equivalent using tensorflow given that we are going back to basics it may be best to think of this as part of the series it will hopefully give an intuition into what is really happening in q learning that we can then build on going forward when we eventually combine the policy gradient and q learning approaches to build state of the art rl agents if you are more interested in policy networks or already have a grasp on q learning feel free to start the tutorial series here instead unlike policy gradient methods which attempt to learn functions which directly map an observation to an action q learning attempts to learn the value of being in a given state and taking a specific action there while both approaches ultimately allow us to take intelligent actions given a situation the means of getting to that action differ significantly you may have heard about deepq networks which can play atari games these are really just larger and more complex implementations of the q learning algorithm we are going to discuss here for this tutorial we are going to be attempting to solve the frozenlake environment from the openai gym for those unfamiliar the openai gym provides an easy way for people to experiment with their learning agents in an array of provided toy games the frozenlake environment consists of a x grid of blocks each one either being the start block the goal block a safe frozen block or a dangerous hole the objective is to have an agent learn to navigate from the start to the goal without moving onto a hole at any given time the agent can choose to move either up down left or right the catch is that there is a wind which occasionally blows the agent onto a space they didn t choose as such perfect performance every time is impossible but learning to avoid the holes and reach the goal are certainly still doable the reward at every step is except for entering the goal which provides a reward of thus we will need an algorithm that learns long term expected rewards this is exactly what q learning is designed to provide in it s simplest implementation q learning is a table of values for every state row and action column possible in the environment within each cell of the table we learn a value for how good it is to take a given action within a given state in the case of the frozenlake environment we have possible states one for each block and possible actions the four directions of movement giving us a x table of q values we start by initializing the table to be uniform all zeros and then as we observe the rewards we obtain for various actions we update the table accordingly we make updates to our q table using something called the bellman equation which states that the expected long term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state in this way we reuse our own q table when estimating how to update our table for future actions in equation form the rule looks like this this says that the q value for a given state s and action a should represent the current reward r plus the maximum discounted future reward expected according to our own table for the next state s we would end up in the discount variable allows us to decide how important the possible future rewards are compared to the present reward by updating in this way the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state below is a python walkthrough of the q table algorithm implemented in the frozenlake environment thanks to praneet d for finding the optimal hyperparameters for this approach now you may be thinking tables are great but they don t really scale do they while it is easy to have a x table for a simple grid world the number of possible states in any modern game or real world environment is nearly infinitely larger for most interesting problems tables simply don t work we instead need some way to take a description of our state and produce q values for actions without a table that is where neural networks come in by acting as a function approximator we can take any number of possible states that can be represented as a vector and learn to map them to q values in the case of the frozenlake example we will be using a one layer network which takes the state encoded in a one hot vector x and produces a vector of q values one for each action such a simple network acts kind of like a glorified table with the network weights serving as the old cells the key difference is that we can easily expand the tensorflow network with added layers activation functions and different input types whereas all that is impossible with a regular table the method of updating is a little different as well instead of directly updating our table with a network we will be using backpropagation and a loss function our loss function will be sum of squares loss where the difference between the current predicted q values and the target value is computed and the gradients passed through the network in this case our q target for the chosen action is the equivalent to the q value computed in equation above below is the tensorflow walkthrough of implementing our simple q network while the network learns to solve the frozenlake problem it turns out it doesn t do so quite as efficiently as the q table while neural networks allow for greater flexibility they do so at the cost of stability when it comes to q learning there are a number of possible extensions to our simple q network which allow for greater performance and more robust learning two tricks in particular are referred to as experience replay and freezing target networks those improvements and other tweaks were the key to getting atari playing deep q networks and we will be exploring those additions in the future for more info on the theory behind q learning see this great post by tambet matiisen i hope this tutorial has been helpful for those curious about how to implement simple q learning algorithms if this post has been valuable to you please consider donating to help support future tutorials articles and implementations any contribution is greatly appreciated if you d like to follow my work on deep learning ai and cognitive science follow me on medium arthur juliani or on twitter awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning unity d cognitive neuroscience phd student exploring frontier technology through the lens of artificial intelligence data science and the shape of things to come
Stefan Kojouharov,14200,7,https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?source=tag_archive---------2----------------,"Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data",over the past few months i have been collecting ai cheat sheets from time to time i share them with friends and colleagues and recently i have been getting asked a lot so i decided to organize and share the entire collection to make things more interesting and give context i added descriptions and or excerpts for each major topic this is the most complete list and the big o is at the very end enjoy this machine learning cheat sheet will help you find the right estimator for the job which is the most difficult part the flowchart will help you check the documentation and rough guide of each estimator that will help you to know more about the problems and how to solve it scikit learn formerly scikits learn is a free software machine learning library for the python programming language it features various classification regression and clustering algorithms including support vector machines random forests gradient boosting k means and dbscan and is designed to interoperate with the python numerical and scientific libraries numpy and scipy in may google announced the second generation of the tpu as well as the availability of the tpus in google compute engine the second generation tpus deliver up to teraflops of performance and when organized into clusters of tpus provide up to petaflops in google s tensorflow team decided to support keras in tensorflow s core library chollet explained that keras was conceived to be an interface rather than an end to end machine learning framework it presents a higher level more intuitive set of abstractions that make it easy to configure neural networks regardless of the backend scientific computing library numpy targets the cpython reference implementation of python which is a non optimizing bytecode interpreter mathematical algorithms written for this version of python often run much slower than compiled equivalents numpy address the slowness problem partly by providing multidimensional arrays and functions and operators that operate efficiently on arrays requiring rewriting some code mostly inner loops using numpy the name pandas is derived from the term panel data an econometrics term for multidimensional structured data sets the term data wrangler is starting to infiltrate pop culture in the movie kong skull island one of the characters played by actor marc evan jackson is introduced as steve woodward our data wrangler scipy builds on the numpy array object and is part of the numpy stack which includes tools like matplotlib pandas and sympy and an expanding set of scientific computing libraries this numpy stack has similar users to other applications such as matlab gnu octave and scilab the numpy stack is also sometimes referred to as the scipy stack matplotlib is a plotting library for the python programming language and its numerical mathematics extension numpy it provides an object oriented api for embedding plots into applications using general purpose gui toolkits like tkinter wxpython qt or gtk there is also a procedural pylab interface based on a state machine like opengl designed to closely resemble that of matlab though its use is discouraged scipy makes use of matplotlib pyplot is a matplotlib module which provides a matlab like interface matplotlib is designed to be as usable as matlab with the ability to use python with the advantage that it is free if you like this list you can let me know here stefan is the founder of chatbot s life a chatbot media and consulting firm chatbot s life has grown to over k views per month and has become the premium place to learn about bots ai online chatbot s life has also consulted many of the top bot companies like swelly instavest outbrain neargroup and a number of enterprises big o algorithm cheat sheet http bigocheatsheet com bokeh cheat sheet https s amazonaws com assets datacamp com blog assets python bokeh cheat sheet pdf data science cheat sheet https www datacamp com community tutorials python data science cheat sheet basics data wrangling cheat sheet https www rstudio com wp content uploads data wrangling cheatsheet pdf data wrangling https en wikipedia org wiki data wrangling ggplot cheat sheet https www rstudio com wp content uploads ggplot cheatsheet pdf keras cheat sheet https www datacamp com community blog keras cheat sheet gs drkenms keras https en wikipedia org wiki keras machine learning cheat sheet https ai icymi email new machinelearning cheat sheet by emily barry abdsc machine learning cheat sheet https docs microsoft com en in azure machine learning machine learning algorithm cheat sheet ml cheat sheet http peekaboo vision blogspot com machine learning cheat sheet for scikit html matplotlib cheat sheet https www datacamp com community blog python matplotlib cheat sheet gs uekyspy matpotlib https en wikipedia org wiki matplotlib neural networks cheat sheet http www asimovinstitute org neural network zoo neural networks graph cheat sheet http www asimovinstitute org blog neural networks https www quora com where can find a cheat sheet for neural network numpy cheat sheet https www datacamp com community blog python numpy cheat sheet gs ak zbge numpy https en wikipedia org wiki numpy pandas cheat sheet https www datacamp com community blog python pandas cheat sheet gs oundfxm pandas https en wikipedia org wiki pandas software pandas cheat sheet https www datacamp com community blog pandas cheat sheet python gs hpforic pyspark cheat sheet https www datacamp com community blog pyspark cheat sheet python gs l j zxq scikit cheat sheet https www datacamp com community blog scikit learn cheat sheet scikit learn https en wikipedia org wiki scikit learn scikit learn cheat sheet http peekaboo vision blogspot com machine learning cheat sheet for scikit html scipy cheat sheet https www datacamp com community blog python scipy cheat sheet gs jdsg oi scipy https en wikipedia org wiki scipy tesorflow cheat sheet https www altoros com tensorflow cheat sheet html tensor flow https en wikipedia org wiki tensorflow from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder of chatbots life i help companies create great chatbots ai systems and share my insights along the way latest news info and tutorials on artificial intelligence machine learning deep learning big data and what it means for humanity
Andrej Karpathy,9200,7,https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b?source=tag_archive---------3----------------,Yes you should understand backprop – Andrej Karpathy – Medium,when we offered cs n deep learning class at stanford we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level the students had to implement the forward and the backward pass of each layer in raw numpy inevitably some students complained on the class message boards this is seemingly a perfectly sensible appeal if you re never going to write backward passes once the class is over why practice writing them are we just torturing the students for our own amusement some easy answers could make arguments along the lines of it s worth knowing what s under the hood as an intellectual curiosity or perhaps you might want to improve on the core algorithm later but there is a much stronger and practical argument which i wanted to devote a whole post to the problem with backpropagation is that it is a leaky abstraction in other words it is easy to fall into the trap of abstracting away the learning process believing that you can simply stack arbitrary layers together and backprop will magically make them work on your data so lets look at a few explicit examples where this is not the case in quite unintuitive ways we re starting off easy here at one point it was fashionable to use sigmoid or tanh non linearities in the fully connected layers the tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non linearities can saturate and entirely stop learning your training loss will be flat and refuse to go down for example a fully connected layer with sigmoid non linearity computes using raw numpy if your weight matrix w is initialized too large the output of the matrix multiply could have a very large range e g numbers between and which will make all outputs in the vector z almost binary either or but if that is the case z z which is local gradient of the sigmoid non linearity will in both cases become zero vanish making the gradient for both x and w be zero the rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule another non obvious fun fact about sigmoid is that its local gradient z z achieves a maximum at when z that means that every time the gradient signal flows through a sigmoid gate its magnitude always diminishes by one quarter or more if you re using basic sgd this would make the lower layers of a network train much slower than the higher ones tldr if you re using sigmoids or tanh non linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn t cause them to be fully saturated see a longer explanation in this cs n lecture video another fun non linearity is the relu which thresholds neurons at zero from below the forward and backward pass for a fully connected layer that uses relu would at the core include if you stare at this for a while you ll see that if a neuron gets clamped to zero in the forward pass i e z it doesn t fire then its weights will get zero gradient this can lead to what is called the dead relu problem where if a relu neuron is unfortunately initialized such that it never fires or if a neuron s weights ever get knocked off with a large update during training into this regime then this neuron will remain permanently dead it s like permanent irrecoverable brain damage sometimes you can forward the entire training set through a trained network and find that a large fraction e g of your neurons were zero the entire time tldr if you understand backpropagation and your network has relus you re always nervous about dead relus these are neurons that never turn on for any example in your entire training set and will remain permanently dead neurons can also die during training usually as a symptom of aggressive learning rates see a longer explanation in cs n lecture video vanilla rnns feature another good example of unintuitive effects of backpropagation i ll copy paste a slide from cs n that has a simplified rnn that does not take any input x and only computes the recurrence on the hidden state equivalently the input x could always be zero this rnn is unrolled for t time steps when you stare at what the backward pass is doing you ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix the recurrence matrix whh interspersed with non linearity backprop what happens when you take one number a and start multiplying it by some other number b i e a b b b b b b this sequence either goes to zero if b or explodes to infinity when b the same thing happens in the backward pass of an rnn except b is a matrix and not just a number so we have to reason about its largest eigenvalue instead tldr if you understand backpropagation and you re using rnns you are nervous about having to do gradient clipping or you prefer to use an lstm see a longer explanation in this cs n lecture video lets look at one more the one that actually inspired this post yesterday i was browsing for a deep q learning implementation in tensorflow to see how others deal with computing the numpy equivalent of q a where a is an integer vector turns out this trivial operation is not supported in tf anyway i searched dqn tensorflow clicked the first link and found the core code here is an excerpt if you re familiar with dqn you can see that there is the target q t which is just reward gamma argmax a q s a and then there is q acted which is q s a of the action that was taken the authors here subtract the two into variable delta which they then want to minimize on line with the l loss with tf reduce mean tf square so far so good the problem is on line the authors are trying to be robust to outliers so if the delta is too large they clip it with tf clip by value this is well intentioned and looks sensible from the perspective of the forward pass but it introduces a major bug if you think about the backward pass the clip by value function has a local gradient of zero outside of the range min delta to max delta so whenever the delta is above min max delta the gradient becomes exactly zero during backprop the authors are clipping the raw q delta when they are likely trying to clip the gradient for added robustness in that case the correct thing to do is to use the huber loss in place of tf square it s a bit gross in tensorflow because all we want to do is clip the gradient if it is above a threshold but since we can t meddle with the gradients directly we have to do it in this round about way of defining the huber loss in torch this would be much more simple i submitted an issue on the dqn repo and this was promptly fixed backpropagation is a leaky abstraction it is a credit assignment scheme with non trivial consequences if you try to ignore how it works under the hood because tensorflow automagically makes my networks learn you will not be ready to wrestle with the dangers it presents and you will be much less effective at building and debugging neural networks the good news is that backpropagation is not that difficult to understand if presented properly i have relatively strong feelings on this topic because it seems to me that of backpropagation materials out there present it all wrong filling pages with mechanical math instead i would recommend the cs n lecture on backprop which emphasizes intuition yay for shameless self advertising and if you can spare the time as a bonus work through the cs n assignments which get you to write backprop manually and help you solidify your understanding that s it for now i hope you ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing also i m aware that this post has unintentionally turned into several cs n ads apologies for that from a quick cheer to a standing ovation clap to show how much you enjoyed this story director of ai at tesla previously research scientist at openai and phd student at stanford i like to train deep neural nets on large datasets
Avinash Sharma V,6900,10,https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0?source=tag_archive---------4----------------,Understanding Activation Functions in Neural Networks,recently a colleague of mine asked me a few questions like why do we have so many activation functions why is that one works better than the other how do we know which one to use is it hardcore maths and so on so i thought why not write an article on it for those who are familiar with neural network only at a basic level and is therefore wondering about activation functions and their why how mathematics note this article assumes that you have a basic knowledge of an artificial neuron i would recommend reading up on the basics of neural networks before reading this article for better understanding so what does an artificial neuron do simply put it calculates a weighted sum of its input adds a bias and then decides whether it should be fired or not yeah right an activation function does this but let s go with the flow for a moment so consider a neuron now the value of y can be anything ranging from inf to inf the neuron really doesn t know the bounds of the value so how do we decide whether the neuron should fire or not why this firing pattern because we learnt it from biology that s the way brain works and brain is a working testimony of an awesome and intelligent system we decided to add activation functions for this purpose to check the y value produced by a neuron and decide whether outside connections should consider this neuron as fired or not or rather let s say activated or not the first thing that comes to our minds is how about a threshold based activation function if the value of y is above a certain value declare it activated if it s less than the threshold then say it s not hmm great this could work activation function a activated if y threshold else not alternatively a if y threshold otherwise well what we just did is a step function see the below figure its output is activated when value threshold and outputs a not activated otherwise great so this makes an activation function for a neuron no confusions however there are certain drawbacks with this to understand it better think about the following suppose you are creating a binary classifier something which should say a yes or no activate or not activate a step function could do that for you that s exactly what it does say a or now think about the use case where you would want multiple such neurons to be connected to bring in more classes class class class etc what will happen if more than neuron is activated all neurons will output a from step function now what would you decide which class is it hmm hard complicated you would want the network to activate only neuron and others should be only then would you be able to say it classified properly identified the class ah this is harder to train and converge this way it would have been better if the activation was not binary and it instead would say activated or activated and so on and then if more than neuron activates you could find which neuron has the highest activation and so on better than max a softmax but let s leave that for now in this case as well if more than neuron says activated the problem still persists i know but since there are intermediate activation values for the output learning can be smoother and easier less wiggly and chances of more than neuron being activated is lesser when compared to step function while training also depending on what you are training and the data ok so we want something to give us intermediate analog activation values rather than saying activated or not binary the first thing that comes to our minds would be linear function a cx a straight line function where activation is proportional to input which is the weighted sum from neuron this way it gives a range of activations so it is not binary activation we can definitely connect a few neurons together and if more than fires we could take the max or softmax and decide based on that so that is ok too then what is the problem with this if you are familiar with gradient descent for training you would notice that for this function derivative is a constant a cx derivative with respect to x is c that means the gradient has no relationship with x it is a constant gradient and the descent is going to be on constant gradient if there is an error in prediction the changes made by back propagation is constant and not depending on the change in input delta x this is not that good not always but bear with me there is another problem too think about connected layers each layer is activated by a linear function that activation in turn goes into the next level as input and the second layer calculates weighted sum on that input and it in turn fires based on another linear activation function no matter how many layers we have if all are linear in nature the final activation function of last layer is nothing but just a linear function of the input of first layer pause for a bit and think about it that means these two layers or n layers can be replaced by a single layer ah we just lost the ability of stacking layers this way no matter how we stack the whole network is still equivalent to a single layer with linear activation a combination of linear functions in a linear manner is still another linear function let s move on shall we well this looks smooth and step function like what are the benefits of this think about it for a moment first things first it is nonlinear in nature combinations of this function are also nonlinear great now we can stack layers what about non binary activations yes that too it will give an analog activation unlike step function it has a smooth gradient too and if you notice between x values to y values are very steep which means any small changes in the values of x in that region will cause values of y to change significantly ah that means this function has a tendency to bring the y values to either end of the curve looks like it s good for a classifier considering its property yes it indeed is it tends to bring the activations to either side of the curve above x and below x for example making clear distinctions on prediction another advantage of this activation function is unlike linear function the output of the activation function is always going to be in range compared to inf inf of linear function so we have our activations bound in a range nice it won t blow up the activations then this is great sigmoid functions are one of the most widely used activation functions today then what are the problems with this if you notice towards either end of the sigmoid function the y values tend to respond very less to changes in x what does that mean the gradient at that region is going to be small it gives rise to a problem of vanishing gradients hmm so what happens when the activations reach near the near horizontal part of the curve on either sides gradient is small or has vanished cannot make significant change because of the extremely small value the network refuses to learn further or is drastically slow depending on use case and until gradient computation gets hit by floating point value limits there are ways to work around this problem and sigmoid is still very popular in classification problems another activation function that is used is the tanh function hm this looks very similar to sigmoid in fact it is a scaled sigmoid function ok now this has characteristics similar to sigmoid that we discussed above it is nonlinear in nature so great we can stack layers it is bound to range so no worries of activations blowing up one point to mention is that the gradient is stronger for tanh than sigmoid derivatives are steeper deciding between the sigmoid or tanh will depend on your requirement of gradient strength like sigmoid tanh also has the vanishing gradient problem tanh is also a very popular and widely used activation function later comes the relu function a x max x the relu function is as shown above it gives an output x if x is positive and otherwise at first look this would look like having the same problems of linear function as it is linear in positive axis first of all relu is nonlinear in nature and combinations of relu are also non linear in fact it is a good approximator any function can be approximated with combinations of relu great so this means we can stack layers it is not bound though the range of relu is inf this means it can blow up the activation another point that i would like to discuss here is the sparsity of the activation imagine a big neural network with a lot of neurons using a sigmoid or tanh will cause almost all neurons to fire in an analog way remember that means almost all activations will be processed to describe the output of a network in other words the activation is dense this is costly we would ideally want a few neurons in the network to not activate and thereby making the activations sparse and efficient relu give us this benefit imagine a network with random initialized weights or normalised and almost of the network yields activation because of the characteristic of relu output for negative values of x this means a fewer neurons are firing sparse activation and the network is lighter woah nice relu seems to be awesome yes it is but nothing is flawless not even relu because of the horizontal line in relu for negative x the gradient can go towards for activations in that region of relu gradient will be because of which the weights will not get adjusted during descent that means those neurons which go into that state will stop responding to variations in error input simply because gradient is nothing changes this is called dying relu problem this problem can cause several neurons to just die and not respond making a substantial part of the network passive there are variations in relu to mitigate this issue by simply making the horizontal line into non horizontal component for example y x for x will make it a slightly inclined line rather than horizontal line this is leaky relu there are other variations too the main idea is to let the gradient be non zero and recover during training eventually relu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations that is a good point to consider when we are designing deep neural nets now which activation functions to use does that mean we just use relu for everything we do or sigmoid or tanh well yes and no when you know the function you are trying to approximate has certain characteristics you can choose an activation function which will approximate the function faster leading to faster training process for example a sigmoid works well for a classifier see the graph of sigmoid doesn t it show the properties of an ideal classifier because approximating a classifier function as combinations of sigmoid is easier than maybe relu for example which will lead to faster training process and convergence you can use your own custom functions too if you don t know the nature of the function you are trying to learn then maybe i would suggest start with relu and then work backwards relu works most of the time as a general approximator in this article i tried to describe a few activation functions used commonly there are other activation functions too but the general idea remains the same research for better activation functions is still ongoing hope you got the idea behind activation function why they are used and how do we decide which one to use from a quick cheer to a standing ovation clap to show how much you enjoyed this story musings of an ai deep learning mathematics addict
Arthur Juliani,3500,8,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=tag_archive---------5----------------,Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C),in this article i want to provide a tutorial on implementing the asynchronous advantage actor critic a c algorithm in tensorflow we will use it to solve a simple challenge in a d doom environment with the holidays right around the corner this will be my final post for the year and i hope it will serve as a culmination of all the previous topics in the series if you haven t yet or are new to deep learning and reinforcement learning i suggest checking out the earlier entries in the series before going through this post in order to understand all the building blocks which will be utilized here if you have been following the series thank you i have learned so much about rl in the past year and am happy to have shared it with everyone through this article series so what is a c the a c algorithm was released by google s deepmind group earlier this year and it made a splash by essentially obsoleting dqn it was faster simpler more robust and able to achieve much better scores on the standard battery of deep rl tasks on top of all that it could work in continuous as well as discrete action spaces given this it has become the go to deep rl algorithm for new challenging problems with complex state and action spaces in fact openai just released a version of a c as their universal starter agent for working with their new and very diverse set of universe environments asynchronous advantage actor critic is quite a mouthful let s start by unpacking the name and from there begin to unpack the mechanics of the algorithm itself asynchronous unlike dqn where a single agent represented by a single neural network interacts with a single environment a c utilizes multiple incarnations of the above in order to learn more efficiently in a c there is a global network and multiple worker agents which each have their own set of network parameters each of these agents interacts with it s own copy of the environment at the same time as the other agents are interacting with their environments the reason this works better than having a single agent beyond the speedup of getting more work done is that the experience of each agent is independent of the experience of the others in this way the overall experience available for training becomes more diverse actor critic so far this series has focused on value iteration methods such as q learning or policy iteration methods such as policy gradient actor critic combines the benefits of both approaches in the case of a c our network will estimate both a value function v s how good a certain state is to be in and a policy s a set of action probability outputs these will each be separate fully connected layers sitting at the top of the network critically the agent uses the value estimate the critic to update the policy the actor more intelligently than traditional policy gradient methods advantage if we think back to our implementation of policy gradient the update rule used the discounted returns from a set of experiences in order to tell the agent which of its actions were good and which were bad the network was then updated in order to encourage and discourage actions appropriately the insight of using advantage estimates rather than just discounted returns is to allow the agent to determine not just how good its actions were but how much better they turned out to be than expected intuitively this allows the algorithm to focus on where the network s predictions were lacking if you recall from the dueling q network architecture the advantage function is as follow since we won t be determining the q values directly in a c we can use the discounted returns r as an estimate of q s a to allow us to generate an estimate of the advantage in this tutorial we will go even further and utilize a slightly different version of advantage estimation with lower variance referred to as generalized advantage estimation in the process of building this implementation of the a c algorithm i used as reference the quality implementations by dennybritz and openai both of which i highly recommend if you d like to see alternatives to my code here each section embedded here is taken out of context for instructional purposes and won t run on its own to view and run the full functional a c implementation see my github repository the general outline of the code architecture is the a c algorithm begins by constructing the global network this network will consist of convolutional layers to process spatial dependencies followed by an lstm layer to process temporal dependencies and finally value and policy output layers below is example code for establishing the network graph itself next a set of worker agents each with their own network and environment are created each of these workers are run on a separate processor thread so there should be no more workers than there are threads on your cpu from here we go asynchronous each worker begins by setting its network parameters to those of the global network we can do this by constructing a tensorflow op which sets each variable in the local worker network to the equivalent variable value in the global network each worker then interacts with its own copy of the environment and collects experience each keeps a list of experience tuples observation action reward done value that is constantly added to from interactions with the environment once the worker s experience history is large enough we use it to determine discounted return and advantage and use those to calculate value and policy losses we also calculate an entropy h of the policy this corresponds to the spread of action probabilities if the policy outputs actions with relatively similar probabilities then entropy will be high but if the policy suggests a single action with a large probability then entropy will be low we use the entropy as a means of improving exploration by encouraging the model to be conservative regarding its sureness of the correct action a worker then uses these losses to obtain gradients with respect to its network parameters each of these gradients are typically clipped in order to prevent overly large parameter updates which can destabilize the policy a worker then uses the gradients to update the global network parameters in this way the global network is constantly being updated by each of the agents as they interact with their environment once a successful update is made to the global network the whole process repeats the worker then resets its own network parameters to those of the global network and the process begins again to view the full and functional code see the github repository here the robustness of a c allows us to tackle a new generation of reinforcement learning challenges one of which is d environments we have come a long way from multi armed bandits and grid worlds and in this tutorial i have set up the code to allow for playing through the first vizdoom challenge vizdoom is a system to allow for rl research using the classic doom game engine the maintainers of vizdoom recently created a pip package so installing it is as simple as pip install vizdoom once it is installed we will be using the basic wad environment which is provided in the github repository and needs to be placed in the working directory the challenge consists of controlling an avatar from a first person perspective in a single square room there is a single enemy on the opposite side of the room which appears in a random location each episode the agent can only move to the left or right and fire a gun the goal is to shoot the enemy as quickly as possible using as few bullets as possible the agent has time steps per episode to shoot the enemy shooting the enemy yields a reward of and each time step as well as each shot yields a small penalty after about episodes per worker agent the network learns a policy to quickly solve the challenge feel free to adjust parameters such as learning rate clipping magnitude update frequency etc to attempt to achieve ever greater performance or utilize a c in your own rl tasks i hope this tutorial has been helpful to those new to a c and asynchronous reinforcement learning now go forth and build ais there are a lot of moving parts in a c so if you discover a bug or find a better way to do something please don t hesitate to bring it up here or in the github i am more than happy to incorporate changes and feedback to improve the algorithm if you d like to follow my writing on deep learning ai and cognitive science follow me on medium arthur juliani or on twitter awjuliani if this post has been valuable to you please consider donating to help support future tutorials articles and implementations any contribution is greatly appreciated more from my simple reinforcement learning with tensorflow series from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning unity d cognitive neuroscience phd student exploring frontier technology through the lens of artificial intelligence data science and the shape of things to come
Elle O'Brien,2300,6,https://towardsdatascience.com/romance-novels-generated-by-artificial-intelligence-1b31d9c872b2?source=tag_archive---------6----------------,"Romance Novels, Generated by Artificial Intelligence",i ve always been fascinated with romance novels the kind they sell at the drugstore for a couple of dollars usually with some attractive soft lit couples on the cover so when i started futzing around with text generating neural networks a few weeks ago i developed an urgent curiosity to discover what artificial intelligence could contribute to the ever popular genre maybe one day there will be entire books written by computers for now let s start with titles i gathered over harlequin romance novel titles and gave them to a neural network a type of artificial intelligence that learns the structure of text it s powerful enough to string together words in a way that seems almost human human the other is all wackiness i was not disappointed with what came out i even photoshopped some of my favorites into existence the author names are synthesized from machine learning too let s have a look by theme a common theme in romance novels is pregnancy and the word baby had a strong showing in the titles i trained the neural network on naturally the neural network came up with a lot of baby themed titles there s an unusually high concentration of sheikhs vikings and billionaires in the harlequin world likewise the neural network generated some colorful new bachelor types i have so many questions how is the prince pregnant what sort of consulting does the count do who is butterfly earl and what makes the sheikh s desires so convenient although there are exceptions most romance novels end in happily ever afters a lot of them even start with an unexpected wedding a marriage of convenience or a stipulation of a business contract or a sham that turns into real love the neural network seems to have internalized something about matrimony doctors and surgeons are common paramours for mistresses headed towards the marriage valley christmas is a magical time for surgeons sheikhs playboys dads consultants and the women who love them what or where is knith i just like mission christmas this neural network has never seen the big montana sky but it has some questionable ideas about cowboys the neural network generated some decidedly pg titles they can t all live happily ever after some of the generated titles sounded like m night shyamalan was a collaborator how did the word fear get in there it s possible the network generated it without having fear in the training set but a subset of the harlequin empire is geared towards paranormal and gothic romance that might have included the word note i checked and there was veil of fear published in to wrap it up some of the adorable failures and near misses generated by the neural network i hope you ve enjoyed computer generated romance novel titles half as much as i have maybe someone out there can write about the virgin viking or the consultant count or the baby surgeon seduction i d buy it i built a webscraper in python thanks beautiful soup that grabbed about romance novel titles published under the harlequin brand off of fictiondb com harlequin is to me synonymous with the romance genre although it comprises only a fraction albeit a healthy one of the entire market i fed this list of book titles into a recurrent neural network using software i got from github and waited a few hours for the magic to happen the model i fit was a layer node recurrent neural network i also trained the network on the author list in to create some new pen names for more about the neural network i used have a look at the fabulous work of andrej karpathy i discovered that surgery by the sea is actually a real novel written by sheila douglas and published in so this one isn t an original neural network creation because the training set is rather small only about mb of text data it s to be expected that sometimes the machine will spit out one of the titles it was trained on one of the more challenging aspects of this project was discerning when that happened since the real published titles can be more surprising than anything born out of artificial intelligence for example the daddy and grinch are both real in fact the very first romance novel published by harlequin was called the manatee from a quick cheer to a standing ovation clap to show how much you enjoyed this story computational scientist software developer science writer sharing concepts ideas and codes
Slav Ivanov,2900,9,https://blog.slavv.com/picking-a-gpu-for-deep-learning-3d4795c273b9?source=tag_archive---------8----------------,Picking a GPU for Deep Learning – Slav,quite a few people have asked me recently about choosing a gpu for machine learning as it stands success with deep learning heavily dependents on having the right hardware to work with when i was building my personal deep learning box i reviewed all the gpus on the market in this article i m going to share my insights about choosing the right graphics processor also we ll go over deep learning dl is part of the field of machine learning ml dl works by approximating a solution to a problem using neural networks one of the nice properties of about neural networks is that they find patterns in the data features by themselves this is opposed to having to tell your algorithm what to look for as in the olde times however often this means the model starts with a blank state unless we are transfer learning to capture the nature of the data from scratch the neural net needs to process a lot of information there are two ways to do so with a cpu or a gpu the main computational module in a computer is the central processing unit better known as cpu it is designed to do computation rapidly on a small amount of data for example multiplying a few numbers on a cpu is blazingly fast but it struggles when operating on a large amount of data e g multiplying matrices of tens or hundreds thousand numbers behind the scenes dl is mostly comprised of operations like matrix multiplication amusingly d computer games rely on these same operations to render that beautiful landscape you see in rise of the tomb raider thus gpus were developed to handle lots of parallel computations using thousands of cores also they have a large memory bandwidth to deal with the data for these computations this makes them the ideal commodity hardware to do dl on or at least until asics for machine learning like google s tpu make their way to market for me the most important reason for picking a powerful graphics processor is saving time while prototyping models if the networks train faster the feedback time will be shorter thus it would be easier for my brain to connect the dots between the assumptions i had for the model and its results see tim dettmers answer to why are gpus well suited to deep learning on quora for a better explanation also for an in depth albeit slightly outdated gpus comparison see his article which gpu s to get for deep learning there are main characteristics of a gpu related to dl are there are two reasons for having multiple gpus you want to train several models at once or you want to do distributed training of a single model we ll go over each one training several models at once is a great technique to test different prototypes and hyperparameters it also shortens your feedback cycle and lets you try out many things at once distributed training or training a single network on several video cards is slowly but surely gaining traction nowadays there are easy to use approaches to this for tensorflow and keras via horovod cntk and pytorch the distributed training libraries offer almost linear speed ups to the number of cards for example with gpus you get x faster training pcie lanes updated the caveat to using multiple video cards is that you need to be able to feed them with data for this purpose each gpu should have pcie lanes available for data transfer tim dettmers points out that having pcie lanes per card should only decrease performance by for two gpus for a single card any desktop processor and chipset like intel i and asus tuf z will use lanes however for two gpus you can go x x lanes or get a processor and a motherboard that support pcie lanes lanes are outside the realm of desktop cpus an intel xeon with a msi x a sli plus will do the job for or gpus go with x lanes per card with a xeon with to pcie lanes to have pcie lanes available for or gpus you need a monstrous processor something in the class of or amd threadripper lanes with a corresponding motherboard also for more gpus you need a faster processor and hard disk to be able to feed them data quickly enough so they don t sit idle nvidia has been focusing on deep learning for a while now and the head start is paying off their cuda toolkit is deeply entrenched it works with all major dl frameworks tensoflow pytorch caffe cntk etc as of now none of these work out of the box with opencl cuda alternative which runs on amd gpus i hope support for opencl comes soon as there are great inexpensive gpus from amd on the market also some amd cards support half precision computation which doubles their performance and vram size currently if you want to do dl and want to avoid major headaches choose nvidia your gpu needs a computer around it hard disk first you need to read the data off the disk an ssd is recommended here but an hdd can work as well cpu that data might have to be decoded by the cpu e g jpegs fortunately any mid range modern processor will do just fine motherboard the data passes via the motherboard to reach the gpu for a single video card almost any chipset will work if you are planning on working with multiple graphic cards read this section ram it is recommended to have gigabytes of memory for every gigabyte of video card ram having more certainly helps in some situations like when you want to keep an entire dataset in memory power supply it should provide enough power for the cpu and the gpus plus watts extra you can get all of this for to or even less if you buy a used workstation here is performance comparison between all cards check the individual card profiles below notably the performance of titan xp and gtx ti is very close despite the huge price gap between them the price comparison reveals that gtx ti gtx and gtx have great value for the compute performance they provide all the cards are in the same league value wise except titan xp the king of the hill when every gb of vram matters this card has more than any other on the consumer market it s only a recommended buy if you know why you want it for the price of titan x you could get two gtx s which is a lot of power and gbs of vram this card is what i currently use it s a great high end option with lots of ram and high throughput very good value i recommend this gpu if you can afford it it works great for computer vision or kaggle competitions quite capable mid to high end card the price was reduced from to when ti was introduced gb is enough for most computer vision tasks people regularly compete on kaggle with these the newest card in nvidia s lineup if is over budget this will get you the same amount of vram gb also of the performance for of the price pretty sweet deal it s hard to get these nowadays because they are used for cryptocurrency mining with a considerable amount of vram for this price but somewhat slower if you can get it or a couple second hand at a good price go for it it s quite cheap but gb vram is limiting that s probably the minimum you want to have if you are doing computer vision it will be okay for nlp and categorical data models also available as p for cryptocurrency mining but it s the same card without a display output the entry level card which will get you started but not much more still if you are unsure about getting in deep learning this might be a cheap way to get your feet wet titan x pascal it used to be the best consumer gpu nvidia had to offer made obsolete by ti which has the same specs and is cheaper tesla gpusthis includes k k which is x k in one p and others you might already be using these via amazon web services google cloud platform or another cloud provider in my previous article i did some benchmarks on gtx ti vs k the performed five times faster than the tesla card and x faster than k k has gb vram and k a whopping gbs in theory the p and gtx ti should be in the same league performance wise however this cryptocurrency comparison has p lagging in every benchmark it is worth noting that you can do half precision on p effectively doubling the performance and vram size on top of all this k goes for over k for over and p is about and they get still get eaten alive by a desktop grade card obviously as it stands i don t recommend getting them all the specs in the world won t help you if you don t know what you are looking for here are my gpu recommendations depending on your budget i have over get as many gtx ti or gtx as you can if you have or gpus running in the same box beware of issues with feeding them with data also keep in mind the airflow in the case and the space on the motherboard i have to gtx ti is highly recommended if you want to go multi gpu get x gtx if you can find them or x gtx ti kaggle here i come i have to get the gtx or gtx ti maybe x gtx if you really want gpus however know that gb per model can be limiting i have to gtx will get you started unless you can find a used gtx i have less than get gtx ti or save for gtx if you are serious about deep learning deep learning has the great promise of transforming many areas of our life unfortunately learning to wield this powerful tool requires good hardware hopefully i ve given you some clarity on where to start in this quest disclosure the above are affiliate links to help me pay for well more gpus from a quick cheer to a standing ovation clap to show how much you enjoyed this story entrepreneur hacker machine learning deep learning and other types of learning
Datafiniti,3,5,https://blog.datafiniti.co/classifying-websites-with-neural-networks-39123a464055?source=tag_archive---------0----------------,Classifying Websites with Neural Networks – Knowledge from Data: The Datafiniti Blog,at datafiniti we have a strong need for converting unstructured web content into structured data for example we d like to find a page like and do the following both of these are hard things for a computer to do in an automated manner while it s easy for you or me to realize that the above web page is selling some jeans a computer would have a hard time making the distinction from the above page from either of the following web pages or both of these pages share many similarities to the actual product page but also have many key differences the real challenge though is that if we look at the entire set of possible web pages those similarities and differences become somewhat blurred which means hard and fast rules for classifications will fail often in fact we can t even rely on just looking at the underlying html since there are huge variations in how product pages are laid out in html while we could try and develop a complicated set of rules to account for all the conditions that perfectly identify a product page doing so would be extremely time consuming and frankly incredibly boring work instead we can try using a classical technique out of the artificial intelligence handbook neural networks here s a quick primer on neural networks let s say we want to know whether any particular mushroom is poisonous or not we re not entirely sure what determines this but we do have a record of mushrooms with their diameters and heights along with which of these mushrooms were poisonous to eat for sure in order to see if we could use diameter and heights to determine poisonous ness we could set up the following equation a diameter b height or for not poisonous poisonous we would then try various combinations of a and b for all possible diameters and heights until we found a combination that correctly determined poisonous ness for as many mushrooms as possible neural networks provide a structure for using the output of one set of input data to adjust a and b to the most likely best values for the next set of input data by constantly adjusting a and b this way we can quickly get to the best possible values for them in order to introduce more complex relationships in our data we can introduce hidden layers in this model which would end up looking something like for a more detailed explanation of neural networks you can check out the following links in our product page classifier algorithm we setup a neural network with input layer with nodes hidden layer with nodes and output layer with output nodes our input layer modeled several features including our output layer had the following our algorithm for the neural network took the following steps the ultimate output is two sets of input layers t and t that we can use in a matrix equation to predict page type for any given web page this works like so so how did we do in order to determine how successful we were in our predictions we need to determine how to measure success in general we want to measure how many true positive tp results as compared to false positives fp and false negatives fn conventional measurements for these are our implementation had the following results these scores are just over our training set of course the actual scores on real life data may be a bit lower but not by much this is pretty good we should have an algorithm on our hands that can accurately classify product pages about of the time of course identifying product pages isn t enough we also want to pull out the actual structured data in particular we re interested in product name price and any unique identifiers e g upc ean isbn this information would help us fill out our product search we don t actually use neural networks for doing this neural networks are better suited toward classification problems and extracting data from a web page is a different type of problem instead we use a variety of heuristics specific to each attribute we re trying to extract for example for product name we look at the h and h tags and use a few metrics to determine the best choice we ve been able to achieve around a accuracy here we may go into the actual metrics and methodology for developing them in a separate post we feel pretty good about our ability to classify and extract product data the extraction part could be better but it s steadily being improved in the meantime we re also working on classifying other types of pages such as business data company team pages event data and more as we roll out these classifiers and data extractors we re including each one in our crawl of the entire internet this means that we can scan the entire internet and pull out any available data that exists out there exciting stuff you can connect with us and learn more about our business people product and property apis and datasets by selecting one of the options below from a quick cheer to a standing ovation clap to show how much you enjoyed this story instant access to web data building the world s largest database of web data follow our journey
Yingjie Miao ,43,6,https://medium.com/kifi-engineering/from-word2vec-to-doc2vec-an-approach-driven-by-chinese-restaurant-process-93d3602eaa31?source=tag_archive---------0----------------,From word2vec to doc2vec: an approach driven by Chinese restaurant process,google s word vec project has created lots of interests in the text mining community it s a neural network language model that is both supervised and unsupervised unsupervised in the sense that you only have to provide a big corpus say english wiki supervised in the sense that the model cleverly generates supervised learning tasks from the corpus how two approaches known as continuous bag of words cbow and skip gram see figure in this paper cbow forces the neural net to predict current word by surrounding words and skip gram forces the neural net to predict surrounding words of the current word training is essentially a classic back propagation method with a few optimization and approximation tricks e g hierarchical softmax word vectors generated by the neural net have nice semantic and syntactic behaviors semantically ios is close to android syntactically boys minus boy is close to girls minus girl one can checkout more examples here although this provides high quality word vectors there is still no clear way to combine them into a high quality document vector in this article we discuss one possible heuristic inspired by a stochastic process called chinese restaurant process crp basic idea is to use crp to drive a clustering process and summing word vectors in the right cluster imagine we have an document about chicken recipe it contains words like chicken pepper salt cheese it also contains words like use buy definitely my the the word vec model gives us a vector for each word one could naively sum up every word vector as the doc vector this clearly introduces lots of noise a better heuristic is to use a weighted sum based on other information like idf or part of speech pos tag the question is could we be more selective when adding terms if this is a chicken recipe document i shouldn t even consider words like definitely use my in the summation one can argue that idf based weights can significantly reduce noise of boring words like the and is however for words like definitely overwhelming the idfs are not necessarily small as you would hope it s natural to think that if we can first group words into clusters words like chicken pepper may stay in one cluster along with other clusters of junk words if we can identify the relevant clusters and only summing up word vectors from relevant clusters we should have a good doc vector this boils down to clustering the words in the document one can of course use off the shelf algorithms like k means but most these algorithms require a distance metric word vec behaves nicely by cosine similarity this doesn t necessarily mean it behaves as well under eucledian distance even after projection to unit sphere it s perhaps best to use geodesic distance it would be nice if we can directly work with cosine similarity we have done a quick experiment on clustering words driven by crp like stochastic process it worked surprisingly well so far now let s explain crp imagine you go to a chinese restaurant there are already n tables with different number of peoples there is also an empty table crp has a hyperparamter r which can be regarded as the imagined number of people on the empty table you go to one of the n tables with probability proportional to existing number of people on the table for the empty table the number is r if you go to one of the n existing tables you are done if you decide to sit down at the empty table the chinese restaurant will automatically create a new empty table in that case the next customer comes in will choose from n tables including the new empty table inspired by crp we tried the following variations of crp to include the similarity factor common setup is the following we are given m vectors to be clustered we maintain two things cluster sum not centroid and vectors in clusters we iterate through vectors for current vector v suppose we have n clusters already now we find the cluster c whose cluster sum is most similar to current vector call this score sim v c variant v creates a new cluster with probability n otherwise v goes to cluster c variant if sim v c n goes to cluster c otherwise with probability n it creates a new cluster and with probability n n it goes to c in any of the two variants if v goes to a cluster we update cluster sum and cluster membership there is one distinct difference to traditional crp if we don t go to empty table we deterministically go to the most similar table in practice we find these variants create similar results one difference is that variant tend to have more clusters and smaller clusters variant tend to have fewer but larger clusters the examples below are from variant for example for a chicken recipe document the clusters look like this apparently the first cluster is most relevant now let s take the cluster sum vector which is the sum of all vectors from this cluster and test if it really preserves semantic below is a snippet of python console we trained word vector using the c implementation on a fraction of english wiki and read the model file using python library gensim model word vec c below denotes the cluster looks like the semantic is preserved well it s convincing that we can use this as the doc vector the recipe document seems easy now let s try something more challenging like a news article news articles tend to tell stories and thus has less concentrated topic words we tried the clustering on this article titled signals on radar puzzle officials in hunt for malaysian jet we got clusters again looks decent note that this is a simple pass clustering process and we don t have to specify number of clusters could be very helpful for latency sensitive services there is still a missing step how to find out the relevant cluster s we haven t yet done extensive experiments on this part a few heuristics to consider there are other problems to think about how do we merge clusters based on similarity among cluster sum vectors or averaging similarity between cluster members what is the minimal set of words that can reconstruct cluster sum vector in the sense of cosine similarity this could be used as a semantic keyword extraction method conclusion google s word vec provides powerful word vectors we are interested in using these vectors to generate high quality document vectors in an efficient way we tried a strategy based on a variant of chinese restaurant process and obtained interesting results there are some open problems to explore and we would like to hear what you think appendix python style pseudo code for similarity driven crp we wrote this post while working on kifi connecting people with knowledge learn more originally published at eng kifi com on march from a quick cheer to a standing ovation clap to show how much you enjoyed this story the kifi engineering blog
Milo Spencer-Harper,2200,3,https://medium.com/technology-invention-and-more/how-to-build-a-multi-layered-neural-network-in-python-53ec3d1d326a?source=tag_archive---------1----------------,How to build a multi-layered neural network in Python,in my last blog post thanks to an excellent blog post by andrew trask i learned how to build a neural network for the first time it was super simple lines of python code modelling the behaviour of a single neuron but what if we are faced with a more difficult problem can you guess what the should be the trick is to notice that the third column is irrelevant but the first two columns exhibit the behaviour of a xor gate if either the first column or the second column is then the output is however if both columns are or both columns are then the output is so the correct answer is however this would be too much for our single neuron to handle this is considered a nonlinear pattern because there is no direct one to one relationship between the inputs and the output instead we must create an additional hidden layer consisting of four neurons layer this layer enables the neural network to think about combinations of inputs you can see from the diagram that the output of layer feeds into layer it is now possible for the neural network to discover correlations between the output of layer and the output in the training set as the neural network learns it will amplify those correlations by adjusting the weights in both layers in fact image recognition is very similar there is no direct relationship between pixels and apples but there is a direct relationship between combinations of pixels and apples the process of adding more layers to a neural network so it can think about combinations is called deep learning ok are we ready for the python code first i ll give you the code and then i ll explain further also available here https github com miloharper multi layer neural network this code is an adaptation from my previous neural network so for a more comprehensive explanation it s worth looking back at my earlier blog post what s different this time is that there are multiple layers when the neural network calculates the error in layer it propagates the error backwards to layer adjusting the weights as it goes this is called back propagation ok let s try running it using the terminal command python main py you should get a result that looks like this first the neural network assigned herself random weights to her synaptic connections then she trained herself using the training set then she considered a new situation that she hadn t seen before and predicted the correct answer is so she was pretty close you might have noticed that as my neural network has become smarter i ve inadvertently personified her by using she instead of it that s pretty cool but the computer is doing lots of matrix multiplication behind the scenes which is hard to visualise in my next blog post i ll visually represent our neural network with an animated diagram of her neurons and synaptic connections so we can see her thinking from a quick cheer to a standing ovation clap to show how much you enjoyed this story studied economics at oxford university founder of www moju io interested in politics and ai technology trends and new invention follow this collection to update the latest trend update as a collection editor i don t have any permission to add your articles in the wild please submit your article and i will approve also follow this collection please
Josh,462,9,https://medium.com/technology-invention-and-more/everything-you-need-to-know-about-artificial-neural-networks-57fac18245a1?source=tag_archive---------3----------------,Everything You Need to Know About Artificial Neural Networks,the year was a monumental year in the field of artificial intelligence not only are computers learning more and learning faster but we re learning more about how to improve their systems everything is starting to align and because of it we re seeing strides we ve never thought possible until now we have programs that can tell stories about pictures we have cars that are driving themselves we even have programs that create art if you want to read more about advancements in read this article here at josh ai with ai technology becoming the core of just about everything we do we think it s important to understand some of the common terminology and to get a rough idea of how it all works a lot of the advances in artificial intelligence are new statistical models but the overwhelming majority of the advances are in a technology called artificial neural networks ann if you ve read anything about them before you ll have read that these anns are a very rough model of how the human brain is structured take note that there is a difference between artificial neural networks and neural networks though most people drop the artificial for the sake of brevity the word artificial was prepended to the phrase so that people in computational neurobiology could still use the term neural network to refer to their work below is a diagram of actual neurons and synapses in the brain compared to artificial ones fear not if the diagram doesn t come through very clearly what s important to understand here is that in our anns we have these units of calculation called neurons these artificial neurons are connected by synapses which are really just weighted values what this means is that given a number a neuron will perform some sort of calculation for example the sigmoid function and then the result of this calculation will be multiplied by a weight as it travels the weighted result can sometimes be the output of your neural network or as i ll talk about soon you can have more neurons configured in layers which is the basic concept to an idea that we call deep learning artificial neural networks are not a new concept in fact we didn t even always call them neural networks and they certainly don t look the same now as they did at their inception back during the s we had what was called a perceptron perceptrons were made of mcculloch pitts neurons we even had biased perceptrons and ultimately people started creating multilayer perceptrons which is synonymous with the general artificial neural network we hear about now but wait if we ve had neural networks since the s why are they just now getting huge it s a long story and i encourage you to listen to this podcast episode to listen to the fathers of modern anns talk about their perspective of the topic to quickly summarize there s a hand full of factors that kept anns from becoming more popular we didn t have the computer processing power and we didn t have the data to train them using them was frowned upon due to them having a seemingly arbitrary ability to perform well each one of these factors is changing our computers are getting faster and more powerful and with the internet we have all kinds of data being shared for use you see i mentioned above that the neurons and synapses perform calculations the question on your mind should be how do they learn what calculations to perform was i right the answer is that we need to essentially ask them a large amount of questions and provide them with answers this is a field called supervised learning with enough examples of question answer pairs the calculations and values stored at each neuron and synapse are slowly adjusted usually this is through a process called backpropagation imagine you re walking down a sidewalk and you see a lamp post you ve never seen a lamp post before so you walk right into it and say ouch the next time you see a lamp post you scoot a few inches to the side and keep walking this time your shoulder hits the lamp post and again you say ouch the third time you see a lamp post you move all the way over to ensure you don t hit the lamp post except now something terrible has happened now you ve walked directly into the path of a mailbox and you ve never seen a mailbox before you walk into it and the whole process happens again obviously this is an oversimplification but it is effectively what backpropogation does an artificial neural network is given a multitude of examples and then it tries to get the same answer as the example given when it is wrong an error is calculated and the values at each neuron and synapse are propagated backwards through the ann for the next time this process takes a lot of examples for real world applications the number of examples can be in the millions now that we have an understanding of artificial neural networks and somewhat of an understanding in how they work there s another question that should be on your mind how do we know how many neurons we need to use and why did you bold the word layers earlier layers are just sets of neurons we have an input layer which is the data we provide to the ann we have the hidden layers which is where the magic happens lastly we have the output layer which is where the finished computations of the network are placed for us to use layers themselves are just sets of neurons in the early days of multilayer perceptrons we originally thought that having just one input layer one hidden layer and one output layer was sufficient it makes sense right given some numbers you just need one set of computations and then you get an output if your ann wasn t calculating the correct value you just added more neurons to the single hidden layer eventually we learned that in doing this we were really just creating a linear mapping from each input to the output in other words we learned that a certain input would always map to a certain output we had no flexibility and really could only handle inputs we d seen before this was by no means what we wanted now introduce deep learning which is when we have more than one hidden layer this is one of the reasons we have better anns now because we need hundreds of nodes with tens if not more layers this leads to a massive amount of variables that we need to keep track of at a time advances in parallel programming also allow us to run even larger anns in batches our artificial neural networks are now getting so large that we can no longer run a single epoch which is an iteration through the entire network at once we need to do everything in batches which are just subsets of the entire network and once we complete an entire epoch then we apply the backpropagation along with now using deep learning it s important to know that there are a multitude of different architectures of artificial neural networks the typical ann is setup in a way where each neuron is connected to every other neuron in the next layer these are specifically called feed forward artificial neural networks even though anns are generally all feed forward we ve learned that by connecting neurons to other neurons in certain patterns we can get even better results in specific scenarios recurrent neural networks rnn were created to address the flaw in artificial neural networks that didn t make decisions based on previous knowledge a typical ann had learned to make decisions based on context in training but once it was making decisions for use the decisions were made independent of each other when would we want something like this well think about playing a game of blackjack if you were given a and a to start you know that low cards are out of the deck information like this could help you determine whether or not you should hit rnns are very useful in natural language processing since prior words or characters are useful in understanding the context of another word there are plenty of different implementations but the intention is always the same we want to retain information we can achieve this through having bi directional rnns or we can implement a recurrent hidden layer that gets modified with each feedforward if you want to learn more about rnns check out either this tutorial where you implement an rnn in python or this blog post where uses for an rnn are more thoroughly explained an honorable mention goes to memory networks the concept is that we need to retain more information than what an rnn or lstm keeps if we want to understand something like a movie or book where a lot of events might occur that build on each other convolutional neural networks cnn sometimes called lenets named after yann lecun are artificial neural networks where the connections between layers appear to be somewhat arbitrary however the reason for the synapses to be setup the way they are is to help reduce the number of parameters that need to be optimized this is done by noting a certain symmetry in how the neurons are connected and so you can essentially re use neurons to have identical copies without necessarily needing the same number of synapses cnns are commonly used in working with images thanks to their ability to recognize patterns in surrounding pixels there s redundant information contained when you look at each individual pixel compared to its surrounding pixels and you can actually compress some of this information thanks to their symmetrical properties sounds like the perfect situation for a cnn if you ask me christopher olah has a great blog post about understanding cnns as well as other types of anns which you can find here another great resource for understanding cnns is this blog post the last ann type that i m going to talk about is the type called reinforcement learning reinforcement learning is a generic term used for the behavior that computers exhibit when trying to maximize a certain reward which means that it in itself isn t an artificial neural network architecture however you can apply reinforcement learning or genetic algorithms to build an artificial neural network architecture that you might not have thought to use before a great example and explanation can be found in this video where youtube user sethbling creates a reinforcement learning system that builds an artificial neural network architecture that plays a mario game entirely on its own another successful example of reinforcement learning can be seen in this video where the company deepmind was able to teach a program to master various atari games now you should have a basic understanding of what s going on with the state of the art work in artificial intelligence neural networks are powering just about everything we do including language translation animal recognition picture captioning text summarization and just about anything else you can think of you re sure to hear more about them in the future so it s good that you understand them now this post was written by aaron at josh ai previously aaron worked at northrop grumman before joining the josh team where he works on natural language programming nlp and artificial intelligence ai aaron is a skilled yoyo expert loves video games and music has been programming since middle school and recently turned josh ai is an ai agent for your home if you re interested in following josh and getting early access to the beta enter your email at https josh ai like josh on facebook http facebook com joshdotai follow josh on twitter http twitter com joshdotai from a quick cheer to a standing ovation clap to show how much you enjoyed this story technology trends and new invention follow this collection to update the latest trend update as a collection editor i don t have any permission to add your articles in the wild please submit your article and i will approve also follow this collection please
Milo Spencer-Harper,317,6,https://medium.com/deep-learning-101/how-to-create-a-mind-the-secret-of-human-thought-revealed-6211bbdb092a?source=tag_archive---------4----------------,How to create a mind: The secret of human thought revealed,in my quest to learn about ai i read how to create a mind the secret of human thought revealed by ray kurzweil it was incredibly exciting and i m going to share what i ve learned if i was going to summarise the book in one sentence i could do no better than kurzweil s own words kurzweil argues convincingly that it is both possible and desirable he goes on to suggest that the algorithm may be simpler than we would expect and that it will be based on the pattern recognition theory of the mind prtm the human brain is the most incredible thing in the known universe a three pound object it can discover relativity imagine the universe create music build the taj mahal and write a book about the brain however it also has limitations and this gives us clues as to how it works recite the alphabet ok good now recite it backwards the former was easy the latter likely impossible yet a computer finds it trivial to reverse a list this tells us that the human brain can only retrieve information sequentially studies have also revealed that when thinking about something we can only hold around four high level concepts in our brain at a time that s why we use tools such as pen and paper to solve a maths problem to help us think so how does the human brain work mammals actually have two brains the old reptilian brain called the amygdala and the conscious part called the neocortex the amygdala is pre programmed through evolution to seek pleasure and avoid pain we call this instinct but what distinguishes mammals from other animals is that we have also evolved to have a neocortex our neocortex rationalises the world around us and makes predictions it allows us to learn the two brains are tightly bound and work together however when reading the book i wondered if these two brains might also be in conflict it would explain why the idea of internal struggle is present throughout literature and religion good vs evil social conformity vs hedonism what s slightly more alarming is we may have more minds than that our brain is divided into two hemispheres left and right studies of split brain patients where the connection between them has been severed shows that these patients are not necessarily aware that the other mind exists if one mind moves the right hand the other mind will post rationalise this decision by creating a false memory a process known as confabulation this has implications for us all we may not have the free will which we perceive to have our conscious part of the brain may simply be creating explanations for what the unconscious parts have already done so how does the neocortex work we know that it consists of around billion cells which we call neurons these neurons are connected together and transmit information using electrical impulses if the sum of the electrical pulses across multiple inputs to a neuron exceeds a certain threshold that neuron fires causing the next neuron in the chain to fire and this goes on continuously we call these processes thoughts at first scientists thought this neural network was such a complicated and tangled web that it would be impossible to ever understand however kurzweil uses the example of the einstein s famous equation e mc to demonstrate that sometimes the solutions to complex problems are surprisingly simple there are many examples in science from newtonian mechanics to thermodynamics which show that moving up a level of abstraction dramatically simplifies modelling complex systems recent innovations in brain imaging techniques have revealed that the neocortex contains modules each consisting of around neurons repeating over and over again there are around million of these modules arranged in a grid so if we could discover the equations which model this module repeat it on a computer million times and expose it to sensory input we could create an intelligent being but what do these modules do kurzweil who has spent decades researching ai proposes that these modules are pattern recognisers when reading this page one pattern recogniser might be responsible for detecting a horizontal stroke this module links upward to a module responsible for the letter a and if the other relevant stroke modules light up the a module also lights up the modules a p p and l link to the apple module which in turn is linked to higher level pattern recognisers such as thoughts about apples you don t actually need to see the e because the apple pattern recogniser fires downward telling the one responsible for the letter e that there is a high probability of seeing one conversely inhibitory signals suppress pattern recognisers from firing if a higher level pattern recogniser has detected such an event is unlikely given the context we literally see what we expect to see kurzweil calls this the pattern recogniser theory of the mind prtm although it is hard for us to imagine all of our thoughts and decisions can be explained by huge numbers of these pattern recognisers hooked together we organise these thoughts to explain the world in a hierarchal fashion and use words to give meaning to these modules the world is naturally hierarchal and the brain mirrors this leaves are on trees trees make up a forest and a forest covers a mountain language is closely related to our thoughts because language directly evolved from and mirrors our brain this helps to explain why different languages follow remarkably similar structures it explains why we think using our native language we use language not only to express ideas to others but to express ideas within our own mind what s interesting is that when ai researchers have worked independently of neuroscientists their most successful methods turned out to be equivalent to the human brain s methods thus the human brain offers us clues for how to create an intelligent nonbiological entity if we work out the algorithm for a single pattern recogniser we can repeat it on a computer creating a neural network kurzweil argues that these neural networks could become conscious like a human mind free from biological constraints and benefiting from the exponential growth in computing power these entities could create even smarter entities and surpass us in intelligence this prediction is called technological singularity i ll discuss the ethical and social considerations in a future blog post but for now let s assume it is desirable the question then becomes what is the algorithm for a single pattern recogniser kurzweil recommends using a mathematical technique called hierarchal hidden markov models named after the russian mathematician andrey markov however this technique is too technical to be properly explained in kurzweil s book so my next two goals are to learn as much as i can about hierarchal hidden markov models to build a simple neural network written in python from scratch which can be trained to complete a simple task in my next blog post i learn how to build a neural network in lines of python code note submissions do not necessarily represent the views of the editors from a quick cheer to a standing ovation clap to show how much you enjoyed this story studied economics at oxford university founder of www moju io interested in politics and ai fundamentals and latest developments in deeplearning
Karl N.,10,7,https://gab41.lab41.org/taking-keras-to-the-zoo-9a76243152cb?source=tag_archive---------5----------------,Taking Keras to the Zoo – Gab41,if you follow any of the popular blogs like google s research fastml smola s adventures in data land or one of the indie pop ones like edwin chen s blog you ve probably also used modelzoo actually if you re like our boss you affectionately call it the zoo actually x if you have interesting blogs that you read feel free to let us know unfortunately modelzoo is only supported in caffe fortunately we ve taken a look at the difference between the kernels in keras theano and caffe for you and after reading this blog you ll be able to load models from modelzoo into any of your favorite python tools why this post why not just download our github code in short it s better you figure out how these things work before you use them that way you re better armed to use the latest tensorflow and neon toolboxes if you re prototyping and transitioning your code to caffe so there s hinton s dropout and then there s caffe s dropout and they re different you might be wondering what s the big deal well sir i have a name of a guy for you and it s willy mr willy nilly one thing willy nilly likes is the number another thing he likes is to introduce regularization which includes dropout arbitrarily and bayesian theorists aren t a fan those people try to fit their work into the probabilistic framework and they re trying to hold onto what semblance of theoretical bounds exist for neural networks however for you as a practitioner understanding who s doing what will save you hours of debugging code we singled out dropout because the way people have implemented it spans the gamut there s actually some history as to this variation but no one really cared because optimizing for it has almost universally produced similar results much of the discussion stems from how the chain rule is implemented since randomly throwing stuff away is apparently not really a differentiable operation passing gradients back i e backpropagation is a fun thing to do there s a technically right way to do it and then there s what s works back to modelzoo where we d recommend you note the only sentence of any substance in this section and the sentence is as follows while keras and perhaps other packages multiply the gradients by the retention probability at inference time caffe does not that is to say if you have a dropout level of your retention probability is and at inference time keras will scale the output of your prediction by so download the modelzoo caffemodels but know that deploying them on caffe will produce non scaled results whereas keras will hinton explains the reason why you need to scale and the intuition is as follows if you ve only got a portion of your signal seeping through to the next layer during training you should scale the expectation of what the energy of your final result should be seems like a weird thing to care about right the argument that minimizes x is still the same as the argument that minimizes x this turns out to be a problem when you re passing multiple gradients back and don t implement your layers uniformly caffe works in instances like siamese networks or bilinear networks but should you scale your networks on two sides differently don t be surprised if you re getting unexpected results what does this look like in francois s code look at the dropout code on github or in your installation folder under keras layers core py if you want to make your own layer for loading in the dropout module just comment out the part of the code that does this scaling you can modify the original code or you can create your own custom layer we ve opted to keep our installation of keras clean and just implemented a new class that extended maskedlayer btw you should be careful in your use of dropout our experience with them is that they regularize okay but could contribute to vanishing gradients really quickly everyday except for sunday and some holidays a select few machine learning professors and some signal processing leaders meet in an undisclosed location in the early hours of the morning the topic of their discussion is almost universally how do we get researchers and deep learning practitioners to code bugs into their programs one of the conclusions a while back was that the definition of convolution and dense matrix multiplication or cross correlation should be exactly opposite of each other that way when people are building algorithms that call themselves convolutional neural networks no one will know which implementation is actually being used for the convolution portion itself for those who don t know convolutions and sweeping matrix multiplication across an array of data differ in that convolutions will be flipped before being slid across the array from wikipedia the definition is on the other hand if you re sweeping matrix multiplications across the array of data you re essentially doing cross correlation which on wikipedia looks like like we said the only difference is that darned minus plus sign which caused us some headache we happen to know that theano and caffe follow different philosophies once again caffe doesn t bother with pleasantries and straight up codes efficient matrix multiplies to load models from modelzoo into either keras and theano will require the transformation because they strictly follow the definition of convolution the easy fix is to flip it yourself when you re loading the weights into your model for d convolution this looks like weights weights here the variable weights will be inserted into your model s parameters you can set weights by indexing into the model for example say i want to set the th layer s weights i would type model layers set weights weights incidentally and this is important when loading any caffemodel into python you may have to transpose it in order to use it you can quickly find this out by loading it if you get an error but we thought it worth noting alright alright we know what you re really here for just getting the code and running with it so we ve got some example code that classifies using keras and the vgg net from the web at our git see the link below but let s go through it just a bit here s a step by step account of what you need to do to use the vgg caffe model and now you have the basics go ahead and take a look at our github for some goodies let us know originally published at www lab org on december from a quick cheer to a standing ovation clap to show how much you enjoyed this story gab is lab s blog exploring data science machine learning and artificial intelligence geek out with us
Milo Spencer-Harper,42,3,https://medium.com/@miloharper/thanks-so-much-for-your-response-jared-really-glad-to-hear-you-enjoyed-reading-it-9d73caa469ff?source=tag_archive---------6----------------,Thanks so much for your response Jared. Really glad you enjoyed reading it.,thanks so much for your response jared really glad you enjoyed reading it could you go into more detail about finding the error on layer that s a really great question i ve changed this response quite a bit as i wrote it because your question helped me improve my own understanding it sounds like you know quite a lot about neural networks already however i m going to explain everything fully for readers who are new to the field in the article you read i modelled the neural network using matrices grids of numbers that s the most common method as it is computationally faster and mathematically equivalent but it hides a lot of the details for example line calculates the error in layer but it is hard to visualise what it is doing to help me learn i ve re written that same code by modelling the layers neurons and synapses explicitly and have created a video of the neural network learning i m going to use this new version of my code to answer your question for clarity i ll describe how i m going to refer to the layers the three input neurons are layer the four neurons in the hidden layer are layer and the single output neuron is layer in my code i chose to associate the synapses with the neuron they flow into how do i find the error in layer first i calculate the error of the output neuron layer which is the difference between its output and the output in the training set example then i work my way backwards through the neural network so i look at the incoming synapses into layer and estimate how much each of the neurons in layer were responsible for the error this is called back propagation in my new version of the code the neural network is represented by a class called neuralnetwork and it has a method called train which is shown below you can see me calculating the error of the ouput neuron lines and then i work backwards through the layers line next i cycle through all the neurons in a layer line and call each individual neuron s train method line but what does the neuron s train method do here it is you can see that i cycle through every incoming synapse into the neuron the two key things to note are let s consider line even more carefully since this is the line which answers your question directly for each neuron in layer its error is equal to the error in the output neuron layer multiplied by the weight of its synapse into the output neuron multiplied by the sensitivity of the output neuron to input the sensitivity of a neuron to input is described by the gradient of its output function since i used the sigmoid curve as my ouput function the gradient is the derivative of the sigmoid curve as well as using the gradient to calculate the errors i also used the gradient to adjust the weights so this method of learning is called gradient descent if you look back at my old code which uses matrices you can see that it is mathematically equivalent unless i made a mistake with the matrices method i calculated the error for all the neurons in layer simultaneously with the new code i iterated through each neuron separately i hope that helps answer your question also i m curious if there is any theory or rule of thumb on how many hidden layers and how many neurons in each layer should be used to solve a problem another good question i m not sure i m pretty new to neural networks i only started learning about them recently i did read a book by the ai researcher ray kurzweil which said that an evolutionary approach works better than consulting experts when selecting the overall parameters for a neural network those neural networks which learned the best would be selected he would make random mutations to the parameters and then pit the offspring against one another from a quick cheer to a standing ovation clap to show how much you enjoyed this story studied economics at oxford university founder of www moju io interested in politics and ai
Nikolai Savas,50,10,https://medium.com/@savas/craig-using-neural-networks-to-learn-mario-a76036b639ad?source=tag_archive---------7----------------,CrAIg: Using Neural Networks to learn Mario – Nikolai Savas – Medium,joe crozier and i recently came back from yhack a hour person hackathon held by yale university this is our second year in a row attending and for the second time we managed to place in the top our project named craig is a self teaching algorithm that learns to play super mario bros for the nintendo entertainment system nes it begins with absolutely no knowledge of what mario is how to play it or what winning looks like and using neuroevolution slowly builds up a foundation for itself to be able to progress in the game my focus on this project was the gritty details of the implementation of craig s evolution algorithm so i figured i d make a relatively indepth blog post about it craig s evolution is based on a paper titled evolving neural networks through augmented topologies specifically an algorithm titled neat the rest of the blog post is going to cover my implementation of it hopefully in relatively layman s terms before we jump right into the algorithm i m going to lay a foundation for the makeup of craig s brain his brain at any given point playing the game is made up of a collection of neurons and synapses alternatively titled nodes and connections links essentially his brain is a directed graph above is the second part of this project a node js server that displays the current state of craig s brain or what he is thinking let s go through it quickly to understand what it s representing on the left you see a big grid of squares this is what the game looks like right now or what craig can see he doesn t know what any of the squares mean but he knows that an air tile is different from a ground tile in some way each of the squares is actually an input neuron on the right side you can see the output neurons or the buttons that craig can press you can also see a line going from one of the black squares on the left grid to the r neuron labelled this is a synapse and when the input neuron fires on the left it will send a signal down the synapse and tell craig to press the r button in this way craig walks right as craig evolves more neurons and synapses are created until his brain might look something more like this in this one i ll just point out a couple things first of all the green square on the left is a goomba second you can see another neuron at the very bottom labelled this is called a hidden neuron and represents a neuron that is neither input nor output they appear in craig s brain for added complexity as he evolves you can also see that at his time of death mario just died to a goomba he was trying to press the r and b buttons while learning mario is a neat application of neural networks and neuroevolution it serves mostly as a means to demonstrate the power of these self evolving neural networks in reality the applications for neural networks is endless while craig only learned how to play a simple nes game the exact same algorithm that was implemented could also be applied to a robot that cleans your house works in a factory or even paints beautiful paintings craig is a cool peek into the future where machines no longer need to be programmed to complete specific tasks but are instead given guidelines and can teach themselves and learn from experience as the tasks we expect machines to complete become more and more complex it becomes less possible to hard code their tasks in we need more versatile machines to work for us and evolving neural networks are a step in that direction if you re curious about some history behind the problems encountered by neuroevolution i highly recommend reading the paper that this algorithm is based off the first section of the paper covers many different approaches to neuroevolution and their benefits neat is a genetic algorithm that puts every iteration of craig s brain to the test and then selectively breeds them in a very similar way to the evolution of species in nature the hierarchy is as follows synapse neuron building blocks of craig s brain genome an iteration of craig s brain essentially a collection of neurons and synapses species a collection of genomes generation an iteration of the neat algorithm this is repeated over and over to evolve craig the first step every generation is to calculate the fitness of every individual genome from the previous generation this involves running the same function on each genome so that neat knows how successful each one is for craig this means running through a mario level using a particular genome or brain after running through the level we determine the fitness of the genome by this function once the fitness of every genome has been calculated we can move on to the next portion of the algorithm this part of the algorithm is probably the least intuitive the reason for this adjusted fitness is to discourage species from growing too big as the population in a species goes up their adjusted fitness goes down forcing the genetic algorithm to diversify the proper implementation of this algorithm is relatively intensive so for craig s implementation we simplified it to the following the important part here is that each genome now has an adjusted fitness value associated with it here s where the natural selection part comes in the survival of the fittest portion is all about determining how many genomes survive another generation as well as how many offspring will be born in the species the algorithms used here aren t outlined directly in the paper so most of these algorithms were created through trial and error the first step is to determine how many off a species will die to make room for more babies this is done proportionally to a species adjusted fitness the higher the adjusted fitness the more die off to make room for babies the second step is to determine how many children should be born in the species this is also proportional to the adjusted fitness of the species by the end of these two functions the species will have a certain number of genomes left as well as a baby quota the difference between the number of genomes and the populationsize this algorithm is necessary to allow for species to be left behind sometimes a species will go down the completely wrong path and there s no point in keeping them around this algorithm works in a very simple way if a species is in the bottom of the entire generation it is marked for extinction if a species is marked for extinction times in a row then all genomes in the species are killed off now comes the fun genetics part each species should have a certain number of genomes as well as a certain number of allotted spots for new offspring those spots now need to be populated each empty population spot needs to be filled but can be filled through either asexual or sexual reproduction in other words offspring can result from either two genomes in the species being merged or from a mutation of a single genome in the species before i discuss the process of merging two genomes i ll first discuss mutations there are three kinds of mutations that can happen to a genome in neat they are as follows this involves a re distribution of all synapse weights in a genome they can be either completely re distributed or simply perturbed meaning changed slightly mutate add synapse adding a synapse means finding two previously unconnected nodes and connecting them with a synapse this new synapse is given a random weight mutate add node this is the trickiest of the mutations when adding a node you need to split an already existing synapse into two synapses and add a node in between them the weight of the original synapse is copied on to the second synapse while the first synapse is given a weight of one important fact to note is that the first synapse bright red in the above picture is not actually deleted but merely disabled this means that it exists in the genome but it is marked as inactive synapses added in either mutate add node or mutate add synapse are given a unique id called a historical marking that is used in the crossover mating algorithm when two genomes mate to produce an offspring there is an algorithm detailed in the neat paper that must be followed the intuition behind it is to match up common ancestor synapses remember we ve been keeping their historical marking s then take the mutations that don t match up and mix and match them to create the child once a child has been created in this way it undergoes the mutation process outlined above i won t go into too much detail on this algorithm but if you re curious about it you can find a more detailed explanation of it in section of the original paper or you can see the code i used to implement it here once all the babies have been created in every species we can finally progress to the final stage of the genetic algorithm respeciation essentially we first select a candidate genome from each species this genome is now the representative for the species all genomes that are not selected as candidates are put into a generic pool and re organized the re organization relies on an equation called the compatibility distance equation this equation determines how similar or different any two given genomes are i won t go into the gritty details of how the equation works as it is well explain in section of the original paper as well as here in craig s code if a genome is too different from any of the candidate genomes it is placed in its own species using this process all of the genomes in the generic pool are re placed into species once this process has completed the generation is done and we are ready to re calculate the fitness of each of the genomes while creating craig meant getting very little sleep at yhack it was well worth it for a couple reasons first of all the neat algorithm is a very complex one learning how to implement a complex algorithm without losing myself in its complexity was an exercise in code cleanliness despite being pressed for time because of the hackathon it was also very interesting to create an algorithm that is mostly based off a paper as opposed to one that i have example code to work with often this meant carefully looking into the wording used in the paper to determine whether i should be using a or a for example one of the most difficult parts of this project was that i was unable to test as i was programming i essentially wrote all of the code blind and then was able to test and debug it once it had all been created this was for a couple reasons partially because of the time constraints of a hackathon and partially because the algorithm as a whole has a lot of interlocking parts meaning they needed to be in a working state to be able to see if the algorithm worked overall i m happy and proud by how joe and i were able to deal with the stress of creating such a deep and complex project from scratch in a short hour period not only did we enjoy ourselves and place well but we also managed to teach craig some cool skills like jumping over the second pipe in level from a quick cheer to a standing ovation clap to show how much you enjoyed this story http savas ca niko savas ca
Dr Ben Medlock,32,4,https://medium.com/@Ben_Medlock/why-turing-s-legacy-demands-a-smarter-keyboard-9e7324463306?source=tag_archive---------8----------------,Why Turing’s legacy demands a smarter keyboard – Dr Ben Medlock – Medium,why turing s legacy demands a smarter keyboard when you start a company you dream of walking in the footsteps of your heroes for those working in artificial intelligence the british computer scientist and father of the field alan turing always comes to mind i thought of him when i did my phd when i co founded an ai keyboard company in and when we pasted his name on a meeting room door in our first real office as a british tech company today is a big day for swiftkey we ve introduced some of the principles originally conceived of by turing artificial neural networks into our smartphone keyboard for the first time i want to explain how we managed to do it and how a technology like this something you may never have heard of before will help define the smartphone experience of the future this is my personal take for the official version check out the swiftkey blog frustration free typing on a smartphone relies on complex software to automatically fix typos and predict the words you might want to use swiftkey has been at the forefront of this area since and today our software is used across the world on more than half a billion handsets soon after we launched the first version of our app in i started to think about using neural networks to power smartphone typing rather than the more traditional n gram approach a sophisticated form of word frequency counting at the time it seemed little more than theoretical as mobile hardware wasn t up to the task however three years later the situation began to look more favorable and in late our team started working on the idea in earnest in order to build a neural network powered swiftkey our engineers were tasked with the enormous challenge of coming up with a solution that would run locally on a smartphone without any perceptible lag neural network language models are typically deployed on large servers requiring huge computational resources getting the tech to fit into a handheld mobile device would be no small feat after many months of trial error and lots of experimentation the team realized they might have found an answer with a combination of two approaches the first was to make use of the graphical processing unit gpu on the phone utilizing the powerful hardware acceleration designed for rendering complex graphical images but thanks to some clever programming they were also able to run the same code on the standard processing unit when the gpu wasn t available this combo turned out to be the winning ticket so back to turing in he published a little known essay called intelligent machinery in which he outlined two forms of computing he felt could ultimately lead to machines exhibiting intelligent behavior the first was a variant of his highly influential universal turing machine destined to become the foundation for hardware design in all modern digital computers the second was an idea he called an unorganized machine a type of computer that would use a network of artificial neurons to accept inputs and translate them into predicted outputs connecting together many small computing units each with the ability to receive modify and pass on basic signals is inspired by the structure of the human brain that s why the appropriation of this concept in software form is called an artificial neural network or a neural network for short the idea is that a collection of artificial neurons are connected together in a specific way called a topology such that a given set of inputs what you ve just typed for example can be turned into a useful output e g your most likely next word the network is then trained on millions or even billions of data samples and the behavior of the individual neurons is automatically tweaked to achieve the desired overall results in the last few years neural network approaches have facilitated great progress on tough problems such as image recognition and speech processing researchers have also begun to demonstrate advances in non traditional tasks such as automatically generating whole sentence descriptions of images such techniques will allow us to better manage the explosion of uncategorized visual data on the web and will lead to smarter search engines and aids for the visually impaired among a host of other applications the fact that the human brain is so adept at working with language suggests that neural networks inspired by the brain s internal structure are a good bet for the future of smartphone typing in principle neural networks also allow us to integrate powerful contextual cues to improve accuracy for instance a user s current location and the time of day these will be stepping stones to more efficient and personal device interactions the keyboard of the future will provide an experience that feels less like typing and more like working with a close friend or personal assistant applying neural networks to real world problems is part of a wider technology movement that s changing the face of consumer electronics for good devices are getting smarter more useful and more personal my goal is that swiftkey contributes to this revolution we should all be spending less time fixing typos and more time saying what we mean when it matters it s the legacy we owe to turing the photograph alan turing by joncallas is licensed under cc by from a quick cheer to a standing ovation clap to show how much you enjoyed this story technopreneur swiftkey co founder
Nieves Ábalos,18,7,https://labs.beeva.com/sem%C3%A1ntica-desde-informaci%C3%B3n-desestructurada-90ce87736812?source=tag_archive---------9----------------,Semántica desde información desestructurada – BEEVA Labs,detectar patrones es un nu cleo importante en el mundo del procesamiento del lenguaje natural esta deteccio n de patrones nos permite clasificar documentos lo que tiene muchas aplicaciones ana lisis de sentimiento sentiment analysis recuperacio n de documentos document retrieval bu squeda web filtrado de spam etc esta clasificacio n se hace de manera automa tica de forma supervisada o no supervisada tambie n conocida como clustering de documentos entre las te cnicas ma s cla sicas y utilizadas generalmente supervisadas encontramos clasificadores naive bayes a rboles de decisio n id o c tf idf latent semantic indexing lsi y support vector machines svm algunas te cnicas utilizadas para extraer caracteri sticas suelen inspirarse en co mo el ser humano es capaz de aprender de informacio n simple y llegar a informacio n ma s compleja se pueden diferenciar entre redes neuronales algunas topologi as de redes neuronales se engloban dentro del concepto deep learning y te cnicas que no usan estas redes para reconocer patrones en beeva nos hemos encontrado varias veces con un mismo problema co mo sabemos si dos documentos son semejantes y con semejantes queremos decir que tratan de lo mismo esto entre otras cosas nos permitiri a categorizar documentos dentro del mismo tema de manera automa tica asi que a priori nos encontramos con dos retos necesitamos representar los documentos de manera que los algoritmos que usemos los puedan entender normalmente estas representaciones o modelos esta n basadas en matrices de caracteri sticas que posee cada documento para representar textos podemos usar te cnicas de representacio n de manera local o de manera continua la representacio n local es aquella en la que solo tenemos en cuenta las palabras de forma aislada y se representa como un conjunto de te rminos i ndice o palabras clave n gramas bag of words este tipo de representacio n no tiene en cuenta la relacio n entre te rminos la representacio ncontinua es aquella en la que si se tiene en cuenta el contexto de las palabras y la relacio n entre ellas y se representan como matrices vectores conjuntos e incluso nodos lsa o lsi lds lda representaciones distribuidas o predictivas usando redes neuronales para nuestro primer reto extraer sema ntica vamos a probar una representacio n continua llamada representacio n distribuida de palabras distributed representations of words esta consiste en aprender representaciones vectoriales de palabras es decir vamos a tener un espacio multidimensional en el que una palabra es representada como un vector una de las cosas interesantes de estos vectores es que son capaces de extraer caracteri sticas tan relevantes como propiedades sinta cticas y sema nticas de las palabras turian et al la otra es que este aprendizaje automa tico se realiza con datos de entrada no etiquetados es decir es no supervisado estos vectores pueden ser utilizados como entrada de muchas aplicaciones de procesamiento del lenguaje natural y machine learning de hecho es nuestro segundo reto utilizaremos estos vectores para intentar extraer temas de documentos para aplicar esta te cnica usamos la herramienta word vec mikolov et al google que utiliza como entrada un corpusde textos o documentos cualquiera y obtener como salida vectores representando las palabras la arquitectura en la que se basa word vec utiliza redes neuronales para aprender estas representaciones aunque tambie n se pueden obtener vectores que representen frases pa rrafos o incluso documentos completos le and mikolov primero utilizamos la implementacio n en python de la herramienta word vec incluida en la libreri a gensim como entrada para generar los vectores tenemos dos datasets con documentos en castellano wikipedia y yahoo answers de este dataset solo los que esta n en espan ol el proceso es el siguiente figura dado el conjunto de textos se construye un vocabulario y word vec aprende las representaciones vectoriales de palabras los algoritmos de aprendizaje que utiliza word vec son bag of words continuo y skip gram continuo ambos algoritmos aprenden las representaciones de una palabra las cuales son u tiles para predecir otras palabras en la frase como sabemos que los vectores capturan muchas regularidades lingu i sticas podemos aplicar operaciones vectoriales para extraer muchas propiedades interesantes por ejemplo si queremos saber que palabras son las ma s similares a una dada buscamos cuales esta n ma s cerca aplicando distancia coseno cosine distance o similitud coseno cosine similarity por ejemplo con el modelo de wikipedia que cinco palabras se parecen ma s a una dada tambie n podemos obtener que seis palabras se parecen ma s a dos dadas con el modelo de la wikipedia y el de yahoo para ver las diferencias otra propiedad interesante es que las operaciones vectoriales vector rey vector hombre vector mujer nos da como resultado un vector muy cercano a vector reina por ejemplo vector pareja vector hombre vector novio nos da como resultado estos vectores al haber trabajado con dos conjuntos de datos diferentes wikipedia y yahoo answers podemos crear dos espacios de representaciones vectoriales ligeramente diferentes con respecto al vocabulario usado y la sema ntica inherente en ellos en el de yahoo encontramos entre las palabras ma s similares la misma palabra mal escrita de diferentes maneras en wikipedia esto no pasa pues la escritura es mucho ma s correcta adema s en el conjunto de yahoo tenemos no so lo preguntas en castellano sino que tambie n encontramos otras en mejicano argentino y otros dialectos de sudame rica esto nos permite encontrar palabras similares en diferentes dialectos con respecto al tiempo que tardamos en crear nuestro espacio de vectores la mayori a del tiempo se dedica al preprocesamiento y limpieza de esos documentos la implementacio n de gensim permite modificar los para metros de creacio n del modelo e incluso utilizar varios workers con cython para que el entrenamiento sea ma s ra pido la calidad de estos vectores dependera de la cantidad de datos de entrenamiento del taman o de los vectores y del algoritmo elegido para entrenar para obtener mejores resultados es necesario entrenar los modelos con datasets grandes y con suficiente dimensionalidad para ma s detalles os recomendamos la lectura del trabajo de mikolov y le en la siguiente tabla os mostramos el tiempo que se tarda aproximadamente en entrenar unos mb de datos suficientes para obtener un buen modelo de vectores el tiempo total es el tiempo que tardamos en preprocesar los datos entrenar y guardar el modelo para posteriores usos para usar a representaciones vectoriales de documentos hemos utilizado doc vec tambie n de gensim como entrada de datos hemos considerado documento como una pa gina de la wikipedia o una pregunta de yahoo con sus respuestas hemos variado el taman o del fichero de entrada de documentos a para un worker y una dimensio n de y el tiempo de entrenamiento se reduce bastante lo podemos ver en la siguiente tabla los tests ejecutados para ver el comportamiento del espacio de vectores no han sido tan satisfactorios como con word vec los resultados para palabras similares son peores que con word vec y para encontrar documentos similares a uno dado vemos que no devuelve nada con mucho sentido como alternativa buscamos otros me todos que nos puedan decir que documentos son parecidos entre si os los presentaremos en el siguiente post word vec es considerado como un me todo inspirado en deep learning recomiendo la lectura de este arti culo para aclarar conceptos en ciertos grupos de especialistas en la materia y no tanto deep learning sino shallow learning en otros grupos sea como sea la creacio n de espacios vectoriales para extraer propiedades sinta cticas y sema nticas de las palabras de manera automa tica y no supervisada nos abre todo un mundo de posibilidades a explorar estos vectores sirven de entrada para muchas aplicaciones como traduccio n automa tica clusterizacio n categorizacio n e incluso puede ser entrada de otros modelos basados en deep learning y es que adema s de aplicarse al lenguaje natural se esta aplicando tambie n en ima genes y reconocimiento de voz ya que doc vec no nos ha gustado mucho nuestro siguiente paso es aplicar estos espacios vectoriales a extraer temas y categori as de documentos con te cnicas habituales en el mundo del procesamiento del lenguaje natural y de machine learning como tf idf de ello hablaremos en un siguiente post el corpus de datos de yahoo l yahoo answers comprehensive questions and answers version multi part ha sido obtenido gracias a yahoo webscope para procesar estos datos hemos utilizado la libreri a gensim para python que implementa word vec fuente imagen principal freedigitalphotos net kangshutters from a quick cheer to a standing ovation clap to show how much you enjoyed this story conversational interfaces expert indie maker product manager entrepreneur voicefirst chatbots ai nlproc creating future concepts at monoceros xyz innovative knowledge
Arthur Juliani,9000,6,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=tag_archive---------0----------------,Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks,for this tutorial in my reinforcement learning series we are going to be exploring a family of rl algorithms called q learning algorithms these are a little different than the policy based algorithms that will be looked at in the the following tutorials parts instead of starting with a complex and unwieldy deep neural network we will begin by implementing a simple lookup table version of the algorithm and then show how to implement a neural network equivalent using tensorflow given that we are going back to basics it may be best to think of this as part of the series it will hopefully give an intuition into what is really happening in q learning that we can then build on going forward when we eventually combine the policy gradient and q learning approaches to build state of the art rl agents if you are more interested in policy networks or already have a grasp on q learning feel free to start the tutorial series here instead unlike policy gradient methods which attempt to learn functions which directly map an observation to an action q learning attempts to learn the value of being in a given state and taking a specific action there while both approaches ultimately allow us to take intelligent actions given a situation the means of getting to that action differ significantly you may have heard about deepq networks which can play atari games these are really just larger and more complex implementations of the q learning algorithm we are going to discuss here for this tutorial we are going to be attempting to solve the frozenlake environment from the openai gym for those unfamiliar the openai gym provides an easy way for people to experiment with their learning agents in an array of provided toy games the frozenlake environment consists of a x grid of blocks each one either being the start block the goal block a safe frozen block or a dangerous hole the objective is to have an agent learn to navigate from the start to the goal without moving onto a hole at any given time the agent can choose to move either up down left or right the catch is that there is a wind which occasionally blows the agent onto a space they didn t choose as such perfect performance every time is impossible but learning to avoid the holes and reach the goal are certainly still doable the reward at every step is except for entering the goal which provides a reward of thus we will need an algorithm that learns long term expected rewards this is exactly what q learning is designed to provide in it s simplest implementation q learning is a table of values for every state row and action column possible in the environment within each cell of the table we learn a value for how good it is to take a given action within a given state in the case of the frozenlake environment we have possible states one for each block and possible actions the four directions of movement giving us a x table of q values we start by initializing the table to be uniform all zeros and then as we observe the rewards we obtain for various actions we update the table accordingly we make updates to our q table using something called the bellman equation which states that the expected long term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state in this way we reuse our own q table when estimating how to update our table for future actions in equation form the rule looks like this this says that the q value for a given state s and action a should represent the current reward r plus the maximum discounted future reward expected according to our own table for the next state s we would end up in the discount variable allows us to decide how important the possible future rewards are compared to the present reward by updating in this way the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state below is a python walkthrough of the q table algorithm implemented in the frozenlake environment thanks to praneet d for finding the optimal hyperparameters for this approach now you may be thinking tables are great but they don t really scale do they while it is easy to have a x table for a simple grid world the number of possible states in any modern game or real world environment is nearly infinitely larger for most interesting problems tables simply don t work we instead need some way to take a description of our state and produce q values for actions without a table that is where neural networks come in by acting as a function approximator we can take any number of possible states that can be represented as a vector and learn to map them to q values in the case of the frozenlake example we will be using a one layer network which takes the state encoded in a one hot vector x and produces a vector of q values one for each action such a simple network acts kind of like a glorified table with the network weights serving as the old cells the key difference is that we can easily expand the tensorflow network with added layers activation functions and different input types whereas all that is impossible with a regular table the method of updating is a little different as well instead of directly updating our table with a network we will be using backpropagation and a loss function our loss function will be sum of squares loss where the difference between the current predicted q values and the target value is computed and the gradients passed through the network in this case our q target for the chosen action is the equivalent to the q value computed in equation above below is the tensorflow walkthrough of implementing our simple q network while the network learns to solve the frozenlake problem it turns out it doesn t do so quite as efficiently as the q table while neural networks allow for greater flexibility they do so at the cost of stability when it comes to q learning there are a number of possible extensions to our simple q network which allow for greater performance and more robust learning two tricks in particular are referred to as experience replay and freezing target networks those improvements and other tweaks were the key to getting atari playing deep q networks and we will be exploring those additions in the future for more info on the theory behind q learning see this great post by tambet matiisen i hope this tutorial has been helpful for those curious about how to implement simple q learning algorithms if this post has been valuable to you please consider donating to help support future tutorials articles and implementations any contribution is greatly appreciated if you d like to follow my work on deep learning ai and cognitive science follow me on medium arthur juliani or on twitter awjliani more from my simple reinforcement learning with tensorflow series from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning unity d cognitive neuroscience phd student exploring frontier technology through the lens of artificial intelligence data science and the shape of things to come
Andrej Karpathy,9200,7,https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b?source=tag_archive---------1----------------,Yes you should understand backprop – Andrej Karpathy – Medium,when we offered cs n deep learning class at stanford we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level the students had to implement the forward and the backward pass of each layer in raw numpy inevitably some students complained on the class message boards this is seemingly a perfectly sensible appeal if you re never going to write backward passes once the class is over why practice writing them are we just torturing the students for our own amusement some easy answers could make arguments along the lines of it s worth knowing what s under the hood as an intellectual curiosity or perhaps you might want to improve on the core algorithm later but there is a much stronger and practical argument which i wanted to devote a whole post to the problem with backpropagation is that it is a leaky abstraction in other words it is easy to fall into the trap of abstracting away the learning process believing that you can simply stack arbitrary layers together and backprop will magically make them work on your data so lets look at a few explicit examples where this is not the case in quite unintuitive ways we re starting off easy here at one point it was fashionable to use sigmoid or tanh non linearities in the fully connected layers the tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non linearities can saturate and entirely stop learning your training loss will be flat and refuse to go down for example a fully connected layer with sigmoid non linearity computes using raw numpy if your weight matrix w is initialized too large the output of the matrix multiply could have a very large range e g numbers between and which will make all outputs in the vector z almost binary either or but if that is the case z z which is local gradient of the sigmoid non linearity will in both cases become zero vanish making the gradient for both x and w be zero the rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule another non obvious fun fact about sigmoid is that its local gradient z z achieves a maximum at when z that means that every time the gradient signal flows through a sigmoid gate its magnitude always diminishes by one quarter or more if you re using basic sgd this would make the lower layers of a network train much slower than the higher ones tldr if you re using sigmoids or tanh non linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn t cause them to be fully saturated see a longer explanation in this cs n lecture video another fun non linearity is the relu which thresholds neurons at zero from below the forward and backward pass for a fully connected layer that uses relu would at the core include if you stare at this for a while you ll see that if a neuron gets clamped to zero in the forward pass i e z it doesn t fire then its weights will get zero gradient this can lead to what is called the dead relu problem where if a relu neuron is unfortunately initialized such that it never fires or if a neuron s weights ever get knocked off with a large update during training into this regime then this neuron will remain permanently dead it s like permanent irrecoverable brain damage sometimes you can forward the entire training set through a trained network and find that a large fraction e g of your neurons were zero the entire time tldr if you understand backpropagation and your network has relus you re always nervous about dead relus these are neurons that never turn on for any example in your entire training set and will remain permanently dead neurons can also die during training usually as a symptom of aggressive learning rates see a longer explanation in cs n lecture video vanilla rnns feature another good example of unintuitive effects of backpropagation i ll copy paste a slide from cs n that has a simplified rnn that does not take any input x and only computes the recurrence on the hidden state equivalently the input x could always be zero this rnn is unrolled for t time steps when you stare at what the backward pass is doing you ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix the recurrence matrix whh interspersed with non linearity backprop what happens when you take one number a and start multiplying it by some other number b i e a b b b b b b this sequence either goes to zero if b or explodes to infinity when b the same thing happens in the backward pass of an rnn except b is a matrix and not just a number so we have to reason about its largest eigenvalue instead tldr if you understand backpropagation and you re using rnns you are nervous about having to do gradient clipping or you prefer to use an lstm see a longer explanation in this cs n lecture video lets look at one more the one that actually inspired this post yesterday i was browsing for a deep q learning implementation in tensorflow to see how others deal with computing the numpy equivalent of q a where a is an integer vector turns out this trivial operation is not supported in tf anyway i searched dqn tensorflow clicked the first link and found the core code here is an excerpt if you re familiar with dqn you can see that there is the target q t which is just reward gamma argmax a q s a and then there is q acted which is q s a of the action that was taken the authors here subtract the two into variable delta which they then want to minimize on line with the l loss with tf reduce mean tf square so far so good the problem is on line the authors are trying to be robust to outliers so if the delta is too large they clip it with tf clip by value this is well intentioned and looks sensible from the perspective of the forward pass but it introduces a major bug if you think about the backward pass the clip by value function has a local gradient of zero outside of the range min delta to max delta so whenever the delta is above min max delta the gradient becomes exactly zero during backprop the authors are clipping the raw q delta when they are likely trying to clip the gradient for added robustness in that case the correct thing to do is to use the huber loss in place of tf square it s a bit gross in tensorflow because all we want to do is clip the gradient if it is above a threshold but since we can t meddle with the gradients directly we have to do it in this round about way of defining the huber loss in torch this would be much more simple i submitted an issue on the dqn repo and this was promptly fixed backpropagation is a leaky abstraction it is a credit assignment scheme with non trivial consequences if you try to ignore how it works under the hood because tensorflow automagically makes my networks learn you will not be ready to wrestle with the dangers it presents and you will be much less effective at building and debugging neural networks the good news is that backpropagation is not that difficult to understand if presented properly i have relatively strong feelings on this topic because it seems to me that of backpropagation materials out there present it all wrong filling pages with mechanical math instead i would recommend the cs n lecture on backprop which emphasizes intuition yay for shameless self advertising and if you can spare the time as a bonus work through the cs n assignments which get you to write backprop manually and help you solidify your understanding that s it for now i hope you ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing also i m aware that this post has unintentionally turned into several cs n ads apologies for that from a quick cheer to a standing ovation clap to show how much you enjoyed this story director of ai at tesla previously research scientist at openai and phd student at stanford i like to train deep neural nets on large datasets
Arthur Juliani,3500,8,https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=tag_archive---------2----------------,Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C),in this article i want to provide a tutorial on implementing the asynchronous advantage actor critic a c algorithm in tensorflow we will use it to solve a simple challenge in a d doom environment with the holidays right around the corner this will be my final post for the year and i hope it will serve as a culmination of all the previous topics in the series if you haven t yet or are new to deep learning and reinforcement learning i suggest checking out the earlier entries in the series before going through this post in order to understand all the building blocks which will be utilized here if you have been following the series thank you i have learned so much about rl in the past year and am happy to have shared it with everyone through this article series so what is a c the a c algorithm was released by google s deepmind group earlier this year and it made a splash by essentially obsoleting dqn it was faster simpler more robust and able to achieve much better scores on the standard battery of deep rl tasks on top of all that it could work in continuous as well as discrete action spaces given this it has become the go to deep rl algorithm for new challenging problems with complex state and action spaces in fact openai just released a version of a c as their universal starter agent for working with their new and very diverse set of universe environments asynchronous advantage actor critic is quite a mouthful let s start by unpacking the name and from there begin to unpack the mechanics of the algorithm itself asynchronous unlike dqn where a single agent represented by a single neural network interacts with a single environment a c utilizes multiple incarnations of the above in order to learn more efficiently in a c there is a global network and multiple worker agents which each have their own set of network parameters each of these agents interacts with it s own copy of the environment at the same time as the other agents are interacting with their environments the reason this works better than having a single agent beyond the speedup of getting more work done is that the experience of each agent is independent of the experience of the others in this way the overall experience available for training becomes more diverse actor critic so far this series has focused on value iteration methods such as q learning or policy iteration methods such as policy gradient actor critic combines the benefits of both approaches in the case of a c our network will estimate both a value function v s how good a certain state is to be in and a policy s a set of action probability outputs these will each be separate fully connected layers sitting at the top of the network critically the agent uses the value estimate the critic to update the policy the actor more intelligently than traditional policy gradient methods advantage if we think back to our implementation of policy gradient the update rule used the discounted returns from a set of experiences in order to tell the agent which of its actions were good and which were bad the network was then updated in order to encourage and discourage actions appropriately the insight of using advantage estimates rather than just discounted returns is to allow the agent to determine not just how good its actions were but how much better they turned out to be than expected intuitively this allows the algorithm to focus on where the network s predictions were lacking if you recall from the dueling q network architecture the advantage function is as follow since we won t be determining the q values directly in a c we can use the discounted returns r as an estimate of q s a to allow us to generate an estimate of the advantage in this tutorial we will go even further and utilize a slightly different version of advantage estimation with lower variance referred to as generalized advantage estimation in the process of building this implementation of the a c algorithm i used as reference the quality implementations by dennybritz and openai both of which i highly recommend if you d like to see alternatives to my code here each section embedded here is taken out of context for instructional purposes and won t run on its own to view and run the full functional a c implementation see my github repository the general outline of the code architecture is the a c algorithm begins by constructing the global network this network will consist of convolutional layers to process spatial dependencies followed by an lstm layer to process temporal dependencies and finally value and policy output layers below is example code for establishing the network graph itself next a set of worker agents each with their own network and environment are created each of these workers are run on a separate processor thread so there should be no more workers than there are threads on your cpu from here we go asynchronous each worker begins by setting its network parameters to those of the global network we can do this by constructing a tensorflow op which sets each variable in the local worker network to the equivalent variable value in the global network each worker then interacts with its own copy of the environment and collects experience each keeps a list of experience tuples observation action reward done value that is constantly added to from interactions with the environment once the worker s experience history is large enough we use it to determine discounted return and advantage and use those to calculate value and policy losses we also calculate an entropy h of the policy this corresponds to the spread of action probabilities if the policy outputs actions with relatively similar probabilities then entropy will be high but if the policy suggests a single action with a large probability then entropy will be low we use the entropy as a means of improving exploration by encouraging the model to be conservative regarding its sureness of the correct action a worker then uses these losses to obtain gradients with respect to its network parameters each of these gradients are typically clipped in order to prevent overly large parameter updates which can destabilize the policy a worker then uses the gradients to update the global network parameters in this way the global network is constantly being updated by each of the agents as they interact with their environment once a successful update is made to the global network the whole process repeats the worker then resets its own network parameters to those of the global network and the process begins again to view the full and functional code see the github repository here the robustness of a c allows us to tackle a new generation of reinforcement learning challenges one of which is d environments we have come a long way from multi armed bandits and grid worlds and in this tutorial i have set up the code to allow for playing through the first vizdoom challenge vizdoom is a system to allow for rl research using the classic doom game engine the maintainers of vizdoom recently created a pip package so installing it is as simple as pip install vizdoom once it is installed we will be using the basic wad environment which is provided in the github repository and needs to be placed in the working directory the challenge consists of controlling an avatar from a first person perspective in a single square room there is a single enemy on the opposite side of the room which appears in a random location each episode the agent can only move to the left or right and fire a gun the goal is to shoot the enemy as quickly as possible using as few bullets as possible the agent has time steps per episode to shoot the enemy shooting the enemy yields a reward of and each time step as well as each shot yields a small penalty after about episodes per worker agent the network learns a policy to quickly solve the challenge feel free to adjust parameters such as learning rate clipping magnitude update frequency etc to attempt to achieve ever greater performance or utilize a c in your own rl tasks i hope this tutorial has been helpful to those new to a c and asynchronous reinforcement learning now go forth and build ais there are a lot of moving parts in a c so if you discover a bug or find a better way to do something please don t hesitate to bring it up here or in the github i am more than happy to incorporate changes and feedback to improve the algorithm if you d like to follow my writing on deep learning ai and cognitive science follow me on medium arthur juliani or on twitter awjuliani if this post has been valuable to you please consider donating to help support future tutorials articles and implementations any contribution is greatly appreciated more from my simple reinforcement learning with tensorflow series from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning unity d cognitive neuroscience phd student exploring frontier technology through the lens of artificial intelligence data science and the shape of things to come
Rohan Kapur,1000,30,https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d?source=tag_archive---------3----------------,"Rohan & Lenny #1: Neural Networks & The Backpropagation Algorithm, Explained",in rohan s last post he talked about evaluating and plugging holes in his knowledge of machine learning thus far the backpropagation algorithm the process of training a neural network was a glaring one for both of us in particular together we embarked on mastering backprop through some great online lectures from professors at mit stanford after attempting a few programming implementations and hand solutions we felt equipped to write an article for ayoai together today we ll do our best to explain backpropagation and neural networks from the beginning if you have an elementary understanding of differential calculus and perhaps an intuition of what machine learning is we hope you come out of this blog post with an acute but existent nonetheless understanding of neural networks and how to train them let us know if we succeeded let s start off with a quick introduction to the concept of neural networks fundamentally neural networks are nothing more than really good function approximators you give a trained network an input vector it performs a series of operations and it produces an output vector to train our network to estimate an unknown function we give it a collection of data points which we denote the training set that the network will learn from and generalize on to make future inferences neural networks are structured as a series of layers each composed of one or more neurons as depicted above each neuron produces an output or activation based on the outputs of the previous layer and a set of weights when using a neural network to approximate a function the data is forwarded through the network layer by layer until it reaches the final layer the final layer s activations are the predictions that the network actually makes all this probably seems kind of magical but it actually works the key is finding the right set of weights for all of the connections to make the right decisions this happens in a process known as training and that s what most of this post is going to be about when we re training the network it s often convenient to have some metric of how good or bad we re doing we call this metric the cost function generally speaking the cost function looks at the function the network has inferred and uses it to estimate values for the data points in our training set the discrepancies between the outputs in the estimations and the training set data points are the principle values for our cost function when training our network the goal will be to get the value of this cost function as low as possible we ll see how to do that in just a bit but for now just focus on the intuition of what a cost function is and what it s good for generally speaking the cost function should be more or less convex like so in reality it s impossible for any network or cost function to be truly convex however as we ll soon see local minima may not be a big deal as long as there is still a general trend for us to follow to get to the bottom also notice that the cost function is parameterized by our network s weights we control our loss function by changing the weights one last thing to keep in mind about the loss function is that it doesn t just have to capture how correctly your network estimates it can specify any objective that needs to be optimized for example you generally want to penalize larger weights as they could lead to overfitting if this is the case simply adding a regularization term to your cost function that expresses how big your weights will mean that in the process of training your network it will look for a solution that has the best estimates possible while preventing overfitting now let s take a look at how we can actually minimize the cost function during the training process to find a set of weights that work the best for our objective now that we ve developed a metric for scoring our network which we ll denote as j w we need to find the weights that will make that score as low as possible if you think back to your pre calculus days your first instinct might be to set the derivative of the cost function to zero and solve which would give us the locations of every minimum maximum in the function unfortunately there are a few problems with this approach especially as the size of networks begins to scale up solving for the weights directly becomes increasingly infeasible instead we look at a different class of algorithms called iterative optimization algorithms that progressively work their way towards the optimal solution the most basic of these algorithms is gradient descent recall that our cost function will be essentially convex and we want to get as close as possible to the global minimum instead of solving for it analytically gradient descent follows the derivatives to essentially roll down the slope until it finds its way to the center let s take the example of a single weight neural network whose cost function is depicted below we start off by initializing our weight randomly which puts us at the red dot on the diagram above taking the derivative we see the slope at this point is a pretty big positive number we want to move closer to the center so naturally we should take a pretty big step in the opposite direction of the slope if we repeat the process enough we soon find ourselves nearly at the bottom of our curve and much closer to the optimal weight configuration for our network more formally gradient descent looks something like this let s dissect every time we want to update our weights we subtract the derivative of the cost function w r t the weight itself scaled by a learning rate and that s it you ll see that as it gets closer and closer to the center the derivative term gets smaller and smaller converging to zero as it approaches the solution the same process applies with networks that have tens hundreds thousands or more parameters compute the gradient of the cost function w r t each of the weights and update each of your weights accordingly i do want to say a few more words on the learning rate because it s one of the more important hyperparameters settings for your neural network that you have control over if the learning rate is too high it could jump too far in the other direction and you never get to the minimum you re searching for set it too low and your network will take ages to find the right weights or it will get stuck in a local minimum there s no magic number to use when it comes to a learning rate and it s usually best to try several and pick the one that works the best for your individual network and dataset in practice many choose to anneal the learning rate over time it starts out high because it s furthest from the solution and decays as it gets closer but as it turns out gradient descent is kind of slow really slow actually earlier i used the analogy of the weights rolling down the gradient to get to the bottom but that doesn t actually make any sense it should pick up speed as it gets to the bottom not slow down another iterative optimization algorithm known as momentum does just that as the weights begin to roll down the slope they pick up speed when they get closer to the solution the momentum that they picked up carries them closer to the optima while gradient descent would simply stop as a result training with momentum updates is both faster and can provide better results here s what the update rule looks like for momentum as we train we accumulate a velocity value v at each training step we update v with the gradient at the current position once again scaled by the learning rate also notice that with each time step we decay velocity v by a factor mu usually somewhere around so that over time we lose momentum instead of bouncing around by the minimum forever we then update our weight in the direction of the velocity and repeat the process again over the first few training iterations v will grow as our weights pick up speed and take successively bigger leaps as we approach the minimum our velocity stops accumulating as quickly and eventually begins to decay until we ve essentially reached the minimum an important thing to note is that we accumulate a velocity independently for each weight just because one weight is changing particularly clearly doesn t mean any of the other weights need to be there are lots of other iterative optimization algorithms that are commonly used with neural networks but i won t go into all of them here if you re curious some of the more popular ones include adagrad and adam the basic principle remains the same throughout gradually update the weights to get them closer to the minimum but regardless of which optimization algorithm you use we still need to be able to compute the gradient of the cost function w r t each weight but our cost function isn t a simple parabola anymore it s a complicated many dimensional function with countless local optima that we need to watch out for that s where backpropagation comes in the backpropagation algorithm was a major milestone in machine learning because before it was discovered optimization methods were extremely unsatisfactory one popular method was to perturb adjust the weights in a random uninformed direction ie increase or decrease and see if the performance of the ann increased if it did not one would attempt to either a go in the other direction b reduce the perturbation size or c a combination of both another attempt was to use genetic algorithms which became popular in ai at the same time to evolve a high performance neural network in both cases without analytically being informed on the correct direction results and efficiency were suboptimal this is where the backpropagation algorithm comes into play recall that for any given supervised machine learning problem we aim to select weights that provide the optimal estimation of a function that models our training data in other words we want to find a set of weights w that minimizes on the output of j w we discussed the gradient descent algorithm one where we update each weight by some negative scalar reduction of the error derivative with respect to that weight if we do choose to use gradient descent or almost any other convex optimization algorithm we need to find said derivatives in numerical form for other machine learning algorithms like logistic regression or linear regression computing the derivatives is an elementary application of differentiation this is because the outputs of these models are just the inputs multiplied by some chosen weights and at most fed through a single activation function the sigmoid function in logistic regression the same however cannot be said for neural networks to demonstrate this here is a diagram of a double layered neural network as you can see each neuron is a function of the previous one connected to it in other words if one were to change the value of w both hidden and hidden and ultimately the output neurons would change because of this notion of functional dependencies we can mathematically formulate the output as an extensive composite function and thus here the output is a composite function of the weights inputs and activation function s it is important to realize that the hidden units nodes are simply intermediary computations that in actuality can be reduced down to computations of the input layer if we were to then take the derivative of said function with respect to some arbitrary weight for example w we would iteratively apply the chain rule which i m sure you all remember from your calculus classes the result would look similar to the following now let s attach a black box to the tail of our neural network this black box will compute and return the error using the cost function from our output all we ve done is add another functional dependency our error is now a function of the output and hence a function of the input weights and activation function if we were to compute the derivative of the error with any arbitrary weight again we ll choose w the result would be each of these derivatives can be simplified once we choose an activation and error function such that the entire result would represent a numerical value at that point any abstraction has been removed and the error derivative can be used in gradient descent as discussed earlier to iteratively improve upon the weight we compute the error derivatives w r t every other weight in the network and apply gradient descent in the same way this is backpropagation simply the computation of derivatives that are fed to a convex optimization algorithm we call it backpropagation because it almost seems as if we are traversing from the output error to the weights taking iterative steps using chain the rule until we reach our weight when i first truly understood the backprop algorithm just a couple of weeks ago i was taken aback by how simple it was sure the actual arithmetic computations can be difficult but this process is handled by our computers in reality backpropagation is just a rather tedious but again for a generalized implementation computers will handle this application of the chain rule since neural networks are convoluted multilayer machine learning model structures at least relative to other ones each weight contributes to the overall error in a more complex manner and hence the actual derivatives require a lot of effort to produce however once we get past the calculus backpropagation of neural nets is equivalent to typical gradient descent for logistic linear regression thus far i ve walked through a very abstract form of backprop for a simple neural network however it is unlikely that you will ever use a single layered ann in applications so now let s make our black boxes the activation and error functions more concrete such that we can perform backprop on a multilayer neural net recall that our error function j w will compute the error of our neural network based on the output predictions it produces vs the correct a priori outputs we know in our training set more formally if we denote our predicted output estimations as vector p and our actual output as vector a then we can use this is just one example of a possible cost function the log likelihood is also a popular one and we use it because of its mathematical convenience this is a notion one will frequently encounter in machine learning the squared expression exaggerates poor solutions and ensures each discrepancy is positive it will soon become clear why we multiply the expression by half the derivative of the error w r t the output was the first term in the error w r t weight derivative expression we formulated earlier let s now compute it our result is simply our predictions take away our actual outputs now let s move on to the activation function the activation function used depends on the context of the neural network if we aren t in a classification context relu rectified linear unit which is zero if input is negative and the identity function when the input is positive is commonly used today if we re in a classification context that is predicting on a discrete state with a probability ie if an email is spam we can use the sigmoid or tanh hyperbolic tangent function such that we can squeeze any value into the range to these are used instead of a typical step function because their smoothness properties allows for the derivatives to be non zero the derivative of the step function before and after the origin is zero this will pose issues when we try to update our weights nothing much will happen now let s say we re in a classification context and we choose to use the sigmoid function which is of the following equation as per usual we ll compute the derivative using differentiation rules as edit on the nd line the denominator should be raised to not thanks to a reader for pointing this out sidenote relu activation functions are also commonly used in classification contexts there are downsides to using the sigmoid function particularly the vanishing gradient problem which you can read more about here the sigmoid function is mathematically convenient there it is again because we can represent its derivative in terms of the output of the function isn t that cool we are now in a good place to perform backpropagation on a multilayer neural network let me introduce you to the net we are going to work with this net is still not as complex as one you may use in your programming but its architecture allows us to nevertheless get a good grasp on backprop in this net we have input neurons and one output neuron there are four layers in total one input one output and two hidden layers there are neurons in each hidden layer too which by the way need not be the case the network is fully connected there are no missing connections each neuron node save the inputs which are usually pre processed anyways is an activity it is the weighted sum of the previous neurons activities applied to the sigmoid activation function to perform backprop by hand we need to introduce the different variables states at each point layer wise in the neural network it is important to note that every variable you see here is a generalization on the entire layer at that point for example when i say x i i am referring to the input to any input neuron arbitrary value of i i chose to place it in the middle of the layer for visibility purposes but that does not mean that x i refers to the middle neuron i ll demonstrate and discuss the implications of this later on x refers to the input layer y refers to hidden layer z refers to hidden layer and p refers to the prediction output layer which fits in nicely with the notation used in our cost function if a variable has the subscript i it means that the variable is the input to the relevant neuron at that layer if a variable has the subscript j it means that the variable is the output of the relevant neuron at that layer for example x i refers to any input value we enter into the network x j is actually equal to x i but this is only because we choose not to use an activation function or rather we use the identity activation function in the input layer s activities we only include these two separate variables to retain consistency y i is the input to any neuron in the first hidden layer it is the weighted sum of all previous neurons each neuron in the input layer multiplied by the corresponding connecting weights y j is the output of any neuron at the hidden layer so it is equal to activation function y i sigmoid y i sigmoid weighted sum of x j we can apply the same logic for z and p ultimately p j is the sigmoid output of p i and hence is the output of the entire neural network that we pass to the error cost function the weights are organized into three separate variables w w and w each w is a matrix if you are not comfortable with linear algebra think of a d array of all the weights at the given layer for example w are the weights that connect the input layer to the hidden layer wlayer ij refers to any arbitrary single weight at a given layer to get an intuition of ij which is really i j wlayer i are all the weights that connect arbitrary neuron i at a given layer to the next layer wlayer ij adding the j component is the weight that connects arbitrary neuron i at a given layer to an arbitrary neuron j at the next layer essentially wlayer is a vector of wlayer is which is a vector of real valued wlayer ijs note please note that the i s and j s in the weights and other variables are completely different these indices do not correspond in any way in fact for x y z p i and j do not represent tensor indices at all they simply represent the input and output of a neuron wlayer ij represents an arbitrary weight at an index in a weight matrix and x j y j z j p j represent an arbitrary input output point of a neuron unit that last part about weights was tedious it s crucial to understand how we re separating the neural network here especially the notion of generalizing on an entire layer before moving forward to acquire a comprehensive intuition of backpropagation we re going to backprop this neural net as discussed before more specifically we re going to find the derivative of the error w r t an arbitrary weight in the input layer w ij we could find the derivative of the error w r t an arbitrary weight in the first or second hidden layer but let s go as far back as we can the more backprop the better so mathematically we are trying to obtain to perform our iterative optimization algorithm with we can express this graphically visually using the same principles as earlier chain rule like so in two layers we have three red lines pointing in three different directions instead of just one this is a reinforcement of and why it is important to understand the fact that variable j is just a generalization represents any point in the layer so when we differentiate p i with respect to the layer before that there are three different weights as i hope you can see in w ij that contribute to the value p i there also happen to be three weights in w in total but this isn t the case for the layers before it is only the case because layer p has one neuron the output in it we stop backprop at the input layer and so we just point to the single weight we are looking for wonderful now let s work out all this great stuff mathematically immediately we know we have already established the left hand side so now we just need to use the chain rule to simplify it further the derivative of the error w r t the weight can be written as the derivative of the error w r t the output prediction multiplied by the derivative of the output prediction w r t the weight at this point we ve traversed one red line back we know this because is reducible to a numerical value specifically the derivative of the error w r t the output prediction is hence going one more layer backwards we can determine that in other words the derivative of the output prediction w r t the weight is the derivative of the output w r t the input to the output layer p i multiplied by the derivative of that value w r t the weight this represents our second red line we can solve the first term like so this corresponds with the derivative of the sigmoid function we solved earlier which was equal to the output multiplied by one minus the output in this case p j is the output of the sigmoid function now we have let s move on to the third red line s this one is interesting because we begin to spread out since there are multiple different weights that contribute to the value of p i we need to take into account their individual pull factors into our derivative if you re a mathematician this notation may irk you slightly sorry if that s the case in computer science we tend to stray from the notion of completely legal mathematical expressions this is yet again again another reason why it s key to understand the role of layer generalization z j here is not just referring to the middle neuron it s referring to an arbitrary neuron the actual value of j in the summation is not changing it s not even an index or a value in the first place and we don t really consider it it s less of a mathematical expression and more of a statement that we will iterate through each generalized neuron z j and use it in other words we iterate over the derivative terms and sum them up using z z and z before we could write p j as any single value because the output layer just contains one node there is just one p j but we see here that this is no longer the case we have multiple z j values and p i is functionally dependent on each of these z j values so when we traverse from p j to the preceding layer we need to consider each contribution from layer z to p j separately and add them up to create one total contribution there s no upper bound to the summation we just assume that we start at zero and end at our maximum value for the number of neurons in the layer please again note that the same changes are not reflected in w ij where j refers to an entirely different thing instead we re just stating that we will use the different z j neurons in layer z since p i is a summation of each weight multiplied by each z j weighted sum if we were to take the derivative of p i with any arbitrary z j the result would be the connecting weight since said weight would be the coefficient of the term derivative of m x w r t x is just m w ij is loosely defined here ij still refers to any arbitrary weight where ij are still separate from the j used in p i z j but again as computer scientists and not mathematicians we need not be pedantic about the legality and intricacy of expressions we just need an intuition of what the expressions imply mean it s almost a succinct form of psuedo code so even though this defines an arbitrary weight we know it means the connecting weight we can also see this from the diagram when we walk from p j to an arbitrary z j we walk along the connecting weight so now we have at this point i like to continue playing the reduction test the reduction test states that if we can further simplify a derivative term we still have more backprop to do since we can t yet quite put the derivative of z j w r t w ij into a numerical term let s keep going and fast forward a bit using chain rule we follow the fourth line back to determine that since z j is the sigmoid of z i we use the same logic as the previous layer and apply the sigmoid derivative the derivative of z i w r t w ij demonstrated by the fifth line s back requires the same idea of spreading out and summation of contributions briefly since z i is the weighted sum of each y j in y we sum over the derivatives which similar to before simplifies to the relevant connecting weights in the preceding layer w in this case we re almost there let s go further there s still more reduction to do we have of course another sigmoid activation function to deal with this is the sixth red line notice now that we have just one line remaining in fact our last derivative term here passes or rather fails the reduction test the last line traverses from the input at y i to x j walking along w ij wait a second is this not what we are attempting to backprop to yes it is since we are for the first time directly deriving y i w r t the weight w ij we can think of the coefficient of w ij as being x j in our weighted sum instead of the vice versa as used previously hence the simplification follows of course since each x j in layer x contributes to the weighted sum y i we sum over the effects and that s it we can t reduce any further from here now let s tie all these individual expressions together edit the denominator on the left hand side should say dw ij instead of layer with no more partial derivative terms left our work is complete this gives us the derivative of the error w r t any arbitrary weight in the input layer w that was a lot of work maybe now we can sympathize with the poor computers something you should notice is that values such as p j a z j y j x j etc are the values of the network at the different points but where do they come from actually we would need to perform a feed forward of the neural network first and then capture these variables our task is to now perform gradient descent to train the neural net we perform gradient descent on each weight in each layer notice that the resulting gradient should change each time because the weight itself changes and as a result the performance and output of the entire net should change even if it s a small perturbation this means that at each update we need to do a feed forward of the neural net not just once before but once each iteration these are then the steps to train an entire neural network it s important to note that one must not initialize the weights to zero similar to what may be done in other machine learning algorithms if weights are initialized to zero after each update the outgoing weights of each neuron will be identical because the gradients will be identical which can be proved because of this the proceeding hidden units will remain the same value and will continue to follow each other ultimately this means that our training will become extremely constrained due to the symmetry and we won t be able to build interesting functions also neural networks may get stuck at local optima places where the gradient is zero but are not the global minima so random weight initialization allows one to hopefully have a chance of circumventing that by starting at many different random values perform one feed forward using the training data perform backpropagation to get the error derivatives w r t each and every weight in the neural network perform gradient descent to update each weight by the negative scalar reduction w r t some learning rate alpha of the respective error derivative increment the number of iterations if we have converged in reality though we just stop when we have reached the number of maximum iterations training is complete else repeat starting at step if we initialize our weights randomly and not to zero and then perform gradient descent with derivatives computed from backpropagation we should expect to train a neural network in no time i hope this example brought clarity to how backprop works and the intuition behind it if you didn t understand the intricacies of the example but understand and appreciate the concept of backprop as a whole you re still in a great place next we ll go ahead and explain backprop code that works on any generalized architecture of a neural network using the relu activation function now that we ve developed the math and intuition behind backpropagation let s try to implement it we ll divide our implementation into three distinct steps let s start off by defining what the api we re implementing looks like we ll define our network as a series of layer instances that our data passes through this means that instead of modeling each individual neuron we group neurons from a single layer together this makes it a bit easier to reason about larger networks but also makes the actual computations faster as we ll see shortly also we re going to write the code in python each layer will have the following api this isn t great api design ideally we would decouple the backprop and weight update from the rest of the object so the specific algorithm we use for updating weights isn t tied to the layer itself but that s not the point so we ll stick with this design for the purposes of explaining how backpropagation works in a real life scenario also we ll be using numpy throughout the implementation it s an awesome tool for mathematical operations in python especially tensor based ones but we don t have the time to get into how it works if you want a good introduction here ya go we can start by implementing the weight initialization as it turns out how you initialize your weights is actually kind of a big deal for both network performance and convergence rates here s how we ll initialize our weights this initializes a weight matrix of the appropriate dimensions with random values sampled from a normal distribution we then scale it rad self size in giving us a variance of self size in derivation here and that s all we need for layer initialization let s move on to implementing our first objective feed forward this is actually pretty simple a dot product of our input activations with the weight matrix followed by our activation function will give us the activations we need the dot product part should make intuitive sense if it doesn t you should sit down and try to work through it on a piece of paper this is where the performance gains of grouping neurons into layers comes from instead of keeping an individual weight vector for each neuron and performing a series of vector dot products we can just do a single matrix operation which thanks to the wonders of modern processors is significantly faster in fact we can compute all of the activations from a layer in just two lines simple enough let s move on to backpropagation this one s a bit more involved first we compute the derivative of the output w r t the weights then the derivative of the cost w r t the output followed by chain rule to get the derivative of the cost w r t the weights let s start with the first part the derivative of the output w r t the weights that should be simple enough because you re multiplying the weight by the corresponding input activation the derivative will just be the corresponding input activation except because we re using the relu activation function the weights have no effect if the corresponding output is because it gets capped anyway this should take care of that hiccup more formally you re multiplying by the derivative of the activation function which is when the activation is and elsewhere let s take a brief detour to talk about the out grad parameter that our backward method gets let s say we have a network with two layers the first has m neurons and the second has n each of the m neurons produces an activation and each of the n neurons looks at each of the m activations the out grad parameter is an m x n matrix of how each m affects each of the n neurons it feeds into now we need the derivative of the cost w r t each of the outputs which is essentially the out grad parameter we re given we just need to sum up each row of the matrix we re given as per the backpropagation formula finally we end up with something like this now we need to compute the derivative of our inputs to pass along to the next layer we can perform a similar chain rule derivative of the output w r t the inputs times the derivative of the cost w r t the outputs and that s it for the backpropagation step the final step is the weight update assuming we re sticking with gradient descent for this example this can be a simple one liner to actually train our network we take one of our training samples and call forward on each layer consecutively passing the output of the previous layer as the input of the following layer we compute dj passing that as the out grad parameter to the last layer s backward method we call backward on each of the layers in reverse order this time passing the output of the further layer as out grad to the previous layer finally we call update on each of our layers and repeat there s one last detail that we should include which is the concept of a bias akin to that of a constant term in any given equation notice that with our current implementation the activation of a neuron is determined solely based on the activations of the previous layer there s no bias term that can shift the activation up or down independent of the inputs a bias term isn t strictly necessary in fact if you train your network as is it would probably still work fine but if you do need a bias term the code stays almost the same the only difference is that you need to add a column of s to the incoming activations and update your weight matrix accordingly so one of your weights gets treated as a bias term the only other difference is that when returning cost wrt inputs you can cut out the first row nobody cares about the gradients associated with the bias term because the previous layer has no say in the activation of the bias neuron implementing backpropagation can be kind of tricky so it s often a good idea to check your implementation you can do so by computing the gradient numerically by literally perturbing the weight and calculating the difference in your cost function and comparing it to your backpropagation computed gradient this gradient check doesn t need to be run once you ve verified your implementation but it could save a lot of time tracking down potential problems with your network nowadays you often don t even need to implement a neural network on your own as libraries such as caffe torch or tensorflow will have implementations ready to go that being said it s often a good idea to try implementing it on your own to get a better grasp of how everything works under the hood intrigued looking to learn more about neural networks here are some great online classes to get you started stanford s cs n although it s technically about convolutional neural networks the class provides an excellent introduction to and survey of neural networks in general class videos notes and assignments are all posted here and if you have the patience for it i would strongly recommend walking through the assignments so you can really get to know what you re learning mit this class taught by prof patrick henry winston explores many different algorithms and disciplines in artificial intelligence there s a great lecture on backprop that i actually used as a stepping stone to getting setup writing this article i also learned genetic algorithms from prof winston he s a great teacher we hope that if you visited this article without knowing how the backpropagation algorithm works you are reading this with an at least rudimentary mathematical or conceptual intuition of it writing and conveying such a complex algorithm to a supposed beginner has proven to be an extremely difficult task for us but it s helped us truly understand what we ve been learning about with greater knowledge in a fundamental area of machine learning we are now excited to take a look at new interesting algorithms and disciplines in the field we are looking forward to continue documenting these endeavors together from a quick cheer to a standing ovation clap to show how much you enjoyed this story rohankapur com our ongoing effort to make the mathematics science linguistics and philosophy of artificial intelligence fun and simple
Per Harald Borgen,1300,7,https://medium.com/learning-new-stuff/how-to-learn-neural-networks-758b78f2736e?source=tag_archive---------4----------------,Learning How To Code Neural Networks – Learning New Stuff – Medium,this is the second post in a series of me trying to learn something new over a short period of time the first time consisted of learning how to do machine learning in a week this time i ve tried to learn neural networks while i didn t manage to do it within a week due to various reasons i did get a basic understanding of it throughout the summer and autumn of by basic understanding i mean that i finally know how to code simple neural networks from scratch on my own in this post i ll give a few explanations and guide you to the resources i ve used in case you re interested in doing this yourself so what is a neural network let s wait with the network part and start off with one single neuron the circle below illustrates an artificial neuron its input is and its output is the input is the sum of the three synapses connecting to the neuron the three arrows at the left at the far left we see two input values plus a bias value the input values are and the green numbers while the bias holds a value of the brown number the two inputs are then multiplied by their so called weights which are and the blue numbers finally we add it up with the bias and end up with a number in this case the red number this is the input for our artificial neuron the neuron then performs some kind of computation on this number in our case the sigmoid function and then spits out an output this happens to be as sigmoid of equals to if we round the number up more info on the sigmoid function follows later if you connect a network of these neurons together you have a neural network which propagates forward from input output via neurons which are connected to each other through synapses like on the image to the left i can strongly recommend the welch labs videos on youtube for getting a better intuitive explanation of this process after you ve seen the welch labs videos its a good idea to spend some time watching week of the coursera s machine learning course which covers neural networks as it ll give you more intuition of how they work the course is fairly mathematical and its based around octave while i prefer python because of this i did not do the programming exercises instead i used the videos to help me understand what i needed to learn the first thing i realized i needed to investigate further was the sigmoid function as this seemed to be a critical part of many neural networks i knew a little bit about the function as it was also covered in week of the same course so i went back and watched these videos again but watching videos won t get you all the way to really understand it i felt i needed to code it from the ground up so i started to code a logistic regression algorithm from scratch which happened to use the sigmoid function it took a whole day and it s probably not a very good implementation of logistic regression but that doesn t matter as i finally understood how it works check the code here you don t need to perform this entire exercise yourself as it requires some knowledge about and cost functions and gradient descent which you might not have at this point but make sure you understand how the sigmoid function works understanding how a neural network works from input to output isn t that difficult to understand at least conceptually more difficult though is understanding how the neural network actually learns from looking at a set of data samples the concept is called backpropagation the weights were the blue numbers on our neuron in the beginning of the article this process happens backwards because you start at the end of the network observe how wrong the networks guess is and then move backwards through the network while adjusting the weights on the way until you finally reach the inputs to calculate this by hand requires some calculus as it involves getting some derivatives of the networks weights the kahn academy calculus courses seems like a good way to start though i haven t used them myself as i took calculus on university the three best sources i found for understanding backpropagation are these you should definitely code along while you re reading the articles especially the two first ones it ll give you some sample code to look back at when you re confused in the future plus i can t really emphasize this enough the third article is also fantastic but i ve used this more as a wiki than a plain tutorial as it s actually an entire book it contains thorough explanations all the important concepts in neural networks these articles will also help you understand important concepts as cost functions and gradient descent which play equally important roles in neural networks in some articles and tutorials you ll actually end up coding small neural networks as soon as you re comfortable with that i recommend you to go all in on this strategy it s both fun and an extremely effective way of learning one of the articles i also learned a lot from was a neural network in lines of python by iamtrask it contains an extraordinary amount of compressed knowledge and concepts in just lines after you ve coded along with this example you should do as the article states at the bottom which is to implement it once again without looking at the tutorial this forces you to really understand the concepts and will likely reveal holes in your knowledge which isn t fun however when you finally manage it you ll feel like you ve just acquired a new superpower when you ve done this you can continue with this wild ml tutorial by denny britz which guides you through a little more robust neural network at this point you could either try and code your own neural network from scratch or start playing around with some of the networks you have coded up already it s great fun to find a dataset that interests you and try to make some predictions with your neural nets to get a hold of a dataset just visit my side project datasets co shameless self promotion and find one you like anyway the point is that you re now better off experimenting with stuff that interests you rather than following my advices personally i m currently learning how to use python libraries that makes it easier to code up neural networks like theano lasagne and nolearn i m using this to do challenges on kaggle which is both great fun and great learning good luck and don t forget to press the heart button if you liked the article thanks for reading my name is per i m a co founder of scrimba a better way to teach and learn code if you ve read this far i d recommend you to check out this demo from a quick cheer to a standing ovation clap to show how much you enjoyed this story co founder of scrimba the next generation platform for teaching and learning code https scrimba com a publication about improving your technical skills
Shi Yan,4400,7,https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714?source=tag_archive---------5----------------,Understanding LSTM and its diagrams – ML Review – Medium,i just want to reiterate what s said here i m not better at explaining lstm i want to write this down as a way to remember it myself i think the above blog post written by christopher olah is the best lstm material you would find please visit the original link if you want to learn lstm but i did create some nice diagrams although we don t know how brain functions yet we have the feeling that it must have a logic unit and a memory unit we make decisions by reasoning and by experience so do computers we have the logic units cpus and gpus and we also have memories but when you look at a neural network it functions like a black box you feed in some inputs from one side you receive some outputs from the other side the decision it makes is mostly based on the current inputs i think it s unfair to say that neural network has no memory at all after all those learnt weights are some kind of memory of the training data but this memory is more static sometimes we want to remember an input for later use there are many examples of such a situation such as the stock market to make a good investment judgement we have to at least look at the stock data from a time window the naive way to let neural network accept a time series data is connecting several neural networks together each of the neural networks handles one time step instead of feeding the data at each individual time step you provide data at all time steps within a window or a context to the neural network a lot of times you need to process data that has periodic patterns as a silly example suppose you want to predict christmas tree sales this is a very seasonal thing and likely to peak only once a year so a good strategy to predict christmas tree sale is looking at the data from exactly a year back for this kind of problems you either need to have a big context to include ancient data points or you have a good memory you know what data is valuable to remember for later use and what needs to be forgotten when it is useless theoretically the naively connected neural network so called recurrent neural network can work but in practice it suffers from two problems vanishing gradient and exploding gradient which make it unusable then later lstm long short term memory was invented to solve this issue by explicitly introducing a memory unit called the cell into the network this is the diagram of a lstm building block at a first sight this looks intimidating let s ignore the internals but only look at the inputs and outputs of the unit the network takes three inputs x t is the input of the current time step h t is the output from the previous lstm unit and c t is the memory of the previous unit which i think is the most important input as for outputs h t is the output of the current network c t is the memory of the current unit therefore this single unit makes decision by considering the current input previous output and previous memory and it generates a new output and alters its memory the way its internal memory c t changes is pretty similar to piping water through a pipe assuming the memory is water it flows into a pipe you want to change this memory flow along the way and this change is controlled by two valves the first valve is called the forget valve if you shut it no old memory will be kept if you fully open this valve all old memory will pass through the second valve is the new memory valve new memory will come in through a t shaped joint like above and merge with the old memory exactly how much new memory should come in is controlled by the second valve on the lstm diagram the top pipe is the memory pipe the input is the old memory a vector the first cross it passes through is the forget valve it is actually an element wise multiplication operation so if you multiply the old memory c t with a vector that is close to that means you want to forget most of the old memory you let the old memory goes through if your forget valve equals then the second operation the memory flow will go through is this operator this operator means piece wise summation it resembles the t shape joint pipe new memory and the old memory will merge by this operation how much new memory should be added to the old memory is controlled by another valve the below the sign after these two operations you have the old memory c t changed to the new memory c t now lets look at the valves the first one is called the forget valve it is controlled by a simple one layer neural network the inputs of the neural network is h t the output of the previous lstm block x t the input for the current lstm block c t the memory of the previous block and finally a bias vector b this neural network has a sigmoid function as activation and it s output vector is the forget valve which will applied to the old memory c t by element wise multiplication now the second valve is called the new memory valve again it is a one layer simple neural network that takes the same inputs as the forget valve this valve controls how much the new memory should influence the old memory the new memory itself however is generated by another neural network it is also a one layer network but uses tanh as the activation function the output of this network will element wise multiple the new memory valve and add to the old memory to form the new memory these two signs are the forget valve and the new memory valve and finally we need to generate the output for this lstm unit this step has an output valve that is controlled by the new memory the previous output h t the input x t and a bias vector this valve controls how much new memory should output to the next lstm unit the above diagram is inspired by christopher s blog post but most of the time you will see a diagram like below the major difference between the two variations is that the following diagram doesn t treat the memory unit c as an input to the unit instead it treats it as an internal thing cell i like the christopher s diagram in that it explicitly shows how this memory c gets passed from the previous unit to the next but in the following image you can t easily see that c t is actually from the previous unit and c t is part of the output the second reason i don t like the following diagram is that the computation you perform within the unit should be ordered but you can t see it clearly from the following diagram for example to calculate the output of this unit you need to have c t the new memory ready therefore the first step should be evaluating c t the following diagram tries to represent this delay or order with dash lines and solid lines there are errors in this picture dash lines means the old memory which is available at the beginning some solid lines means the new memory operations require the new memory have to wait until c t is available but these two diagrams are essentially the same here i want to use the same symbols and colors of the first diagram to redraw the above diagram this is the forget gate valve that shuts the old memory this is the new memory valve and the new memory these are the two valves and the element wise summation to merge the old memory and the new memory to form c t in green flows back to the big cell this is the output valve and output of the lstm unit from a quick cheer to a standing ovation clap to show how much you enjoyed this story software engineer wantrepreneur interested in computer graphics bitcoin and deep learning highlights from machine learning research projects and learning materials from and for ml scientists engineers an enthusiasts
Ross Goodwin,686,23,https://medium.com/artists-and-machine-intelligence/adventures-in-narrated-reality-6516ff395ba3?source=tag_archive---------6----------------,Adventures in Narrated Reality – Artists and Machine Intelligence – Medium,by ross goodwin in may stanford phd student andrej karpathy wrote a blog post entitled the unreasonable effectiveness of recurrent neural networks and released a code repository called char rnn both received quite a lot of attention from the machine learning community in the months that followed spurring commentary and a number of response posts from other researchers i remember reading these posts early last summer initially i was somewhat underwhelmed as at least one commentator pointed out much of the generated text that karpathy chose to highlight did not seem much better than results one might expect from high order character level markov chains here is a snippet of karpathy s char rnn generated shakespeare and here is a snippet of generated shakespeare from a high order character level markov chain via the post linked above so i was discouraged and without access to affordable gpus for training recurrent neural networks i continued to experiment with markov chains generative grammars template systems and other ml free solutions for generating text in december new york university was kind enough to grant me access to their high performance computing facilities i began to train my own recurrent neural networks using karpathy s code and i finally discovered the quasi magical capacities of these machines since then i have been training a collection of recurrent neural network models for my thesis project at nyu and exploring possibilities for devices that could enable such models to serve as expressive real time narrators in our everyday lives at this point since this is my very first medium post perhaps i should introduce myself my name is ross goodwin i m a graduate student at nyu itp in my final semester and computational creative writing is my personal obsession before i began my studies at itp i was a political ghostwriter i graduated from mit in with a b s degree in economics and during my undergraduate years i had worked on barack obama s presidential campaign at the time i wanted to be a political speechwriter and my first job after graduation was a presidential writer position at the white house in this role i wrote presidential proclamations which are statements of national days weeks and months of things everything from thanksgiving and african american history month to lesser known observances like safe boating week it was a very strange job but i thoroughly enjoyed it i left the white house in for a position at the u s department of the treasury where i worked for two years mostly putting together briefing binders for then secretary timothy geithner and deputy secretary neal wolin in the department s front office i didn t get many speechwriting opportunities and pursuing a future in the financial world did not appeal to me so i left to work as a freelance ghostwriter this was a rather dark time in my life as i rapidly found myself writing for a variety of unsavory clients and causes in order to pay my rent every month in completing these assignments i began to integrate algorithms into my writing process to improve my productivity at the time i didn t think about these techniques as algorithmic but it s obvious in retrospect for example if i had to write letters i d write them in a spreadsheet with a paragraph in each cell each letter would exist in a column and i would write across the rows first i d write all the first paragraphs as one group then all the second paragraphs then all the thirds and so on if i had to write a similar group of letters the next day for the same client i would use an excel macro to randomly shuffle the cells then edit the paragraphs for cohesion and turn the results in as an entirely new batch of letters writing this way i found i could complete an hour day of work in about hours i used the rest of my time to work on a novel that s still not finished but that s a story for another time with help from some friends i turned the technique into a game we called the diagonalization argument after georg cantor s mathematical proof of the same name in early a client asked me to write reviews of all the guides available online to learn the python programming language one guide stood out above all others in the sheer number of times i saw users reference it on various online forums and in the countless glowing reviews it had earned across the internet learn python the hard way by zed shaw so to make my reviews better i decided i might as well try to learn python my past attempts at learning to code had failed due to lack of commitment lack of interest or lack of a good project to get started but this time was different somehow zed s guide worked for me and just like that i found myself completely and hopelessly addicted to programming as a writer i gravitated immediately to the broad and expanding world of natural language processing and generation my first few projects were simple poetry generators and once i moved to new york city and started itp i discovered a local community of likeminded individuals leveraging computation to produce and enhance textual work i hosted a code poetry slam in november and began attending todd anderson s monthly wordhack events at babycastles in early i developed and launched word camera a web app and set of physical devices that use the clarifai api to tag images with nouns conceptnet to find related words and a template system to string the results together into descriptive though often bizarre prose poems related to the captured photographs the project was about redefining the photographic experience and it earned more attention than i expected in november i was invited to exhibit this work at idfa doclab in amsterdam at that point it became obvious that word camera or some extension thereof would become my itp thesis project and while searching for ways to improve its output i began to experiment with training my own neural networks rather than using those others had trained via apis as i mentioned above i started using nyu s high performance computing facilities in december this supercomputing cluster includes a staggering array of computational resources in particular at least nvidia tesla k gpus each with gb of gpu memory while gpus aren t strictly required to train deep neural networks the massively parallel processes involved make them all but a necessity for training a larger model that will perform well in a reasonable amount of time using two of andrej karpathy s repositories neuraltalk and char rnn respectively i trained an image captioning model and a number of models for generating text as a result of having free access to the largest gpus in the world i was able to start training very large models right away neuraltalk uses a convolutional neural network to classify images then transfers that classification data to a recurrent neural network that generates a brief caption for my first attempt at training a neuraltalk model i wanted to do something less traditional than simply captioning images in my opinion the idea of machine image captioning is problematic because it s so limited in scope fundamentally a machine that can caption images is a machine that can describe or relate to what it sees in a highly intelligent way i do understand that image captioning is an important benchmark for machine intelligence however i also believe that thinking such a machine s primary use case will be to replace human image captioning represents a highly restrictive and narrow point of view so i tried training a model on frames and corresponding captions from every episode of the tv show the x files my idea was to create a model that if given an image would generate a plausible line of dialogue from what it saw unfortunately it simply did not work most likely due to the dialogue for a particular scene bearing no direct relationship to that scene s imagery rather than generating a different line of dialogue for different images the machine seemed to want to assign the same line to every image indiscriminately strangely these repetitive lines tended to say things like i don t know i m not sure what you want and i don t know what to do one of my faculty advisors patrick hebron jokingly suggested this may be a sign of metacognition needless to say i was slightly creeped out but excited to continue these explorations i tried two other less than traditional approaches with neuraltalk training on reddit image posts and corresponding comments and training on pictures of recreational drugs and corresponding erowid experience reports both worked better than my x files experiment but neither produced particularly interesting results so i resigned myself to training a traditional image captioning model using the microsoft common objects in context mscoco caption set in terms of objects represented mscoco is far from exhaustive but it does contain over images with captions each which is more than i could ve expected to produce on my own from any source furthermore i figured i could always do something less traditional with such a model once trained i made just one adjustment to karpathy s default training parameters decreased the word frequency threshold from five to three by default neuraltalk ignores any word that appears fewer than five times in the caption corpus it trains on i guessed that reducing this threshold would result in some extra verbosity in the generated captions possibly at the expense of accuracy as a more verbose model might describe details that were not actually present in an image however after about five days of training i had produced a model that exceeded cider in tests which is about as good as karpathy suggested the model could get in his documentation as opposed to neuraltalk which is designed to caption images karpathy s char rnn employs a character level lstm recurrent neural network simply for generating text a recurrent neural network is fundamentally a linear pattern machine given a character or set of characters as a seed a char rnn model will predict which character would come next based on what it has learned from its input corpus by doing this again and again the model can generate text in the same manner as a markov chain though its internal processes are far more sophisticated lstm stands for long short term memory which remains a popular architecture for recurrent neural networks unlike a no frills vanilla rnn an lstm protects its fragile underlying neural net with gates that determine which connections will persist in the machine s weight matrices i ve been told that others are using something called a gru but i have yet to investigate this architecture i trained my first text generating lstm on the same prose corpus i used for word camera s literary epitaphs after about hours i was getting results like this this paragraph struck me as highly poetic compared to what i d seen in the past from a computer the language wasn t entirely sensical but it certainly conjured imagery and employed relatively solid grammar furthermore it was original originality has always been important to me in computer generated text because what good is a generator if it just plagiarizes your input corpus this is a major issue with high order markov chains but due to its more sophisticated internal mechanisms the lstm didn t seem to have the same tendency unfortunately much of the prose trained model output that contained less poetic language was also less interesting than the passage above but given that i could produce poetic language with a prose trained model i wondered what results i could get from a poetry trained model the output above comes from the first model i trained on poetry i used the most readily available books i could find mostly those of poets from the th century and earlier whose work had entered the public domain the consistent line breaks and capitalization schemes were encouraging but i still wasn t satisfied with the language due to the predominant age of the corpus it seemed too ornate and formal i wanted more modern sounding poetic language and so i knew i had to train a model on modern poetry i assembled a corpus of all the modern poetry books i could find online it wasn t nearly as easy as assembling the prior corpus unfortunately i can t go into detail on how i got all the books for fear of being sued the results were much closer to what i was looking for in terms of language but they were also inconsistent in quality at the time i believed this was because the corpus was too small so i began to supplement my modern poetry corpus with select prose works to increase its size it remains likely that this was the case however i had not yet discovered the seeding techniques i would later learn can dramatically improve lstm output another idea occurred to me i could seed a poetic language lstm model with a generated image caption to make a new more poetic version of word camera some of the initial results see left were striking i showed them to one of my mentors allison parrish who suggested that i find a way to integrate the caption throughout the poetic text rather than just at the beginning i had showed her some longer examples where the language had strayed quite far from the subject matter of the caption after a few lines i thought about how to accomplish this and settled on a technique of seeding the poetic language lstm multiple times with the same image caption at different temperatures temperature is a parameter a number between zero and one that controls the riskiness of a recurrent neural network s character predictions a low temperature value will result in text that s repetitive but highly grammatical accordingly high temperature results will be more innovative and surprising the model may even invent its own words while containing more mistakes by iterating through temperature values with the same seed the subject matter would remain consistent while the language varied resulting in longer pieces that seemed more cohesive than anything i d ever produced with a computer as i refined the aforementioned technique i trained more lstm models attempting to discover the best training parameters the performance of a neural network model is measured by its loss which drops during training and eventually should be as close to zero as possible a model s loss is a statistical measurement indicating how well a model can predict the character sequences in its own corpus during training there are two loss figures to monitor the training loss which is defined by how well the model predicts the part of the corpus it s actually training on and the validation loss which is defined by how well the model predicts an unknown validation sample that was removed from the corpus prior to training the goal of training a model is to reduce its validation loss as much as possible because we want a model that accurately predicts unknown character sequences not just those it s already seen to this end there are a number of parameters to adjust among which are the training process largely consists of monitoring the validation loss as it drops across model checkpoints and monitoring the difference between training loss and validation loss as karpathy writes in his char rnn documentation in january i released my code on github along with a set of trained neural network models an image captioning model and two poetic language lstm models in my github readme i highlighted a few results i felt were particularly strong unlike prior versions of word camera that mostly relied on a strong connection between the image and the output i found that i could still enjoy the result when the image caption was totally incorrect and there often seemed to be some other accidental or perhaps slightly less than accidental element connecting the image to the words i then shifted my focus to developing a new physical prototype with the prior version of word camera i believed one of the most important parts of the experience was its portability that s why i developed a mobile web app first and why i ensured all the physical prototypes i built were fully portable for the new version i started with a physical prototype rather than a mobile web application because developing an app initially seemed infeasible due to computational requirements though i have since thought of some possible solutions since this would be a rapid prototype i decided to use a very small messenger bag as the case rather than fabricating my own also my research suggested that some of karpathy s code may not run on the raspberry pi s arm architecture so i needed a slightly larger computer that would require a larger power source i decided to use an intel nuc that i powered with a backup battery for a laptop i mounted an elp wide angle camera to the strap alongside a set of controls a rotary potentiometer and a button that communicated with the main computer via an arduino originally i planned to dump the text output to a hacked kindle but ultimately decided the tactile nature of thermal printer paper would provide for a superior experience and allow me to hand out the output on the street like i d done with prior word camera models i found a large format thermal printer model with built in batteries that uses wide paper previous printers i d used had taken paper half as wide and i was able to pick up a couple of them on ebay for less than each based on a suggestion from my friend anthony kesich i decided to add an ascii image of the photo above the text in february i was invited to speak at an art and machine learning symposium at gray area in san francisco in amsterdam at idfa in november i had met jessica brillhart who is a vr director on google s cardboard team in january i began to collaborate with her and some other folks at google on deep dream vr experiences with automated poetic voiceover if you re unfamiliar with deep dream check out this blog post from last summer along with the related github repo and wikipedia article we demonstrated these experiences at the event which was also an auction to sell deep dream artwork to benefit the gray area foundation mike tyka an artist whose deep dream work was prominently featured in the auction had asked me to use my poetic language lstm to generate titles for his artwork i had a lot of fun doing this and i thought the titles came out well they even earned a brief mention in the wired article about the show during my talk the day after the auction i demonstrated my prototype i walked onto the stage wearing my messenger bag snapped a quick photo before i started speaking and revealed the output at the end i would have been more nervous about sharing the machine s poetic output in front of so many people but the poetry had already passed what was in my opinion a more genuine test of its integrity a small reading at a library in brooklyn alongside traditional poets earlier in february i was invited to share some work at the leonard library in williamsburg the theme of the evening s event was love and romance so i generated several poems from images i considered romantic my reading was met with overwhelming approval from the other poets at the event one of whom said that the poem i had generated from the iconic times square v j day kiss photograph by alfred eisenstaedt messed him up as it seemed to contain a plausible description of a flashback from the man s perspective i had been worried because as i once heard allison parrish say so much commentary about computational creative writing focuses on computers replacing humans but as anyone who has worked with computers and language knows that perspective which allison summarized as now they re even taking the poet s job is highly uninformed when we teach computers to write the computers don t replace us any more than pianos replace pianists in a certain way they become our pens and we become more than writers we become writers of writers nietzsche who was the first philosopher to use a typewriter famously wrote our writing tools are also working on our thoughts which media theorist friedrich kittler analyzes in his book gramophone film typewriter p if we employ machine intelligence to augment our writing activities it s worth asking how such technology would affect how we think about writing as well as how we think in the general sense i m inclined to believe that such a transformation would be positive as it would enable us to reach beyond our native writing capacities and produce work that might better reflect our wordless internal thoughts and notions i hesitate to repeat the piano pianist analogy for fear of stomping out its impact but i think it applies here too in producing fully automated writing machines i am only attempting to demonstrate what is possible with a machine alone in my research i am ultimately striving to produce devices that allow humans to work in concert with machines to produce written work my ambition is to augment our creativity not to replace it another ambition of mine is to promote a new framework that i ve been calling narrated reality we already have virtual reality vr and augmented reality ar so it only makes sense to provide another option nr perhaps one that s less visual and more about supplementing existing experiences with expressive narration that way we can enjoy our experiences while we re having them then revisit them later in an augmented format for my itp thesis i had originally planned to produce one general purpose device that used photographs gps coordinates supplemented with foursquare locations and the time to narrate everyday experiences however after receiving some sage advice from taeyoon choi i have decided to split that project into three devices a camera a compass and a clock that respectively use image location and time to realize narrated reality along with designing and building those devices i am in the process of training a library of interchangeable lstm models in order to experience a variety of options with each device in this new space after training a number of models on fiction and poetry i decided to try something different i trained a model on the oxford english dictionary the result was better than i ever could have anticipated an automated balderdash player that could generate plausible definitions for made up words i made a twitter bot so that people could submit their linguistic inventions and a tumblr blog for the complete unabridged definitions i was amazed by the machine s ability to take in and parrot back strings of arbitrary characters it had never seen before and how it often seemed to understand them in the context of actual words the fictional definitions it created for real words were also frequently entertaining my favorite of these was its definition for love although a prior version of the model had defined love as past tense of leave which i found equally amusing one particularly fascinating discovery i made with this bot concerned the importance of a certain seeding technique that kyle mcdonald taught me as discussed above when you generate text with a recurrent neural network you can provide a seed to get the machine started for example if you wanted to know the machine s feelings on the meaning of life you might seed your lstm with the following text and the machine would logically complete your sentence based on the patterns it had absorbed from its training corpus however to get better and more consistent results it makes sense to prepend the seed with a pre seed another paragraph of text to push the lstm into a desired state in practice it s good to use a high quality sample of output from the model you re seeding with length approximately equal to the sequence length see above you set during training this means the seed will now look something like this and the raw output will look like this though usually i remove the pre seed when i present the output the difference was more than apparent when i began using this technique with the dictionary model without the pre seed the bot would usually fail to repeat an unknown word within its generated definition with the pre seed it would reliably parrot back whatever gibberish it had received in the end the oxford english dictionary model trained to a significantly lower final validation loss than any other model i had trained or have trained since one commenter on hacker news noted after considering what to do next i decided to try integrating dictionary definitions into the prose and poetry corpora i had been training before additionally another stanford phd student named justin johnson released a new and improved version of karpathy s char rnn torch rnn which promised to use x less memory which would in turn allow for me to train even larger models than i had been training before on the same gpus it took me an evening to get torch rnn working on nyu s supercomputing cluster but once i had it running i was immediately able to start training models four times as large as those i d trained on before my initial models had million parameters and now i was training with million with some extra room to increase batch size and sequence length parameters the results i got from the first model were stunning the corpus was about poetry prose and dictionary definitions and the output appeared more prose like while remaining somewhat cohesive and painting vivid imagery next i decided to train a model on noam chomsky s complete works most individuals have not produced enough publicly available text mb raw text or novels to train an lstm this size noam chomsky is an exception and the corpus of his writing i was able to assemble weighs in at a hefty mb this project was complicated by the fact that i worked for noam chomsky as an undergraduate at mit but that s a story for another time here is a sample of the output from that model unfortunately i ve had trouble making it say anything interesting about language as it prefers to rattle on and on about the u s and israel and palestine perhaps i ll have to train the next model on academic papers alone and see what happens most recently i ve been training machines on movie screenplays and getting some interesting results if you train an lstm on continuous dialogue you can ask the model questions and receive plausible responses i promised myself i wouldn t write more than words for this article and i ve already passed that threshold so rather than attempting some sort of eloquent conclusion i ll leave you with this brief video there s much more to come in the near future stay tuned edit check out part ii from a quick cheer to a standing ovation clap to show how much you enjoyed this story not a poet new forms interfaces for written language narrated reality c ami is a program at google that brings together artists and engineers to realize projects using machine intelligence works are developed together alongside artists current practices and shown at galleries biennials festivals or online
Eric Elliott,947,9,https://medium.com/javascript-scene/how-to-build-a-neuron-exploring-ai-in-javascript-pt-1-c2726f1f02b2?source=tag_archive---------7----------------,How to Build a Neuron: Exploring AI in JavaScript Pt 1,years ago i was working on a project that needed to be adaptive essentially the software needed to learn and get better at a frequently repeated task over time i d read about neural networks and some early success people had achieved with them so i decided to try it out myself that marked the beginning of a life long fascination with ai ai is a really big deal there are a small handful of technologies that will dramatically change the world over the course of the next years three of the biggest disruptors rely deeply on ai self driving cars alone will disrupt more than million jobs in america radically improve transportation and shipping efficiency and may lead to a huge change in car ownership as we outsource transportation and the pains of car ownership and maintenance to apps like uber you ve probably heard about google s self driving cars but tesla mercedes bmw and other car manufacturers are also making big bets on self driving technology regulations not technology are the primary obstacles for drone based commercial services such as amazon air and just a few days ago the faa relaxed restrictions on commercial drone flights it s still not legal for amazon to deliver packages to your door with drones but that will soon change and when that happens commerce will never be the same of course half a million consumer drone sales over the last holiday season implies that drones are going to change a lot more than commerce expect to see a lot more of them hovering obnoxiously in every metro area in the world in the coming years augmented and virtual reality will fundamentally transform what it means to be human as our senses are augmented by virtual constructs mixed seamlessly with the real world we ll find new ways to work new ways to play and new ways to interact with each other including ar assisted learning telepresence and radical new experiences we haven t dreamed of yet all of these technologies require our gadgets to have an awareness of the surrounding environment and the ability to respond behaviorally to environmental inputs self driving cars need to see obstacles and make corrections to avoid them drones need to detect collision hazards wind and the ground to land on room scale vr needs to alert you of the room boundaries so you don t wander into walls and ar devices need to detect tables chairs and desks and walls and allow virtual elements and characters to interact with them processing sensory inputs and figuring out what they mean is one of the most important jobs that our brain is responsible for how does the human brain deal with the complexity of that job with neurons taken alone a single neuron doesn t do anything particularly interesting but when combined together neural networks are responsible for our ability to recognize the world around us solve problems and interact with our environment and the people around us neural networks are the mechanism that allows us to use language build tools catch balls type read this article remember things and basically do all the things we consider to be thinking recently scientists have been scanning sections of small animal brains on the road to whole brain emulation for example a molecular level model of the neurons in the c elegans roundworm the blue brain project is an attempt to do the same thing with a human brain the research uses microscopes to scan slices of living human brain tissue it s an ambitious project that is still in its infancy a decade after it launched but nobody expects it to be finished tomorrow we are still a long way from whole brain emulation for anything but the simplest organisms but eventually we may be able to emulate a whole human brain on a computer at the molecular level before we try to emulate even basic neuron functionality ourselves we should learn more about how neurons work a neuron is a cell that collects input signals electrical potentials from synaptic terminals typically from dendrites but sometimes directly on the cell membrane when those signals sum past a certain threshold potential at the axon hillock trigger zone it triggers an output signal called an action potential the action potential travels along the output nerve fiber called an axon the axon splits into collateral branches which can carry the output signal to different parts of the neural network each axon branch terminates by splitting into clusters of tiny terminal branches which interface with other neurons through synapses synapse is the word used to describe the transmission mechanism from one neuron to the next there are two kinds of synapse receptors on the postsynaptic terminal wall ion channels and metabolic channels ion channels are fast tens of milliseconds and can either excite or inhibit the potential in the postsynaptic neuron by opening channels for positively or negatively charged ions to enter the cell respectively in an ionotropic transmission the neurotransmitter is released from the presynaptic neuron into the synaptic cleft a tiny gap between the terminals of the presynaptic neuron and the postsynaptic neuron it binds to receptors on the postsynaptic terminal wall which causes them to open allowing electrically charged ions to flow into the postsynaptic cell causing a change to the cell s potential metabolic channels are slower and more controlled than ion channels in chemical transmissions the action potential triggers the release of chemical transmitters from the presynaptic terminal into the synaptic cleft those chemical transmitters bind to metabolic receptors which do not have ion channels of their own that binding triggers chemical reactions on the inside of the cell wall to release g proteins which can open ion channels connected to different receptors as the g proteins must first diffuse and rebind to neighboring channels this process naturally takes longer the duration of metabolic effect can vary from about ms to several minutes depending on how long it takes for neurotransmitters to be absorbed released diffused or recycled back into the presynaptic terminal like ion channels the signal can be either exciting or inhibitory to the postsynaptic neuron potential there is also another type of synapse called an electrical synapse unlike the chemical synapses described above which rely on chemical neurotransmitters and receptors at axon terminals an electrical synapse connects dendrites from one cell directly to dendrites of another cell by a gap junction which is a channel that allows ions and other small molecules to pass directly between the cells effectively creating one large neuron with multiple axons cells connected by electrical synapses almost always fire simultaneously when any connected cell fires all connected cells fire with it however some gap junctions are one way among other things electrical synapses connect cells that control muscle groups such as the heart where it s important that all related cells cooperate creating simultaneous muscle contractions different synapses can have different strengths called weights a synapse weight can change over time through a process known as synaptic plasticity it is believed that changes in synapse connection strength is how we form memory in other words in order to learn and form memories our brain literally rewires itself an increase in synaptic weight is called long term potentiation ltp a decrease in synaptic weight is called long term depression ltd if the postsynaptic neuron tends to fire a lot when the presynaptic neuron fires the synaptic weight increases if the cells don t tend to fire together often the connection weakens in other words the key to synaptic plasticity is hidden in a pair of ms windows if the presynaptic neuron fires before the postsynaptic neuron within ms the weight increases ltp if the presynaptic neuron fires after the postsynaptic neuron within ms the weight decreases ltd this process is called spike timing dependent plasticity spike timing dependent plasticity was discovered in the s and is still being explored but it is believed that action potential backpropagation from the cell s axon to the dendrites is involved in the ltp process during a typical forward propagating event glutamate will be released from the presynaptic terminal which binds to ampa receptors in the postsynaptic terminal wall allowing positively charged sodium ions na into the cell if a large enough depolarization event occurs inside the cell perhaps a backpropagation potential from the axon trigger point electrostatic repulsion will open a magnesium block in nmda receptors allowing even more sodium to flood the cell along with calcium ca at the same time potassium k flows out of the cell these events themselves only last tens of milliseconds but they have indirect lasting effects an influx of calcium causes extra ampa receptors to be inserted into the cell membrane which will allow more sodium ions into the cell during future action potential events from the presynaptic neuron a similar process works in reverse to trigger ltd during ltp events a special class of proteins called growth factors can also form which can cause new synapses to grow strengthening the bond between the two cells the impact of new synapse growth can be permanent assuming that the neurons continue to fire together frequently many artificial neurons act less like neurons and more like transistors with two simple states on or off if enough upstream neurons are on rather than off the neuron is on otherwise it s off other neural nets use input values from to the basic math looks a little like the following this is a good idea if you want to conserve cpu power so you can emulate a lot more neurons and we ve been able to use these basic principles to accomplish very simple pattern recognition tasks such as optical character recognition ocr using pre trained networks however there s a problem as i ve described above real neurons don t behave that way instead synapses transmit fluctuating continuous value potentials over time through the soma cell body to the axon hillock trigger zone where the sum of the signal may or may not trigger an action potential at any given moment in time if the potential in the soma remains high pulses may continue as the cell triggers at high frequency once every few milliseconds lots of variables influence the process the trigger frequencies and the pattern of action potential bursts with the model presented above how would you determine whether or not triggers occurred within the ltp ltd windows what critical element is our basic model missing time but that s a story for a different article stay tuned for part eric elliott is the author of programming javascript applications o reilly and learn javascript with eric elliott he has contributed to software experiences for adobe systems zumba fitness the wall street journal espn bbc and top recording artists including usher frank ocean metallica and many more he spends most of his time in the san francisco bay area with the most beautiful woman in the world from a quick cheer to a standing ovation clap to show how much you enjoyed this story make some magic javascript to submit dm your proposal to js cheerleader on twitter
Dhruv Parthasarathy,665,11,https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0?source=tag_archive---------8----------------,Write an AI to win at Pong from scratch with Reinforcement Learning,there s a huge difference between reading about reinforcement learning and actually implementing it in this post you ll implement a neural network for reinforcement learning and see it learn more and more as it finally becomes good enough to beat the computer in pong you can play around with other such atari games at the openai gym by the end of this post you ll be able to do the following the code and the idea are all tightly based on andrej karpathy s blog post the code in me pong py is intended to be a simpler to follow version of pong py which was written by dr karpathy to follow along you ll need to know the following if you want a deeper dive into the material at hand read the blog post on which all of this is based this post is meant to be a simpler introduction to that material great let s get started we are given the following can we use these pieces to train our agent to beat the computer moreover can we make our solution generic enough so it can be reused to win in games that aren t pong indeed we can andrej does this by building a neural network that takes in each image and outputs a command to our ai to move up or down we can break this down a bit more into the following steps our neural network based heavily on andrej s solution will do the following ok now that we ve described the problem and its solution let s get to writing some code we re now going to follow the code in me pong py please keep it open and read along the code starts here first let s use openai gym to make a game environment and get our very first image of the game next we set a bunch of parameters based off of andrej s blog post we aren t going to worry about tuning them but note that you can probably get better performance by doing so the parameters we will use are then we set counters initial values and the initial weights in our neural network weights are stored in matrices layer of our neural network is a x matrix representing the weights for our hidden layer for layer element w ij represents the weight of neuron i for input pixel j in layer layer is a x matrix representing the weights of the output of the hidden layer on our final output for layer element w i represents the weights we place on the activation of neuron i in the hidden layer we initialize each layer s weights with random numbers for now we divide by the square root of the number of the dimension size to normalize our weights next we set up the initial parameters for rmsprop a method for updating weights that we will discuss later don t worry too much about understanding what you see below i m mainly bringing it up here so we can continue to follow along the main code block we ll need to collect a bunch of observations and intermediate values across the episode and use those to compute the gradient at the end based on the result the below sets up the arrays where we ll collect all that information ok we re all done with the setup if you were following it should look something like this phew now for the fun part the crux of our algorithm is going to live in a loop where we continually make a move and then learn based on the results of the move we ll put everything in a while block for now but in reality you might set up a break condition to stop the process the first step to our algorithm is processing the image of the game that openai gym passed us we really don t care about the entire image just certain details we do this below let s dive into preprocess observations to see how we convert the image openai gym gives us into something we can use to train our neural network the basic steps are now that we ve preprocessed the observations let s move on to actually sending the observations through our neural net to generate the probability of telling our ai to move up here are the steps we ll take how exactly does apply neural nets take observations and weights and generate a probability of going up this is just the forward pass of the neural network let s look at the code below for more information as you can see it s not many steps at all let s go step by step let s return to the main algorithm and continue on now that we have obtained a probability of going up we need to now record the results for later learning and choose an action to tell our ai to implement we choose an action by flipping an imaginary coin that lands up with probability up probability and down with up probability if it lands up we choose tell our ai to go up and if not we tell it to go down we also having done that we pass the action to openai gym via env step action ok we ve covered the first half of the solution we know what action to tell our ai to take if you ve been following along your code should look like this now that we ve made our move it s time to start learning so we figure out the right weights in our neural network learning is all about seeing the result of the action i e whether or not we won the round and changing our weights accordingly the first step to learning is asking the following question mathematically this is just the derivative of our result with respect to the outputs of our final layer if l is the value of our result to us and f is the function that gives us the activations of our final layer this derivative is just l f in a binary classification context i e we just have to tell the ai one of two actions up or down this derivative turns out to be note that in the above equation represents the sigmoid function read the attribute classification section here for more information about how we get the above derivative we simplify this further below after one action moving the paddle up or down we don t really have an idea of whether or not this was the right action so we re going to cheat and treat the action we end up sampling from our probability as the correct action our predicion for this round is going to be the probability of going up we calculated using that we have that l f can be computed by awesome we have the gradient per action the next step is to figure out how we learn after the end of an episode i e when we or our opponent miss the ball and someone gets a point we do this by computing the policy gradient of the network at the end of each episode the intuition here is that if we won the round we d like our network to generate more of the actions that led to us winning alternatively if we lose we re going to try and generate less of these actions openai gym provides us the handy done variable to tell us when an episode finishes i e we missed the ball or our opponent missed the ball when we notice we are done the first thing we do is compile all our observations and gradient calculations for the episode this allows us to apply our learnings over all the actions in the episode next we want to learn in such a way that actions taken towards the end of an episode more heavily influence our learning than actions taken at the beginning this is called discounting think about it this way if you moved up at the first frame of the episode it probably had very little impact on whether or not you win however closer to the end of the episode your actions probably have a much larger effect as they determine whether or not your paddle reaches the ball and how your paddle hits the ball we re going to take this weighting into account by discounting our rewards such that rewards from earlier frames are discounted a lot more than rewards for later frames after this we re going to finally use backpropagation to compute the gradient i e the direction we need to move our weights to improve let s dig in a bit into how the policy gradient for the episode is computed this is one of the most important parts of reinforcement learning as it s how our agent figures out how to improve over time to begin with if you haven t already read this excerpt on backpropagation from michael nielsen s excellent free book on deep learning as you ll see in that excerpt there are four fundamental equations of backpropogation a technique for computing the gradient for our weights our goal is to find c w bp the derivative of the cost function with respect to the first layer s weights and c w the derivative of the cost function with respect to the second layer s weights these gradients will help us understand what direction to move our weights in for the greatest improvement to begin with let s start with c w if a l is the activations of the hidden layer layer we see that the formula is indeed this is exactly what we do here next we need to calculate c w the formula for that is and we also know that a l is just our observation values so all we need now is l once we have that we can calculate c w and return we do just that below if you ve been following along your function should look like this with that we ve finished backpropagation and computed our gradients after we have finished batch size episodes we finally update our weights for our neural network and implement our learnings to update the weights we simply apply rmsprop an algorithm for updating weights described by sebastian reuder here we implement this below this is the step that tweaks our weights and allows us to get better over time this is basically it putting it altogether it should look like this you just coded a full neural network for playing pong uncomment env render and run it for days to see it finally beat the computer you ll need to do some pickling as done in andrej karpathy s solution to be able to visualize your results when you win according to the blog post this algorithm should take around days of training on a macbook to start beating the computer consider tweaking the parameters or using convolutional neural nets to boost the performance further if you want a further primer into neural networks and reinforcement learning there are some great resources to learn more i work at udacity as the director of machine learning programs from a quick cheer to a standing ovation clap to show how much you enjoyed this story dhruvp vp eng athelas mit math and cs undergrad mit cs masters previously director of ai programs udacity
Waleed Abdulla,507,12,https://medium.com/@waleedka/traffic-sign-recognition-with-tensorflow-629dffc391a6?source=tag_archive---------9----------------,Traffic Sign Recognition with TensorFlow – Waleed Abdulla – Medium,this is part of a series about building a deep learning model to recognize traffic signs it s intended to be a learning experience for myself and for anyone else who likes to follow along there are a lot of resources that cover the theory and math of neural networks so i ll focus on the practical aspects instead i ll describe my own experience building this model and share the source code and relevant materials this is suitable for those who know python and the basics of machine learning already but want hands on experience and to practice building a real application in this part i ll talk about image classification and i ll keep the model as simple as possible in later parts i ll cover convolutional networks data augmentation and object detection the source code is available in this jupyter notebook i m using python and tensorflow if you prefer to run the code in docker you can use my docker image that contains many popular deep learning tools run it with this command note that my project directory is in traffic and i m mapping it to the traffic directory in the docker container modify this if you re using a different directory my first challenge was finding a good training dataset traffic sign recognition is a well studied problem so i figured i ll find something online i started by googling traffic sign dataset and found several options i picked the belgian traffic sign dataset because it was big enough to train on and yet small enough to be easy to work with you can download the dataset from http btsd ethz ch shareddata there are a lot of datasets on that page but you only need the two files listed under belgiumts for classification cropped images after expanding the files this is my directory structure try to match it so you can run the code without having to change the paths each of the two directories contain subdirectories named sequentially from to the directory names represent the labels and the images inside each directory are samples of each label or if you prefer to sound more formal do exploratory data analysis it s tempting to skip this part but i ve found that the code i write to examine the data ends up being used a lot throughout the project i usually do this in jupyter notebooks and share them with the team knowing your data well from the start saves you a lot of time later the images in this dataset are in an old ppm format so old in fact that most tools don t support it which meant that i couldn t casually browse the folders to take a look at the images luckily the scikit image library recognizes this format this code will load the data and return two lists images and labels this is a small dataset so i m loading everything into ram to keep it simple for larger datasets you d want to load the data in batches after loading the images into numpy arrays i display a sample image of each label see code in the notebook this is our dataset looks like a good training set the image quality is great and there are a variety of angles and lighting conditions more importantly the traffic signs occupy most of the area of each image which allows me to focus on object classification and not have to worry about finding the location of the traffic sign in the image object detection i ll get to object detection in a future post the first thing i noticed from the samples above is that images are square ish but have different aspect ratios my neural network will take a fixed size input so i have some preprocessing to do i ll get to that soon but first let s pick one label and see more of its images here is an example of label it looks like the dataset considers all speed limit signs to be of the same class regardless of the numbers on them that s fine as long as we know about it beforehand and know what to expect that s why understanding your dataset is so important and can save you a lot of pain and confusion later i ll leave exploring the other labels to you labels and are interesting to check they also have numbers in red circles so the model will have to get really good to differentiate between them most image classification networks expect images of a fixed size and our first model will do as well so we need to resize all the images to the same size but since the images have different aspect ratios then some of them will be stretched vertically or horizontally is that a problem i think it s not in this case because the differences in aspect ratios are not that large my own criteria is that if a person can recognize the images when they re stretched then the model should be able to do so as well what are the sizes of the images anyway let s print a few examples the sizes seem to hover around x i could use that size to preserve as much information as possible but in early development i prefer to use a smaller size because it leads to faster training which allows me to iterate faster i experimented with x and x but they were too small i ended up picking x which is easy to recognize see below and reduces the size of the model and training data by a factor of compared to x i m also in the habit of printing the min and max values often it s a simple way to verify the range of the data and catch bugs early this tells me that the image colors are the standard range of we re getting to the interesting part continuing the theme of keeping it simple i started with the simplest possible model a one layer network that consists of one neuron per label this network has neurons and each neuron takes the rgb values of all pixels as input effectively each neuron receives inputs this is a fully connected layer because every neuron connects to every input value you re probably familiar with its equation i start with a simple model because it s easy to explain easy to debug and fast to train once this works end to end expanding on it is much easier than building something complex from the start tensorflow encapsulates the architecture of a neural network in an execution graph the graph consists of operations ops for short such as add multiply reshape etc these ops perform actions on data in tensors multidimensional arrays i ll go through the code to build the graph step by step below but here is the full code if you prefer to scan it first first i create the graph object tensorflow has a default global graph but i don t recommend using it global variables are bad in general because they make it too easy to introduce bugs i prefer to create the graph explicitly then i define placeholders for the images and labels the placeholders are tensorflow s way of receiving input from the main program notice that i create the placeholders and all other ops inside the block of with graph as default this is so they become part of my graph object rather than the global graph the shape of the images ph placeholder is none it stands for batch size height width channels often shortened as nhwc the none for batch size means that the batch size is flexible which means that we can feed different batch sizes to the model without having to change the code pay attention to the order of your inputs because some models and frameworks might use a different arrangement such as nchw next i define the fully connected layer rather than implementing the raw equation y xw b i use a handy function that does that in one line and also applies the activation function it expects input as a one dimensional vector though so i flatten the images first i m using the relu activation function here it simply converts all negative values to zeros it s been shown to work well in classification tasks and trains faster than sigmoid or tanh for more background check here and here the output of the fully connected layer is a logits vector of length technically it s none because we re dealing with a batch of logits vectors a row in the logits tensor might look like this the higher the value the more likely that the image represents that label logits are not probabilities though they can have any value and they don t add up to the actual absolute values of the logits are not important just their values relative to each other it s easy to convert logits to probabilities using the softmax function if needed it s not needed here in this application we just need the index of the largest value which corresponds to the id of the label the argmax op does that the argmax output will be integers in the range to choosing the right loss function is an area of research in and of itself which i won t delve into it here other than to say that cross entropy is the most common function for classification tasks if you re not familiar with it there is a really good explanation here and here cross entropy is a measure of difference between two vectors of probabilities so we need to convert labels and the logits to probability vectors the function sparse softmax cross entropy with logits simplifies that it takes the generated logits and the groundtruth labels and does three things converts the label indexes of shape none to logits of shape none one hot vectors then it runs softmax to convert both prediction logits and label logits to probabilities and finally calculates the cross entropy between the two this generates a loss vector of shape none d of length batch size which we pass through reduce mean to get one single number that represents the loss value choosing the optimization algorithm is another decision to make i usually use the adam optimizer because it s been shown to converge faster than simple gradient descent this post does a great job comparing different gradient descent optimizers the last node in the graph is the initialization op which simply sets the values of all variables to zeros or to random values or whatever the variables are set to initialize to notice that the code above doesn t execute any of the ops yet it s just building the graph and describing its inputs the variables we defined above such as init loss predicted labels don t contain numerical values they are references to ops that we ll execute next this is where we iteratively train the model to minimize the loss function before we start training though we need to create a session object i mentioned the graph object earlier and how it holds all the ops of the model the session on the other hand holds the values of all the variables if a graph holds the equation y xw b then the session holds the actual values of these variables usually the first thing to run after starting a session is the initialization op init to initialize the variables then we start the training loop and run the train op repeatedly while not necessary it s useful to run the loss op as well to print its values and monitor the progress of the training in case you re wondering i set the loop to so that the i condition is satisfied in the last round and prints the last loss value the output should look something like this now we have a trained model in memory in the session object to use it we call session run just like in the training code the predicted labels op returns the output of the argmax function so that s what we need to run here i classify random images and print both the predictions and the groundtruth labels for comparison in the notebook i include a function to visualize the results as well it generates something like this the visualization shows that the model is working but doesn t quantify how accurate it is and you might ve noticed that it s classifying the training images so we don t know yet if the model generalizes to images that it hasn t seen before next we calculate a better evaluation metric to properly measure how the model generalizes to data it hasn t seen i do the evaluation on test data that i didn t use in training the belgiumts dataset makes this easy by providing two separate sets one for training and one for testing in the notebook i load the test set resize the images to x and then calculate the accuracy this is the relevant part of the code that calculates the accuracy the accuracy i get in each run ranges between and depending on whether the model lands on a local minimum or a global minimum this is expected when running a simple model like this one in a future post i ll talk about ways to improve the consistency of the results congratulations we have a working simple neural network given how simple this neural network is training takes just a minute on my laptop so i didn t bother saving the trained model in the next part i ll add code to save and load trained models and expand to use multiple layers convolutional networks and data augmentation stay tuned from a quick cheer to a standing ovation clap to show how much you enjoyed this story startups deep learning computer vision
Stefan Kojouharov,14200,7,https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463?source=tag_archive---------0----------------,"Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data",over the past few months i have been collecting ai cheat sheets from time to time i share them with friends and colleagues and recently i have been getting asked a lot so i decided to organize and share the entire collection to make things more interesting and give context i added descriptions and or excerpts for each major topic this is the most complete list and the big o is at the very end enjoy this machine learning cheat sheet will help you find the right estimator for the job which is the most difficult part the flowchart will help you check the documentation and rough guide of each estimator that will help you to know more about the problems and how to solve it scikit learn formerly scikits learn is a free software machine learning library for the python programming language it features various classification regression and clustering algorithms including support vector machines random forests gradient boosting k means and dbscan and is designed to interoperate with the python numerical and scientific libraries numpy and scipy in may google announced the second generation of the tpu as well as the availability of the tpus in google compute engine the second generation tpus deliver up to teraflops of performance and when organized into clusters of tpus provide up to petaflops in google s tensorflow team decided to support keras in tensorflow s core library chollet explained that keras was conceived to be an interface rather than an end to end machine learning framework it presents a higher level more intuitive set of abstractions that make it easy to configure neural networks regardless of the backend scientific computing library numpy targets the cpython reference implementation of python which is a non optimizing bytecode interpreter mathematical algorithms written for this version of python often run much slower than compiled equivalents numpy address the slowness problem partly by providing multidimensional arrays and functions and operators that operate efficiently on arrays requiring rewriting some code mostly inner loops using numpy the name pandas is derived from the term panel data an econometrics term for multidimensional structured data sets the term data wrangler is starting to infiltrate pop culture in the movie kong skull island one of the characters played by actor marc evan jackson is introduced as steve woodward our data wrangler scipy builds on the numpy array object and is part of the numpy stack which includes tools like matplotlib pandas and sympy and an expanding set of scientific computing libraries this numpy stack has similar users to other applications such as matlab gnu octave and scilab the numpy stack is also sometimes referred to as the scipy stack matplotlib is a plotting library for the python programming language and its numerical mathematics extension numpy it provides an object oriented api for embedding plots into applications using general purpose gui toolkits like tkinter wxpython qt or gtk there is also a procedural pylab interface based on a state machine like opengl designed to closely resemble that of matlab though its use is discouraged scipy makes use of matplotlib pyplot is a matplotlib module which provides a matlab like interface matplotlib is designed to be as usable as matlab with the ability to use python with the advantage that it is free if you like this list you can let me know here stefan is the founder of chatbot s life a chatbot media and consulting firm chatbot s life has grown to over k views per month and has become the premium place to learn about bots ai online chatbot s life has also consulted many of the top bot companies like swelly instavest outbrain neargroup and a number of enterprises big o algorithm cheat sheet http bigocheatsheet com bokeh cheat sheet https s amazonaws com assets datacamp com blog assets python bokeh cheat sheet pdf data science cheat sheet https www datacamp com community tutorials python data science cheat sheet basics data wrangling cheat sheet https www rstudio com wp content uploads data wrangling cheatsheet pdf data wrangling https en wikipedia org wiki data wrangling ggplot cheat sheet https www rstudio com wp content uploads ggplot cheatsheet pdf keras cheat sheet https www datacamp com community blog keras cheat sheet gs drkenms keras https en wikipedia org wiki keras machine learning cheat sheet https ai icymi email new machinelearning cheat sheet by emily barry abdsc machine learning cheat sheet https docs microsoft com en in azure machine learning machine learning algorithm cheat sheet ml cheat sheet http peekaboo vision blogspot com machine learning cheat sheet for scikit html matplotlib cheat sheet https www datacamp com community blog python matplotlib cheat sheet gs uekyspy matpotlib https en wikipedia org wiki matplotlib neural networks cheat sheet http www asimovinstitute org neural network zoo neural networks graph cheat sheet http www asimovinstitute org blog neural networks https www quora com where can find a cheat sheet for neural network numpy cheat sheet https www datacamp com community blog python numpy cheat sheet gs ak zbge numpy https en wikipedia org wiki numpy pandas cheat sheet https www datacamp com community blog python pandas cheat sheet gs oundfxm pandas https en wikipedia org wiki pandas software pandas cheat sheet https www datacamp com community blog pandas cheat sheet python gs hpforic pyspark cheat sheet https www datacamp com community blog pyspark cheat sheet python gs l j zxq scikit cheat sheet https www datacamp com community blog scikit learn cheat sheet scikit learn https en wikipedia org wiki scikit learn scikit learn cheat sheet http peekaboo vision blogspot com machine learning cheat sheet for scikit html scipy cheat sheet https www datacamp com community blog python scipy cheat sheet gs jdsg oi scipy https en wikipedia org wiki scipy tesorflow cheat sheet https www altoros com tensorflow cheat sheet html tensor flow https en wikipedia org wiki tensorflow from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder of chatbots life i help companies create great chatbots ai systems and share my insights along the way latest news info and tutorials on artificial intelligence machine learning deep learning big data and what it means for humanity
Avinash Sharma V,6900,10,https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0?source=tag_archive---------1----------------,Understanding Activation Functions in Neural Networks,recently a colleague of mine asked me a few questions like why do we have so many activation functions why is that one works better than the other how do we know which one to use is it hardcore maths and so on so i thought why not write an article on it for those who are familiar with neural network only at a basic level and is therefore wondering about activation functions and their why how mathematics note this article assumes that you have a basic knowledge of an artificial neuron i would recommend reading up on the basics of neural networks before reading this article for better understanding so what does an artificial neuron do simply put it calculates a weighted sum of its input adds a bias and then decides whether it should be fired or not yeah right an activation function does this but let s go with the flow for a moment so consider a neuron now the value of y can be anything ranging from inf to inf the neuron really doesn t know the bounds of the value so how do we decide whether the neuron should fire or not why this firing pattern because we learnt it from biology that s the way brain works and brain is a working testimony of an awesome and intelligent system we decided to add activation functions for this purpose to check the y value produced by a neuron and decide whether outside connections should consider this neuron as fired or not or rather let s say activated or not the first thing that comes to our minds is how about a threshold based activation function if the value of y is above a certain value declare it activated if it s less than the threshold then say it s not hmm great this could work activation function a activated if y threshold else not alternatively a if y threshold otherwise well what we just did is a step function see the below figure its output is activated when value threshold and outputs a not activated otherwise great so this makes an activation function for a neuron no confusions however there are certain drawbacks with this to understand it better think about the following suppose you are creating a binary classifier something which should say a yes or no activate or not activate a step function could do that for you that s exactly what it does say a or now think about the use case where you would want multiple such neurons to be connected to bring in more classes class class class etc what will happen if more than neuron is activated all neurons will output a from step function now what would you decide which class is it hmm hard complicated you would want the network to activate only neuron and others should be only then would you be able to say it classified properly identified the class ah this is harder to train and converge this way it would have been better if the activation was not binary and it instead would say activated or activated and so on and then if more than neuron activates you could find which neuron has the highest activation and so on better than max a softmax but let s leave that for now in this case as well if more than neuron says activated the problem still persists i know but since there are intermediate activation values for the output learning can be smoother and easier less wiggly and chances of more than neuron being activated is lesser when compared to step function while training also depending on what you are training and the data ok so we want something to give us intermediate analog activation values rather than saying activated or not binary the first thing that comes to our minds would be linear function a cx a straight line function where activation is proportional to input which is the weighted sum from neuron this way it gives a range of activations so it is not binary activation we can definitely connect a few neurons together and if more than fires we could take the max or softmax and decide based on that so that is ok too then what is the problem with this if you are familiar with gradient descent for training you would notice that for this function derivative is a constant a cx derivative with respect to x is c that means the gradient has no relationship with x it is a constant gradient and the descent is going to be on constant gradient if there is an error in prediction the changes made by back propagation is constant and not depending on the change in input delta x this is not that good not always but bear with me there is another problem too think about connected layers each layer is activated by a linear function that activation in turn goes into the next level as input and the second layer calculates weighted sum on that input and it in turn fires based on another linear activation function no matter how many layers we have if all are linear in nature the final activation function of last layer is nothing but just a linear function of the input of first layer pause for a bit and think about it that means these two layers or n layers can be replaced by a single layer ah we just lost the ability of stacking layers this way no matter how we stack the whole network is still equivalent to a single layer with linear activation a combination of linear functions in a linear manner is still another linear function let s move on shall we well this looks smooth and step function like what are the benefits of this think about it for a moment first things first it is nonlinear in nature combinations of this function are also nonlinear great now we can stack layers what about non binary activations yes that too it will give an analog activation unlike step function it has a smooth gradient too and if you notice between x values to y values are very steep which means any small changes in the values of x in that region will cause values of y to change significantly ah that means this function has a tendency to bring the y values to either end of the curve looks like it s good for a classifier considering its property yes it indeed is it tends to bring the activations to either side of the curve above x and below x for example making clear distinctions on prediction another advantage of this activation function is unlike linear function the output of the activation function is always going to be in range compared to inf inf of linear function so we have our activations bound in a range nice it won t blow up the activations then this is great sigmoid functions are one of the most widely used activation functions today then what are the problems with this if you notice towards either end of the sigmoid function the y values tend to respond very less to changes in x what does that mean the gradient at that region is going to be small it gives rise to a problem of vanishing gradients hmm so what happens when the activations reach near the near horizontal part of the curve on either sides gradient is small or has vanished cannot make significant change because of the extremely small value the network refuses to learn further or is drastically slow depending on use case and until gradient computation gets hit by floating point value limits there are ways to work around this problem and sigmoid is still very popular in classification problems another activation function that is used is the tanh function hm this looks very similar to sigmoid in fact it is a scaled sigmoid function ok now this has characteristics similar to sigmoid that we discussed above it is nonlinear in nature so great we can stack layers it is bound to range so no worries of activations blowing up one point to mention is that the gradient is stronger for tanh than sigmoid derivatives are steeper deciding between the sigmoid or tanh will depend on your requirement of gradient strength like sigmoid tanh also has the vanishing gradient problem tanh is also a very popular and widely used activation function later comes the relu function a x max x the relu function is as shown above it gives an output x if x is positive and otherwise at first look this would look like having the same problems of linear function as it is linear in positive axis first of all relu is nonlinear in nature and combinations of relu are also non linear in fact it is a good approximator any function can be approximated with combinations of relu great so this means we can stack layers it is not bound though the range of relu is inf this means it can blow up the activation another point that i would like to discuss here is the sparsity of the activation imagine a big neural network with a lot of neurons using a sigmoid or tanh will cause almost all neurons to fire in an analog way remember that means almost all activations will be processed to describe the output of a network in other words the activation is dense this is costly we would ideally want a few neurons in the network to not activate and thereby making the activations sparse and efficient relu give us this benefit imagine a network with random initialized weights or normalised and almost of the network yields activation because of the characteristic of relu output for negative values of x this means a fewer neurons are firing sparse activation and the network is lighter woah nice relu seems to be awesome yes it is but nothing is flawless not even relu because of the horizontal line in relu for negative x the gradient can go towards for activations in that region of relu gradient will be because of which the weights will not get adjusted during descent that means those neurons which go into that state will stop responding to variations in error input simply because gradient is nothing changes this is called dying relu problem this problem can cause several neurons to just die and not respond making a substantial part of the network passive there are variations in relu to mitigate this issue by simply making the horizontal line into non horizontal component for example y x for x will make it a slightly inclined line rather than horizontal line this is leaky relu there are other variations too the main idea is to let the gradient be non zero and recover during training eventually relu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations that is a good point to consider when we are designing deep neural nets now which activation functions to use does that mean we just use relu for everything we do or sigmoid or tanh well yes and no when you know the function you are trying to approximate has certain characteristics you can choose an activation function which will approximate the function faster leading to faster training process for example a sigmoid works well for a classifier see the graph of sigmoid doesn t it show the properties of an ideal classifier because approximating a classifier function as combinations of sigmoid is easier than maybe relu for example which will lead to faster training process and convergence you can use your own custom functions too if you don t know the nature of the function you are trying to learn then maybe i would suggest start with relu and then work backwards relu works most of the time as a general approximator in this article i tried to describe a few activation functions used commonly there are other activation functions too but the general idea remains the same research for better activation functions is still ongoing hope you got the idea behind activation function why they are used and how do we decide which one to use from a quick cheer to a standing ovation clap to show how much you enjoyed this story musings of an ai deep learning mathematics addict
Elle O'Brien,2300,6,https://towardsdatascience.com/romance-novels-generated-by-artificial-intelligence-1b31d9c872b2?source=tag_archive---------2----------------,"Romance Novels, Generated by Artificial Intelligence",i ve always been fascinated with romance novels the kind they sell at the drugstore for a couple of dollars usually with some attractive soft lit couples on the cover so when i started futzing around with text generating neural networks a few weeks ago i developed an urgent curiosity to discover what artificial intelligence could contribute to the ever popular genre maybe one day there will be entire books written by computers for now let s start with titles i gathered over harlequin romance novel titles and gave them to a neural network a type of artificial intelligence that learns the structure of text it s powerful enough to string together words in a way that seems almost human human the other is all wackiness i was not disappointed with what came out i even photoshopped some of my favorites into existence the author names are synthesized from machine learning too let s have a look by theme a common theme in romance novels is pregnancy and the word baby had a strong showing in the titles i trained the neural network on naturally the neural network came up with a lot of baby themed titles there s an unusually high concentration of sheikhs vikings and billionaires in the harlequin world likewise the neural network generated some colorful new bachelor types i have so many questions how is the prince pregnant what sort of consulting does the count do who is butterfly earl and what makes the sheikh s desires so convenient although there are exceptions most romance novels end in happily ever afters a lot of them even start with an unexpected wedding a marriage of convenience or a stipulation of a business contract or a sham that turns into real love the neural network seems to have internalized something about matrimony doctors and surgeons are common paramours for mistresses headed towards the marriage valley christmas is a magical time for surgeons sheikhs playboys dads consultants and the women who love them what or where is knith i just like mission christmas this neural network has never seen the big montana sky but it has some questionable ideas about cowboys the neural network generated some decidedly pg titles they can t all live happily ever after some of the generated titles sounded like m night shyamalan was a collaborator how did the word fear get in there it s possible the network generated it without having fear in the training set but a subset of the harlequin empire is geared towards paranormal and gothic romance that might have included the word note i checked and there was veil of fear published in to wrap it up some of the adorable failures and near misses generated by the neural network i hope you ve enjoyed computer generated romance novel titles half as much as i have maybe someone out there can write about the virgin viking or the consultant count or the baby surgeon seduction i d buy it i built a webscraper in python thanks beautiful soup that grabbed about romance novel titles published under the harlequin brand off of fictiondb com harlequin is to me synonymous with the romance genre although it comprises only a fraction albeit a healthy one of the entire market i fed this list of book titles into a recurrent neural network using software i got from github and waited a few hours for the magic to happen the model i fit was a layer node recurrent neural network i also trained the network on the author list in to create some new pen names for more about the neural network i used have a look at the fabulous work of andrej karpathy i discovered that surgery by the sea is actually a real novel written by sheila douglas and published in so this one isn t an original neural network creation because the training set is rather small only about mb of text data it s to be expected that sometimes the machine will spit out one of the titles it was trained on one of the more challenging aspects of this project was discerning when that happened since the real published titles can be more surprising than anything born out of artificial intelligence for example the daddy and grinch are both real in fact the very first romance novel published by harlequin was called the manatee from a quick cheer to a standing ovation clap to show how much you enjoyed this story computational scientist software developer science writer sharing concepts ideas and codes
Slav Ivanov,4400,10,https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607?source=tag_archive---------3----------------,37 Reasons why your Neural Network is not working – Slav,the network had been training for the last hours it all looked good the gradients were flowing and the loss was decreasing but then came the predictions all zeroes all background nothing detected what did i do wrong i asked my computer who didn t answer where do you start checking if your model is outputting garbage for example predicting the mean of all outputs or it has really poor accuracy a network might not be training for a number of reasons over the course of many debugging sessions i would often find myself doing the same checks i ve compiled my experience along with the best ideas around in this handy list i hope they would be of use to you too a lot of things can go wrong but some of them are more likely to be broken than others i usually start with this short list as an emergency first response if the steps above don t do it start going down the following big list and verify things one by one check if the input data you are feeding the network makes sense for example i ve more than once mixed the width and the height of an image sometimes i would feed all zeroes by mistake or i would use the same batch over and over so print display a couple of batches of input and target output and make sure they are ok try passing random numbers instead of actual data and see if the error behaves the same way if it does it s a sure sign that your net is turning data into garbage at some point try debugging layer by layer op by op and see where things go wrong your data might be fine but the code that passes the input to the net might be broken print the input of the first layer before any operations and check it check if a few input samples have the correct labels also make sure shuffling input samples works the same way for output labels maybe the non random part of the relationship between the input and output is too small compared to the random part one could argue that stock prices are like this i e the input are not sufficiently related to the output there isn t an universal way to detect this as it depends on the nature of the data this happened to me once when i scraped an image dataset off a food site there were so many bad labels that the network couldn t learn check a bunch of input samples manually and see if labels seem off the cutoff point is up for debate as this paper got above accuracy on mnist using corrupted labels if your dataset hasn t been shuffled and has a particular order to it ordered by label this could negatively impact the learning shuffle your dataset to avoid this make sure you are shuffling input and labels together are there a class a images for every class b image then you might need to balance your loss function or try other class imbalance approaches if you are training a net from scratch i e not finetuning you probably need lots of data for image classification people say you need a images per class or more this can happen in a sorted dataset i e the first k samples contain the same class easily fixable by shuffling the dataset this paper points out that having a very large batch can reduce the generalization ability of the model thanks to hengcherkeng for this one did you standardize your input to have zero mean and unit variance augmentation has a regularizing effect too much of this combined with other forms of regularization weight l dropout etc can cause the net to underfit if you are using a pretrained model make sure you are using the same normalization and preprocessing as the model was when training for example should an image pixel be in the range or cs n points out a common pitfall also check for different preprocessing in each sample or batch this will help with finding where the issue is for example if the target output is an object class and coordinates try limiting the prediction to object class only again from the excellent cs n initialize with small parameters without regularization for example if we have classes at chance means we will get the correct class of the time and the softmax loss is the negative log probability of the correct class so ln after this try increasing the regularization strength which should increase the loss if you implemented your own loss function check it for bugs and add unit tests often my loss would be slightly incorrect and hurt the performance of the network in a subtle way if you are using a loss function provided by your framework make sure you are passing to it what it expects for example in pytorch i would mix up the nllloss and crossentropyloss as the former requires a softmax input and the latter doesn t if your loss is composed of several smaller loss functions make sure their magnitude relative to each is correct this might involve testing different combinations of loss weights sometimes the loss is not the best predictor of whether your network is training properly if you can use other metrics like accuracy did you implement any of the layers in the network yourself check and double check to make sure they are working as intended check if you unintentionally disabled gradient updates for some layers variables that should be learnable maybe the expressive power of your network is not enough to capture the target function try adding more layers or more hidden units in fully connected layers if your input looks like k h w it s easy to miss errors related to wrong dimensions use weird numbers for input dimensions for example different prime numbers for each dimension and check how they propagate through the network if you implemented gradient descent by hand gradient checking makes sure that your backpropagation works like it should more info overfit a small subset of the data and make sure it works for example train with just or examples and see if your network can learn to differentiate these move on to more samples per class if unsure use xavier or he initialization also your initialization might be leading you to a bad local minimum so try a different initialization and see if it helps maybe you using a particularly bad set of hyperparameters if feasible try a grid search too much regularization can cause the network to underfit badly reduce regularization such as dropout batch norm weight bias l regularization etc in the excellent practical deep learning for coders course jeremy howard advises getting rid of underfitting first this means you overfit the training data sufficiently and only then addressing overfitting maybe your network needs more time to train before it starts making meaningful predictions if your loss is steadily decreasing let it train some more some frameworks have layers like batch norm dropout and other layers behave differently during training and testing switching to the appropriate mode might help your network to predict properly your choice of optimizer shouldn t prevent your network from training unless you have selected particularly bad hyperparameters however the proper optimizer for a task can be helpful in getting the most training in the shortest amount of time the paper which describes the algorithm you are using should specify the optimizer if not i tend to use adam or plain sgd with momentum check this excellent post by sebastian ruder to learn more about gradient descent optimizers a low learning rate will cause your model to converge very slowly a high learning rate will quickly decrease the loss in the beginning but might have a hard time finding a good solution play around with your current learning rate by multiplying it by or getting a nan non a number is a much bigger issue when training rnns from what i hear some approaches to fix it did i miss anything is anything wrong let me know by leaving a reply below from a quick cheer to a standing ovation clap to show how much you enjoyed this story entrepreneur hacker machine learning deep learning and other types of learning
Slav Ivanov,2900,9,https://blog.slavv.com/picking-a-gpu-for-deep-learning-3d4795c273b9?source=tag_archive---------4----------------,Picking a GPU for Deep Learning – Slav,quite a few people have asked me recently about choosing a gpu for machine learning as it stands success with deep learning heavily dependents on having the right hardware to work with when i was building my personal deep learning box i reviewed all the gpus on the market in this article i m going to share my insights about choosing the right graphics processor also we ll go over deep learning dl is part of the field of machine learning ml dl works by approximating a solution to a problem using neural networks one of the nice properties of about neural networks is that they find patterns in the data features by themselves this is opposed to having to tell your algorithm what to look for as in the olde times however often this means the model starts with a blank state unless we are transfer learning to capture the nature of the data from scratch the neural net needs to process a lot of information there are two ways to do so with a cpu or a gpu the main computational module in a computer is the central processing unit better known as cpu it is designed to do computation rapidly on a small amount of data for example multiplying a few numbers on a cpu is blazingly fast but it struggles when operating on a large amount of data e g multiplying matrices of tens or hundreds thousand numbers behind the scenes dl is mostly comprised of operations like matrix multiplication amusingly d computer games rely on these same operations to render that beautiful landscape you see in rise of the tomb raider thus gpus were developed to handle lots of parallel computations using thousands of cores also they have a large memory bandwidth to deal with the data for these computations this makes them the ideal commodity hardware to do dl on or at least until asics for machine learning like google s tpu make their way to market for me the most important reason for picking a powerful graphics processor is saving time while prototyping models if the networks train faster the feedback time will be shorter thus it would be easier for my brain to connect the dots between the assumptions i had for the model and its results see tim dettmers answer to why are gpus well suited to deep learning on quora for a better explanation also for an in depth albeit slightly outdated gpus comparison see his article which gpu s to get for deep learning there are main characteristics of a gpu related to dl are there are two reasons for having multiple gpus you want to train several models at once or you want to do distributed training of a single model we ll go over each one training several models at once is a great technique to test different prototypes and hyperparameters it also shortens your feedback cycle and lets you try out many things at once distributed training or training a single network on several video cards is slowly but surely gaining traction nowadays there are easy to use approaches to this for tensorflow and keras via horovod cntk and pytorch the distributed training libraries offer almost linear speed ups to the number of cards for example with gpus you get x faster training pcie lanes updated the caveat to using multiple video cards is that you need to be able to feed them with data for this purpose each gpu should have pcie lanes available for data transfer tim dettmers points out that having pcie lanes per card should only decrease performance by for two gpus for a single card any desktop processor and chipset like intel i and asus tuf z will use lanes however for two gpus you can go x x lanes or get a processor and a motherboard that support pcie lanes lanes are outside the realm of desktop cpus an intel xeon with a msi x a sli plus will do the job for or gpus go with x lanes per card with a xeon with to pcie lanes to have pcie lanes available for or gpus you need a monstrous processor something in the class of or amd threadripper lanes with a corresponding motherboard also for more gpus you need a faster processor and hard disk to be able to feed them data quickly enough so they don t sit idle nvidia has been focusing on deep learning for a while now and the head start is paying off their cuda toolkit is deeply entrenched it works with all major dl frameworks tensoflow pytorch caffe cntk etc as of now none of these work out of the box with opencl cuda alternative which runs on amd gpus i hope support for opencl comes soon as there are great inexpensive gpus from amd on the market also some amd cards support half precision computation which doubles their performance and vram size currently if you want to do dl and want to avoid major headaches choose nvidia your gpu needs a computer around it hard disk first you need to read the data off the disk an ssd is recommended here but an hdd can work as well cpu that data might have to be decoded by the cpu e g jpegs fortunately any mid range modern processor will do just fine motherboard the data passes via the motherboard to reach the gpu for a single video card almost any chipset will work if you are planning on working with multiple graphic cards read this section ram it is recommended to have gigabytes of memory for every gigabyte of video card ram having more certainly helps in some situations like when you want to keep an entire dataset in memory power supply it should provide enough power for the cpu and the gpus plus watts extra you can get all of this for to or even less if you buy a used workstation here is performance comparison between all cards check the individual card profiles below notably the performance of titan xp and gtx ti is very close despite the huge price gap between them the price comparison reveals that gtx ti gtx and gtx have great value for the compute performance they provide all the cards are in the same league value wise except titan xp the king of the hill when every gb of vram matters this card has more than any other on the consumer market it s only a recommended buy if you know why you want it for the price of titan x you could get two gtx s which is a lot of power and gbs of vram this card is what i currently use it s a great high end option with lots of ram and high throughput very good value i recommend this gpu if you can afford it it works great for computer vision or kaggle competitions quite capable mid to high end card the price was reduced from to when ti was introduced gb is enough for most computer vision tasks people regularly compete on kaggle with these the newest card in nvidia s lineup if is over budget this will get you the same amount of vram gb also of the performance for of the price pretty sweet deal it s hard to get these nowadays because they are used for cryptocurrency mining with a considerable amount of vram for this price but somewhat slower if you can get it or a couple second hand at a good price go for it it s quite cheap but gb vram is limiting that s probably the minimum you want to have if you are doing computer vision it will be okay for nlp and categorical data models also available as p for cryptocurrency mining but it s the same card without a display output the entry level card which will get you started but not much more still if you are unsure about getting in deep learning this might be a cheap way to get your feet wet titan x pascal it used to be the best consumer gpu nvidia had to offer made obsolete by ti which has the same specs and is cheaper tesla gpusthis includes k k which is x k in one p and others you might already be using these via amazon web services google cloud platform or another cloud provider in my previous article i did some benchmarks on gtx ti vs k the performed five times faster than the tesla card and x faster than k k has gb vram and k a whopping gbs in theory the p and gtx ti should be in the same league performance wise however this cryptocurrency comparison has p lagging in every benchmark it is worth noting that you can do half precision on p effectively doubling the performance and vram size on top of all this k goes for over k for over and p is about and they get still get eaten alive by a desktop grade card obviously as it stands i don t recommend getting them all the specs in the world won t help you if you don t know what you are looking for here are my gpu recommendations depending on your budget i have over get as many gtx ti or gtx as you can if you have or gpus running in the same box beware of issues with feeding them with data also keep in mind the airflow in the case and the space on the motherboard i have to gtx ti is highly recommended if you want to go multi gpu get x gtx if you can find them or x gtx ti kaggle here i come i have to get the gtx or gtx ti maybe x gtx if you really want gpus however know that gb per model can be limiting i have to gtx will get you started unless you can find a used gtx i have less than get gtx ti or save for gtx if you are serious about deep learning deep learning has the great promise of transforming many areas of our life unfortunately learning to wield this powerful tool requires good hardware hopefully i ve given you some clarity on where to start in this quest disclosure the above are affiliate links to help me pay for well more gpus from a quick cheer to a standing ovation clap to show how much you enjoyed this story entrepreneur hacker machine learning deep learning and other types of learning
gk_,1800,6,https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6?source=tag_archive---------5----------------,Text Classification using Neural Networks – Machine Learnings,understanding how chatbots work is important a fundamental piece of machinery inside a chat bot is the text classifier let s look at the inner workings of an artificial neural network ann for text classification we ll use layers of neurons hidden layer and a bag of words approach to organizing our training data text classification comes in flavors pattern matching algorithms neural nets while the algorithmic approach using multinomial naive bayes is surprisingly effective it suffers from fundamental flaws as with its naive counterpart this classifier isn t attempting to understand the meaning of a sentence it s trying to classify it in fact so called ai chat bots do not understand language but that s another story let s examine our text classifier one section at a time we will take the following steps the code is here we re using ipython notebook which is a super productive way of working on data science projects the code syntax is python we begin by importing our natural language toolkit we need a way to reliably tokenize sentences into words and a way to stem words and our training data sentences belonging to classes intents we can now organize our data structures for documents classes and words notice that each word is stemmed and lower cased stemming helps the machine equate words like have and having we don t care about case our training data is transformed into bag of words for each sentence the above step is a classic in text classification each training sentence is reduced to an array of s and s against the array of unique words in the corpus is stemmed then transformed to input a for each word in the bag the is ignored and output the first class note that a sentence could be given multiple classes or none make sure the above makes sense and play with the code until you grok it next we have our core functions for our layer neural network if you are new to artificial neural networks here is how they work we use numpy because we want our matrix multiplication to be fast we use a sigmoid function to normalize values and its derivative to measure the error rate iterating and adjusting until our error rate is acceptably low also below we implement our bag of words function transforming an input sentence into an array of s and s this matches precisely with our transform for training data always crucial to get this right and now we code our neural network training function to create synaptic weights don t get too excited this is mostly matrix multiplication from middle school math class we are now ready to build our neural network model we will save this as a json structure to represent our synaptic weights you should experiment with different alpha gradient descent parameter and see how it affects the error rate this parameter helps our error adjustment find the lowest error rate synapse alpha synapse weight update we use neurons in our hidden layer you can adjust this easily these parameters will vary depending on the dimensions and shape of your training data tune them down to as a reasonable error rate the synapse json file contains all of our synaptic weights this is our model this classify function is all that s needed for the classification once synapse weights have been calculated lines of code the catch if there s a change to the training data our model will need to be re calculated for a very large dataset this could take a non insignificant amount of time we can now generate the probability of a sentence belonging to one or more of our classes this is super fast because it s dot product calculation in our previously defined think function experiment with other sentences and different probabilities you can then add training data and improve expand the model notice the solid predictions with scant training data some sentences will produce multiple predictions above a threshold you will need to establish the right threshold level for your application not all text classification scenarios are the same some predictive situations require more confidence than others the last classification shows some internal details notice the bag of words bow for the sentence words matched our corpus the neural net also learns from the s the non matching words a low probability classification is easily shown by providing a sentence where a common word is the only match for example here you have a fundamental piece of machinery for building a chat bot capable of handling a large of classes intents and suitable for classes with limited or extensive training data patterns adding one or more responses to an intent is trivial from a quick cheer to a standing ovation clap to show how much you enjoyed this story philosopher entrepreneur investor understand how machine learning and artificial intelligence will change your work life
nafrondel,1700,5,https://medium.com/@nafrondel/you-requested-someone-with-a-degree-in-this-holds-up-hand-d4bf18e96ff?source=tag_archive---------6----------------,You requested someone with a degree in this? *Holds up hand*,you requested someone with a degree in this holds up hand so there are two main schools of artificial intelligence symbolic and non symbolic symbolic says the best way to make ai is to make an expert ai e g if you want a doctor ai you feed it medical text books and it answers questions by looking it up in the text book non symbolic says the best way to make ai is to decide that computers are better at understanding in computer so give the information to the ai and let it turn that in to something it understands as a bit of an apt aside consider the chinese room thought experiment imagine you put someone in a room with shelves full of books the books are filled with symbols and look up tables and the person inside is told you will be given a sheet of paper with symbols on use the books in the room to look up the symbols to write in reply then a person outside the room posts messages in to the room in mandarin and gets messages back in mandarin the person inside the room doesn t understand mandarin the knowledge is all in the books but to the person outside the room it looks like they understand mandarin that is how symbolic ai works it has no inate knowledge of the subject mater it just follows instructions even if some if those instructions are to update the books non symbolic ai says that it d be better if the ai wrote the books itself so looking back at the chinese room this is like teaching the person in the room mandarin and the books are their study notes the trouble is teaching someone mandarin takes time and effort as we re starting with a blank slate here but consider that it takes decades to teach a child their first language yet it takes only a little more effort to teach them a second language so back to the ai once we teach it one language we want it to be like the child we want it to be easy for it to learn a second language this is where artificial neural networks come in these are our blank slate children they re made up of three parts inputs neurones outputs the neurones are where the magic happens they re modelled on brains they re a blob of neurones that can connect up to one another or cut links so they can join one bit of the brain up to another and let a signal go from one place to another this is what joins the input up to the output and in the pavlovian way when something good happens the brain remembers by strengthening the link between neurones but just like a baby these start out pretty much random so all you get out is baby babble but we don t want baby babble we have to teach it how to get from dog to chien not dog to goobababaa when teaching the ann you give it an input and if the output is wrong give it a tap on the nose and the neurones remember whatever we just did was wrong don t do it again by decreasing the value it has on the links between the neurones that led to the wrong answer and of it gets it right give it a rub on the head and it does the opposite it increases the numbers meaning it ll be more likely to take that path next time this means that over time it ll join up the input dog to the output chien so how does this explain the article well anns work in both directions we can give it outputs and it ll give us back inputs by following the path of neurones back in the opposite direction so by teaching it dog means chien it also knows chien could mean dog that also means we can teach it that perro means dog when we re speaking spanish so when we teach it the fastest way for it to go from perro to dog is to follow the same path that took chien to dog meaning over time it will pull the neurones linking chien and dog closer to perro as well which links perro to chien as well this three way link in the middle of perro dog and chien is the language the google ai is creating for itself backing up a bit to our imaginary child learning a new language when they learn their first language e g english they don t write an english dictionary in their head they hear the words and map them to an idea that the words represent this is why people frequently misquote films they remember what the quote meant not what the words were so when the child learns a second language they hear chien as being french but map it to the idea of dog then when they hear perro they hear it as spanish but map that to the idea of dog too this means the child only has to learn about the idea of a dog once but can then link that idea up to many languages or synonyms for dog and this is what the google ai is doing instead of thinking if dog chien and chien perro perro must dog it thinks dog x b chien x b perro x b where x b is the idea of dog meaning it can then turn x b in to whichever language you ask for tl dr it wasn t big news because artificial neural networks have been doing this since they were invented in the s and the entire non symbolic branch of ai is all about having computers invent their own language to understand and learn things p s it really is smart enough to warrant that excitement most people have no idea how much they rely on ai from the relatively simple ai that runs their washing machine to the ai that reads the address hand written on mail and then figures out the best way to deliver it these are real everyday machines making decisions for us even your computer mouse has ai in it to determine what you wanted to point at rather than what you actually pointed at on a p screen there are million points you could click on it s not by accident that it s pretty easy to pick the correct one mobile phones constantly run ai to decide which phone tower to connect to while the backbone of the internet is a huge interconnected ai deciding the fastest way to get data from one computer to another thinking decision making ai is in our hands beneath our feet in our cars and almost every electronic device we have the robots have already taken over from a quick cheer to a standing ovation clap to show how much you enjoyed this story
Neelabh Pant,2000,11,https://blog.statsbot.co/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f?source=tag_archive---------7----------------,A Guide For Time Series Prediction Using Recurrent Neural Networks (LSTMs),the statsbot team has already published the article about using time series analysis for anomaly detection today we d like to discuss time series prediction with a long short term memory model lstms we asked a data scientist neelabh pant to tell you about his experience of forecasting exchange rates using recurrent neural networks as an indian guy living in the us i have a constant flow of money from home to me and vice versa if the usd is stronger in the market then the indian rupee inr goes down hence a person from india buys a dollar for more rupees if the dollar is weaker you spend less rupees to buy the same dollar if one can predict how much a dollar will cost tomorrow then this can guide one s decision making and can be very important in minimizing risks and maximizing returns looking at the strengths of a neural network especially a recurrent neural network i came up with the idea of predicting the exchange rate between the usd and the inr there are a lot of methods of forecasting exchange rates such as in this article we ll tell you how to predict the future exchange rate behavior using time series analysis and by making use of machine learning with time series let us begin by talking about sequence problems the simplest machine learning problem involving a sequence is a one to one problem in this case we have one data input or tensor to the model and the model generates a prediction with the given input linear regression classification and even image classification with convolutional network fall into this category we can extend this formulation to allow for the model to make use of the pass values of the input and the output it is known as the one to many problem the one to many problem starts like the one to one problem where we have an input to the model and the model generates one output however the output of the model is now fed back to the model as a new input the model now can generate a new output and we can continue like this indefinitely you can now see why these are known as recurrent neural networks a recurrent neural network deals with sequence problems because their connections form a directed cycle in other words they can retain state from one iteration to the next by using their own output as input for the next step in programming terms this is like running a fixed program with certain inputs and some internal variables the simplest recurrent neural network can be viewed as a fully connected neural network if we unroll the time axes in this univariate case only two weights are involved the weight multiplying the current input xt which is u and the weight multiplying the previous output yt which is w this formula is like the exponential weighted moving average ewma by making its pass values of the output with the current values of the input one can build a deep recurrent neural network by simply stacking units to one another a simple recurrent neural network works well only for a short term memory we will see that it suffers from a fundamental problem if we have a longer time dependency as we have talked about a simple recurrent network suffers from a fundamental problem of not being able to capture long term dependencies in a sequence this is a problem because we want our rnns to analyze text and answer questions which involves keeping track of long sequences of words in late s lstm was proposed by sepp hochreiter and jurgen schmidhuber which is relatively insensitive to gap length over alternatives rnns hidden markov models and other sequence learning methods in numerous applications this model is organized in cells which include several operations lstm has an internal state variable which is passed from one cell to another and modified by operation gates forget gate it is a sigmoid layer that takes the output at t and the current input at time t and concatenates them into a single tensor and applies a linear transformation followed by a sigmoid because of the sigmoid the output of this gate is between and this number is multiplied with the internal state and that is why the gate is called a forget gate if ft then the previous internal state is completely forgotten while if ft it will be passed through unaltered input gate the input gate takes the previous output and the new input and passes them through another sigmoid layer this gate returns a value between and the value of the input gate is multiplied with the output of the candidate layer this layer applies a hyperbolic tangent to the mix of input and previous output returning a candidate vector to be added to the internal state the internal state is updated with this rule the previous state is multiplied by the forget gate and then added to the fraction of the new candidate allowed by the output gate output gate this gate controls how much of the internal state is passed to the output and it works in a similar way to the other gates these three gates described above have independent weights and biases hence the network will learn how much of the past output to keep how much of the current input to keep and how much of the internal state to send out to the output in a recurrent neural network you not only give the network the data but also the state of the network one moment before for example if i say hey something crazy happened to me when i was driving there is a part of your brain that is flipping a switch that s saying oh this is a story neelabh is telling me it is a story where the main character is neelabh and something happened on the road now you carry a little part of that one sentence i just told you as you listen to all my other sentences you have to keep a bit of information from all past sentences around in order to understand the entire story another example is video processing where you would again need a recurrent neural network what happens in the current frame is heavily dependent upon what was in the last frame of the movie most of the time over a period of time a recurrent neural network tries to learn what to keep and how much to keep from the past and how much information to keep from the present state which makes it so powerful as compared to a simple feed forward neural network i was impressed with the strengths of a recurrent neural network and decided to use them to predict the exchange rate between the usd and the inr the dataset used in this project is the exchange rate data between january and august later i ll give you a link to download this dataset and experiment with it the dataset displays the value of in rupees we have a total of records starting from january to august over the period the price to buy in rupees has been rising one can see that there was a huge dip in the american economy during which was hugely caused by the great recession during that period it was a period of general economic decline observed in world markets during the late s and early s this period was not very good for the world s developed economies particularly in north america and europe including russia which fell into a definitive recession many of the newer developed economies suffered far less impact particularly china and india whose economies grew substantially during this period now to train the machine we need to divide the dataset into test and training sets it is very important when you do time series to split train and test with respect to a certain date so you don t want your test data to come before your training data in our experiment we will define a date say january as our split date the training data is the data between january and december which are about training data points the test dataset is between january and august which are about points the next thing to do is normalize the dataset you only need to fit and transform your training data and just transform your test data the reason you do that is you don t want to assume that you know the scale of your test data normalizing or transforming the data means that the new scale variables will be between zero and one a fully connected model is a simple neural network model which is built as a simple regression model that will take one input and will spit out one output this basically takes the price from the previous day and forecasts the price of the next day as a loss function we use mean squared error and stochastic gradient descent as an optimizer which after enough numbers of epochs will try to look for a good local optimum below is the summary of the fully connected layer after training this model for epochs or early callbacks whichever came first the model tries to learn the pattern and the behavior of the data since we split the data into training and testing sets we can now predict the value of testing data and compare them with the ground truth as you can see the model is not good it essentially is repeating the previous values and there is a slight shift the fully connected model is not able to predict the future from the single previous value let us now try using a recurrent neural network and see how well it does the recurrent model we have used is a one layer sequential model we used lstm nodes in the layer to which we gave input of shape which is one input given to the network with one value the last layer is a dense layer where the loss is mean squared error with stochastic gradient descent as an optimizer we train this model for epochs with early stopping callback the summary of the model is shown above this model has learned to reproduce the yearly shape of the data and doesn t have the lag it used to have with a simple feed forward neural network it is still underestimating some observations by certain amounts and there is definitely room for improvement in this model there can be a lot of changes to be made in this model to make it better one can always try to change the configuration by changing the optimizer another important change i see is by using the sliding time window method which comes from the field of stream data management system this approach comes from the idea that only the most recent data are important one can show the model data from a year and try to make a prediction for the first day of the next year sliding time window methods are very useful in terms of fetching important patterns in the dataset that are highly dependent on the past bulk of observations try to make changes to this model as you like and see how the model reacts to those changes i made the dataset available on my github account under deep learning in python repository feel free to download the dataset and play with it i personally follow some of my favorite data scientists like kirill eremenko jose portilla dan van boxel better known as dan does data and many more most of them are available on different podcast stations where they talk about different current subjects like rnn convolutional neural networks lstm and even the most recent technology neural turing machine try to keep up with the news of different artificial intelligence conferences by the way if you are interested then kirill eremenko is coming to san diego this november with his amazing team to give talks on machine learning neural networks and data science lstm models are powerful enough to learn the most important past behaviors and understand whether or not those past behaviors are important features in making future predictions there are several applications where lstms are highly used applications like speech recognition music composition handwriting recognition and even in my current research of human mobility and travel predictions according to me lstm is like a model which has its own memory and which can behave like an intelligent human in making decisions thank you again and happy machine learning from a quick cheer to a standing ovation clap to show how much you enjoyed this story i love data science let s build some intelligent bots together data stories on machine learning and analytics from statsbot s makers
Eugenio Culurciello,2200,15,https://towardsdatascience.com/neural-network-architectures-156e5bad51ba?source=tag_archive---------8----------------,Neural Network Architectures – Towards Data Science,deep neural networks and deep learning are powerful and popular algorithms and a lot of their success lays in the careful design of the neural network architecture i wanted to revisit the history of neural network design in the last few years and in the context of deep learning for a more in depth analysis and comparison of all the networks reported here please see our recent article one representative figure from this article is here reporting top one crop accuracy versus amount of operations required for a single forward pass in multiple popular neural network architectures it is the year and this is one of the very first convolutional neural networks and what propelled the field of deep learning this pioneering work by yann lecun was named lenet after many previous successful iterations since the year the lenet architecture was fundamental in particular the insight that image features are distributed across the entire image and convolutions with learnable parameters are an effective way to extract similar features at multiple location with few parameters at the time there was no gpu to help training and even cpus were slow therefore being able to save parameters and computation was a key advantage this is in contrast to using each pixel as a separate input of a large multi layer neural network lenet explained that those should not be used in the first layer because images are highly spatially correlated and using individual pixel of the image as separate input features would not take advantage of these correlations lenet features can be summarized as in overall this network was the origin of much of the recent architectures and a true inspiration for many people in the field in the years from to neural network were in incubation most people did not notice their increasing power while many other researchers slowly progressed more and more data was available because of the rise of cell phone cameras and cheap digital cameras and computing power was on the rise cpus were becoming faster and gpus became a general purpose computing tool both of these trends made neural network progress albeit at a slow rate both data and computing power made the tasks that neural networks tackled more and more interesting and then it became clear in dan claudiu ciresan and jurgen schmidhuber published one of the very fist implementations of gpu neural nets this implementation had both forward and backward implemented on a a nvidia gtx graphic processor of an up to layers neural network in alex krizhevsky released alexnet which was a deeper and much wider version of the lenet and won by a large margin the difficult imagenet competition alexnet scaled the insights of lenet into a much larger neural network that could be used to learn much more complex objects and object hierarchies the contribution of this work were at the time gpu offered a much larger number of cores than cpus and allowed x faster training time which in turn allowed to use larger datasets and also bigger images the success of alexnet started a small revolution convolutional neural network were now the workhorse of deep learning which became the new name for large neural networks that can now solve useful tasks in december the nyu lab from yann lecun came up with overfeat which is a derivative of alexnet the article also proposed learning bounding boxes which later gave rise to many other papers on the same topic i believe it is better to learn to segment objects rather than learn artificial bounding boxes the vgg networks from oxford were the first to use much smaller filters in each convolutional layers and also combined them as a sequence of convolutions this seems to be contrary to the principles of lenet where large convolutions were used to capture similar features in an image instead of the or filters of alexnet filters started to become smaller too dangerously close to the infamous convolutions that lenet wanted to avoid at least on the first layers of the network but the great advantage of vgg was the insight that multiple convolution in sequence can emulate the effect of larger receptive fields for examples and these ideas will be also used in more recent network architectures as inception and resnet the vgg networks uses multiple x convolutional layers to represent complex features notice blocks of vgg e and filters are used multiple times in sequence to extract more complex features and the combination of such features this is effectively like having large classifiers with layers which are convolutional this obviously amounts to a massive number of parameters and also learning power but training of these network was difficult and had to be split into smaller networks with layers added one by one all this because of the lack of strong ways to regularize the model or to somehow restrict the massive search space promoted by the large amount of parameters vgg used large feature sizes in many layers and thus inference was quite costly at run time reducing the number of features as done in inception bottlenecks will save some of the computational cost network in network nin had the great and simple insight of using x convolutions to provide more combinational power to the features of a convolutional layers the nin architecture used spatial mlp layers after each convolution in order to better combine features before another layer again one can think the x convolutions are against the original principles of lenet but really they instead help to combine convolutional features in a better way which is not possible by simply stacking more convolutional layers this is different from using raw pixels as input to the next layer here convolution are used to spatially combine features across features maps after convolution so they effectively use very few parameters shared across all pixels of these features the power of mlp can greatly increase the effectiveness of individual convolutional features by combining them into more complex groups this idea will be later used in most recent architectures as resnet and inception and derivatives nin also used an average pooling layer as part of the last classifier another practice that will become common this was done to average the response of the network to multiple are of the input image before classification christian szegedy from google begun a quest aimed at reducing the computational burden of deep neural networks and devised the googlenet the first inception architecture by now fall deep learning models were becoming extermely useful in categorizing the content of images and video frames most skeptics had given in that deep learning and neural nets came back to stay this time given the usefulness of these techniques the internet giants like google were very interested in efficient and large deployments of architectures on their server farms christian thought a lot about ways to reduce the computational burden of deep neural nets while obtaining state of art performance on imagenet for example or be able to keep the computational cost the same while offering improved performance he and his team came up with the inception module which at a first glance is basically the parallel combination of and convolutional filters but the great insight of the inception module was the use of convolutional blocks nin to reduce the number of features before the expensive parallel blocks this is commonly referred as bottleneck this deserves its own section to explain see bottleneck layer section below googlenet used a stem without inception modules as initial layers and an average pooling plus softmax classifier similar to nin this classifier is also extremely low number of operations compared to the ones of alexnet and vgg this also contributed to a very efficient network design inspired by nin the bottleneck layer of inception was reducing the number of features and thus operations at each layer so the inference time could be kept low before passing data to the expensive convolution modules the number of features was reduce by say times this led to large savings in computational cost and the success of this architecture let s examine this in detail let s say you have features coming in and coming out and let s say the inception layer only performs x convolutions that is x x x convolutions that have to be performed s multiply accumulate or mac operations that may be more than the computational budget we have say to run this layer in milli seconds on a google server instead of doing this we decide to reduce the number of features that will have to be convolved say to or in this case we first perform convolutions then convolution on all inception branches and then we use again a x convolution from features back again the operations are now for a total of about versus the almost we had before almost x less operations and although we are doing less operations we are not losing generality in this layer in fact the bottleneck layers have been proven to perform at state of art on the imagenet dataset for example and will be also used in later architectures such as resnet the reason for the success is that the input features are correlated and thus redundancy can be removed by combining them appropriately with the x convolutions then after convolution with a smaller number of features they can be expanded again into meaningful combination for the next layer christian and his team are very efficient researchers in february batch normalized inception was introduced as inception v batch normalization computes the mean and standard deviation of all feature maps at the output of a layer and normalizes their responses with these values this corresponds to whitening the data and thus making all the neural maps have responses in the same range and with zero mean this helps training as the next layer does not have to learn offsets in the input data and can focus on how to best combine features in december they released a new version of the inception modules and the corresponding architecture this article better explains the original googlenet architecture giving a lot more detail on the design choices a list of the original ideas are inception still uses a pooling layer plus softmax as final classifier the revolution then came in december at about the same time as inception v resnet have a simple ideas feed the output of two successive convolutional layer and also bypass the input to the next layers this is similar to older ideas like this one but here they bypass two layers and are applied to large scales bypassing after layers is a key intuition as bypassing a single layer did not give much improvements by layers can be thought as a small classifier or a network in network this is also the very first time that a network of hundred even layers was trained resnet with a large number of layers started to use a bottleneck layer similar to the inception bottleneck this layer reduces the number of features at each layer by first using a x convolution with a smaller output usually of the input and then a x layer and then again a x convolution to a larger number of features like in the case of inception modules this allows to keep the computation low while providing rich combination of features see bottleneck layer section after googlenet and inception resnet uses a fairly simple initial layers at the input stem a x conv layer followed with a pool of contrast this to more complex and less intuitive stems as in inception v v resnet also uses a pooling layer plus softmax as final classifier additional insights about the resnet architecture are appearing every day and christian and team are at it again with a new version of inception the inception module after the stem is rather similar to inception v they also combined the inception module with the resnet module this time though the solution is in my opinion less elegant and more complex but also full of less transparent heuristics it is hard to understand the choices and it is also hard for the authors to justify them in this regard the prize for a clean and simple network that can be easily understood and modified now goes to resnet squeezenet has been recently released it is a re hash of many concepts from resnet and inception and show that after all a better design of architecture will deliver small network sizes and parameters without needing complex compression algorithms our team set up to combine all the features of the recent architectures into a very efficient and light weight network that uses very few parameters and computation to achieve state of the art results this network architecture is dubbed enet and was designed by adam paszke we have used it to perform pixel wise labeling and scene parsing here are some videos of enet in action these videos are not part of the training dataset the technical report on enet is available here enet is a encoder plus decoder network the encoder is a regular cnn design for categorization while the decoder is a upsampling network designed to propagate the categories back into the original image size for segmentation this worked used only neural networks and no other algorithm to perform image segmentation as you can see in this figure enet has the highest accuracy per parameter used of any neural network out there enet was designed to use the minimum number of resources possible from the start as such it achieves such a small footprint that both encoder and decoder network together only occupies mb with fp precision even at this small size enet is similar or above other pure neural network solutions in accuracy of segmentation a systematic evaluation of cnn modules has been presented the found out that is advantageous to use use elu non linearity without batchnorm or relu with it apply a learned colorspace transformation of rgb use the linear learning rate decay policy use a sum of the average and max pooling layers use mini batch size around or if this is too big for your gpu decrease the learning rate proportionally to the batch size use fully connected layers as convolutional and average the predictions for the final decision when investing in increasing training set size check if a plateau has not been reach cleanliness of the data is more important then the size if you cannot increase the input image size reduce the stride in the con sequent layers it has roughly the same effect if your network has a complex and highly optimized architecture like e g googlenet be careful with modifications xception improves on the inception module and architecture with a simple and more elegant architecture that is as effective as resnet and inception v the xception module is presented here this network can be anyone s favorite given the simplicity and elegance of the architecture presented here the architecture has convolutional stages making it close in similarity to a resnet but the model and code is as simple as resnet and much more comprehensible than inception v a torch implementation of this network is available here an implementation in keras tf is availble here it is interesting to note that the recent xception architecture was also inspired by our work on separable convolutional filters a new mobilenets architecture is also available since april this architecture uses separable convolutions to reduce the number of parameters the separate convolution is the same as xception above now the claim of the paper is that there is a great reduction in parameters about in case of facenet as reported in the paper here is the complete model architecture unfortunately we have tested this network in actual application and found it to be abysmally slow on a batch of on a titan xp gpu look at a comparison here of inference time per image clearly this is not a contender in fast inference it may reduce the parameters and size of network on disk but is not usable fractalnet uses a recursive architecture that was not tested on imagenet and is a derivative or the more general resnet we believe that crafting neural network architectures is of paramount importance for the progress of the deep learning field our group highly recommends reading carefully and understanding all the papers in this post but one could now wonder why we have to spend so much time in crafting architectures and why instead we do not use data to tell us what to use and how to combine modules this would be nice but now it is work in progress some initial interesting results are here note also that here we mostly talked about architectures for computer vision similarly neural network architectures developed in other areas and it is interesting to study the evolution of architectures for all other tasks also if you are interested in a comparison of neural network architecture and computational performance see our recent paper this post was inspired by discussions with abhishek chaurasia adam paszke sangpil kim alfredo canziani and others in our e lab at purdue university i have almost years of experience in neural networks in both hardware and software a rare combination see about me here medium webpage scholar linkedin and more if you found this article useful please consider a donation to support more tutorials and blogs any contribution can make a difference from a quick cheer to a standing ovation clap to show how much you enjoyed this story i dream and build new technology sharing concepts ideas and codes
Gary Marcus,1300,27,https://medium.com/@GaryMarcus/in-defense-of-skepticism-about-deep-learning-6e8bfd5ae0f1?source=tag_archive---------0----------------,In defense of skepticism about deep learning – Gary Marcus – Medium,in a recent appraisal of deep learning marcus i outlined ten challenges for deep learning and suggested that deep learning by itself although useful was unlikely to lead on its own to artificial general intelligence i suggested instead the deep learning be viewed not as a universal solvent but simply as one tool among many in place of pure deep learning i called for hybrid models that would incorporate not just supervised forms of deep learning but also other techniques as well such as symbol manipulation and unsupervised learning itself possibly reconceptualized i also urged the community to consider incorporating more innate structure into ai systems within a few days thousands of people had weighed in over twitter some enthusiastic e g the best discussion of deeplearning and ai i ve read in many years some not thoughtful but mostly wrong nevertheless because i think clarity around these issues is so important i ve compiled a list of fourteen commonly asked queries where does unsupervised learning fit in why didn t i say more nice things about deep learning what gives me the right to talk about this stuff in the first place what s up with asking a neural network to generalize from even numbers to odd numbers hint that s the most important one and lots more i haven t addressed literally every question i have seen but i have tried to be representative what is general intelligence thomas dietterich an eminent professor of machine learning and my most thorough and explicit critic thus far gave a nice answer that i am very comfortable with marcus wasn t very nice to deep learning he should have said more nice things about all of its vast accomplishments and he minimizes others dietterich mentioned above made both of these points writing on the first part of that true i could have said more positive things but it s not like i didn t say any or even like i forgot to mention dietterich s best example i mentioned it on the first page more generally later in the article i cited a couple of great texts and excellent blogs that have pointers to numerous examples a lot of them though would not really count as agi which was the main focus of my paper google translate for example is extremely impressive but it s not general it can t for example answer questions about what it has translated the way a human translator could the second part is more substantive is categories really very finite well yes compared to the flexibility of cognition cognitive scientists generally place the number of atomic concepts known by an individual as being on the order of and we can easily compose those into a vastly greater number of complex thoughts pets and fish are probably counted in those pet fish which is something different probably isn t counted and i can easily entertain the concept of a pet fish that is suffering from ick or note that it is always disappointing to buy a pet fish only to discover that it was infected with ick an experience that i had as a child and evidently still resent how many ideas like that i can express it s a lot more than i am not precisely sure how many visual categories a person can recognize but suspect the math is roughly similar try google images on pet fish and you do ok try it on pet fish wearing goggles and you mostly find dogs wearing goggles with a false alarm rate of over machines win over nonexpert humans on distinguishing similar dog breeds but people win by a wide margin on interpreting complex scenes like what would happen to a skydiver who was wearing a backpack rather than a parachute in focusing on category chunks the machine learning field is in my view doing itself a disservice trading a short term feeling of success for a denial of harder more open ended problems like scene and sentence comprehension that must eventually be addressed compared to the essentially infinite range of sentences and scenes we can see and comprehend of anything really is small see also note at bottom marcus says deep learning is useless but it s great for many things of course it is useful i never said otherwise only that a in its current supervised form deep learning might be approaching its limits and b that those limits would stop short from full artificial general intelligence unless maybe we started incorporating a bunch of other stuff like symbol manipulation and innateness the core of my conclusion was this one thing that i don t understand garymarcus says that dl is not good for hierarchical structures but in ylecun nature review paper says that that dl is particularly suited for exploiting such hierarchies this is an astute question from ram shankar and i should have been a lot clearer about the answer there are many different types of hierarchy one could think about deep learning is really good probably the best ever at the sort of feature wise hierarchy lecun talked about which i typically refer to as hierarchical feature detection you build lines out of pixels letters out of lines words out of letters and so forth kurzweil and hawkins have emphasized this sort of thing too and it really goes back to hubel and wiesel in neuroscience experiments and to fukushima fukushima miyake ito in ai fukushima in his neocognitron model hand wired his hierarchy of successively more abstract features lecun and many others after showed that at least in some cases you don t have to hand engineer them but you don t have to keep track of the subcomponents you encounter along the way the top level system need not explicitly encode the structure of the overall output in terms of which parts were seen along the way this is part of why a deep learning system can be fooled into thinking a pattern of a black and yellow stripes is a school bus nguyen yosinski clune that stripe pattern is strongly correlated with activation of the school bus output units which is in turn correlated with a bunch of lower level features but in a typical image recognition deep network there is no fully realized representation of a school bus as being made up of wheels a chassis windows etc virtually the whole spoofing literature can be thought of in these terms note the structural sense of hierarchy which i was discussing was different and focused around systems that can make explicit reference to the parts of larger wholes the classic illustration would be chomsky s sense of hierarchy in which a sentence is composed of increasingly complex grammatical units e g using a novel phrase like the man who mistook his hamburger for a hot dog with a larger sentence like the actress insisted that she would not be outdone by the man who mistook his hamburger for a hot dog i don t think deep learning does well here e g in discerning the relation between the actress the man and the misidentified hot dog though attempts have certainly been made even in vision the problem is not entirely licked hinton s recent capsule work sabour frosst hinton for example is an attempt to build in more robust part whole directions for image recognition by using more structured networks i see this as a good trend and one potential way to begin to address the spoofing problem but also as a reflection of trouble with the standard deep learning approach it s weird to discuss deep learning in the context of general ai general ai is not the goal of deep learning best twitter response to this came from university of quebec professor daniel lemire oh come on hinton bengio are openly going for a model of human intelligence second prize goes to a math phd at google jeremy kun who countered the dubious claim that general ai is not the goal of deep learning with if that s true then deep learning experts sure let everyone believe it is without correcting them andrew ng s recent harvard business review article which i cited implies that deep learning can do anything a person can do in a second thomas dietterich s tweet that said in part it is hard to argue that there are limits to dl jeremy howard worried that the idea that deep learning is overhyped might itself be overhyped and then suggested that every known limit had been countered deepmind s recent alphago paper see note is positioned somewhat similarly with silver et al silver et al enthusiastically reporting that in that paper s concluding discussion not one of the challenges to deep learning that i reviewed was mentioned as i will discuss in a paper coming out soon it s not actually a pure deep learning system but that s a story for another day the main reason people keep benchmarking their ai systems against humans is precisely because agi is the goal what marcus said is a problem with supervised learning not deep learning yann lecun presented a version of this in a comment on my facebook page the part about my allegedly not recognizing lecun s recent work is well odd it s true that i couldn t find a good summary article to cite when i asked lecun he told me by email that there wasn t one yet but i did mention his interest explicitly i also noted that my conclusion was positive too although i expressed reservations about current approaches to building unsupervised systems i ended optimistically what lecun s remark does get right is that many of the problems i addressed are a general problem with supervised learning not something unique to deep learning i could have been more clear about this many other supervised learning techniques face similar challenges such as problems in generalization and dependence on massive data sets relatively little of what i said is unique to deep learning in my focus on assessing deep learning at the five year resurgence mark i neglected to say that but it doesn t really help deep learning that other supervised learning techniques are in the same boat if someone could come up with a truly impressive way of using deep learning in an unsupervised way a reassessment might be required but i don t see that unsupervised learning at least as it currently pursued particularly remedies the challenges i raised e g with respect to reasoning hierarchical representations transfer robustness and interpretability it s simply a promissory note note as portland state and santa fe institute professor melanie mitchell s put it in a thus far unanswered tweet i would too in the meantime i see no principled reason to believe that unsupervised learning can solve the problems i raise unless we add in more abstract symbolic representations first deep learning is not just convolutional networks of the sort marcus critiqued it s essentially a new style of programming differentiable programming and the field is trying to work out the reusable constructs in this style we have some convolution pooling lstm gan vae memory units routing units etc tom dietterich this seemed in the context of dietterich s longer series of tweets to have been proposed as a criticism but i am puzzled by that as i am a fan of differentiable programming and said so perhaps the point was that deep learning can be taken in a broader way in any event i would not equate deep learning and differentiable programming e g approaches that i cited like neural turing machines and neural programming deep learning is a component of many differentiable systems but such systems also build in exactly the sort of elements drawn from symbol manipulation that i am and have been urging the field to integrate marcus marcus marblestone dean a marcus marblestone dean b including memory units and operations over variables and other systems like routing units stressed in the more recent two essays if integrating all this stuff into deep learning is what gets us to agi my conclusion quoted below will have turned out to be dead on now vs the future maybe deep learning doesn t work now but it s offspring will get us to agi possibly i do think that deep learning might play an important role in getting us to agi if some key things many not yet discovered are added in first but what we add matters and whether it is reasonable to call some future system an instance of deep learning per se or more sensible to call the ultimate system a such and such that uses deep learning depends on where deep learning fits into the ultimate solution maybe for example in truly adequate natural language understanding systems symbol manipulation will play an equally large role as deep learning or an even larger one part of the issue here is of course terminological a very good friend recently asked me why can t we just call anything that includes deep learning deep learning even if it includes symbol manipulation some enhancement to deep learning ought to work to which i respond why not call anything that includes symbol manipulation symbol manipulation even if it includes deep learning gradient based optimization should get its due but so should symbol manipulation which as yet is the only known tool for systematically representing and achieving high level abstraction bedrock to virtually all of the world s complex computer systems from spreadsheets to programming environments to operating systems eventually i conjecture credit will also be due to the inevitable marriage between the two hybrid systems that bring together the two great ideas of th century ai symbol processing and neural networks both initially developed in the s other new tools yet to be invented may be critical as well to a true acolyte of deep learning anything is deep learning no matter what it s incorporating and no matter how different it might be from current techniques viva imperialism if you replaced every transistor in a classic symbolic microprocessor with a neuron but kept the chip s logic entirely unchanged a true deep learning acolyte would still declare victory but we won t understand the principles driving eventual success if we lump everything together note no machine can extrapolate it s not fair to expect a neural network to generalize from even numbers to odd numbers here s a function expressed over binary digits f f f what s f if you are an ordinary human you are probably going to guess if you are neural network of the sort i discussed you probably won t if you have been told many times that hidden layers in neural networks abstract functions you should be a little bit surprised by this if you are a human you might think of the function as something like reversal easily expressed in a line of computer code if you are a neural network of a certain sort it s very hard to learn the abstraction of reversal in a way that extends from evens in that context to odds but is that impossible certainly not if you have a prior notion of an integer try another this time in decimal f f what s f none of my human readers would care that questions happens to require you to extrapolate from even numbers to odds a lot of neural networks would be flummoxed sure the function is undetermined by the sparse number of examples like all functions but it is interesting and important that most people would amid the infinite range of a priori possible inductions would alight on f and just as interesting that most standard multilayer perceptrons representing the numbers as binary digits wouldn t that s telling us something but many people in the neural network community franc ois chollet being one very salient exception don t want to listen importantly recognizing that a rule applies to any integer is roughly the same kind of generalization that allows one to recognize that a novel noun that can be used in one context can be used in a huge variety of other contexts from the first time i hear the word blicket used as an object i can guess that it will fit into a wide range of frames like i thought i saw a blicket i had a close encounter with a blicket and exceptionally large blickets frighten me etc and i can both generate and interpret such sentences without specific further training it doesn t matter whether blicket is or not similar in for example phonology to other words i have heard nor whether i pile on the adjectives or use the word as a subject or an object if most machine learning ml paradigms have a problem with this we should have problem with most ml paradigms am i being fair well yes and no it s true that i am asking neural networks to do something that violates their assumptions a neural network advocate might for example say hey wait a minute in your reversal example there are three dimensions in your input space representing the left binary digit the middle binary digit and rightmost binary digit the rightmost binary digit has only been a zero in the training there is no way a network can know what to do when you get to one in that position for example vincent lostenlan a postdoc at cornell said dietterich made essentially the same point more concisely but although both are right about why odds and evens are in this context hard for deep learning they are both wrong about the larger issues for three reasons first it can t be that people can t extrapolate you just did in two different examples at the top of this section paraphrasing chico marx who are you going to believe me or your own eyes to someone immersed deeply perhaps too deeply in contemporary machine learning my odds and evens problem seems unfair because a certain dimension the one which contains the value of in the rightmost digit hasn t been illustrated in the training regime but when you a human look at my examples above you will not be stymied by this particular gap in the training data you won t even notice it because your attention is on higher level regularities people routinely extrapolate in exactly the fashion that i have been describing like recognizing string reversal from the three training examples i gave above in a technical sense that is extrapolation and you just did it in the algebraic mind i referred to this specific kind of extrapolation as generalizing universally quantified one to one mappings outside of a space of training examples as a field we desperately need a solution to this challenge if we are ever to catch up to human learning even if it means shaking up our assumptions now it might reasonably be objected that it s not a fair fight humans manifestly depend on prior knowledge when they generalize such mappings in some sense dieterrich proposed this objection later in his tweet stream true enough but in a way that s the point neural networks of a certain sort don t have a good way of incorporating the right sort of prior knowledge in the place it is precisely because those networks don t have a way of incorporating prior knowledge like many generalizations hold for all elements of unbounded classes or odd numbers leave a remainder of one when divided by two that neural networks that lack operations over variables fail the right sort of prior knowledge that would allow neural networks to acquire and represent universally quantified one to one mappings standard neural networks can t represent such mappings except in certain limited ways convolution is a way of building in one particular such mapping prior to learning second saying that no current system deep learning or otherwise can extrapolate in the way that i have described is no excuse once again other architectures may be in the choppy water but that doesn t mean we shouldn t be trying to swim to shore if we want to get to agi we have to solve the problem put differently yes one could certainly hack together solutions to get deep learning to solve my specific number series problems by for example playing games with the input encoding schemes the real question if we want to get to agi is how to have a system learn the sort of generalizations i am describing in a general way third the claim that no current system can extrapolate turns out to be well false there are already ml systems that can extrapolate at least some functions of exactly the sort i described and you probably own one microsoft excel its flash fill function in particular gulwani powered by a very different approach to machine learning it can do certain kinds of extrapolation albeit in a narrow context by the bushel e g try typing the decimal digits in a series of rows and see if the system can extrapolate via flash fill to the eleventh item in the sequence spoiler alert it can in exactly the same way as you probably would even though there were no positive examples in the training dimension of the hundreds digit the systems learns from examples the function you want and extrapolates it piece of cake can any deep learning system do that with three training examples even with a range of experience on other small counting functions like and well maybe but only the ones that are likely do so are likely to be hybrids that build in operations over variables which are quite different from the sort of typical convolutional neural networks that most people associate with deep learning putting all this very differently one crude way to think about where we are with most ml systems that we have today note is that they just aren t designed to think outside the box they are designed to be awesome interpolators inside the box that s fine for some purposes but not others humans are better at thinking outside boxes than contemporary ai i don t think anyone can seriously doubt that but that kind of extrapolation that microsoft can do in a narrow context but that no machine can do with human like breadth is precisely what machine learning engineers really ought to be working on if they want to get to agi everybody in the field already knew this there is nothing new here well certainly not everybody as noted there were many critics who think we still don t know the limits of deep learning and others who believe that there might be some but none yet discovered that said i never said that any of my points was entirely new for virtually all i cited other scholars who had independently reached similar conclusions marcus failed to cite x definitely true the literature review was incomplete one favorite among the papers i failed to cite is shanahan s deep symbolic reinforcement garnelo arulkumaran shanahan i also can t believe i forgot richardson and domingos markov logic networks i also wish i had cited evans and edward grefenstette a great paper from deepmind and smolensky s tensor calculus work smolensky et al and work on inductive programming in various forms gulwani et al and probabilistic programming too by noah goodman goodman mansinghka roy bonawitz tenenbaum all seek to bring rules and networks close to together and older stuff by pioneers like jordan pollack smolensky et al and forbus and gentner s falkenhainer forbus gentner and hofstadter and mitchell s work on analogy and many others i am sure there is a lot more i could and should have cited overall i tried to be representative rather than fully comprehensive but i still could have done better chagrin marcus has no standing in the field he isn t a practitioner he is just a critic hesitant to raise this one but it came up in all kinds of different responses even from the mouths of certain well known professionals as ram shankar noted as a community we must circumscribe our criticism to science and merit based arguments what really matters is not my credentials which i believe do in fact qualify me to write but the validity of the arguments either my arguments are correct or they are not still for those who are curious i supply an optional mini history of some of my relevant credentials in note at the end re hierarchy what about socher s tree rnns i have written to him in hopes of having a better understanding of its current status i ve also privately pushed several other teams towards trying out tasks like lake and baroni presented pengfei et al offers some interesting discussion you could have been more critical of deep learning nobody quite said that not in exactly those words but a few came close generally privately one colleague for example pointed out that there may be some serious errors of future forecasting around the same colleague added another colleague ml researcher and author pedro domingos pointed out still other shortcomings of current deep learning methods that i didn t mention like other flexible supervised learning methods deep learning systems can be unstable in the sense that slightly changing the training data may result in large changes in the resulting model as domingos notes there s no guarantee this sort of rise and decline won t repeat itself neural networks have risen and fallen several times before all the way back to rosenblatt s first perceptron in we shouldn t mistake cyclical enthusiasm for a complete solution to intelligence which still seems to me anyway to be decades away if we want to reach agi we owe it to ourselves to be as keenly aware of challenges we face as we are of our successes there are other problems too in relying on these image sets for example in reading a draft of this paper melanie mitchell pointed me to important recent work by loghmani and colleague on assessing how deep learning does in the real world quoting from the abstract the paper analyzes the transferability of deep representations from web images to robotic data in the wild despite the promising results obtained with representations developed from web image the experiments demonstrate that object classification with real life robotic data is far from being solved and that literature is growing fast in late december there was a paper about fooling deep nets into mistaking a pair of skiers for a dog https arxiv org pdf pdf and another on a general purpose tool for building real world adversarial patches https arxiv org pdf pdf see also https arxiv org abs it s frightening to think how vulnerable deep learning can be real world contexts and for that matter consider filip pieknewski s blog on why photo trained deep learning systems have trouble transferring what they have learned to line drawings https blog piekniewski info can a deep net see a cat vision is not as solved as many people seem to think as i will explain in the forthcoming paper alphago is not actually a pure deep reinforcement learning system although the quoted passage presented it as such it s really more of a hybrid with important components that are driven by symbol manipulating algorithms along with a well engineered deep learning component alphazero by the way isn t unsupervised it s self supervised using self play and simulation as a way of generating supervised data i will have a lot more to say about that system in a forthcoming paper consider for example google search and how one might understand it google has recently added in a deep learning algorithm rankbrain to the wide array of algorithms it uses for search and google search certainly takes in data and knowledge and processes them hierarchically which according to maher ibrahim is all you need to count as being deep learning but realistically deep learning is just one cue among many the knowledge graph component for example is based instead primarily on classical ai notions of traversing ontologies by any reasonable measure google search is a hybrid with deep learning as just one strand among many calling google search as a whole a deep learning system would be grossly misleading akin to relabeling carpentry screwdrivery just because screwdrivers happen to be involved important exceptions include inductive logic programming inductive function programming the brains behind microsoft s flash fill and neural programming all are making some progress here some of these even include deep learning but they also all include structured representations and operations over variables among their primitive operations that s all i am asking for my ai experiments begin in adolescence with among other thing a latin english translator that i coded in the programming language logo in graduate school studying with steven pinker i explored the relation between language acquisition symbolic rules and neural networks i also owe a debt to my undergraduate mentor neil stillings the child language data i gathered marcus et al for my dissertation have been cited hundreds of times and were the most frequently modeled data in the s debate about neural networks and how children learned language in the late s i discovered some specific replicable problems with multilayer perceptrons marcus b marcus a based on those observation i designed a widely cited experiment published in science marcus vijayan bandi rao vishton that showed that young infants could extract algebraic rules contra jeff elman s then popular neural network all of this culminated in a mit press book marcus which lobbied for a variety of representational primitives some of which have begun to pop up in recent neural networks in particular that the use of operations over variables in the new field of differentiable programming daniluk rockta schel welbl riedel graves et al owes something to the position outlined in that book there was a strong emphasis on having memory records as well which can be seen in the memory networks being developed e g at facebook bordes usunier chopra weston the next decade saw me work on other problems including innateness marcus which i will discuss at length in the forthcoming piece about alphago and evolution marcus marcus i eventually returned to ai and cognitive modeling publishing a article on cortical computation in science marcus marblestone dean that also anticipates some of what is now happening in differentiable programming more recently i took a leave from academia to found and lead a machine learning company in by any reasonable measure that company was successful acquired by uber roughly two years after founding as co founder and ceo i put together a team of some of the very best machine learning talent in the world including zoubin ghahramani jeff clune noah goodman ken stanley and jason yosinski and played a pivotal role in developing our core intellectual property and shaping our intellectual mission a patent is pending co written by zoubin ghahramani and myself although much of what we did there remains confidential now owned by uber and not by me i can say that a large part of our efforts were addressed towards integrating deep learning with our own techniques which gave me a great deal of familiarity with joys and tribulations of tensorflow and vanishing and exploding gradients we aimed for state of the art results sometimes successfully sometimes not with sparse data using hybridized deep learning systems on a daily basis bordes a usunier n chopra s weston j large scale simple question answering with memory networks arxiv daniluk m rockta schel t welbl j riedel s frustratingly short attention spans in neural language modeling arxiv elman j l finding structure in time cognitive science evans r grefenstette e learning explanatory rules from noisy data arxiv cs ne falkenhainer b forbus k d gentner d the structure mapping engine algorithm and examples artificial intelligence fukushima k miyake s ito t neocognitron a neural network model for a mechanism of visual pattern recognition ieee transactions on systems man and cybernetics garnelo m arulkumaran k shanahan m towards deep symbolic reinforcement learning arxiv cs ai goodman n mansinghka v roy d m bonawitz k tenenbaum j b church a language for generative models arxiv preprint arxiv graves a wayne g reynolds m harley t danihelka i grabska barwin ska a et al hybrid computing using a neural network with dynamic external memory nature gulwani s automating string processing in spreadsheets using input output examples dl acm org gulwani s herna ndez orallo j kitzelmann e muggleton s h schmid u zorn b inductive programming meets the real world communications of the acm hofstadter d r mitchell m the copycat project a model of mental fluidity and analogy making advances in connectionist and neural computation theory hosseini h xiao b jaiswal m poovendran r on the limitation of convolutional neural networks in recognizing negative images arxiv cs cv hubel d h wiesel t n receptive fields of single neurones in the cat s striate cortex the journal of physiology lake b m baroni m still not systematic after all these years on the compositional skills of sequence to sequence recurrent networks arxiv loghmani m r caputo b vincze m recognizing objects in the wild where do we stand arxiv cs ro marcus g f a rethinking eliminative connectionism cogn psychol marcus g f b can connectionism save constructivism cognition marcus g f the algebraic mind integrating connectionism and cognitive science cambridge mass mit press marcus g f the birth of the mind how a tiny number of genes creates the complexities of human thought basic books marcus g f kluge the haphazard construction of the human mind boston houghton mifflin marcus g deep learning a critical appraisal arxiv marcus g f marblestone a dean t a the atoms of neural computation science marcus g f marblestone a h dean t l b frequently asked questions for the atoms of neural computation biorxiv arxiv q bio nc marcus g f the algebraic mind integrating connectionism and cognitive science cambridge mass mit press marcus g f pinker s ullman m hollander m rosen t j xu f overregularization in language acquisition monogr soc res child dev marcus g f vijayan s bandi rao s vishton p m rule learning by seven month old infants science nguyen a yosinski j clune j deep neural networks are easily fooled high confidence predictions for unrecognizable images arxiv cs cv pengfei l xipeng q xuanjing h dynamic compositional neural networks over tree structure ijcai proceedings from proceedings of the twenty sixth international joint conference on artificial intelligence ijcai ribeiro m t singh s guestrin c why should i trust you explaining the predictions of any classifier arxiv cs lg richardson m domingos p markov logic networks machine learning sabour s dffsdfdsf n hinton g e dynamic routing between capsules arxiv cs cv silver d schrittwieser j simonyan k antonoglou i huang a guez a et al mastering the game of go without human knowledge nature smolensky p lee m he x yih w t gao j deng l basic reasoning with tensor product representations arxiv cs ai from a quick cheer to a standing ovation clap to show how much you enjoyed this story ceo founder geometric intelligence acquired by uber professor of psychology and neural science nyu freelancer for the new yorker new york times
Sarthak Jain,3900,10,https://medium.com/nanonets/how-to-easily-detect-objects-with-deep-learning-on-raspberrypi-225f29635c74?source=tag_archive---------1----------------,How to easily Detect Objects with Deep Learning on Raspberry Pi,disclaimer i m building nanonets com to help build ml with less data and no hardware the raspberry pi is a neat piece of hardware that has captured the hearts of a generation with m devices sold with hackers building even cooler projects on it given the popularity of deep learning and the raspberry pi camera we thought it would be nice if we could detect any object using deep learning on the pi now you will be able to detect a photobomber in your selfie someone entering harambe s cage where someone kept the sriracha or an amazon delivery guy entering your house m years of evolution have made human vision fairly evolved the human brain has of it s neurons work on processing vision as compared with percent for touch and just percent for hearing humans have two major advantages when compared with machines one is stereoscopic vision the second is an almost infinite supply of training data an infant of years has had approximately b images sampled at fps to mimic human level performance scientists broke down the visual perception task into four different categories object detection has been good enough for a variety of applications even though image segmentation is a much more precise result it suffers from the complexity of creating training data it typically takes a human annotator x more time to segment an image than draw bounding boxes this is more anecdotal and lacks a source also after detecting objects it is separately possible to segment the object from the bounding box object detection is of significant practical importance and has been used across a variety of industries some of the examples are mentioned below object detection can be used to answer a variety of questions these are the broad categories there are a variety of models architectures that are used for object detection each with trade offs between speed size and accuracy we picked one of the most popular ones yolo you only look once and have shown how it works below in under lines of code if you ignore the comments note this is pseudo code not intended to be a working example it has a black box which is the cnn part of it which is fairly standard and shown in the image below you can read the full paper here https pjreddie com media files papers yolo pdf for this task you probably need a few images per object try to capture data as close to the data you re going to finally make predictions on draw bounding boxes on the images you can use a tool like labelimg you will typically need a few people who will be working on annotating your images this is a fairly intensive and time consuming task you can read more about this at medium com nanonets nanonets how to use deep learning when you have limited data f c b cab you need a pretrained model so you can reduce the amount of data required to train without it you might need a few k images to train the model you can find a bunch of pretrained models here the process of training a model is unnecessarily difficult to simplify the process we created a docker image would make it easy to train to start training the model you can run the docker image has a run sh script that can be called with the following parameters you can find more details at to train a model you need to select the right hyper parameters finding the right parameters the art of deep learning involves a little bit of hit and try to figure out which are the best parameters to get the highest accuracy for your model there is some level of black magic associated with this along with a little bit of theory this is a great resource for finding the right parameters quantize model make it smaller to fit on a small device like the raspberry pi or mobile small devices like mobile phones and rasberry pi have very little memory and computation power training neural networks is done by applying many tiny nudges to the weights and these small increments typically need floating point precision to work though there are research efforts to use quantized representations here too taking a pre trained model and running inference is very different one of the magical qualities of deep neural networks is that they tend to cope very well with high levels of noise in their inputs why quantize neural network models can take up a lot of space on disk with the original alexnet being over mb in float format for example almost all of that size is taken up with the weights for the neural connections since there are often many millions of these in a single model the nodes and weights of a neural network are originally stored as bit floating point numbers the simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer and then compressing each float value to an eight bit integer the size of the files is reduced by code for quantization you need the raspberry pi camera live and working then capture a new image for instructions on how to install checkout this link download model once your done training the model you can download it on to your pi to export the model run then download the model onto the raspberry pi install tensorflow on the raspberry pi depending on your device you might need to change the installation a little run model for predicting on the new image the raspberry pi has constraints on both memory and compute a version of tensorflow compatible with the raspberry pi gpu is still not available therefore it is important to benchmark how much time do each of the models take to make a prediction on a new image we have removed the need to annotate images we have expert annotators who will annotate your images for you we automatically train the best model for you to achieve this we run a battery of model with different parameters to select the best for your data nanonets is entirely in the cloud and runs without using any of your hardware which makes it much easier to use since devices like the raspberry pi and mobile phones were not built to run complex compute heavy tasks you can outsource the workload to our cloud which does all of the compute for you get your free api key from http app nanonets com user api key collect the images of object you want to detect you can annotate them either using our web ui https app nanonets com objectannotation appid your model id or use open source tool like labelimg once you have dataset ready in folders images image files and annotations annotations for the image files start uploading the dataset once the images have been uploaded begin training the model the model takes hours to train you will get an email once the model is trained in the meanwhile you check the state of the model once the model is trained you can make predictions using the model from a quick cheer to a standing ovation clap to show how much you enjoyed this story founder ceo nanonets com nanonets machine learning api
Favio Vázquez,3300,14,https://towardsdatascience.com/a-weird-introduction-to-deep-learning-7828803693b0?source=tag_archive---------2----------------,A “weird” introduction to Deep Learning – Towards Data Science,there are amazing introductions courses and blog posts on deep learning i will name some of them in the resources sections but this is a different kind of introduction but why weird maybe because it won t follow the normal structure of a deep learning post where you start with the math then go into the papers the implementation and then to applications it will be more close to the post i did before about my journey into deep learning i think telling a story can be much more helpful than just throwing information and formulas everywhere so let s begin note there s a companion webinar to this article find it here sometimes is important to have a written backup of your thoughts i tend to talk a lot and be present in several presentations and conference and this is my way of contributing with a little knowledge to everyone deep learning dl is such an important field for data science ai technology and our lives right now and it deserves all of the attention is getting please don t say that deep learning is just adding a layer to a neural net and that s it magic nope i m hoping that after reading this you have a different perspective of what dl is i just created this timeline based on several papers and other timelines with the purpose of everyone seeing that deep learning is much more than just neural networks there has been really theoretical advances software and hardware improvements that were necessary for us to get to this day if you want it just ping me and i ll send it to you find my contact in the end of the article deep learning has been around for quite a while now so why it became so relevant so fast the last years as i said before until the late s we were still missing a reliable way to train very deep neural networks nowadays with the development of several simple but important theoretical and algorithmic improvements the advances in hardware mostly gpus now tpus and the exponential generation and accumulation of data dl came naturally to fit this missing spot to transform the way we do machine learning deep learning is an active field of research too nothing is settle or closed we are still searching for the best models topology of the networks best ways to optimize their hyperparameters and more is very hard as any other active field on science to keep up to date with the investigation but it s not impossible a side note on topology and machine learning deep learning with topological signatures by hofer et al luckily for us there are lots of people helping understand and digest all of this information through courses like the andrew ng one blog posts and much more this for me is weird or uncommon because normally you have to wait for sometime sometime years to be able to digest difficult and advance information in papers or research journals of course most areas of science are now really fast too to get from a paper to a blog post that tells you what yo need to know but in my opinion dl has a different feel we are working with something that is very exciting most people in the field are saying that the last ideas in the papers of deep learning specifically new topologies and configurations for nn or algorithms to improve their usage are the best ideas in machine learning in decades remember that dl is inside of ml i ve used the word learning a lot in this article so far but what is learning in the context of machine learning the word learning describes an automatic search process for better representations of the data you are analyzing and studying please have this in mind is not making a computer learn this is a very important word for this field rep re sen ta tion don t forget about it what is a representation it s a way to look at data let me give you an example let s say i tell you i want you to drive a line that separates the blue circles from the green triangles for this plot so if you want to use a line this is what the author says this is impossible if we remember the concept of a line so is the case lost actually no if we find a way of representing this data in a different way in a way we can draw a straight line to separate the types of data this is somethinkg that math taught us hundreds of years ago in this case what we need is a coordinate transformation so we can plot or represent this data in a way we can draw this line if we look the polar coordinate transformation we have the solution and that s it now we can draw a line so in this simple example we found and chose the transformation to get a better representation by hand but if we create a system a program that can search for different representations in this case a coordinate change and then find a way of calculating the percentage of categories being classified correctly with this new approach in that moment we are doing machine learning this is something very important to have in mind deep learning is representation learning using different kinds of neural networks and optimize the hyperparameters of the net to get learn the best representation for our data this wouldn t be possible without the amazing breakthroughs that led us to the current state of deep learning here i name some of them learning representations by back propagating errors by david e rumelhart geoffrey e hinton ronald j williams a theoretical framework for back propagation by yann lecun idea better initialization of the parameters of the nets something to remember the initialization strategy should be selected according to the activation function used next idea better activation functions this mean better ways of approximating the functions faster leading to faster training process idea dropout better ways of preventing overfitting and more dropout a simple way to prevent neural networks from overfitting a great paper by srivastava hinton and others idea convolutional neural nets cnns gradient based learning applied to document recognition by lecun and others imagenet classification with deep convolutional neural networks by krizhevsky and others idea residual nets resnets idea region based cnns used for object detection and more idea recurrent neural networks rnns and lstms btw it was shown by liao and poggio that resnets rnns arxiv v idea generative adversarial networks gans idea capsule networks and there are many others but i think those are really important theoretical and algorithmic breakthroughs that are changing the world and that gave momentum for the dl revolution it s not easy to get started but i ll try my best to guide you through this process check out this resources but remember this is not only watching videos and reading papers it s about understanding programming coding failing and then making it happen learn python and r andrew ng and coursera you know he doesn t need an intro siraj raval he s amazing he has the power to explain hard concepts in a fun and easy way follow him on his youtube channel specifically this playlists the math of intelligence intro to deep learning franc ois chollet s book deep learning with python and r ibm cognitive class datacamp deep learning is one of the most important tools and theories a data scientist should learn we are so lucky to see amazing people creating both research software tools and hardware specific for dl tasks dl is computationally expensive and even though there s been advances in theory software and hardware we need the developments in big data and distributed machine learning to improve performance and efficiency great people and companies are making amazing efforts to join the distributed frameworks spark and dl libraries tf and keras here s an overview elephas distributed dl with keras pyspark yahoo inc tensorflowonspark cern distributed keras keras spark qubole tutorial keras spark intel corporation bigdl distributed deep learning library for apache spark tensorflow and spark on google cloud as i ve said before one of the most important moments for this field was the creation and open sourced of tensorflow tensorflow is an open source software library for numerical computation using data flow graphs nodes in the graph represent mathematical operations while the graph edges represent the multidimensional data arrays tensors communicated between them the things you are seeing in the image above are tensor manipulations working with the riemann tensor in general relativity tensors defined mathematically are simply arrays of numbers or functions that transform according to certain rules under a change of coordinates but in the scope of machine learning and deep learning a tensor is a generalization of vectors and matrices to potentially higher dimensions internally tensorflow represents tensors as n dimensional arrays of base datatypes we use heavily tensors all the time in dl but you don t need to be an expert in them to use it you may need to understand a little bit about them so here i list some good resources after you check that out the breakthroughs i mentioned before and the programming frameworks like tensorflow or keras for more on keras go here now i think you have an idea of what you need to understand and work with deep learning but what have we achieved so far with dl to name a few from franc ois chollet book on dl and much more here s a list of great and funny applications of dl thinking about the future of deep learning for programming or building applications i ll repeat what i said in other posts i really think guis and automl are the near future of getting things done with deep learning don t get me wrong i love coding but i think the amount of code we will be writing next years will decay we cannot spend so many hours worldwide programming the same stuff over and over again so i think these two features guis and automl will help data scientist on getting more productive and solving more problems on of the best free platforms for doing these tasks in a simple gui is deep cognition their simple drag drop interface helps you design deep learning models with ease deep learning studio can automatically design a deep learning model for your custom dataset thanks to their advance automl feature with nearly one click here you can learn more about them take a look at the prices o it s freeeee i mean it s amazing how fast the development in the area is right now that we can have simple guis to interact with all the hard and interesting concepts i talked about in this post one of the things i like about that platform is that you can still code interact with tensorflow keras caffe mxnet an much more with the command line or their notebook without installing anything you have both the notebook and the cli i take my hat off to them and their contribution to society other interesting applications of deep learning that you can try for free or for little cost are some of them are on private betas thanks for reading this weird introduction to deep learning i hope it helped you getting started in this amazing area or maybe just discover something new if you have questions just add me on linkedin and we ll chat there from a quick cheer to a standing ovation clap to show how much you enjoyed this story data scientist physicist and computer engineer love sharing ideas thoughts and contributing to open source in machine learning and deep learning sharing concepts ideas and codes
Oleksandr Savsunenko,5500,4,https://hackernoon.com/the-new-neural-internet-is-coming-dda85b876adf?source=tag_archive---------3----------------,The New Neural Internet is Coming – Hacker Noon,how it all began the landscape think of the typical and well studied neural networks such as image classifier as a left hemisphere of the neural network technology with this in mind it is easy to understand what is generative adversarial network it is a kind of right hemisphere the one that is claimed to be responsible for creativity the generative adversarial networks gans are the first step of neural networks technology learning creativity typical gan is a neural network trained to generate images on the certain topic using an image dataset and some random noise as a seed up until now images created by gans were of low quality and limited in resolution recent advances by nvidia showed that it is within a reach to generate photorealistic images in high resolution and they published the technology itself in open access there is a plethora of gans types of various complexity architectures and strange acronyms we are mostly interested here in conditional gans and variational autoencoders conditional gans are capable of not just mimicking the broad type of images as bedroom face dog but also dive into more specific categories for example the text image network is capable of translation textual image description into the image itself by varying random seed that is concatenated to the meanings vector we are able to produce an infinite number of birds image matching description let s just close your eyes and see the world in years companies like nvidia will push gan technology to industry ready level same as they did with celebrities faces generation this means that a gan will be able to generate any image on demand on the fly based on textual for example description this will render obsolete a number of photography and design related industries here s how this will work again the network is able to generate an infinite number of images by varying random seed and here s the scary part such a network can receive not only description of the target object it needs to generate but also a vector describing you the ad consumer this ad can have a very deep description of your personality web browsing history recent transactions and geolocation so the gan will generate one time unique and that fits you perfectly ctr is going sky high by measuring your reactions the network will adapt and make ads targeting you more and more precisely hitting your soft spots so at the end of the day we are going to see a fully personalized content everywhere on the internet everyone will see fully custom versions of all content that is adapted to the consumer based on his lifestyle opinions and history we all witnessed arousal of this bubble pattern after latest usa elections and it s gonna be getting worse gans will able to target content precisely to you with no limitations of the medium starting from image ads and up to complex opinions tread and publications generated by machines this will create a constant feedback loop improving based on your interactions and there is going to be a competition of different gans between each other kind of a fully automated war of phycological manipulations having humanity as a battlefield the driving force behind this trend is extremely simple profits and this is not a scary doomsday scenario this actually is happening today i have no idea but surely we need few things broad public discussions about this technology inevitable arrival and a backup plan to stop it so it s better to start thinking now how we can fight this process and benefit from it at the same time we are not there yet due to some technical limitation up until recently images generated by gans were just of bad quality and easily spotted as fake nvidia showed that it is actually doable to generate x extremely real faces to move things forward we would need faster and bigger gpus more theoretical studies on gan more smart hacks around gan training more labeled datasets etc please notice we don t need new power sources quantum processors but they can help general ai to reach this point or some other purely theoretical new cool things all we need is within a reach of few years and likely big corp already have this kind of resources available also we will need smarter neural networks i am definitely looking for progress in capsules approach by hinton et al and of course we will be the first to implement this in super resolution technology that should heavily benefit from gan progress let me know what you think from a quick cheer to a standing ovation clap to show how much you enjoyed this story machine learning engineer doer maker dreamer father how hackers start their afternoons
Max Pechyonkin,3400,8,https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a?source=tag_archive---------4----------------,Stochastic Weight Averaging — a New Way to Get State of the Art Results in Deep Learning,in this article i will discuss two interesting recent papers that provide an easy way to improve performance of any given neural network by using a smart way to ensemble they are additional prerequisite reading that will make context of this post much more easy to understand traditional ensembling combines several different models and makes them predict on the same input then some way of averaging is used to determine the final prediction of the ensemble it can be simple voting an average or even another model that learns to predict correct value or label based on the inputs of models in the ensemble ridge regression is one particular way of combining several predictions which is used by kaggle winning machine learning practitioners when applied in deep learning ensembling can be used to combine predictions of several neural networks to produce one final prediction usually it is a good idea to use neural networks of different architectures in an ensemble because they will likely make mistakes on different training samples and therefore the benefit of ensembling will be larger however you can also ensemble models with the same architecture and it will give surprisingly good results one very cool trick exploiting this approach was proposed in the snapshot ensembling paper the authors take weights snapshot while training the same network and then after training create an ensemble of nets with the same architecture but different weights this allows to improve test performance and it is a very cheap way too because you just train one model once just saving weights from time to time you can refer to this awesome post for more details if you aren t yet using cyclical learning rates then you definitely should as it becomes the standard state of the art training technique that is very simple not computationally heavy and provides significant gains at almost no additional cost all of the examples above are ensembles in the model space because they combine several models and then use models predictions to produce the final prediction in the paper that i am discussing in this post however the authors propose to use a novel ensembling in the weights space this method produces an ensemble by combining weights of the same network at different stages of training and then uses this model with combined weights to make predictions there are benefits from this approach let s see how it works but first we need to understand some important facts about loss surfaces and generalizable solutions the first important insight is that a trained network is a point in multidimensional weight space for a given architecture each distinct combination of network weights produces a separate model since there are infinitely many combinations of weights for any given architecture there will be infinitely many solutions the goal of training of a neural network is to find a particular solution point in the weight space that will provide low value of the loss function both on training and testing data sets during training by changing weights training algorithm changes the network and travel in the weight space gradient descent algorithm travels on a loss plane in this space where plane elevation is given by the value of the loss function it is very hard to visualize and understand the geometry of multidimensional weight space at the same time it is very important to understand it because stochastic gradient descent essentially traverses a loss surface in this highly multidimensional space during training and tries to find a good solution a point on the loss surface where loss value is low it is known that such surfaces have many local optima but it turns out that not all of them are equally good one metric that can distinguish a good solution from a bad one is its flatness the idea being that training data set and testing data set will produce similar but not exactly the same loss surfaces you can imagine that a test surface will be shifted a bit relative to the train surface for a narrow solution during test time a point that gave low loss can have a large loss because of this shift this means that this narrow solution did not generalize well training loss is low while testing loss is large on the other hand for a wide and flat solution this shift will lead to training and testing loss being close to each other i explained the difference between narrow and wide solutions because the new method which is the focus of this post leads to nice and wide solutions initially sgd will make a big jump in the weight space then as the learning rate gets smaller due to cosine annealing sgd will converge to some local solution and the algorithm will take a snapshot of the model by adding it to the ensemble then the rate is reset to high value again and sgd takes a large jump again before converging to some different local solution cycle length in the snapshot ensembling approach is to epochs the idea of long learning rate cycles is to be able to find sufficiently different models in the weight space if the models are too similar then predictions of the separate networks in the ensemble will be too close and the benefit of ensembling will be negligible snapshot ensembling works really well and improves model performance but fast geometric ensembling works even better fast geometric ensembling is very similar to snapshot ensembling but is has some distinguishing features it uses linear piecewise cyclical learning rate schedule instead of cosine secondly the cycle length in fge is much shorter only to epochs per cycle at first intuition the short cycle is wrong because the models at the end of each cycle will be close to each other and therefore ensembling them will not give any benefits however as the authors discovered because there exist connected paths of low loss between sufficiently different models it is possible to travel along those paths in small steps and the models encountered along will be different enough to allow ensembling them with good results thus fge shows improvement compared to snapshot ensembles and it takes smaller steps to find the model which makes it faster to train to benefit from both snapshot ensembling or fge one needs to store multiple models and then make predictions for all of them before averaging for the final prediction thus for additional performance of the ensemble one needs to pay with higher amount of computation so there is no free lunch there or is there this is where the new paper with stochastic weight averaging comes in stochastic weight averaging closely approximates fast geometric ensembling but at a fraction of computational loss swa can be applied to any architecture and data set and shows good result in all of them the paper suggests that swa leads to wider minima the benefits of which i discussed above swa is not an ensemble in its classical understanding at the end of training you get one model but it s performance beats snapshot ensembles and approaches fge intuition for swa comes from empirical observation that local minima at the end of each learning rate cycle tend to accumulate at the border of areas on loss surface where loss value is low points w w and w are at the border of the red area of low loss in the left panel of figure above by taking the average of several such points it is possible to achieve a wide generalizable solution with even lower loss wswa in the left panel of the figure above here is how it works instead of an ensemble of many models you only need two models at the end of each learning rate cycle the current weights of the second model will be used to update the weight of the running average model by taking weighted mean between the old running average weights and the new set of weights from the second model formula provided in the figure on the left by following this approach you only need to train one model and store only two models in memory during training for prediction you only need the running average model and predicting on it is much faster than using ensemble described above where you use many models to predict and then average results authors of the paper provide their own implementation in pytorch also swa is implemented in the awesome fast ai library that everyone should be using and if you haven t yet seen their course then follow the links you can follow me on twitter let s also connect on linkedin from a quick cheer to a standing ovation clap to show how much you enjoyed this story deep learning sharing concepts ideas and codes
Daniel Simmons,3400,8,https://itnext.io/you-can-build-a-neural-network-in-javascript-even-if-you-dont-really-understand-neural-networks-e63e12713a3?source=tag_archive---------5----------------,You can build a neural network in JavaScript even if you don’t really understand neural networks,click here to share this article on linkedin skip this part if you just want to get on with it i should really start by admitting that i m no expert in neural networks or machine learning to be perfectly honest most of it still completely baffles me but hopefully that s encouraging to any fellow non experts who might be reading this eager to get their feet wet in m l machine learning was one of those things that would come up from time to time and i d think to myself yeah that would be pretty cool but i m not sure that i want to spend the next few months learning linear algebra and calculus like a lot of developers however i m pretty handy with javascript and would occasionally look for examples of machine learning implemented in js only to find heaps of articles and stackoverflow posts about how js is a terrible language for m l which admittedly it is then i d get distracted and move on figuring that they were right and i should just get back to validating form inputs and waiting for css grid to take off but then i found brain js and i was blown away where had this been hiding the documentation was well written and easy to follow and within about minutes of getting started i d set up and trained a neural network in fact if you want to just skip this whole article and just read the readme on github be my guest it s really great that said what follows is not an in depth tutorial about neural networks that delves into hidden input layers activation functions or how to use tensorflow instead this is a dead simple beginner level explanation of how to implement brain js that goes a bit beyond the documentation here s a general outline of what we ll be doing if you d prefer to just download a working version of this project rather than follow along with the article then you can clone the github repository here create a new directory and plop a good ol index html boilerplate file in there then create three js files brain js training data js and scripts js or whatever generic term you use for your default js file and of course import all of these at the bottom of your index html file easy enough so far now go here to get the source code for brain js copy paste the whole thing into your empty brain js file hit save and bam out of files are finished next is the fun part deciding what your machine will learn there are countless practical problems that you can solve with something like this sentiment analysis or image classification for example i happen to think that applications of m l that process text as input are particularly interesting because you can find training data virtually everywhere and they have a huge variety of potential use cases so the example that we ll be using here will be one that deals with classifying text we ll be determining whether a tweet was written by donald trump or kim kardashian ok so this might not be the most useful application but twitter is a treasure trove of machine learning fodder and useless though it may be our tweet author identifier will nevertheless illustrate a pretty powerful point once it s been trained our neural network will be able to look at a tweet that it has never seen before and then be able to determine whether it was written by donald trump or by kim kardashian just by recognizing patterns in the things they write in order to do that we ll need to feed it as much training data as we can bear to copy paste into our training data js file and then we can see if we can identify ourselves some tweet authors now all that s left to do is set up brain js in our scripts js file and feed it some training data in our training data js file but before we do any of that let s start with a foot view of how all of this will work setting up brain js is extremely easy so we won t spend too much time on that but there are a few details about how it s going to expect its input data to be formatted that we should go over first let s start by looking at the setup example that s included in the documentation which i ve slightly modified here that illustrates all this pretty well first of all the example above is actually a working a i it looks at a given color and tells you whether black text or white text would be more legible on it which hopefully illustrates how easy brain js is to use just instantiate it train it and run it that s it i mean if you inlined the training data that would be lines of code pretty cool now let s talk about training data for a minute there are two important things to note in the above example other than the overall input output format of the training data first the data do not need to be all the same length as you can see on line above only an r and a b value get passed whereas the other two inputs pass an r g and b value also even though the example above shows the input as objects it s worth mentioning that you could also use arrays i mention this largely because we ll be passing arrays of varying length in our project second those are not valid rgb values every one of them would come out as black if you were to actually use it that s because input values have to be between and in order for brain js to work with them so in the above example each color had to be processed probably just fed through a function that divides it by the max value for rgb in order to make it work and we ll be doing the same thing so if we want out neural network to accept tweets i e strings as an input we ll need to run them through an similar function called encode below that will turn every character in a string into a value between and and store it in an array fortunately javascript has a native method for converting any character into ascii code called charcodeat so we ll use that and divide the outcome by the max value for extended ascii characters we re using extended ascii just in case we encounter any fringe cases like e or which will ensure that we get a value also we ll be storing our training data as plain text not as the encoded data that we ll ultimately be feeding into our a i you ll thank me for this later so we ll need another function called processtrainingdata below that will apply the previously mentioned encoding function to our training data selectively converting the text into encoded characters and returning an array of training data that will play nicely with brain js so here s what all of that code will look like this goes into your scripts js file something that you ll notice here that wasn t present in the example from the documentation shown earlier other than the two helper functions that we ve already gone over is on line in the train function which saves the trained neural network to a global variable called trainednet this prevents us from having to re train our neural network every time we use it once the network is trained and saved to the variable we can just call it like a function and pass in our encoded input as shown on line in the execute function to use our a i alright so now your index html brain js and scripts js files are finished now all we need is to put something into training data js and we ll be ready to go last but not least our training data like i mentioned we re storing all our tweets as text and encoding them into numeric values on the fly which will make your life a whole lot easier when you actually need to copy paste training data no formatting necessary just paste in the text and add a new row add that to your training data js file and you re done note although the above example only shows samples from each person i used of each i just didn t want this sample to take up too much space of course your neural network s accuracy will increase proportionally to the amount of training data that you give it so feel free to use more or less than me and see how it affects your outcomes now to run your newly trained neural network just throw an extra line at the bottom of your script js file that calls the execute function and passes in a tweet from trump or kardashian make sure to console log it because we haven t built a ui here s a tweet from kim kardashian that was not in my training data i e the network has never encountered this tweet before then pull up your index html page on localhost check the console aaand there it is the network correctly identified a tweet that it had never seen before as originating from kim kardashian with a certainty of now let s try it again with a trump tweet and the result again a never before seen tweet and again correctly identified this time with certainty now you have a neural network that can be trained on any text that you want you could easily adapt this to identify the sentiment of an email or your company s online reviews identify spam classify blog posts determine whether a message is urgent or not or any of a thousand different applications and as useless as our tweet identifier is it still illustrates a really interesting point that a neural network like this can perform tasks as nuanced as identifying someone based on the way they write so even if you don t go out and create an innovative or useful tool that s powered by machine learning this is still a great bit of experience to have in your developer tool belt you never know when it might come in handy or even open up new opportunities down the road once again all of this is available in a github repo here from a quick cheer to a standing ovation clap to show how much you enjoyed this story web developer javascript enthusiast boxing fan itnext is a platform for it developers software engineers to share knowledge connect collaborate learn and experience next gen technologies
Eugenio Culurciello,2800,13,https://towardsdatascience.com/artificial-intelligence-ai-in-2018-and-beyond-e06f05167f9c?source=tag_archive---------6----------------,"Artificial Intelligence, AI in 2018 and beyond – Towards Data Science",these are my opinions on where deep neural network and machine learning is headed in the larger field of artificial intelligence and how we can get more and more sophisticated machines that can help us in our daily routines please note that these are not predictions of forecasts but more a detailed analysis of the trajectory of the fields the trends and the technical needs we have to achieve useful artificial intelligence not all machine learning is targeting artificial intelligences and there are low hanging fruits which we will examine here also the goal of the field is to achieve human and super human abilities in machines that can help us in every day lives autonomous vehicles smart homes artificial assistants security cameras are a first target home cooking and cleaning robots are a second target together with surveillance drones and robots another one is assistants on mobile devices or always on assistants another is full time companion assistants that can hear and see what we experience in our life one ultimate goal is a fully autonomous synthetic entity that can behave at or beyond human level performance in everyday tasks see more about these goals here and here and here software is defined here as neural networks architectures trained with an optimization algorithm to solve a specific task today neural networks are the de facto tool for learning to solve tasks that involve learning supervised to categorize from a large dataset but this is not artificial intelligence which requires acting in the real world often learning without supervision and from experiences never seen before often combining previous knowledge in disparate circumstances to solve the current challenge neural network architectures when the field boomed a few years back we often said it had the advantage to learn the parameters of an algorithms automatically from data and as such was superior to hand crafted features but we conveniently forgot to mention one little detail the neural network architecture that is at the foundation of training to solve a specific task is not learned from data in fact it is still designed by hand hand crafted from experience and it is currently one of the major limitations of the field there is research in this direction here and here for example but much more is needed neural network architectures are the fundamental core of learning algorithms even if our learning algorithms are capable of mastering a new task if the neural network is not correct they will not be able to the problem on learning neural network architecture from data is that it currently takes too long to experiment with multiple architectures on a large dataset one has to try training multiple architectures from scratch and see which one works best well this is exactly the time consuming trial and error procedure we are using today we ought to overcome this limitation and put more brain power on this very important issue unsupervised learning we cannot always be there for our neural networks guiding them at every stop of their lives and every experience we cannot afford to correct them at every instance and provide feedback on their performance we have our lives to live but that is exactly what we do today with supervised neural networks we offer help at every instance to make them perform correctly instead humans learn from just a handful of examples and can self correct and learn more complex data in a continuous fashion we have talked about unsupervised learning extensively here predictive neural networks a major limitation of current neural networks is that they do not possess one of the most important features of human brains their predictive power one major theory about how the human brain work is by constantly making predictions predictive coding if you think about it we experience it every day as you lift an object that you thought was light but turned out heavy it surprises you because as you approached to pick it up you have predicted how it was going to affect you and your body or your environment in overall prediction allows not only to understand the world but also to know when we do not and when we should learn in fact we save information about things we do not know and surprise us so next time they will not and cognitive abilities are clearly linked to our attention mechanism in the brain our innate ability to forego of of our sensory inputs only to focus on the very important data for our survival where is the threat and where do we run to to avoid it or in the modern world where is my cell phone as we walk out the door in a rush building predictive neural networks is at the core of interacting with the real world and acting in a complex environment as such this is the core network for any work in reinforcement learning see more below we have talked extensively about the topic of predictive neural networks and were one of the pioneering groups to study them and create them for more details on predictive neural networks see here and here and here limitations of current neural networks we have talked about before on the limitation of neural networks as they are today cannot predict reason on content and have temporal instabilities we need a new kind of neural networks that you can about read here neural network capsules are one approach to solve the limitation of current neural networks we reviewed them here we argue here that capsules have to be extended with a few additional features continuous learning this is important because neural networks need to continue to learn new data points continuously for their life current neural networks are not able to learn new data without being re trained from scratch at every instance neural networks need to be able to self assess the need of new training and the fact that they do know something this is also needed to perform in real life and for reinforcement learning tasks where we want to teach machines to do new tasks without forgetting older ones for more detail see this excellent blog post by vincenzo lomonaco transfer learning or how do we have these algorithms learn on their own by watching videos just like we do when we want to learn how to cook something new that is an ability that requires all the components we listed above and also is important for reinforcement learning now you can really train your machine to do what you want by just giving an example the same way we humans do every reinforcement learning this is the holy grail of deep neural network research teach machines how to learn to act in an environment the real world this requires self learning continuous learning predictive power and a lot more we do not know there is much work in the field of reinforcement learning but to the author it is really only scratching the surface of the problem still millions of miles away from it we already talked about this here reinforcement learning is often referred as the cherry on the cake meaning that it is just minor training on top of a plastic synthetic brain but how can we get a generic brain that then solve all problems easily it is a chicken in the egg problem today to solve reinforcement learning problems one by one we use standard neural networks both these components are obvious solutions to the problem and currently are clearly wrong but that is what everyone uses because they are some of the available building blocks as such results are unimpressive yes we can learn to play video games from scratch and master fully observable games like chess and go but i do not need to tell you that is nothing compared to solving problems in a complex world imagine an ai that can play horizon zero dawn better than humans i want to see that but this is what we want machine that can operate like us our proposal for reinforcement learning work is detailed here it uses a predictive neural network that can operate continuously and an associative memory to store recent experiences no more recurrent neural networks recurrent neural network rnn have their days counted rnn are particularly bad at parallelizing for training and also slow even on special custom machines due to their very high memory bandwidth usage as such they are memory bandwidth bound rather than computation bound see here for more details attention based neural network are more efficient and faster to train and deploy and they suffer much less from scalability in training and deployment attention in neural network has the potential to really revolutionize a lot of architectures yet it has not been as recognized as it should the combination of associative memories and attention is at the heart of the next wave of neural network advancements attention has already showed to be able to learn sequences as well as rnns and at up to x less computation who can ignore that we recognize that attention based neural network are going to slowly supplant speech recognition based on rnn and also find their ways in reinforcement learning architecture and ai in general localization of information in categorization neural networks we have talked about how we can localize and detect key points in images and video extensively here this is practically a solved problem that will be embedded in future neural network architectures hardware for deep learning is at the core of progress let us now forget that the rapid expansion of deep learning in and in the recent years is mainly due to hardware and we have talked about hardware extensively before but we need to give you a recent update last years saw a boom in the are of machine learning hardware and in particular on the one targeting deep neural networks we have significant experience here and we are fwdnxt the makers of snowflake deep neural network accelerator there are several companies working in this space nvidia obviously intel nervana movidius bitmain cambricon cerebras deephi google graphcore groq huawei arm wave computing all are developing custom high performance micro chips that will be able to train and run deep neural networks the key is to provide the lowest power and the highest measured performance while computing recent useful neural networks operations not raw theoretical operations per seconds as many claim to do but few people in the field understand how hardware can really change machine learning neural networks and ai in general and few understand what is important in micro chips and how to develop them here is our list about neuromorphic neural networks hardware please see here we talked briefly about applications in the goals section above but we really need to go into details here how is ai and neural network going to get into our daily life here is our list i have almost years of experience in neural networks in both hardware and software a rare combination see about me here medium webpage scholar linkedin and more if you found this article useful please consider a donation to support more tutorials and blogs any contribution can make a difference for interesting additional reading please see from a quick cheer to a standing ovation clap to show how much you enjoyed this story i dream and build new technology sharing concepts ideas and codes
Devin Soni,5800,4,https://towardsdatascience.com/spiking-neural-networks-the-next-generation-of-machine-learning-84e167f4eb2b?source=tag_archive---------7----------------,"Spiking Neural Networks, the Next Generation of Machine Learning",everyone who has been remotely tuned in to recent progress in machine learning has heard of the current nd generation artificial neural networks used for machine learning these are generally fully connected take in continuous values and output continuous values although they have allowed us to make breakthrough progress in many fields they are biologically inn accurate and do not actually mimic the actual mechanisms of our brain s neurons the rd generation of neural networks spiking neural networks aims to bridge the gap between neuroscience and machine learning using biologically realistic models of neurons to carry out computation a spiking neural network snn is fundamentally different from the neural networks that the machine learning community knows snns operate using spikes which are discrete events that take place at points in time rather than continuous values the occurrence of a spike is determined by differential equations that represent various biological processes the most important of which is the membrane potential of the neuron essentially once a neuron reaches a certain potential it spikes and the potential of that neuron is reset the most common model for this is the leaky integrate and fire lif model additionally snns are often sparsely connected and take advantage of specialized network topologies at first glance this may seem like a step backwards we have moved from continuous outputs to binary and these spike trains are not very interpretable however spike trains offer us enhanced ability to process spatio temporal data or in other words real world sensory data the spatial aspect refers to the fact that neurons are only connected to neurons local to them so these inherently process chunks of the input separately similar to how a cnn would using a filter the temporal aspect refers to the fact that spike trains occur over time so what we lose in binary encoding we gain in the temporal information of the spikes this allows us to naturally process temporal data without the extra complexity that rnns add it has been proven in fact that spiking neurons are fundamentally more powerful computational units than traditional artificial neurons given that these snns are more powerful in theory than nd generation networks it is natural to wonder why we do not see widespread use of them the main issue that currently lies in practical use of snns is that of training although we have unsupervised biological learning methods such as hebbian learning and stdp there are no known effective supervised training methods for snns that offer higher performance than nd generation networks since spike trains are not differentiable we cannot train snns using gradient descent without losing the precise temporal information in spike trains therefore in order to properly use snns for real world tasks we would need to develop an effective supervised learning method this is a very difficult task as doing so would involve determining how the human brain actually learns given the biological realism in these networks another issue that we are much closer to solving is that simulating snns on normal hardware is very computationally intensive since it requires simulating differential equations however neuromorphic hardware such as ibm s truenorth aims to solve this by simulating neurons using specialized hardware that can take advantage of the discrete and sparse nature of neuronal spiking behavior the future of snns therefore remains unclear on one hand they are the natural successor of our current neural networks but on the other they are quite far from being practical tools for most tasks there are some current real world applications of snns in real time image and audio processing but the literature on practical applications remains sparse most papers on snns are either theoretical or show performance under that of a simple fully connected nd generation network however there are many teams working on developing snn supervised learning rules and i remain optimistic for the future of snns make sure you give this post claps and my blog a follow if you enjoyed this post and want to see more from a quick cheer to a standing ovation clap to show how much you enjoyed this story crypto markets data science twitter devin soni website https github io sharing concepts ideas and codes
Carlos E. Perez,3900,7,https://medium.com/intuitionmachine/neurons-are-more-complex-than-what-we-have-imagined-b3dd00a1dcd3?source=tag_archive---------8----------------,Surprise! Neurons are Now More Complex than We Thought!!,one of the biggest misconceptions around is the idea that deep learning dl or artificial neural networks ann mimics biological neurons at best ann mimics a cartoonish version of a model of a neuron anyone claiming deep learning is biologically inspired is doing so for marketing purposes or has never bother to read biological literature neurons in deep learning are essentially mathematical functions that perform a similarity function of its inputs against internal weights the closer a match is made the more likely an action is performed i e not sending a signal to zero there are exceptions to this model see autoregressive networks however it is general enough to include the perceptron convolution networks and rnns neurons are very different from dl constructs they don t maintain continuous signals but rather exhibit spiking or event driven behavior so when you hear about neuromorphic hardware then these are inspired on integrate and spike neurons these kinds of system at best get a lot of press see ibm truenorth but have never been shown to be effective however there has been some research work that has shown some progress see https arxiv org abs v if you ask me if you truly want to build biologically inspired cognition then you should at the very least explore systems are not continuous like dl biological systems by nature will use the least amount of energy to survive dl systems in stark contrast are power hungry that s because dl is a brute force method to achieve cognition we know it works we just don t know how to scale it down jeff hawkins of numenta has always lamented that a more biologically inspired approach is needed so in his research in building cognitive machinery he has architected systems that try to more closely mirror the structure of the neo cortex numenta s model of a neuron is considerably more elaborate than the deep learning model of a neuron as you can see in this graphic the team at numenta is betting on this approach in the hopes of creating something that is more capable than deep learning it hasn t been proved to be anywhere near successful they ve been doing this long enough that the odds of them succeeding are diminishing overtime bycontrast deep learning despite its model of a cartoon neuron has been shown to be unexpectedly effective in performing all kinds of mind boggling feats of cognition deep learning is doing something that is extraordinarily correct we just don t know exactly what that is unfortunately we have to throw in a new monkey wrench on all these research new experiments on the nature of neurons have revealed that biological neurons are even more complex than we have imagined them to be in short there is a lot more going on inside a single neuron than the simple idea of integrate and fire neurons may not be pure functions dependent on a single parameter i e weight but rather they are stateful machines alternatively perhaps the weight may not even be single valued and instead requires complex valued or maybe higher dimensions this is all behavior that research has yet to explore and thus we have little understanding to date if you think this throws a monkey wrench on our understanding there s an even newer discovery that reveals even greater complexity what this research reveals is that there is a mechanism for neurons to communicate with each other by sending packages of rna code to clarify these are packages of instructions and not packages of data there is a profound difference between sending codes and sending data this implies that behavior from one neuron can change the behavior of another neuron not through observation but rather through injection of behavior this code exchange mechanism hints at the validity of my earlier conjecture are biological brains made of only discrete logic experimental evidence reveals a new reality even at the smallest unit of our cognition there is a kind of conversational cognition that is going on between individual neurons that modifies each other s behavior thus not only are neurons machines with state they are also machines with an instruction set and a way to send code to each other i m sorry but this is just another level of complexity there are two obvious ramifications of these experimental discoveries the first is that our estimates of the computational capabilities of the human brain are likely to be at least an order of magnitude off the second is that research will begin in earnest to explore dl architectures with more complex internal node or neuron structures if we were to make the rough argument that a single neuron performs a single operation the total capacity of the human brain is measured at peta operations per second if were then to assume a dl model of operations being equal to floating point operations then a petaflops system would be equivalent in capability the top ranked supercomputer sunway taihulight from china is estimated at petaflops however let s say the new results reveal x more computation then the number should be petaflops and we perhaps have breathing room until what is obvious however is that biological brains actually perform much more cognition with less computation the second consequences it that it s now time to get back to the drawing board and begin to explore more complex kinds of neurons the more complex kinds we ve seen to date are the ones derived from lstm here is the result of a brute force architectural search for lstm like neurons it s not clear why these more complex lstm are more effective only the architectural search algorithm knows but it can t explain itself there is newly released paper that explores more complex hand engineered lstms that reveals measurable improvements over standard lstms in summary a research plan that explores more complex kinds of neurons may bear promising fruit this is not unlike the research that explores the use of complex values in neural networks in these complex valued networks performance improvements are noticed only on rnn networks this should indicate that these internal neuron complexities may be necessary for capabilities beyond simple perception i suspect that these complexities are necessary for advanced cognition that seems to evade current deep learning systems these include robustness to adversarial features learning to forget learning what to ignore learning abstraction and recognizing contextual switching i predict in the near future that we shall see more aggressive research in this area after all nature is already unequivocally telling us that neurons are individually more complex and therefore our own neuron models may also need to be more complex perhaps we need something as complicated as a grassmann algebra to make progress from a quick cheer to a standing ovation clap to show how much you enjoyed this story author of artificial intuition and the deep learning playbook intuition machine inc deep learning patterns methodology and strategy
Nityesh Agarwal,2400,13,https://towardsdatascience.com/wth-does-a-neural-network-even-learn-a-newcomers-dilemma-bd8d1bbbed89?source=tag_archive---------9----------------,“WTH does a neural network even learn??” — a newcomer’s dilemma,i believe we all have that psychologist philosopher in our brains that likes to ponder upon how thinking happens there a simple clear bird s eye view of what neural networks learn they learn increasingly more complex concepts doesn t that feel familiar isn t that how we learn anything at all for instance let s consider how we as kids probably learnt to recognise objects and animals see so neural networks learn like we do it almost eases the mind to believe that we have this intangible sort of man made thing that is analogous to the mind itself it is especially appealing to someone who has just begun his her deep learning journey but no a neural network s learning is not analogous to our own almost all the credible guides and starters packs on the subject of deep learning come with a warning something along the lines of and that s where all the confusion begins i think this was mostly because of the way in which most of the tutorials and beginner level books approach the subject let s see how michael nielsen describes what the hidden neurons are doing in his book neural networks and deep learning he like many others uses the analogy between neural networks and the human mind to try to explain a neural networks the way lines and edges make loops which then help in recognising some digits is what we would think of doing many other tutorials try to use a similar analogy to explain what it means to build a hierarchy of knowledge i have to say that because of this analogy i understand neural nets better but it is one of the paradoxes that the very analogy that makes a difficult concept intelligible to the masses can also create an illusion of knowledge among them readers need to understand that it is just an analogy nothing more nothing less they need to understand that every simple analogy needs to be followed by more rigorous seemingly difficult explanations now don t get me wrong i am deeply thankful to michael nielsen for writing this book it is one of the best books on the subject out there he is careful in mentioning that this is just for the sake of argument but i took it to mean this maybe the network won t use the same exact pieces maybe it will figure out some other pieces and join them in some other way to recognise the digits but the essence will be the same right i mean each of those pieces has to be some kind of an edge or a line or some loopy structure after all it doesn t seem like there are other possibilities if you want to build a hierarchical structure to solve the problem of recognising digits as i gained a better intuition about them and how they work i understood that this view is obviously wrong it hit me let s consider loops being able to identify a loop is essential for us humans to write digits an is two loops joined end to end a is loop with a tail under it and a is loop with a tail up top but when it comes to recognising digits in an image features like loops seem difficult and infeasible for a neural network remember i m talking about your vanilla neutral networks or mlps here i know its just a lot of hand wavy reasoning but i think it is enough to convince probably the edges and all the other hand engineered features will face similar problems and there s the dilemma i had no clue about the answer or how to find it until blue brown released a set of videos about neural networks it was grant sanderson s take at explaining the subject to newcomers maybe even he felt that there were some missing pieces in the explanation by other people and that he could address them in his tutorials and boy did he grant sanderson of blue brown who uses a structure with hidden layers says the very loops and edges that we ruled out above they were not looking for loops or edges or anything even remotely close they were looking for well something inexplicable some strange patterns that can be confused for random noise i found those weight matrix images in the above screenshot really fascinating i thought of them as a lego puzzle the weight matrix images were like the elementary lego blocks and my task was to figure out a way to arrange them together so that i could create all digits this idea was inspired from the excerpt of neural networks and deep learning that i posted above there we saw how we could assemble a using hand made features like edges and curves so i thought that maybe we could do the same with the features that the neural network actually found good all i needed was those weight matrix images that were used in blue brown s video now the problem was that grant had put only images in the video so i was gonna have to generate them on my own and create my very own set of lego blocks i imported the code used in michael nielsen s book to a jupyter notebook then i extended the network class in there to include the methods that would help me visualise the weight matrices one pixel for every connection in the network one image for each neuron showing how much it likes colour blue or dislikes colour red the previous layer neurons so if i was to look at the image belonging to one of the neurons in the hidden layer it would be like a heat map showing one feature one basic lego block that will be used to recognise digits blue pixels would represent connections that it likes whereas red ones would represent the connections that it dislikes i trained a neural network that had notice that we will have different types of basic lego blocks for our lego puzzle here because that s the size of our hidden layer and here s what they look like these are the features that we were looking for the ones that are better than loops and edges according to the network and here s how it classifies all digits and guess what none of them make any sense none of the features seem to capture any isolated distinguishable feature in the input image all of them can be mistaken to be just randomly shaped blobs at randomly chosen places i mean just look at how it identifies a this is the weight matrix image for the output neuron that recognizes s to be clear the pixels in this image represent the weights connecting the hidden layer to the output neuron that recognises s we shall take only a handful of the most useful features for each digit into account to do that we can visually select the most intense blue pixels and the most intense red pixels here the blue ones should give us the most useful features and the red ones should give us the most dreaded ones think of it as the neuron saying the image will absolutely not match this prototype if it is a indices of the three most intense blue pixels indices of the three most intense red pixels matrices and seem to capture something like a blue boundary of sorts that is surrounding inner red pixels exactly what could actually help in identifying a but what about matrix it does not capture any feature we can even explain in words the same goes for matrix why would the neuron not like it it seems quite similar to matrix and let s not even go into the weird blue s in nonsensical see let s do it for indices of the three most intense blue pixels indices of the top two most intense red pixels i have no words for this one i won t even try to comment in what world can those be used to identify s now the much anticipated how will it represent the loops in it top most intense blue pixels top most intense red pixels nope this isn t any good either there seem to be no loops like we were expecting it to have but there is another interesting thing to notice in here a majority of the pixels in the output layer neuron image the one above the collage are red it seems like the network has figured out a way to recognise s using features that it does not like so no i couldn t put digits together using those features as lego blocks i failed real bad at the task but to be fair to myself those features weren t so much lego blocky either here s why so there it is neural networks can be said to learn like us if you consider the way they build hierarchies of features just like we do but when you see the features themselves they are nothing like what we would use the networks give you almost no explanation for the features that they learn neural networks are good function approximators when we build and train one we mostly just care about its accuracy on what percentage of the test samples does it give positive results this works incredibly well for a lot of purposes because modern neural nets can have remarkably high accuracies upward of is not uncommon meaning that the chances of failure are just in a but here s the catch when they are wrong there s no easy way to understand the reason why they are they can t be debugged in the traditional sense for example here s an embarrassing incident that happened with google because of this understanding what neural networks learn is a subject of great importance it is crucial to unleashing the true power of deep learning it will help us in a few weeks ago the new york times magazine ran a story about how neural networks were trained to predict the death of cancer patients with a remarkable accuracy here s what the writer an oncologist said i think i can strongly relate to this because of my little project during the little project that i described earlier i stumbled upon a few other results that i found really cool and worth sharing so here they are smaller networks i wanted to see how low i could make the hidden layer size while still getting a considerable accuracy across my test set it turns out that with neurons the network was able to classify out of test images correctly that s accuracy at classifying images that it has never seen with just hidden neurons just different types of lego blocks to recognise digits i find this incredibly fascinating of course these weights don t make much sense either in case you are curious i tried it with neurons too and i got an accuracy of neurons accuracy below that it dropped very steeply neurons neurons weight initialisation regularisation makes a lot of difference just regularising your network and using good initialisations for the weights can have a huge effect on what your network learns let me demonstrate i used the same network architecture meaning same no of layers and same no of neurons in the layers i then trained network objects one without regularisation and using the same old np random randn whereas in the other one i used regularisation along with np random randn sqrt n this is what i observed yeah i was shocked too note i have shown the weight matrices associated with different index neurons in the above collage this is because due to different initialisations even the ones at the same index learn different features so i chose the ones that appear to make the effect most starking to know more about weight initialisation techniques in neural networks i recommend that you start here if you want to discuss this article or any other project that you have in mind or really anything ai please feel free to comment below or drop me a message on linkedin facebook or twitter i have learnt a lot more about deep learning since i did the project in this article like completing the deep learning specialisation at coursera don t hesitate to reach out if you think i could be of any help thank you for reading you can follow me on twitter https twitter com nityeshaga i won t spam your feed originally published on the zeolearn blog from a quick cheer to a standing ovation clap to show how much you enjoyed this story reader writer and a programmer sharing concepts ideas and codes
